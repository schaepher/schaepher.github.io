[{"categories":null,"content":"说明 偶尔会想要把平常的思考记录下来，通常是直接发到微信的文件传输助手。本来想把相关联的想法汇集起来形成一篇完整的文章，但是这个习惯没有培养出来。只好把混杂的内容都复制出来了。 这些大多是思考后的结论或者关键思考点，通常只有一两句话。为了避免误解，还是得添加一些说明。还有一些我自己都已经看不懂当时思考了什么，这部份会略去。 有些是特定时间点才想起来记录下来，那部份的思考内容可能很早之前就思考过了。 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:1:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"1 月 谈运气与成功 运气不一定只有一个人获得，而是一批人同时获得。但是由于每个人成长的过程不一样，导致只有少数人能够抓住这个运气。 评某些公众号作者的文章 一篇文章会反映出作者的意图。有些作者会把与自己声音不同的那部份内容完全隐去，而有些作者会把其中比较符合事实和逻辑隐去，只留下明显有问题的论据论点，然后把这两部份人作为一个整体批判，达到证明自己观点的效果。 单角度分析的好处：便于筛选读者。因为这能更加吸引与自己观点相同的人，晒选掉与自己观点不同的人的关注。如果客观地多角度地分析，则有得罪各方论点的人的风险。 单角度的坏处：难以符合客观情况。细心的人容易发现其上下文逻辑的问题。 观察到的解决方式：扩大情绪因素，用情感掩盖逻辑的不足。当人的情绪被调动起来后，即使逻辑出现跳跃，也不会怀疑。 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:2:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"2 月 谈策略的选择与后果 一个策略确定执行时，意味着其他的策略被否定掉。每一个策略都会导致一部分人的利益受损。但是当其中一个策略确定执行时，就确定了其他策略一部分本来利益会受损的人得到了保护。然而我们看到的和听到的都是那些需要发声和愿意发声的人。 人们在评价一个策略好坏的时候，不会去在意这个策略保护了这些本应该利益受损的人们。因为他们可能自己都意识不到，就算意识到了也不会特地去发声，所以人们容易根据那些由于执行的策略而利益受损的人的评价来否定这个策略。 从现实实践中，我们识别到了一种解决方法，那就是让这群利益受损的人短暂地集中地释放自己的不满，允许他们集中地发声。比如美国的“零元购”，看起来利益受损的人爽了一把，但根本问题没有解决。承担损失的不是政府，政府也不用思考如何解决根本问题。 对于各人而言，在做决策时，要充分考虑各种策略做与不做会带来的损失。 谈考虑远近对决策的影响 年轻人很难理解年长的人的想法。其中一个原因是年轻人没有经历过年长的人以前的经历。 我们可以思考三年、五年、十年，甚至是二十年的安排。但是他们在年轻的时候根本就没有时间去考虑，或者根本就没有稳定的客观条件允许他们考虑。在他们眼中，能在接下来的半年活下来就已经是奇迹了。这样就不敢去考虑更远的事情。谁知道一年后自己还能不能活着呢？久而久之形成了习惯。 即使当代的年轻人，也有一些面临着相同的情况，他们也会像老一辈那样做出类似的选择。 如果当代的年轻人理解不到这点，自然会觉得这些人目光短浅。而这些年轻人，也可能会被比他们考虑得更远的人觉得目光短浅。 谈辩论 辩论本应该是为了无限接近真理，而不是抱着一定要证明对方错误的想法。 正反两方总会有一方由于社会共识的存在而预先占据优势。就像努力和运气哪个重要？踏踏实实做工作成功的人容易认为努力更重要。让这样的人去当评委，拿“努力”观点一方的队更有可能获胜。 但这并不代表正方是完全有道理而反方是完全没有道理的。人们达成一个共识，往往是因为各方都可以接受这个共识，是一种相互妥协。 劣势方的目标应该是把优势方所需要妥协的部分尽可能多地表达出来，而不是否定观点本身。 谈“理中客” “理中客\"是理性，中立，客观的缩略词。 “客观”要求充分肯定正方和反方的优点，并且充分地批判正方和反方的缺点。 对于事物要进行批判继承。既不因为存在缺点而全面否定，也不因为存在优点而全盘接受。全盘否定和全盘接受的做法都会对发展造成阻碍。 谈批评 有的文章想要批评一件事情，但是绕来绕去，没有什么逻辑，全靠情绪支撑。看这种文章是很费力的，因为他真正有价值的信息非常少，大部分内容都是在宣泄情绪。而这仅有的一点有价值的信息却还不是问题的关键。 这些人很奇怪，不把精力花在怎么一针见血地指出问题所在，而是花在怎么调动读者情绪以及怎么阴阳怪气上。 谈权利 从一些在国际上发生的事情可以看出，可以做到通过给别人强加权利以剥夺他们其他权利。 权利的实现，需要客观环境满足其实施条件。而不同权利之间存在着资源上的竞争关系。强调应该给人们一种不符合当前客观条件的权利时，实际上封锁了他们其他的权利，得到这些权利的资源。 谈恶与善 有的人作恶，还会觉得自己了不起。“如果不是我们作恶，你们又怎么通过消灭我们获得正义的荣誉呢？”。 难道真正的勇者喜欢斗恶龙吗？如果没有恶龙威胁到人们的安全，勇者就可以把自己的能力用在让人们更加幸福的事情上了。这种荣誉难道不比“消灭恶龙”的荣誉还更令人向往么？ 谈理性和感性 理性和感性都可以用于解决问题。当想要达成一个目标，理想的情况是综合地使用理性和感性，以效率最高的比例去做。 然而有些人过于强调感性，排斥理性；另外一些人则相反。 理性和感性都不是目的。不能满足于把自己培养成“理性”的人，对于“感性”也是。 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:3:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"3 月 谈原因分析 寻找外因有两种态度： 为了甩锅 为了解决问题 有的人真的是为了解决问题而寻找外因，但是却被他人认为是为了甩锅。但是一般不用计较，把问题解决了就好。除非对方是对自己很重要的人。 碰到问题，内因外因都需要寻找，分清主次才能更好地解决问题。 谈子女教育 对于子女的教育投入，是否可以有一个较为合理的标准？ 例如长大了之后，发现自己小时候由于在哪一方面没有投入导致现在碰到瓶颈，那么就在这方面加强对孩子的教育投入。 像那种盲目把孩子送到国外“长见识”，不见得是有好处的。涨了和当前水平差距过大的见识，反而会因为缺乏对事物的发展过程的理解而盲目地认为“应该这样”。 谈学习 学习是为了创造更多的价值。将这些创造出来的价值与其他人交换，获取我们所需要的资源，帮助我们过得更好。 谈批评 他们主观上是想批评，但没有注意到客观上他们犯了错，他们错在鄙视那些得到帮助的人。并且把本来就有问题的人所受的苦痛，说成是别人对他展开帮助后才有的。(时空错位) 谈自我管理 做自己的领导。无论是生活中还是工作中，都把自己当成要领导的对象，用领导的角度去审视自己。 谈辩论 陷入持有不同观点的一方的预设观点和话语体系里面时，越解释越糟糕。要学会解构，并跳出对方的框架。 谈进入心流的可能条件 阅读长文章的能力 收集尽可能多的视角 每周保持一定量的运动 目标尽可能地纯粹 快连接，慢启动 谈投资 股票不是只能涨 20％，它可以翻倍。 谈社会平均劳动时间 社会平均劳动时间，是否也指社会上所有的人在完成一件商品时所需要的平均时间？包括职业和非职业的人的劳动。(是的，然后由于所有的商品都这样，他们共同的那部分时间可能被我们忽视) 谈满足感 成瘾性 \u003c–\u003e 满足感 奶茶不加糖与别人加糖的满足感或幸福感一致。 玩小游戏和别人玩大型游戏的满足感一致。 通过管理阈值达到以少量代价换取相同幸福感的效果，搭配上自身能力的不断地成长，使得未来的变化对自己的幸福感造成影响极小。 谈自信 自信应来自于自身能创造的价值的量。 谈关系对判断的影响 朋友和陌生人做了同样一件坏事，但人们通常会下意识地觉得陌生人可恨，而朋友是迫不得已。这是得到了情感的加成，以及熟悉程度的加成。 谈认知 聚光灯太强的情况下，人们容易忽略其他地方，导致重要的内容被掩盖。需要通过学习实践和深入思考，让自己成为比聚光灯还亮的光，为自己照亮那些被聚光灯掩盖的地方。 谈成长 思想的成长需要以物质作为基础。物质不是决定成长的所有因素，但是非常重要的基础因素 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:4:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"4 月 谈分析能力的培养 在学校里，语文老师或者班主任是否可以布置作业，让同学们分析微博热搜及评论在逻辑上有哪些问题，以及有哪些误导性质的点。 谈品质 我们应该学习某个人的好的品质吗？不，我们应该学习他获取这个品质的方式。 谈独立思考 是为了证明自己独立思考而相信那些话，还是因为独立思考而得到相同的结论？ 谈理性与感性 理性思考，感性选择。 谈解放 解放，而不是施舍。扫除竞争中的特殊障碍，让他们参与到平等的竞争中，而不是以施舍的态度让他们享受更简单的竞争条件（例如按比例平均）。按比例平均反而是压迫了那些付出努力平等竞争人。 读技术评论文章 “伟大的项目往往由一个人或者一个极小数量的群体发起并成长起来”。 人少容易做决策，人多意见多。越重要的项目，如果领导的人能力不足以统一意见，仅保留一种声音，项目就越难以成功。 谈效率 通过重复的练习，把行为固化为特定突触。就像把一些本来用软件实现的功能，做成专门的硬件加速器，使用时用硬件加速。避免思维卡壳，这样就节省出一些空闲的时间做更高层次的思考。 谈“贤者之石” 只听到“贤者之石”能无视等价交换的规则，就容易形成崇拜。用实际行动去了解的人，会知道“贤者之石”是用大量人作为材料做出来的。 普世价值虽然很迷人，但了解它代价的人有多少呢？ 谈信任 人们更愿意相信那些他们看不出与他们有直接利益冲突的人。即使可能因此伤害其他无辜的人。 谈信息密度 在围绕某个具体对象作描述的时候，画图的密度高。 单纯的描述可以用数学符号表达，但信息密度仍然不够。 在描述抽象对象时，文字的信息密度高。 谈资源 愿意承受多少代价也是一种有限的资源。 谈取舍 增加 1% 的性能要多花 20% 的资源。那么是否要增加这 1% 的性能呢？ 换成生活中的事情，又有多少人认识到这一点呢？ 谈工程与系统 领导知道在做工程，底下的人认为是在做系统。 做工程会面临更多取舍，需要确保整体效果最大化。而做系统可能会花大量精力在某一件事情上，但对整体的使用效果贡献很小。 谈实践与理论 通过分析过去几十年犯的错误作为结合理论的实践。 别人的传记会隐藏很多信息，但自己的传记很难骗自己，就算美化过去，也会在自己问自己的时候戳破谎言。 谈人的变化 参考过往行为判断，但过往行为随着时间的推移，可参考性会不断降低。 人是会变化的，有好有坏。只根据过往行为判断，而不去了解最近的行为和想法，就容易误判。 谈抽象与具体 从抽象思想生成具体思想，再进行实践验证和调整。 谈分析事情 避免只有两个角度。这样容易让自己变成极端的人。 谈开源 开源精神对于推动世界进步的贡献远大于一般人的理解。 谈发展 立足于前辈们的理论，但要努力超越他们的理论，这样才能做到随心所欲不逾矩。 谈理论与实践 理论是指导作用，指导不是强制。理论与实践不一致，要以实践为准。并通过实践反过去补充和完善理论 理论和实践不一致，是因为实际的资源与理论预设的资源不一致。这样实践会促进理论的发展。 人们为了固化实践结果，想了个办法，那就是写论文。 论文是对实践的总结。实践可以是证明理论在某个条件下正确，也可以证明理论在某个条件下错误，如果能够根据实践结果补充理论，那就是更加优秀的论文。 谈文学作品 文学作品的作用是让难以领悟真理的人，以一种低门槛的方式做出和领悟的人一样的事情。以低成本的方式达成社会共识，推动社会发展。 谈对等 有人会说 A 是为了自己好，对 B 做了坏事，B 其实也可以反过来对 A 做同样的事。 这是 A 为 A 做坏事辩护，甚至受害者为 A 辩护。以 B 会做同样的事为前提。 B 反驳的点是。因为 B 不会做这样的事，所以拒绝 A 做这样的事。必须对等。 谈文明的韧性 兼收并蓄，为我所用。中华文明最强的地方，正在于此。 引入外来思想，一部分是因为有新的东西，但最重要的是因为它契合了中华文明的内核，并提供了系统化以及科学的解释。 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:5:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"5 月 谈对等 选择对等不代表代价对等 谈道德 道德声誉上的强者，同时是道德行为上的弱者。因为同样一个道德问题，他所要付出的代价更大了。 谈自我限制 通过给自己添加限制，作为代价交换相应的保障。 谈生命最后时间想做的事 看《钢之炼金术师FA》。 谈个人发展 由分裂的个人逐渐抛弃阻碍个人发展的因素，最终发展为统一。（忘记是什么意思了） 谈传承 科技传承，对于真理的追求 谈解放自己 人类的好奇心促使其不断解放自己，去揭开世界的神秘面纱。 思考《资本论》 不同商品的价值表现为同名的量（即多少金） 货币起到类似消息队列的作用，解藕了商品的价值形式，简化了等式 有什么显著的价值？（不使用\"意义\"这个词，可以用\"显著意义”，避免\"为什么要在任何事情中寻找意义\"的反驳） 谈焦虑和不确定性 高中的时候曾经想过，如果计划好未来并按照计划走，那么未来岂不是很没意思？恰好这个月的时候在微博上看到高中老同学发了条微博，说很焦虑，他在困惑这个问题。 不知道是不是我以前思考的东西太多了，其实差不多这个年纪有这种烦恼才是正常的？ 总之，以下是我给老同学的评论。虽然是给他的微博评论，但其实我更想告诉高中时的自己。 从不确定性中获取满足感的一般都是赌徒。而通过主动放弃所有可控性来创造不确定性以获取的只能算初级的赌徒。 个人的未来的完全确定性不是想创造就创造的，任何一个普通人都做不到。人只能创造部分确定性。 主动放弃无法争取的确定性，与主动放弃能够争取的确定性不一样。前者积极，后者消极。 个人主动创造确定性，在一定的发展阶段，会发现有能力选择更多的发展方向，也可以重新规划个人的未来。人不是死板的在确定一个未来之后就一条路走到黑。这种有能力选择不确定性比没有能力只能任由不确定性宰割所获得的满足感不是同一个层次的。 人的焦虑来自于自以为对真实的世界了解很多，实际上只是对想象中的世界了解很多，对真实的世界了解得太少。对自己也了解得太少。 谈小孩子的价值误判 从客观的角度看待孩子，虽然他们不懂得价值交换的原理，他们仍然能根据外部的反馈判断自己可以做哪些事（自我价值判断）。 当家庭内部溺爱孩子时，意味着在家庭这个环境中，对于其他家庭成员来说，小孩子给他们带来的价值非常高，以至于需要用相当多的资源去交换。 但是当范围扩大到家庭外部时，小孩子无法天然地做出调整。这个思考通路无法自发地建立。这就导致了小孩子在外面做出各种出格的事情。因为他仍然认为其他人必须按照其家人的标准拿出对等的价值与之交换。 价值是由社会平均评价为基础，而不是某个人对另一个人的价值评价。但是被评价的人无法接受现实。 谈生命的延续 人是会死亡的，但是却能够通过繁衍后代，用另一种方式延续自己的生命。这种生命不是个人的生命，而是思想、意志这些宝贵的东西的延续。而这些东西，是上一代人的积累，经过我们的学习和发展，再传递给下一代的。 大自然给人的繁衍后代的本能，无意中激活了这一点。无数的人无意识地做着一件伟大的事情，而其中很多人在无意识中失败了，有些人无意识地成功了。这些不断积累不断更新的东西，形成了独特的文化（群体共识）。推动着人们不断发展，解决更多问题，过上更好的生活。 谈极端动保 不吃动物，但吃人。。 谈创新 万众创新的主要参与者其实是有大量实践积累的人民群众，这种鼓励其实是将他们暂时地从繁杂的劳动中解放出来，让实践反作用于理论，然后再让理论指导创新。 谈分析事物 当一个人的分析的关注范围受到自身局限的束缚时，会首先地抗拒分析范围之外的客观因素。进而否定这种客观因素的影响，而不是分析其在事件中影响的程度大小。以至于将自己困在信息茧房中，锁死了分析能力，阻碍自身的发展。 分析得具体，才不会被事物本身的特点局限住，从而找到与其他事物的共同点，形成对客观规律的认识。 谈网络舆论 通过反驳非黑即白言论来表达自己非黑即白的言论。 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:6:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"6 月 思考《资本论》 价值通过多个节点组成的链条，逐渐积累。在某个阶段，付出的价值和得到的价值一致。但如果再往前一步，就得到了绝对剩余价值。 延长了的劳动过程的产品。 六个小时的劳动时间所产生的价值与生活资料的价值相等，这与购买劳动力一天的价格相等。在延长劳动过程后，得到剩余价值。 通常所说的“没有价值”指的是“没有使用价值” 谈抬杠与好奇 带强烈感情地提问（质问）叫抬杠，只带基本感情地提问叫好奇（想了解对方的想法） 谈积累 每天进步一点点。不是纯线性积累，而是到达某一个程度时，会有质的提升。即跨越式的发展。 谈了解一个人 认为一个人会怎么想。把所有可能性列出来后，再根据对这个人的了解，过滤一些可能性。（忘记是什么意思了） 谈选择 选择，而不是想象。（忘记是什么意思了） 谈分析与猜测 没有从客观分析，而且从主观去猜测，就容易有一种命运掌握在强者手上的感觉。 谈信任 信任来自于可预期性。一个人的行为越容易被预判，就容易增加同类人对他的信任程度。 谈可预期性 由于发展而变化的可预期性。（忘记是什么意思了） 谈想象与现实 活在自己的世界：那个在美国得新冠病毒的人（heshanshuo），在美国因为医疗资源不足而抛弃他的时候，他仍然认为美国的医疗资源丰富，证据是他被分配的房间还有很多床。 谈底层逻辑 了解底层技术，就可以在开发过程中不受过多规则的限制，从而发挥创造性。 谈感谢 不应该感谢那些伤害过你的人，而是应该感谢那些让你成长的事情。 人的伤害是故意的，不是无意的。事情是客观的，是他们让人成长。 谈了解与理解 从多个面同时看待同一个事物，得到共同的特征，才能叫理解。 只从一面去看待，只能叫了解。 谈需求的满足 一些人在买东西的时候，一部分是为了满足自己的需要，另一部分以为是满足自己的需要，实际上是满足别人的需要。例如买一台电脑，其中有个功能自己实际上并不需要，但看到了别人对这个功能的差评，自然就产生了不太想买这台电脑的想法。 ","date":"2023-02-11","objectID":"/2023/02/11/2022-insight/:7:0","tags":["想法"],"title":"2021一些想法","uri":"/2023/02/11/2022-insight/"},{"categories":null,"content":"总体感受 最近几个月过得不是很好。倒不是生活艰苦或者工作上没做好。恰恰相反，比如生活上，这几个月吃了很多好吃的，也去了趟苏州玩，和亲戚家的小朋友建立起很不错的关系；在工作上也得到领导的认可，跟其他同事的合作依然很和谐顺利。这些经历不仅为我带来了快乐，还证明了我仍然在不断地做出改变，不断地进步。 但是感觉自己好像失去了什么。当我在审视自己的时候不禁发出这样的疑问“这是我吗？现在的我是之前的我的延续吗？我以前掌握的那些学习和做事的方法哪里去了？”。 这段时间好像对一切都不在意了，大脑放空。有一次去楼下点了酸菜鱼放餐盘上，走的时候碗在餐盘上滑了一下，汤撒到我裤子上和腿上。我一点反应都没，就像什么事情都没发生一样走到位置上坐下来吃饭。 这跟我之前开悟的时候不一样，那时候是用近乎无限的积极情绪把一切负面情绪抵消掉了。但这个时期却是像强行锁在平衡点上一样，所有的情感都被剥夺。照理来说应该是会感觉非常痛苦，但是无法产生痛苦的情绪。 我想尽快结束这种状态，但是找不到方法。以前曾经对自己说过，未来一定会再次进入低谷，但无论如何都要耐心地等待，一定会碰到一些转机，再次恢复的。这段时间偶尔会想起这句话。 我在以前的文章中说过，理性只是用来分析，最终要由感性决定。这段时间就像是完全失去了感性一样，只剩下分析，甚至连理性分析的动力都没有。由于缺少决定，最后什么都没做。 现在我也没办法确定，这种状态是不是因为脱产了两个月，工作的状态消失了。还是可能在更早的时候就已经逐步地进入这种状态。因为我之前确实经历了太长时间的低谷，累积到一定程度后产生了变化。 在这段时间里面，我甚至有些时候大脑活跃程度会像以前一样特别地灵活。只是这种状态持续不超过一天。 说起来我写这篇的时候，已经是 2023 年的 4 月了。好多事情都不记得。当时又很难去把这些事情写下来，毕竟进入了什么事情都不想做的状态了。 如果我有女朋友，是不是就会很快脱离这种状态呢？甚至都不会进入这样的状态。 我以前也说过，自己一个人是有极限的。只是后来进入了开悟的状态，认识到自己还没达到个人极限，所以又把找女朋友的计划推后了。结果被制裁了。 我现在终于知道那个时候为什么身体状况会变得一副要死的样子了。除了当时咽喉发炎之外，还有一个是长期在跑步的时候长时间保持高心率。我最近跑步有了很大的突破，以前通常以 4'39\" 的配速跑 4 公里已经非常累了。最近却先后跑了 5.85 公里， 10 公里，7.5 公里，10 公里。跑 10 公里的时候，心率在 185 左右保持了 40 几分钟，对于心脏的压力太大了。以至于后面好几天都感到难受，让我想起了 21 年 9 月和 10 月那段时间的感觉。我现在已经改变目标了，不追求跑得远，而是追求心率压到 135 以下的前提下提升速度，不过现在几乎变成走路了。这样我下次就不会因为身体原因断了开悟态了。 尽管这段时间各方面都做得不好，但我仍然需要尽可能地记录下来。不能让我在未来回忆自己的成长时，在这段时间出现了断层。 我相信处于低谷不是坏事，它反而会成为我未来向上突破的垫脚石。 ","date":"2022-12-14","objectID":"/2022/12/14/2022-life-part3/:1:0","tags":["life"],"title":"成了自己的陌生人 —— 2022.08~12","uri":"/2022/12/14/2022-life-part3/"},{"categories":null,"content":"新公司 11 月 19 日搬到另一个办公区。 ","date":"2022-12-14","objectID":"/2022/12/14/2022-life-part3/:2:0","tags":["life"],"title":"成了自己的陌生人 —— 2022.08~12","uri":"/2022/12/14/2022-life-part3/"},{"categories":null,"content":"国庆苏州 ","date":"2022-12-14","objectID":"/2022/12/14/2022-life-part3/:3:0","tags":["life"],"title":"成了自己的陌生人 —— 2022.08~12","uri":"/2022/12/14/2022-life-part3/"},{"categories":null,"content":"运动情况 刚进公司的时候，福利还是很不错的。每周一都有羽毛球活动，分两个不同位置的场馆，共 16 个场地。只是要抽签，不过基本上每周都能抽中。 公司的健身房也不错。 第一个办公区有两个健身房。我的工位是在三楼，二楼就有一个健身房。可惜健身房没有浴室。据说大厦有些 VIP 房间可以洗澡，但是我没去过。二楼健身房有四台跑步机。与上一家公司不同，因为这里人多，跑步机经常被占满，需要等人跑完才能上。 第二个办公区是一个大健身房。我的工位是在五楼，健身房在六楼。这里的健身房有浴室!!!跑完之后洗个澡真是太幸福了。浴室有三个位置，不用等。健身房有八台跑步机，去晚了还是要等。 ","date":"2022-12-14","objectID":"/2022/12/14/2022-life-part3/:4:0","tags":["life"],"title":"成了自己的陌生人 —— 2022.08~12","uri":"/2022/12/14/2022-life-part3/"},{"categories":null,"content":"数据同步 服务端和服务端的同步 客户端和服务端的同步 ","date":"2022-10-13","objectID":"/2022/10/13/naming-service/:1:0","tags":["分布式"],"title":"名字服务（Naming Service）","uri":"/2022/10/13/naming-service/"},{"categories":null,"content":"服务端和服务端的同步 ","date":"2022-10-13","objectID":"/2022/10/13/naming-service/:1:1","tags":["分布式"],"title":"名字服务（Naming Service）","uri":"/2022/10/13/naming-service/"},{"categories":["马克思"],"content":"上一篇《\u003c资本论\u003e中的“价值”》讲了价值、使用价值和交换价值的一些基础概念。这一篇讲价值的转移，核心仍然是人：人生产价值，价值是社会属性而不是自然属性。 ","date":"2022-09-17","objectID":"/2022/09/17/das-capital-2/:0:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”转移（草稿）","uri":"/2022/09/17/das-capital-2/"},{"categories":["马克思"],"content":"商品的组成及价值 通常一件商品是由多种材料组合而成的。这些组成成分不会凭空聚集在一起，它们至少会有人的劳动的参与。于是一件商品的价值不只是这件商品本身制作时所凝结的人类无差别劳动，它还包括了组成它的那些凝结了人类无差别劳动的材料的价值。 这样就形成了一种视角：一个商品的价值是其所有组成部分从人类劳动参与开始所凝结的无差别人类劳动。或者另一种说法：一个商品是不同阶段的必要劳动时间的累积。我们看到的不再是一个个物质上的东西，而是人类无差别劳动的不断积累。和上一篇文章所说的那样，只是人类无差别劳动决定了商品的价值，而不是商品本身物质上的使用价值。 严格来说，以上的表述不够严谨，因此我们继续展开。除了直接组成商品的材料外，还有一些在商品生产过程中不会在物质上加入商品，但是商品生产过程中的一部分。最显然的例子就是工业机器。对于这个例子的理解，也会解开我们的另一个问题：机器在生产的时候是否和人一样是在创造价值？ 工业机器是生产商品的过程的参与者，它不会将自身融化以融入商品，成为商品的一部分。那么是否它生产出来的商品就等于商品的组成材料？我们先假设是这样的，从理论上思考，似乎没有什么问题。但我们回到现实就会发现问题。 首先机器是由人通过劳动制造的，机器中凝结着人类无差别劳动，机器本身具有价值。在现实中，机器会由于这样或那样的原因损坏，并以低于（通常是远低于）购买时的价格（比如作为废铜烂铁）卖出最终进入熔炉回到原始状态，或者直接废弃。如果机器生产出的商品的价值等于其组成材料的价值，那么机器损坏意味着包括机器、商品和商品的生产材料在内的总价值减少了。 这样商品的生产者岂不就是在做慈善么？实际上我们买的绝大多数商品的价格都会比生产该商品的原料的价格高。这是因为商品的价值确实比其原料的价值总和多。 于是有的人就会得出这样的结论：机器也能创造价值。这就与“人是价值的唯一创造者”的核心产生了矛盾。 那么造成矛盾的根源是什么呢？ 我个人的看法是，其根源在于看问题的视角是否是以人为核心。换句话说，是否是从社会的角度去看待价值。如果脱离了价值的社会属性，那么就超出了我们的讨论范围。那为什么我们要把价值的讨论缩小到社会属性？扩大范围是否更具有意义？ ","date":"2022-09-17","objectID":"/2022/09/17/das-capital-2/:1:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”转移（草稿）","uri":"/2022/09/17/das-capital-2/"},{"categories":["马克思"],"content":"价值的社会属性 我们把价值定义为人类无差别劳动的凝结，就决定了其出发点是人，而且是社会中的人。一个产品只有在社会中，才会成为商品。定义价值是为了讨论社会中的商品，而不是与社会隔绝时的产品。 把人作为一切的出发点，或者说以人为本，是这套理论的根基。人始终是目的，我们的最终目标是为了让人幸福。把价值定义为人类无差别劳动的凝结，把范围缩小到社会，我认为有三个效果： 这是为了让每个人的劳动都平等。从客观上讲，人人生而不平等。但至少在社会角度，要让人人的劳动平等。在交换商品的时候，应该是等量的（无差别）劳动交换。 这样定义的价值，使得人能够区分哪些行为使得社会总价值增长，哪些行为没有增加社会总价值，从而把精力放在创造价值上，确保了人类的发展是健康且持久的。如果人类发展到最后导致了人类的毁灭，那是为了啥发展？ 为社会的进一步发展指出了一条路。人类社会隐藏着巨大的发展潜力。这是因为很多人具有劳动能力，但不能充分发挥他们的劳动能力，特别是具有绝对庞大规模的农村人口。一旦他们的劳动力得到完全解放，社会的发展会上升很多个台阶。 ……..这里还有很多内容没有说明。提前把后面的内容写一写，是为了把理论与现实结合起来，指导实践。 ","date":"2022-09-17","objectID":"/2022/09/17/das-capital-2/:2:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”转移（草稿）","uri":"/2022/09/17/das-capital-2/"},{"categories":["马克思"],"content":"钱生钱的问题 以下都会使用“钱”来作为“货币”的表达，这是为了能够更加直接地将我们生活中的认知与理论联系在一起。 人是价值的唯一制造者，这是以人为本的观念。我们在讨论价值的时候，始终要考虑到人，而不是把人和商品隔离开来并纯粹地讨论商品、钱和经济。 尊重劳动者。 当人们热衷于讨论钱生钱的时候，忽略了这些新增的钱，并不一定是（或者绝大部分都不是）由于他们自身通过劳动创造了价值。 由上一篇可知，钱是商品与商品交换过程的一种中间媒介，提高了商品交换的效率。当我们获取到钱的时候，这些钱表示有人通过劳动生产了商品，并出卖商品获取到钱。这个人可能是我们自己，也可能是别人。前者无需解释，这里主要解释后者。 当我们通过钱生钱获取更多钱的时候，这些多出来的钱代表的是别人出卖劳动产品所得的一部分或者全部。 这个过程分成两种情况： 这些钱用于支持他人生产，帮助他人创造更多价值。最终按照这些钱占生产总成本的比例，从新增的价值中取得一部分，称作分红。取得的分红的比例不能超过占总成本比例。从公平的角度讲，生产者由于投资的部分而创造出的价值，应和投资者一起分享，但投资者不能独占。这使得生产方能够具有创造价值的条件，或者能够更加充分地创造价值。 没有帮助他人创造更多价值，而是通过交易将他人的钱转移到自己手中。在理想情况下，应该是提供给他人等量价值的商品，用于交换代表该价值量的钱。但上面这个钱生钱的过程并没有这种公平的交换，达到了用少量价值交换更多价值的效果。而上一篇文章说过，价值是人的抽象劳动的凝结。少量价值交换更多价值，对于提供更多价值的一方来说是不公平的交易。因为他的劳动就像是被夺走，只不过被用\"赚钱\"和\"亏钱\"将这一实质掩盖住，使人们只看到\"钱\"这一层，这样似乎就没有那么难受了。因为\"赚他人的钱\"看起来对所有人都是公平的，只要不去考虑钱的本质，是在一定程度上被接受的。 这就变成了比运气或者比谁更能利用规则漏洞赚钱，而不是创造价值。 我们要把“劳动”和“为他人劳动”区分清楚。通常来说，人们认为只要是付出劳动，赚到钱就是正当的。例如利用信息差购买一些东西赚钱。那么想要获取这些信息差，也是要付出劳动的，所以容易认为这是应得的。但是在上面的讨论中，会有一些违和感。问题出现在哪呢？ “价值”的全称是“商品价值”。一个物成为商品，是因为它凝结了人类的无差别劳动，并且（重点来了）它会通过交易与他人的劳动产品交换，将凝结了的无差别劳动转移给他人，以此从他人那里获取同等价值的商品。 如果没有为他人提供与所赚的钱表示的同等价值，那么就相当于什么都没有给对方，但从对方那里拿到了东西。 ","date":"2022-09-17","objectID":"/2022/09/17/das-capital-2/:3:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”转移（草稿）","uri":"/2022/09/17/das-capital-2/"},{"categories":null,"content":"如果想将 sed 操作的结果写入源文件，加上 -i 参数即可。 ","date":"2022-09-10","objectID":"/2022/09/10/sed/:0:0","tags":["Linux"],"title":"sed 命令 —— Linux","uri":"/2022/09/10/sed/"},{"categories":null,"content":"行删除 行删除表达式主要包含两部分： 匹配规则 表达式以 d 或者 !d 结尾，前者表示删除匹配到的行，后者表示删除未匹配到的行 匹配规则是一个范围，用逗号隔开，左右闭区间。例如表达式 1,3d 表示删除 1, 2, 3 这三行。逗号和后面的行号省略时，表示匹配单行。例如 1d 表示删除第一行。 匹配规则可以是一个正则表达式，正则表达式左右使用斜杠包裹。例如 /a/d 表示删除所有包含字符 a 的行。左区间和右区间都可以是正则表达式。表示一直删除到匹配到右区间的正则表达式为止，然后继续搜索左区间。 ","date":"2022-09-10","objectID":"/2022/09/10/sed/:1:0","tags":["Linux"],"title":"sed 命令 —— Linux","uri":"/2022/09/10/sed/"},{"categories":null,"content":"示例 cat a.txt hello world three four five six eight nine 删除匹配到 world 的行 sed '/world/d' a.txt hello three four five six eight nine 2. 删除第一行 sed '1d' a.txt world three four five six eight nine 从第三行开始,每隔一行删除 sed '3~2d' a.txt hello world four six eight 4. 删除从第４行到第８行 sed '4,8d' a.txt hello world three nine `` 5. 删除最后一行 ```bash sed '$d' a.txt hello world three four five six eight 6. 删除空行 sed '/^$/d' a.txt hello world three four five six eight nine 有些空行其实包含了空格或者 tab sed '/^\\s+$/d' a.txt 7. 从匹配行到末尾行 sed '/eight/,$d' a.txt hello world three four five six 8. 删除匹配行和之后两行 sed '/hello/,+2d' a.txt four five six eight nine ","date":"2022-09-10","objectID":"/2022/09/10/sed/:1:1","tags":["Linux"],"title":"sed 命令 —— Linux","uri":"/2022/09/10/sed/"},{"categories":null,"content":"参考链接 https://cloud.tencent.com/developer/article/1924364 https://www.ibm.com/docs/en/elm/6.0.2?topic=cases-use https://www.wumaow.org/tu/4834527.html https://blog.csdn.net/Tir_zhang/article/details/124174730 https://max.book118.com/html/2018/0425/163067362.shtm http://lib.uml.com.cn/ebook/UML2.5/UML-53.asp https://www.edrawmax.cn/templates/file/1014712 https://blog.csdn.net/weixin_34261415/article/details/92119483 https://developer.aliyun.com/article/51636 https://blog.csdn.net/m0_53291740/article/details/122073445 https://online.visual-paradigm.com/cn/diagrams/templates/use-case-diagram/online-shopping-system-use-case-diagram/ https://blog.csdn.net/zhaokx3/article/details/68958218 https://support.microsoft.com/en-us/office/create-a-uml-use-case-diagram-92cc948d-fc74-466c-9457-e82d62ee1298#:~:text=You%20can%20create%20a%20UML%20use%20case%20diagram,diagrams%20show%20the%20expected%20behavior%20of%20the%20system. https://venngage.com/blog/use-case-diagram-example/ https://www.uml-diagrams.org/examples/online-shopping-use-case-diagram-example.html https://www.researchgate.net/figure/Use-Case-Diagram-of-the-Online-Games-Marketplace_fig2_339259369 ","date":"2022-08-07","objectID":"/2022/08/07/uml-use-case/:1:0","tags":["UML"],"title":"UML 用例图","uri":"/2022/08/07/uml-use-case/"},{"categories":null,"content":"在上一篇文章中，我记录了两个身体上没有解决的问题。其中眼睛的问题在 6 月 19 日已经恢复了，胃胀也在眼睛恢复的这段时间消失了。除此之外，个人状态在 7 月也有一定的恢复，下文详说。 这段时间的主基调是离职和找工作。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:0:0","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"离职 2022 年深圳互联网公司裁人很多。我所在的产品线亏损不小，公司就决定把整个产品线撤掉。领导问我想不想留着，我觉得留着能创造的价值有限，就说还是离开吧。 虽然各个大公司裁了大量人员导致找工作比以往更加困难，但是我觉得没有担心的必要。最终能去哪家公司，并不是有能力就行，还得看缘分。就像我两年前换工作地点的时候也是错过了最佳的换工作时间，但是仍然在偶然的机会下面到了这家公司，岗位和我的需要完全匹配。 说是离职，但也不是主动提的，毕竟主动提就没有 N+1 了。拿这个 N+1 有很大的好处。为我接下来一段时间的规划提供了物质保障，会更有底气一些。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:1:0","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"规划 我的规划大致分为三件事情： 写一写今年前面部分的事情。这些内容已经在 5 月 19 日发布了。除了这一篇，我还想把我对人生观的思考写出来； 把我觉得对我未来职业发展的一些重要内容补上来； 做好面试的准备。 由于是 N+1，总共拿到了 3 个月的补偿，因此我在最开始的时候给上面三项分别分配了一个月的时间，不过后面实际上总体分配的时间会少一些。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:2:0","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"输出内容 我的想法始终是：不管写得怎么样，总是要输出一些东西的。不是主要为了给别人看，而是主要为了以后能够回顾，所以无论遇到什么都要这么做。 我知道自己成长到现在这样其实很不容易，以前经历了很多内心上的痛苦，但还是咬牙挺过来了。只是当时很少记录，有些记录的也随着课本一起丢失了。我也知道，中国有这么多人口，总会有一些人跟我情况相似，但在某一个节点上没有突破，导致走上了另一条道路。客观地说，也有一些走上比我好很多的道路。我虽然也希望能够从他们那里得到建议，但我现在更加能做的其实是把我的情况和思考记录下来。我希望能帮助到一些人，起码如果我穿越回过去，我希望我写的内容能够帮助到我自己。更现实一些的是，我的状态会有起伏，因此希望状态差的时候能够记录足够多的信息，当状态好的时候解读这些信息，然后给自己一些建议，毕竟以后还有可能再状态下滑。就像我去年状态好的时候就想要给状态差的自己一些建议，不过碰到了一些麻烦导致没能留下多少有用的信息。 这篇内容只有少数的部分是 7 月的时候写的，多数是在 9 月底写的。8 月和 9 月发生了很大的变化，也正因如此才想单独写一篇。我尽量把自己调回到 7 月底，还原当时的感受。 在离职之前的 5 月 7 日，把我的“三观系列”的世界观部分写完了。其实这篇从 2 月就开始写了，最终有 20 次的编辑记录。我觉得最重要的是能通过对客观的认识，获得一种“有根”的乐观，并且让自己主观上持续拥有不断学习进步的意识。 由于离职后给自己分配了一个月的时间完全抛弃事业上的内容，专心把想写的内容写完，感觉时间还是很充裕的。 花了几天时间在惯例的记录上面，19 日发布完就进入下一篇，“三观系列”的人生观了。 人生观的部分在最近的几个月反复出现在脑中，直到这时才正式地开始写下来，但是我又犯了难。平常在脑子里想的是没有先后逻辑顺序，而是根据一个事件或者一个想法引申出的思考。但在真正要写下来的时候，本能地想让这些内容有逻辑上的关联，导致脑子卡壳。因为跟价值观和世界观相比，人生观的内容更难组织各个部分的逻辑关系，并且它受到价值观和世界观的影响，也需要在适当位置插入这两个部分的内容。 我在某些时间段会经常地把自己的想法碎片记录下来，想着以后写东西的时候会用到。但是最近几个月因为状态下滑就没有怎么去记录了，那些思考过的内容大多忘了。这也是卡壳的一个原因。 而且我发现自己大脑的规律就是我一旦把这些内容写下来，就会自动地不去思考这些内容，由此很难在大脑中借着这些内容去展开。 人生观也有一些内容不适合写出来，它们需要有过相同的经历才容易理解。这些不是那种会指导我做出损害他人的内容，但是我认为如果别人不能理解，会对我自己有负面影响。如何恰当处理这部分内容，需要更多时间谨慎地考虑。 所以当写了四千字后，尽管距离完成还有很远的距离，还是决定在 5 月 25 日停了下来。我想大概需要等待一种感觉，就把时间让给其他规划了。 在进入职业相关话题前，我想插一段图书馆的内容。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:2:1","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"图书馆 不知道是否是工作了的关系，让我在心理上和图书馆的距离变远了，觉得去图书馆是一件很麻烦的事情。所以我希望借这个机会，让自己潜意识觉得去图书馆是一件很简单且很自然的事情。 我在 5 月 16 日周一正式离职，于是从 14 日周六就开始去图书馆了。14 日那天晚上 19 点到图书馆，离闭馆只有 2 个小时。因为是第一次来到这个图书馆，就先到处熟悉一下。最终在书架上拿了《共产党宣言》看。我一直很好奇它的全文是怎么样的，以前没有去看，就借这个机会把它看完。我从里面摘抄了一段分享给大家的内容： 资本是集体的产物，它只有通过社会许多成员的共同活动，而且归根到底只有通过社会全体成员的共同活动，才能运动起来。 因此，资本不是一种个人力量，而是一种社会力量。 因此，把资本变为公共的、属于社会全体成员的财产，这并不是把个人财产变为社会财产。这里所改变的只是财产的社会性质。它将失掉它的阶级性质。 ——《共产党宣言》 资本不应该成为资本家的个人财产，而应该成为公共的、属于社会全体成员的财产。由这个属于社会全体成员的资本所产生的利润，也就自然地继续成为公共的、属于社会全体成员的财产，最终这些财产将会用于提升社会全体成员的幸福感。 这些内容跟本篇主题差太多，就不展开了。说回图书馆。 图书馆很大，座位也不少，但由于新冠影响只能隔位座。早上去得晚些就得到处找座位。特别是我需要使用一整天的平板电脑，必然要充电，但只有部分桌子上有插座。这些位置更是难等，只能等到下午 17 点多才空得出来。实在不行的时候，就带上平板电脑和充电器到有插座的地方，找个没有正在使用的插座充电，然后回到自己位置上先看一阵纸质书。 在图书馆可以看到中学生、大学生、考公的人，以及一些已经结婚有孩子的人。里面应该也有和我一样是最近才刚刚没有了工作的。从图书馆整体来看，感觉去那里学习的女性比例高一些。由于这段时间去图书馆对我来说是一种改变，那么也是可以期待一下运气会发挥怎样的作用。但从结果上看，什么都没有发生。 图书馆室外有一个专用于吃饭的地方，我的午饭和晚饭都是叫外卖，在这个地方吃。晚上大多吃粥，中午也尽量少油，感觉比平常健康多了。 一整天泡在图书馆让我心情一直保持很好，而且能感受到自己又学了不少知识。 接下来回到正题。在停下人生观的内容后，就到了我一直觉得难，且一直想要理解的知识了。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:2:2","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"补充知识 我一直有再次长期全身心投入学习重要知识点的想法。除了感受学习本身的快乐，还可以让自己的基础更牢固，走得更远。 这个想法实践起来不容易，因为我还没确定能否承受它的风险。风险归根结底是我自身的问题，分为两方面：1、真的能够全身心投入，不受其他事情干扰吗？2、学习的结果是否能够在未来帮助我获得更多幸福感？ 我本来想展开讨论，但这样就偏离主要内容了，最好单独写一篇。 因此我借着这次离职，提前做了一次短期体验。给了自己一段时间去学习对于找工作没有多大用处但对于长期有用处的基础知识。能这么做，也有一个重要原因是面试相关的知识点在我离职之前就被朋友拉着“卷”过了。 读者或许也有这样的感受：现在再去学习以前学校里教的知识，不仅很容易理解，而且理解得更加深刻。但是如果不去学，那么一些知识就一直都没能理解。 由于在大学的时候侧重点不一样，所以一开始的学习都落下了。其中有一个方向我没有理解清楚并且一直想要解决：数据结构与算法。其中的哈希表已经在之前重点学习过了。剩下的是另外两块：树和动态规划。以前没有好好学这部分的内容，导致有一种畏惧感。最主要的是树，因为比较常见。当认真地去学习，理解和实践后，发现它的内容比想象中的简单很多。这些之中的一部分内容都输出到技术博客上了。只不过分配的时间有限，对于树中的红黑树的证明以及动态规划的复杂情况还没有达到完全理解的状态。这是今后要解决的课题。 这部份大概在 6 月 20 日结束。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:2:3","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"面试 我的简历一直写的很不好，这次多花了些时间改。这期间还有其他事情占了时间，直到 6 月 25 日才改出一份像样的简历来（后来还会改几次），但还是过了几天才发到招聘平台上。 本来可能要多花些时间确定第一版，但是朋友要给我内推，催我快点发给他，就加快进度了。早点写完早点放招聘平台也挺好的。 28 日开始了第一个面试，形式是现场面试。由于准备得还不够充足，跟后来的表现有一些差距。面试确实是面得越多，思路越清晰。 后面既有招聘平台上联系我的，有些面过了，有些面试比较奇怪，有些确实是没回答好。最后进的这家公司，是让我大学同学内推的。工作内容是我感兴趣的，涨幅也不错。 在三面结束后，才知道三面的面试官认识我之前的领导。之前的领导有帮我内推到他们部门，但是过了好久都没有通知我去面试。然后我让同学内推的时候，从那么多部门里面恰好选到他们部门。在双方不知情的情况下过了前两面（一般两面就结束了），后来额外加了第三面，面完了才知道这事。竟然还有这么巧的事！当时脑中就浮现出两个字“缘分”。 要是这样的缘分能发生在我找女朋友的时候就好了，哈哈。 在所有面试中，最后一个阶段面试官问“你有没有什么想问我的”，比较有得说。分为两方面：1、我在这个阶段一直存在的不足；2、我在这个阶段准备的问题。 首先说不足的部分。在面试过程中，有些问题我没能回答上来。这些应该在最后的时候和面试官探讨，但我却没有。面试几次后我才发现这个问题，但是在后面的面试中仍然没有这么做。我想可能是经过一段时间的回答问题，想要转换为问问题需要一段时间的缓冲。来不及转换就只好问提前准备的问题。这里暴露出的问题是面试的时候没有一个全局视野，导致大脑被局限在某个上下文。 那么我提前准备了什么样的问题？主要分为两方面：1、部门及工作的相关问题；2、与工作本身不相关的问题。 其中与工作不相关的问题主要有两个： 让面试官对我面试过程中的表现做一个评价； 问面试官在这个部门有哪些收获。 问第一个问题的时候，面试官大多的反应看起来应该是认为我想马上知道面试结果。其实我并没有这么想。在之前的面试中，我发现有些问题的答案我是知道的，但面试过程中没有理解面试官想要问的点，就不知道应该回答这些内容。问这个问题，其实部分解决了不足的部分，只是不是由我提出某个具体的问题，而是根据面试官指出的问题来回答。另一方面，可以听到面试官对我面试过程中表现好的地方的肯定，在通过别人评价补充对于自己的认识的同时，又可以尝试在后续面试中突出这些优势。 第二个问题可以了解面试官，还可以看出部门的一些情况。比如有的面试官会说得到了想要的工作和生活的平衡，这就可以看出部门应该不卷。有的面试官会说带头把一个小系统逐渐发展成一个大的成熟的系统，这就说明这人技术强，并且拥有大系统设计能力，跟着他能学到不少东西。这个问题有很强的开放性，可以得到各种各样的回答，很有趣。 这两个问题最多在前两面的时候问。之后的面试官通常是大领导了，不适合问这个问题。所以我准备了其他问题，例如问怎么看待 35 岁危机？这个问题我自己的答案是保持学习意识，不断进步。但我想知道渡过 35 岁危机的这些能人，他们是怎么考虑的。而我确实得到了“保持学习”这样的答案，但主要还是专业知识上的学习，所以我总觉得少了些什么，但也不知道该怎么问。这个问题就留待以后再去探索吧。 最终获得了一个让我各方面都非常满意的 offer，8 月 1 日入职。 7 月我的状态又恢复了不少，我想这跟我面试期间​不得不说很多话有关。我之前有注意到，如果我经常和别人交流，状态似乎就会变好​。 ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:2:4","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"最后 最近一年，随着对技术底层实现的细节的了解，会有一种奇妙的感觉。似乎有了更宽阔的视野，能理解和联系的东西变多了。这给了我很多信心去达到我以前所设想的高度。不过我不觉得这种感觉是稳定的，所以需要源源不断地补充细节，然后思考。 继续努力！ ","date":"2022-07-22","objectID":"/2022/07/22/2022-life-part2/:3:0","tags":["life"],"title":"磨刀不误砍柴工 —— 2022.05~07","uri":"/2022/07/22/2022-life-part2/"},{"categories":null,"content":"背景 我有时候想捣鼓个工具用。起初是为了让我及时掌握一些信息，想做一个消息推送功能。但是由于功能简单，不想单独开发个手机 APP。于是就想着能否直接把消息推送到个人微信上。 面临的问题 以前有 web 版的个人微信，有些人就基于 web 版的个人微信接口做了一个开发套件。但是现在 web 版只有少数一些微信号能够登录，我就登录不了。 另一种方式是申请公众号，使用公众号的消息推送。但是公众号主要是对关注者的，只用来给我自己推送消息不太合适。而如果在公众号上增加一些其他功能，等于是把功能开放给所有关注的人了，没什么必要。 后来我才知道公众号的消息推送有次数限制，不过对于个人使用已经足够了。 最终采取的方案是通过企业微信把消息推送到个人微信。 实现后的消息流向 个人微信 -\u003e 企业微信 -\u003e 个人服务器 个人微信 \u003c- 企业微信 \u003c- 个人服务器 实现步骤 在企业微信手机端上创建企业（不会核实真实企业） 在企业微信web端管理页面通过个人微信扫码绑定企业的微信插件 在web端管理页面上创建应用 在个人服务器上启动服务 web端管理页面设置应用接收信息的 API 地址 在个人微信上收发消息 ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:0:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"1. 在企业微信上创建企业 以手机企业微信为例，菜单依次如下： 我 | 设置 | 管理企业 | 全新创建企业 | 企业 填写企业名称 行业类型根据需要选择 员工规模选择1-50人 真实姓名填写自己的姓名 点击创建就能创建成功了。 ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:1:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"2. 绑定插件 上一步创建企业后，会自动将当前帐号切换到这个企业（之后可以来回切换）。 电脑浏览器访问： https://work.weixin.qq.com/wework_admin/frame#profile/wxPlugin 在切换到新创建的企业后，用企业微信的扫码登录。 登录后会自动跳转到【微信插件】功能页。可以看到下图那样的页面。 用个人微信扫【邀请关注】一栏的二维码。关注后，个人微信会绑定该企业。如果企业微信加入了多个企业（如正在任职的企业），不用担心会绑定到这些企业上。 ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:2:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"3. 创建应用 电脑浏览器访问： https://work.weixin.qq.com/wework_admin/frame#apps 进入应用管理模块。 依次访问： 应用 | 自建 | 创建应用 如下图： 填写页面要求的信息，记得在可见范围里把自己选上。 创建成功后，进入应用页面可以看到以下信息： AgentId Secret 功能 | 接收消息 | 设置API接收 其中标记 2 的 Secret 是企业的密码。在请求企业微信接口的时候，要使用企业 ID 和应用密码获取 Token。 接着进入应用页面里标记 3 的【设置API接收】。 注意这里的 Token 仅用于计算签名，和请求企业微信接口时用于验证身份的 Token 不是同一个，因此随机获取就行。 URL 按以下格式填写： http://IP:端口/qiye-wechat/agents/应用的AgentId 这里的 IP 和端口设置为个人服务器的 IP，以及即将在下一步启动服务时开放的端口。 Token 和 EncodingAESKey 随机获取或者自己填都行。 填完之后先不点保存，等到下一步【启动服务】执行完后再保存。因为保存的时候。企业微信会向上面的 URL 发送一个请求验证。 ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:3:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"4. 启动服务 登录个人服务器，下载下面这个项目： https://github.com/schaepher/noty 进入项目文件夹，复制一份 config.json.example 到 config.json。初始内容为： { \"addr\": \"127.0.0.1:55556\", \"corp_id\": \"\", \"agents\": [ { \"id\": 1000002, \"secret\": \"\", \"token\": \"\", \"encoding_aes_key\": \"\", \"type\": \"echo\" } ] } corp_id 是企业的 ID。通过以下链接页面底部获取： https://work.weixin.qq.com/wework_admin/frame#profile 在 agents 里面，填写刚刚创建的应用的信息。其中 secret 是通过应用界面的 Secret 一栏进入获取的。也就是下图的标记 2。 type 是 agent 类型，与企业微信无关，与本项目 qiyewechat 文件夹里 agent.go 的 AgentFactory 有关。默认为 echo，作用是个人微信发什么，服务器就返回什么。 接下来是编译和运行。 go build ./noty ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:4:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"5. 配置企业微信 回到刚才的【设置API接收】界面，点击保存。此时企业微信会发一条验证信息到服务器，如果通过，就能成功保存。 保存成功后，在这个页面下方的【企业可信IP】卡片里选择配置，将刚刚填入到 URL 的服务器 IP 添加进去。这样之后服务器才能主动调用企业微信的 API。 ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:5:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"6. 个人微信收发消息 在个人微信的【我的企业及企业联系人】分组中，找到企业。进入后可以看到应用，发送消息即可。 ↓ ↓ ","date":"2022-06-29","objectID":"/2022/06/29/wechat-message/:6:0","tags":["wechat"],"title":"在个人微信上与自搭服务端交互","uri":"/2022/06/29/wechat-message/"},{"categories":null,"content":"MySQL 的多版本并发控制（Multiversion Concurrency Control，MVCC）解决了 InnoDB 事务隔离级别中读已提交和可重复读的读和写冲突问题。MVCC 使事务中执行普通的 SELECT 读取数据不需要对记录加锁，同时又能根据需要避免脏读、不可重复读、幻读的问题，提高了系统整体效率。 MVCC 要放在事务的框架中学习，因此下文会尽可能多地使用包含了“事务”的表述，用来增强读者脑中的“一个事务包含一条以上的 SQL”的印象。在此基础上要清楚如果没有显示地执行 BEGIN 开启事务或者 set autocommit = 0 ，执行的单条 SQL 语句都会被 MySQL 作为一个事务执行。因此就可以通过事务的结构来统一地理解多条语句和单条语句的情况。 注：下文提到数据库中的“行”的概念时，会使用“记录”这一表述代替。 ","date":"2022-06-14","objectID":"/2022/06/14/mysql-mvcc/:0:0","tags":["MySQL"],"title":"MySQL InnoDB 并发控制之 MVCC","uri":"/2022/06/14/mysql-mvcc/"},{"categories":null,"content":"疑问有哪些？ 在对 InnoDB 了解得很少时，会有以下的疑问： 一条记录的多个版本是用什么区分的？是使用更新时间么？是使用版本号么？ 一条记录应该保留多少个版本？ 更新数据还未提交的时候，是不是有另外一块区域存放新数据，等到提交的时候才覆盖到数据页里原先的位置上？ 在启动事务后，另一个事务更新了数据，那么旧数据会存放在哪？ 是不是为每条记录都设置了独享的一块放置该数据的多个版本的内存或外存区域？ 如果两个事务都想修改同一个数据，会怎么处理？ ","date":"2022-06-14","objectID":"/2022/06/14/mysql-mvcc/:1:0","tags":["MySQL"],"title":"MySQL InnoDB 并发控制之 MVCC","uri":"/2022/06/14/mysql-mvcc/"},{"categories":null,"content":"记录与事务 如果思考的视角是记录本身，就会往记录本身的更新时间和版本号去思考。但是正如前文强调的，无论是 MVCC 还是 SQL 语句对记录的修改都离不开事务。既然如此，就可以把视角转移到记录和事务的关系上。一旦明确这个关系，就可以考虑将事务的 ID 作为记录的版本号了。 在记录上引入事务 ID 字段表示最近一次变更该记录的事务。当一个事务更新该记录或标记删除的时候，将记录的版本号更新为这个事务的 ID。 static inline void row_upd_rec_sys_fields( rec_t *rec, /* 记录（record） */ page_zip_des_t *page_zip, const dict_index_t *index, /* 聚集索引 */ const ulint *offsets, const trx_t *trx, /* 事务 */ roll_ptr_t roll_ptr) { if (page_zip) { // ... 省略 } else { ulint offset = index-\u003etrx_id_offset; // ... 省略 trx_write_trx_id(rec + offset, trx-\u003eid); // \u003c-- 就是这里 // ... 省略 } } ","date":"2022-06-14","objectID":"/2022/06/14/mysql-mvcc/:2:0","tags":["MySQL"],"title":"MySQL InnoDB 并发控制之 MVCC","uri":"/2022/06/14/mysql-mvcc/"},{"categories":null,"content":"未提交的更新存放在哪 通常情况下，如果数据没有提交，我们会认为它应该放在一个单独的地方。只有提交的时候，才把这些数据拿去覆盖原先的数据，否则其他事务在读取的时候，就会读取到未提交的数据。 MVCC 工作在读已提交和可重复读这两个事务级别上，说明使用 MVCC 的情况下不会出现读到未提交的数据。但是 MVCC 没有将未提交的数据单独存放，而是直接写入数据页，甚至会刷入磁盘。 见回答及其评论区： https://www.zhihu.com/question/278643174/answer/522384191 源码以后再找。 ","date":"2022-06-14","objectID":"/2022/06/14/mysql-mvcc/:3:0","tags":["MySQL"],"title":"MySQL InnoDB 并发控制之 MVCC","uri":"/2022/06/14/mysql-mvcc/"},{"categories":null,"content":"旧数据存放在哪 既然未提交的更新直接写入到数据页，那么原先的数据要怎么办？必然需要有一个地方存储旧数据的备份。 了解 InnoDB 的同学会知道 undo 日志的存在。undo 日志用于存储一个操作的反向操作，例如 insert 操作就是 delete；如果是 update，则产生一个更新回旧数据的 update。当事务回滚时执行 undo 日志中的操作达到恢复数据的效果。 ","date":"2022-06-14","objectID":"/2022/06/14/mysql-mvcc/:4:0","tags":["MySQL"],"title":"MySQL InnoDB 并发控制之 MVCC","uri":"/2022/06/14/mysql-mvcc/"},{"categories":null,"content":"旧数据会存多少个版本 与这个问题相关的是：如果两个事务同时修改一个记录，会如何处理？ 其中一个事务会在记录上加锁，在其释放锁之前，另一个事务无法加锁，也就无法操作数据。于是不会出现 undo 日志中存在多个事务未提交的记录。由于 undo 日志存储的是逻辑日志，一个事务有可能多次修改同一个数据，那么一个记录就可能出现多次出现在 undo 日志中。 undo 日志是以事务为粒度划分区域还是全局呢？ undo 日志按事务划分，各自组成一条链。 MySQL 在初始化的时候会创建两个回滚表空间（undo_001 和 undo_002 这两个文件），每个表空间划分了 TRX_SYS_N_RSEGS = 128 个段（Segement），这些在回滚表空间的段又称为回滚段（Rollback Segment，源码中简写为 rseg）。undo 日志存储于回滚段中。 struct trx_t { // ... 省略 UndoMutex undo_mutex; undo_no_t undo_no; space_id_t undo_rseg_space; trx_rsegs_t rsegs; /* 回滚段 */ // ... 省略 } struct trx_rsegs_t { trx_undo_ptr_t m_redo; /* 系统崩溃后需要恢复的回滚日志 */ trx_undo_ptr_t m_noredo; /* 系统崩溃后不需要恢复的回滚日志 */ }; struct trx_undo_ptr_t { trx_rseg_t *rseg; /* 回滚段 */ trx_undo_t *insert_undo; /* 插入操作的 undo 日志指针*/ trx_undo_t *update_undo; /* 更新操作的 undo 日志指针*/ }; /** 事务系统中心内存数据结构 */ struct trx_sys_t { // ... 省略一些字段 /** 多版本并发控制管理器 */ MVCC *mvcc; /** serialisation_list 的互斥量 */ TrxSysMutex serialisation_mutex; /** 追踪未提交的最小事务号 */ UT_LIST_BASE_NODE_T(trx_t, no_list) serialisation_list; /** 为 MySQL 创建的事务列表 */ UT_LIST_BASE_NODE_T(trx_t, mysql_trx_list) mysql_trx_list; /** 用于 MVCC 快照的一组读写事务 ID */ trx_ids_t rw_trx_ids; // ... 省略一些字段 } class ReadView { class ids_t { // ... 省略 private: /** 大于等于此 ID 的事务修改的数据对该快照不可见 */ trx_id_t m_low_limit_id; /** 小于此 ID 的事务对数据的修改对该快照可见 */ trx_id_t m_up_limit_id; /** 创建该快照的事务 */ trx_id_t m_creator_trx_id; /** 当该快照被创建时，处于活动中的所有读写事务 */ ids_t m_ids; // ... 省略 } // ... 省略 public: bool changes_visible(trx_id_t id, const table_name_t \u0026name) const { // ... 省略 if (id \u003c m_up_limit_id || id == m_creator_trx_id) { return (true); } check_trx_id_sanity(id, name); if (id \u003e= m_low_limit_id) { return (false); } else if (m_ids.empty()) { return (true); } const ids_t::value_type *p = m_ids.data(); return (!std::binary_search(p, p + m_ids.size(), id)); } } void ReadView::prepare(trx_id_t id) { // ... 省略 m_creator_trx_id = id; m_low_limit_no = trx_get_serialisation_min_trx_no(); m_low_limit_id = trx_sys_get_next_trx_id_or_no(); if (!trx_sys-\u003erw_trx_ids.empty()) { copy_trx_ids(trx_sys-\u003erw_trx_ids); } else { m_ids.clear(); } m_up_limit_id = !m_ids.empty() ? m_ids.front() : m_low_limit_id; // ... 省略 } ","date":"2022-06-14","objectID":"/2022/06/14/mysql-mvcc/:5:0","tags":["MySQL"],"title":"MySQL InnoDB 并发控制之 MVCC","uri":"/2022/06/14/mysql-mvcc/"},{"categories":null,"content":"在读+写不是一个原子操作的时候，为了提高处理速度而引入的并发访问会使得一些读写操作的结果不符合预期。由于数据库读取数据时的最小粒度是数据页，在修改的时候是操作数据页，因此 从数据库在内存中的存储来看，最容易想到的一个做法是为整份数据块做一份冗余，在 为了解决两个不同的并发连接操作引发的读和写之间的冲突，引入锁（互斥锁）的概念将访问的数据集合锁住，避免被其他连接读或者写。但如果两个连接都只是读数据就不会引起数据变更，然而读和读之间的互斥则会降低访问效率，因此引入了不同的锁（共享锁）使得读和读之间的锁可以兼容从而提高并发读效率。 然而当一个操作涉及写的时候，很有可能是基于原先数据做变更。 通过事务把多个操作组合为一个整体，通过保证事务的原子性来确保多个数据库操作的原子性。 ","date":"2022-06-14","objectID":"/drafts/transaction/:0:0","tags":["MySQL"],"title":"事务","uri":"/drafts/transaction/"},{"categories":null,"content":"2-3 树和 2-3-4 树统称为 B-树（Balanced Tree）。 ","date":"2022-06-11","objectID":"/2022/06/11/red-black-tree/:0:0","tags":["树","平衡树","数据结构"],"title":"红黑树","uri":"/2022/06/11/red-black-tree/"},{"categories":null,"content":"为什么不直接使用 B-树？ 既然以 2-3 树和 2-3-4 树都可以用来转换成红黑树，为什么最终使用 2-3-4 树的转换呢？ https://stackoverflow.com/questions/8765558/why-dont-we-use-2-3-or-2-3-4-5-trees Implementation of 2-3-4 trees typically requires either multiple classes (2NODE, 3NODE, 4NODE) or you have just NODE that has an array of items. In the case of multiple classes you waste lots of time constructing and destructing node instances and reparenting them is cumbersome. If you use a single class with arrays to hold items and children then you are either resizing arrays constantly which is similarly wasteful or you wind up wasting over half your memory on unused array elements. It’s just not very efficient compared to Red-Black trees. Red-Black trees have only one type of node structure. Since Red-Black trees have a duality with 2-3-4 trees, RB trees can use the exact same algorithms as 2-3-4 trees (no need for the stupidly confusing/complex implementations described in Cormen, Leiserson and Rivest that led to AA trees which are not less complex than the 2-3-4 algorithm.) So, Red-Black trees for their ease of implementation plus their memory/CPU efficiency. (AVL trees are nice too. They produce more well balanced trees and are stupid simply to code but they tend to be less efficient due to working too often to maintain only a slightly more compact tree.) Oh, and 2-3-4-5-6… etc aren’t done because nothing is gained. 2-3-4 has a net-gain over 2-3 trees because they can be done without recursion easily (recursion tends to be less efficient, especially when it cannot be coded tail-recursively). However, B-Trees and Bplus-Trees are pretty much 2-3-4-5-6-7-8-9-etc trees where the max size of the nodes, n, is chosen so that n records can be stored in a single disk sector. (i.e. each disk sector is a node in the tree and the size of the sector is equivalent to the number of items stored in the node.) This is because the time to search through 512 records linearly in memory is still MUCH faster than traversing down a level in the tree which requires another disk seek/fetch. and O(512) is still O(1) and thus maintains O(lg n) for the tree. https://read.seas.harvard.edu/~kohler/notes/llrb.html 下一篇将会详细展开红黑树的内容。 ","date":"2022-06-11","objectID":"/2022/06/11/red-black-tree/:0:1","tags":["树","平衡树","数据结构"],"title":"红黑树","uri":"/2022/06/11/red-black-tree/"},{"categories":null,"content":"如果从先易后难的顺序介绍各种树，那么红黑树必然放在 AVL 树后面。但在红黑树之前，还有一种名为 2-3 树的平衡二叉树。2-3 树理解起来比红黑树容易很多，并且在理解它的基础上增加一个变更，就成了红黑树（尽管不是通常使用的那种红黑树）。因此学习红黑树的时候，最好先学习 2-3 树。 2-3 树与 AVL 树有一个不同的地方：AVL 树是从上到下增加树高，根节点只会因为旋转而改变；而 2-3 树是从下到上增加树高，节点值是从下往上挤，原先根节点容纳不下的时候，挤出一个新的根节点。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:0:0","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"2-3 树 2-3 树与二叉树不同的是，它的节点除了可以有 2 个叉之外，还可以有 3 个叉。如下图： 根节点的 4_7 表示该节点有两个值，分别是 4 和 7。为了方便讨论，把较小的值 4 称为低值，把较大的值 7 称为高值。 该节点有三颗子树，子树与节点的关系是：左子树所有节点的值都小于低值，右子树所有节点的值都大于高值，中间子树所有节点的值都介于高值和低值之间。 把这种有两个值且可以有 3 个叉的节点称为 3节点（例如值为 1_2 的节点），把只有一个值且最多只能有 2 个叉的节点（例如值为 6 的节点）称为 2节点。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:1:0","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"新数据插入图示 由于有两种不同的节点，因此应该分情况讨论。 首先是最简单的 2 节点： 当插入值为 2 时，由于该节点还有一个空缺的位置，又没有子树，因此把值放入到节点内部。如下图： 此时该节点成为一个 3节点。 接着，当继续插入值 3 的时候，发现这个节点无法容纳 3 这个值，只好把三个数的中间值 2 挤上去，两边的值作为子节点。 为了方便理解，有些资料会把值强行插进原先节点，变成一个 4节点，再把 4节点转换到上图的双层解构。下图是一个 4节点。 除了这种简单的根节点转换，还需要考虑到原先 3 节点有父节点的情况。例如最开始的图： 8_9 这个节点是一个 3节点，如果往树中插入 10 会发生什么？ 8_9 节点无法容纳 10 这个值，需要把三个数 8_9_10 的中间数 9 往上挤。但 4_7 节点也无法容纳 9，因此需要再把 4_7_9 的中间数 7 往上挤。最终成为下图的样子： 这样，2-3 树不需要有旋转的操作，只需不断地把中间数往上挤，就能保持平衡。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:1:1","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"2-3 树的实现 根据上述过程的需要，可以写出以下结构： const ( NODE_TYPE_2 = 2 // 2 节点 NODE_TYPE_3 = 3 // 3 节点 ) type TreeNode struct { Type int // 表示节点类型：2节点、3节点 LowValue int HighValue int Parent *TreeNode Left *TreeNode Middle *TreeNode Right *TreeNode } // NewTreeNode 创建一个节点并设置为 2 节点 func NewTreeNode(value int) *TreeNode { return \u0026TreeNode{Type: NODE_TYPE_2, LowValue: value} } 根据合并算法的具体实现，结构体还会有不同的变化。例如有的实现中，不使用单独的 LowValue 和 HighValue，而是一个存储三个元素的数组 Values [3]int。先把子节点拆分时的中间数挤到父节点，再让父节点去调整。因为存储的时候节点变成了一个 4节点，所以结构体中还需要加入一个 Tmp *TreeNode 来存储子节点分离后多出来的一颗子树。 现在这个结构体，是我在 GitHub 上找到的一份 C 语言实现的 2-3 树代码翻译过来的。我认为这种实现容易理解，因此接下来会使用完整翻译后的代码做解析。 C 实现的源代码在： https://github.com/Hazeman28/self-balancing-trees/blob/master/2_3tree/2_3tree.c 首先关注 Insert 时的行为。根据 2-3 树的特性，具体处理的时候应该考虑两点： 是否是叶子节点？ 节点类型是 2节点还是 3节点？ 最外层根据是否为叶子节点做不同的处理。 func Insert(node *TreeNode, value int) *TreeNode { if node == nil { return NewTreeNode(value) } if node.IsLeaf() { return AddToLeaf(node, value) } // ... 非叶子节点的情况 } // IsLeaf 判断是否为叶子节点。如果不是叶子节点，则 Left 必不为空。 func (node *TreeNode) IsLeaf() bool { return node.Left == nil } ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:2:0","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"插入叶子节点 确定是否为叶子节点后，要考虑节点类型。2节点的情况比较容易，优先考虑 2 节点。 // AddToLeaf 把新 value 添加到叶子节点 func AddToLeaf(node *TreeNode, newValue int) *TreeNode { if node.Type == NODE_TYPE_2 { if newValue \u003e node.LowValue { node.HighValue = newValue } else { node.HighValue = node.LowValue node.LowValue = newValue } node.Type = NODE_TYPE_3 return GetRoot(node) } // ... 3节点的情况 } // GetRoot 获取 node 所在树的根节点 func GetRoot(node *TreeNode) *TreeNode { if node.Parent == nil { return node } return GetRoot(node.Parent) } 2节点类型的叶子节点只需操作 Value。需要注意的点是，操作完必须把节点类型改为 3节点。 接着考虑插入时叶子节点为 3节点的情况。 3节点必然要拆成几个部分： 三数最小值单独为一个节点 三数中间值提升到父节点 三数最大值单独为一个节点 由于是叶子节点，不需要让单独出来的节点继承原先 3节点的 Left、Middle、Right，因此比较简单。 func AddToLeaf(node *TreeNode, newValue int) *TreeNode { // ... 2节点的情况 // 3节点的情况 var left, right *TreeNode // 要提升到父节点的 value var promotedValue int if newValue \u003c node.LowValue { // new_value \u003c low_value \u003c high_value left = NewTreeNode(newValue) promotedValue = node.LowValue right = NewTreeNode(node.HighValue) } else if newValue \u003e node.HighValue { // low_value \u003c high_value \u003c new_value left = NewTreeNode(node.LowValue) promotedValue = node.HighValue right = NewTreeNode(newValue) } else { // low_value \u003c new_value \u003c high_value left = NewTreeNode(node.LowValue) promotedValue = newValue right = NewTreeNode(node.HighValue) } return MergeWithParent(node.Parent, left, right, promotedValue) } 最后是将这三个部分合并到父节点。注意这里已经不再使用 node 这个节点了，它已经被拆成 3 个部分。 由于三个部分作为参数传入，它们也就不需要放到 node 里面。node 不需要额外添加属性去存储这些信息。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:2:1","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"合并到父节点 当最初的拆分出现时，拆分的节点必然是根节点。因此先考虑根节点的情况。 func MergeWithParent(parent, left, right *TreeNode, promotedValue int) *TreeNode { // 拆分的节点是根节点，因此再往上创建新的根节点。 if parent == nil { parent = NewTreeNode(promotedValue) parent.Left = left parent.Right = right parent.Left.Parent = parent parent.Right.Parent = parent return parent } // ... 非根节点的情况 } 在这种情况下，promotedValue 必然是作为一个新的节点类型为2节点的根节点出现。 由于使用同样的解构存储2节点和3节点，因此需要确定2节点时的使用规则。3节点可以使用 Left\\Middle\\Right 三个属性，2节点只能使用两个。C 的实现中，2节点使用 Left 和 Middle 属性，不使用 Right。我认为使用 Right 会更贴近于 AVL 树，容易理解，因此 2节点不使用 Middle，而是使用 Right。 要特别注意的点是，必须让 left 和 right 的 Parent 指向新节点。由于 2-3 树是向上生长的，Parent 如果没设置好，会在升高的时候没法正确地把 Value 合并到父节点。 接着考虑非根节点的情况。由于节点特性，还需要分 2节点和 3节点的情况。 首先考虑比较简单的非根 2节点。与叶子节点不同的地方在于需要处理子树，其他没有区别。处理子树时，根据子节点提升的值和2节点已有值的大小比较，决定子树存放的位置。下图分别是左右两种情况。 func MergeWithParent(parent, left, right *TreeNode, promotedValue int) *TreeNode { // ... 根节点的情况 // 非根节点的情况 if parent.Type == NODE_TYPE_2 { if promotedValue \u003e parent.LowValue { parent.HighValue = promotedValue parent.Middle = left parent.Right = right } else { parent.HighValue = parent.LowValue parent.LowValue = promotedValue parent.Left = left parent.Middle = right } parent.Left.Parent = parent parent.Middle.Parent = parent parent.Right.Parent = parent parent.Type = NODE_TYPE_3 return GetRoot(parent) } // ... parent.Type == NODE_TYPE_3 } 当然，仍然需要注意设置子节点的父节点。 最后考虑父节点是 3节点的情况。把提升的值的位置分为左中右三种情况讨论就行了。 promoted_value \u003c low_value \u003c high_value low_value \u003c promoted_value \u003c high_value low_value \u003c high_value \u003c promoted_value 代码和插入到叶子节点不同的地方在于处理子节点。 func MergeWithParent(parent, left, right *TreeNode, promotedValue int) *TreeNode { // ... 根节点的情况 // 非根节点的情况 // ... parent.Type == NODE_TYPE_2 // parent.Type == NODE_TYPE_3 var newLeft, newRight *TreeNode var newPromotedValue int if promotedValue \u003c parent.LowValue { // promoted_value \u003c low_value \u003c high_value newLeft = NewTreeNode(promotedValue) newLeft.Left = left newLeft.Right = right newPromotedValue = parent.LowValue newRight = NewTreeNode(parent.HighValue) newRight.Left = parent.Middle newRight.Right = parent.Right } else if promotedValue \u003e parent.HighValue { // low_value \u003c high_value \u003c promoted_value newLeft = NewTreeNode(parent.LowValue) newLeft.Left = parent.Left newLeft.Right = parent.Middle newPromotedValue = parent.HighValue newRight = NewTreeNode(promotedValue) newRight.Left = left newRight.Right = right } else { // low_value \u003c promoted_value \u003c high_value newLeft = NewTreeNode(parent.LowValue) newLeft.Left = parent.Left newLeft.Right = left newPromotedValue = promotedValue newRight = NewTreeNode(parent.HighValue) newRight.Left = right newRight.Right = parent.Right } newLeft.Left.Parent = newLeft newLeft.Right.Parent = newLeft newRight.Left.Parent = newRight newRight.Right.Parent = newRight return MergeWithParent(parent.Parent, newLeft, newRight, newPromotedValue) } 需要再次提醒的是要给子节点设置父节点信息。 新值的插入就到这里了。由于使用了 MergeWithParent 的这种处理方式，使得代码无论是写起来还是理解起来都比较简单。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:2:2","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"删除 如果没有做好总结，那么会发现删除时的情况特别多。不仅要考虑从删除的节点往下的节点，还要考虑其往上的节点。 由于 C 版本代码不包含删除操作，因此这里会先把删除操作总结后的内容写出来，然后再转换成代码实现。删除操作参考以下链接的说明： https://www.geeksforgeeks.org/2-3-trees-search-and-insert/ https://www.cs.princeton.edu/~dpw/courses/cos326-12/ass/2-3-trees.pdf 删除某个已存在节点时的操作有三条： 如果删除的值不在叶子节点，则交换待删除值和它在树的中序遍历结果中的下一个节点值，然后删除； 需要注意的是，中序遍历结果中的下一个值必然在一个叶子节点上。 如果一个值被删除后，所在节点值的个数为 0，就要从父母节点中取出一个值与兄弟合并 如果一个值被取出后，所在节点值的个数为 0，就要从父母节点中取出一个值与兄弟合并，直到根节点为空时删除根节点。 以这张图为例。 删除 13 时，由于它在叶子节点，直接删除就完成了： 删除 9 时，由于它不在叶子节点，交换它和中序遍历下一个值 10 的位置： 然后删除 9： 删除 11 时，由于是叶子节点，直接删除： 11 之前所在的节点成为空节点。由于空节点在父节点的左子树，因此从父节点中取出低值，和空节点最近的兄弟 14 合并。 删除 16 时，由于是叶子节点，直接删除： 删除 17 时，由于是叶子节点，直接删除： 现在出现空节点了。空节点在父节点的右子树，如果父节点是3节点就要从父节点中取出高值，但这里父节点是2节点，因此取仅剩的一个值与 empty 节点的兄弟节点合并。 但兄弟节点 12_14 是一个3节点，所以合并的时候要把 12_14_15 的中间值挤上去： 删除 12 时，由于是叶子节点，直接删除： 原来的节点变成空节点。则让父节点的值合并到空节点的兄弟节点： 空节点仍然存在。由于空节点在中间子树，因此可以选择取父节点的低值然后与左兄弟合并，也可以取高值和右兄弟合并。这里取前者。 由于左兄弟 3_6 是3节点，合并时需要把 3_6_10 的中间值挤上父节点，接着由于这个节点变成了2节点，右子树 7_8 要转移到新节点上： 由于时间有限，代码留着以后再实现吧。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:2:3","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"完整代码 package main import \"fmt\" const ( NODE_TYPE_2 = 2 // 2 节点 NODE_TYPE_3 = 3 // 3 节点 ) type TreeNode struct { Type int // 表示节点类型：2节点、3节点。2节点时 Middle 必为空 LowValue int HighValue int Parent *TreeNode Left *TreeNode Middle *TreeNode Right *TreeNode } // NewTreeNode 创建一个节点并设置为 2 节点 func NewTreeNode(value int) *TreeNode { return \u0026TreeNode{Type: NODE_TYPE_2, LowValue: value} } // IsLeaf 判断是否为叶子节点。如果不是叶子节点，则 Left 必不为空。 func (node *TreeNode) IsLeaf() bool { return node.Left == nil } // Text 获取节点 value 的字符串表示 func (node *TreeNode) Text() string { if node.Type == NODE_TYPE_2 { return fmt.Sprintf(\"%d\", node.LowValue) } return fmt.Sprintf(\"%d_%d\", node.LowValue, node.HighValue) } // GetRoot 获取 node 所在树的根节点 func GetRoot(node *TreeNode) *TreeNode { if node.Parent == nil { return node } return GetRoot(node.Parent) } // Insert 往树中添加一个 value。如果 value 已存在，则不做任何操作 func Insert(node *TreeNode, value int) *TreeNode { if node == nil { return NewTreeNode(value) } if node.IsLeaf() { return AddToLeaf(node, value) } if value \u003c node.LowValue { return Insert(node.Left, value) } if node.Type == NODE_TYPE_2 \u0026\u0026 value \u003e node.LowValue || node.Type == NODE_TYPE_3 \u0026\u0026 value \u003e node.HighValue { return Insert(node.Right, value) } if value == node.LowValue || value == node.HighValue { return GetRoot(node) } return Insert(node.Middle, value) } // AddToLeaf 把新 value 添加到叶子节点。如果添加前叶子节点已是 3 节点，则拆分并合并到父节点 func AddToLeaf(node *TreeNode, newValue int) *TreeNode { if node.Type == NODE_TYPE_2 { if newValue \u003e node.LowValue { node.HighValue = newValue } else { node.HighValue = node.LowValue node.LowValue = newValue } node.Type = NODE_TYPE_3 return GetRoot(node) } // node.Type == NODE_TYPE_3 var left, right *TreeNode var promotedValue int if newValue \u003c node.LowValue { left = NewTreeNode(newValue) promotedValue = node.LowValue right = NewTreeNode(node.HighValue) } else if newValue \u003e node.HighValue { left = NewTreeNode(node.LowValue) promotedValue = node.HighValue right = NewTreeNode(newValue) } else { left = NewTreeNode(node.LowValue) promotedValue = newValue right = NewTreeNode(node.HighValue) } return MergeWithParent(node.Parent, left, right, promotedValue) } // MergeWithParent 把拆分后的左右子树和提升的 value 合并到父母节点。如果父母节点已经是 3 节点，则继续拆分往上合并。 func MergeWithParent(parent, left, right *TreeNode, promotedValue int) *TreeNode { // 拆分的节点是根节点，因此再往上创建新的根节点。 if parent == nil { parent = NewTreeNode(promotedValue) parent.Left = left parent.Right = right parent.Left.Parent = parent parent.Right.Parent = parent return parent } if parent.Type == NODE_TYPE_2 { if promotedValue \u003e parent.LowValue { parent.HighValue = promotedValue parent.Middle = left parent.Right = right } else { parent.HighValue = parent.LowValue parent.LowValue = promotedValue parent.Left = left parent.Middle = right } parent.Left.Parent = parent parent.Middle.Parent = parent parent.Right.Parent = parent parent.Type = NODE_TYPE_3 return GetRoot(parent) } // parent.Type == NODE_TYPE_3 var newLeft, newRight *TreeNode var newPromotedValue int if promotedValue \u003c parent.LowValue { // promoted_value \u003c low_value \u003c high_value newLeft = NewTreeNode(promotedValue) newLeft.Left = left newLeft.Right = right newPromotedValue = parent.LowValue newRight = NewTreeNode(parent.HighValue) newRight.Left = parent.Middle newRight.Right = parent.Right } else if promotedValue \u003e parent.HighValue { // low_value \u003c high_value \u003c promoted_value newLeft = NewTreeNode(parent.LowValue) newLeft.Left = parent.Left newLeft.Right = parent.Middle newPromotedValue = parent.HighValue newRight = NewTreeNode(promotedValue) newRight.Left = left newRight.Right = right } else { // low_value \u003c promoted_value \u003c high_value newLeft = NewTreeNode(parent.LowValue) newLeft.Left = parent.Left newLeft.Right = left newPromotedValue = promotedValue newRight = NewTreeNode(parent.HighValue) newRight.Left = right newRight.Right = parent.Right } newLeft.Left.Parent = newLeft newLeft.Right.Parent = newLeft newRight.Left.Parent = newRight newRight.Right.Parent = newRight return MergeWithParent(pa","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:3:0","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"2-3 树与红黑树的关系 2-3 树理解起来并不复杂，使用这种方式实现也比较简单。从 2-3 树可以看出一个规律：任意节点到叶子节点的所有路径的长度相同。 有没有一点红黑树的味道？如果把 3节点的 LowValue 和 Left、Middle 下放，并且把 LowValue 标记为红色，原先节点标记为黑色，是不是就得到满足红黑树性质要求的树了？ 区别是，红黑树所说的是“从一个节点到一个 NULL 指针的每一条路径必须包含相同数目的黑色节点”。对于 2-3 树来说也是如此。 但是如果对比 2-3 树转换的红黑树与我们通常看到的红黑树，会发现两者不一样。例如在如下红黑树展示网站插入 1、2、3 这三个节点的时候： https://www.cs.usfca.edu/~galles/visualization/RedBlack.html 得到的是两个红色子节点。如果是使用 2-3 树的转换，应该三个节点都是黑色的： 实际使用的红黑树是 2-3-4 树所对应的红黑树。 前面 2-3 树规定 3 节点只能用低值作为红色节点，对应的红黑树是一颗左倾红黑树（LLRB，Left-leaning red-black trees）。左倾指的是连接到红色子节点的线是往左的。 为什么不直接用 2-3 树或者 2-3-4-树？ 为什么不使用 2-3 树转换的红黑树，而是使用 2-3-4 树的？ 数据解构专题的下一篇会继续展开。 ","date":"2022-06-02","objectID":"/2022/06/02/2-3-tree/:4:0","tags":["树","平衡树","数据结构"],"title":"2-3树 —— 理解红黑树的捷径","uri":"/2022/06/02/2-3-tree/"},{"categories":null,"content":"背景 之前在了解二叉树旋转的时候，为了方便查看中间状态，就写了个以树状形式打印二叉树的函数。 起初是使用二叉树中序遍历的结果展开的方式，简单但打印出来的树有一定的倾斜。 例如这棵树： 5 3 7 2 6 8 它的中序遍历结果为： +++++++++++++ |2|3|5|6|7|8| +++++++++++++ 打印出来的结果中，节点 3 和节点 7 不是对称的。因为节点 3 距离其父节点 5 的距离只有 1，而节点 7 距离其父节点 5 的距离则是 2。 于是做了一番改造，打印了对称的树： 5 3 7 2 6 8 对应的数组是： +++++++++++++ |2|3| |5|6|7|8| +++++++++++++ ","date":"2022-06-01","objectID":"/2022/06/01/print-binary-tree/:1:0","tags":["树","二叉树"],"title":"打印二叉树","uri":"/2022/06/01/print-binary-tree/"},{"categories":null,"content":"中序遍历 这里改成返回遍历结果而不是直接打印。 // InorderIteration 中序遍历迭代法 func InorderIteration(root *TreeNode) []*TreeNode { rs := make([]*TreeNode, 0) stack := make([]*TreeNode, 0) for len(stack) != 0 || root != nil { for root != nil { stack = append(stack, root) root = root.Left } root = stack[len(stack)-1] stack = stack[:len(stack)-1] rs = append(rs, root) root = root.Right } return rs } ","date":"2022-06-01","objectID":"/2022/06/01/print-binary-tree/:2:0","tags":["树","二叉树"],"title":"打印二叉树","uri":"/2022/06/01/print-binary-tree/"},{"categories":null,"content":"基于中序遍历结果的展开 // PrintTree 以中序遍历结果展开树 func PrintTree(root *TreeNode) { if root == nil { return } inorder := InorderIteration(root) row := []*TreeNode{root} var cache []*TreeNode for len(row) != 0 { // 每次取一整行出来，并重新申请空间存放下一行 cache = row row = make([]*TreeNode, 0) // 从中序遍历中寻找当前行的数据，不匹配的打印空格 i := 0 for _, node := range inorder { if node.Value != cache[i].Value { fmt.Print(\" \") } else { fmt.Print(node.Value) i++ if i == len(cache) { break } } } fmt.Println() for _, node := range cache { if node.Left != nil { row = append(row, node.Left) } if node.Right != nil { row = append(row, node.Right) } } } } ","date":"2022-06-01","objectID":"/2022/06/01/print-binary-tree/:3:0","tags":["树","二叉树"],"title":"打印二叉树","uri":"/2022/06/01/print-binary-tree/"},{"categories":null,"content":"补全空位置的打印 借鉴中序遍历展开的思路。根据树的高度，申请一个可以容纳这个高度满节点状态的节点数量的数组。从根节点开始，宽度优先遍历。让每个节点都把一个范围平均分成两部分。 为了方便，这里先展示具有破坏性的打印。破坏的地方为 Node 的 Height 属性。在打印时，会发生变化。如果不想破坏，则再增加一个 Layer 属性即可。 func PrintTree(root *TreeNode) { // 把子节点的高度更新为父节点高度-1 UpdateHightToMax(root) data := ToStrictInorderArray(root) height := root.Height for height \u003e 0 { // 每次遍历完整的中序遍历结果，当元素的层级与当前层级相同时打印值，不同时打印空格 for _, element := range data { if element == nil || element.Height != height { fmt.Print(\" \") } else { fmt.Printf(\"%d\", element.Value) } } height-- fmt.Println() } } // UpdateHightToMax 前序遍历更新高度，把子节点的高度更新为父节点高度-1 func UpdateHightToMax(root *TreeNode) { stack := make([]*TreeNode, 0) for len(stack) != 0 || root != nil { for root != nil { if root.Parent != nil { root.Height = root.Parent.Height - 1 } stack = append(stack, root) root = root.Left } node := stack[len(stack)-1] stack = stack[:len(stack)-1] root = node.Right } } // ToStrictInorderArray 对称的树 func ToStrictInorderArray(root *TreeNode) []*TreeNode { if root == nil || root.Height \u003c 0 { return make([]*TreeNode, 0) } // 总节点数为 (2^n)-1 result := make([]*TreeNode, (1\u003c\u003croot.Height)-1) // 确保操作下一层节点时，父节点已放入结果集 queue := BreadthFirstIteration(root) // root 的位置无法从父节点算出，先算出来 index := 1\u003c\u003c(root.Height-1) - 1 result[index] = queue[0] queue = queue[1:] for len(queue) \u003e 0 { iterNode := queue[0] queue = queue[1:] distance := 1 \u003c\u003c (iterNode.Height - 1) idx := FindIndexStrictInorder(result, iterNode.Parent) if iterNode.Value \u003c iterNode.Parent.Value { idx -= distance } else { idx += distance } result[idx] = iterNode } return result } // BreadthFirstIteration 广度优先遍历 func BreadthFirstIteration(node *TreeNode) []*TreeNode { data := []*TreeNode{node} for index := 0; index \u003c len(data); index++ { node = data[index] if node.Left != nil { data = append(data, node.Left) } if node.Right != nil { data = append(data, node.Right) } } return data } // FindIndexStrictInorder 在数组中找到指定节点的位置 func FindIndexStrictInorder(tree []*TreeNode, node *TreeNode) int { cut := (len(tree) + 1) \u003e\u003e 1 middle := cut - 1 for cut \u003e 1 \u0026\u0026 middle \u003c= len(tree) { cut = cut \u003e\u003e 1 if node.Value \u003c tree[middle].Value { middle -= cut } else if node.Value \u003e tree[middle].Value { middle += cut } else { return middle } } return middle } ","date":"2022-06-01","objectID":"/2022/06/01/print-binary-tree/:4:0","tags":["树","二叉树"],"title":"打印二叉树","uri":"/2022/06/01/print-binary-tree/"},{"categories":null,"content":"上一篇把树旋转了解清楚，是为这一篇平衡二叉树准备的。 平衡二叉树，就是在二叉树的基础上加上一个条件：对于任意节点，左子树和右子树的树高之差不超过 1。 从实现的角度看，就是在已具备旋转功能的 Node 上增加一个 height 字段，并且在原先的代码上增加对 height 的操作。关键操作是判断左右子树的树高之差，根据树高之差选择需要执行的旋转。 ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:0:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"示例 如图所示，这是一颗高为 3 的树。根节点左右两边的高度差为 1，满足平衡条件。 此时插入一个值为 1 的节点来破坏这个平衡。当节点 1 插入后，需要逐级往上更新节点的高度。 在高度更新后，发现根节点左右两边的高度差为 2，因此需要通过右旋调整平衡。节点 3 是转轴，按照旋转的规则执行。得到以下结果： 基本思路很简单。剩下的问题放到后面的内容处理。 以下内容会以可旋转的二叉排序树的代码（前篇文章的内容）为基础，添加 Height 这一属性。并根据树高的要求调整代码。 ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:1:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"可旋转的二叉排序树 以下是原始代码，不难。与上一篇文章不同的是把 PutChild 改为 Insert，还有简化了旋转的代码。 package main import \"fmt\" type TreeNode struct { Parent *TreeNode Value int Left *TreeNode Right *TreeNode } // Insert 往树中合适位置插入节点 func Insert(root *TreeNode, value int) *TreeNode { if root == nil { return \u0026TreeNode{Value: value} } if value \u003c root.Value { root.Left = Insert(root.Left, value) root.Left.Parent = root } else if value \u003e root.Value { root.Right = Insert(root.Right, value) root.Right.Parent = root } else { return root } return root } // RotateRight 右旋 func RotateRight(root *TreeNode) *TreeNode { if root.Left == nil { return root } newRoot := root.Left tmp := newRoot.Right newRoot.Right = root root.Left = tmp if tmp != nil { tmp.Parent = root } newRoot.Parent = root.Parent root.Parent = newRoot return newRoot } // RotateLeft 左旋 func RotateLeft(root *TreeNode) *TreeNode { if root.Right == nil { return root } newRoot := root.Right tmp := newRoot.Left newRoot.Left = root root.Right = tmp if tmp != nil { tmp.Parent = root } newRoot.Parent = root.Parent root.Parent = newRoot return newRoot } // PrintTree 以树状形式打印树 func PrintTree(root *TreeNode) { // 这里先不管 } func main() { var root *TreeNode root = Insert(root, 7) root = Insert(root, 3) root = Insert(root, 2) root = Insert(root, 5) root = Insert(root, 8) PrintTree(root) fmt.Println(\"------------\") root = RotateLeft(root) PrintTree(root) fmt.Println(\"------------\") root = RotateRight(root) PrintTree(root) fmt.Println(\"------------\") } ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:2:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"添加 Height 参数 type TreeNode struct { Parent *TreeNode Value int Height int Left *TreeNode Right *TreeNode } func NewTreeNode(value int) *TreeNode { return \u0026TreeNode{Value: value, Height: 1} } 因为每次插入的节点都是作为叶子节点，所以新节点的树高都为 1。这里新增 New 函数，在初始化时自动指定。 ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:3:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"检测树平衡 在修改的时候才需要检测树平衡，因此需要关注三个操作：插入、旋转、删除。这里忽略删除的部分，仅关注插入和旋转。 原始代码： func Insert(root *TreeNode, value int) *TreeNode { if root == nil { return \u0026TreeNode{Value: value} } if value \u003c root.Value { root.Left = Insert(root.Left, value) root.Left.Parent = root } else if value \u003e root.Value { root.Right = Insert(root.Right, value) root.Right.Parent = root } else { return root } return root } 每次插入一个叶子节点，有可能使其父节点树高增加，因此必须更新其父节点的树高。接着由于父节点树高的更新，需要再更新上一层树高，直到根节点。 由于 Insert 是递归调用，在递归下面写插入完成后的代码。分为两部分：1. 更新当前节点的树高；2. 如果左右子树树高相差超过 1，则通过旋转平衡该树。 首先第一部分，更新树高。 由于左右子树高度不一定一致，所以要取较高的那一颗子树的树高，加上 1 就是以当前节点作为根节点的子树的高度。 func max(a, b int) int { if a \u003e b { return a } return b } // GetHeight 用来处理节点为 nil 的情况 func GetHeight(node *TreeNode) int { if node == nil { return 0 } return node.Height } func Insert(root *TreeNode, value int) *TreeNode { // ... root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 return root } 接着第二步，判断树平衡。 引入一个函数来获取平衡因子。当平衡因子小于 -1 的时候，表示右边的子树比左边高大于 1，此时应该左旋。反之，平衡因子大于 1 的时候表示应该右旋。 // GetBalanceFactor 获取平衡因子 func GetBalanceFactor(node *TreeNode) int { if node == nil { return 0 } return GetHeight(node.Left) - GetHeight(node.Right) } func Insert(root *TreeNode, value int) *TreeNode { // ... root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 bf := GetBalanceFactor(root) if bf \u003c -1 { // 应该左旋 root = RotateLeft(root) } else if bf \u003e 1 { // 应该右旋 root = RotateRight(root) } else { // do nothing } return root } ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:4:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"旋转时更新树高 这里要先更新原先 root 节点的树高，因为旋转后它是 newRoot 的子节点。总是要按照先子节点再父节点的顺序更新树高。 另外由于 tmp 子树本身没有修改，因此不需要更新树高。 // RotateRight 右旋 func RotateRight(root *TreeNode) *TreeNode { // ... root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 newRoot.Height = max(GetHeight(newRoot.Left), GetHeight(newRoot.Right)) + 1 return newRoot } // RotateLeft 左旋 func RotateLeft(root *TreeNode) *TreeNode { // ... root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 newRoot.Height = max(GetHeight(newRoot.Left), GetHeight(newRoot.Right)) + 1 return newRoot } ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:5:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"目前为止的完整代码 在进入下一个阶段前，有必要浏览一遍当前的完整代码。 package main import \"fmt\" type TreeNode struct { Parent *TreeNode Value int Height int Left *TreeNode Right *TreeNode } func NewTreeNode(value int) *TreeNode { return \u0026TreeNode{Value: value, Height: 1} } // Insert 往树中合适位置插入节点 func Insert(root *TreeNode, value int) *TreeNode { if root == nil { return \u0026TreeNode{Value: value} } if value \u003c root.Value { root.Left = Insert(root.Left, value) root.Left.Parent = root } else if value \u003e root.Value { root.Right = Insert(root.Right, value) root.Right.Parent = root } else { return root } root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 bf := GetBalanceFactor(root) if bf \u003c -1 { // 应该左旋 root = RotateLeft(root) } else if bf \u003e 1 { // 应该右旋 root = RotateRight(root) } else { // do nothing } return root } func max(a, b int) int { if a \u003e b { return a } return b } // GetHeight 用来处理节点为 nil 的情况 func GetHeight(node *TreeNode) int { if node == nil { return 0 } return node.Height } // GetBalanceFactor 获取平衡因子 func GetBalanceFactor(node *TreeNode) int { if node == nil { return 0 } return GetHeight(node.Left) - GetHeight(node.Right) } // RotateRight 右旋 func RotateRight(root *TreeNode) *TreeNode { if root.Left == nil { return root } // 旋转 newRoot := root.Left tmp := newRoot.Right newRoot.Right = root root.Left = tmp // 更新节点的父节点信息 if tmp != nil { tmp.Parent = root } newRoot.Parent = root.Parent root.Parent = newRoot // 更新树高 root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 newRoot.Height = max(GetHeight(newRoot.Left), GetHeight(newRoot.Right)) + 1 return newRoot } // RotateLeft 左旋 func RotateLeft(root *TreeNode) *TreeNode { if root.Right == nil { return root } // 旋转 newRoot := root.Right tmp := newRoot.Left newRoot.Left = root root.Right = tmp // 更新节点的父节点信息 if tmp != nil { tmp.Parent = root } newRoot.Parent = root.Parent root.Parent = newRoot // 更新树高 root.Height = max(GetHeight(root.Left), GetHeight(root.Right)) + 1 newRoot.Height = max(GetHeight(newRoot.Left), GetHeight(newRoot.Right)) + 1 return newRoot } func PrintTree(root *TreeNode) { } func main() { var root *TreeNode root = Insert(root, 7) root = Insert(root, 3) root = Insert(root, 2) root = Insert(root, 5) root = Insert(root, 8) PrintTree(root) fmt.Println(\"------------\") root = Insert(root, 6) PrintTree(root) fmt.Println(\"------------\") } ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:6:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"旋转的问题 与最开始示例不同的是，上面的代码最后插入的是 6。执行这些代码，发现得到的仍然是一颗不平衡的树。 为什么？ 用图来解释： 这是原始图，插入节点 6 后得到： 此时对于节点 7，左子树比右子树高 2。因此需要右旋，根据规则右旋后得到： 为什么会这样？ 从第二张图可以看到，之所以平衡被打破，是因为 A 的高度发生变化，导致节点 3 的高度变化。当右旋开始时，这个打破平衡的 A 被抽离了。 抽离后节点 3 的高度变回 2，也就是说根节点左子树的高度为 2。如果执行右旋，那么根节点的左子树的高度必定会减 1，变成 1。 不管原先根节点右子树的树高是多少，在旋转后树高为 2 的部分必然要挂到原先根节点 7 上。此时根节点的右子树高度必大于 2。 上图把节点 8 隐藏起来，可以直观地看到问题。 也就是说，继续按照之前的方式，旋转后必处于不平衡状态。而且从这个状态出发，也只能执行左旋，旋转后回到原来的样子。进入死循环。 怎么解决？ 根据上面的描述，右旋是必然要做的，并且右旋时必然使左子树高度减 1。 要解决这个问题，需要让根节点的左子树去掉其右子树剩下的部分的高度增加，然后再右旋。 有没有办法？ 有，让左子树左旋就行。 根据规则旋转后： 接着，对根节点执行右旋： 去掉 A 的部分后，左子树的高度仍然为 3，右旋后为 2。 平衡了。 综上，当一个节点的左右子树不平衡时，要分两步判断： 左子树高还是右子树高 不平衡是由子树的左子树还是右子树引起的 如果是左子树的右子树引起的，则子树需要先旋转。同理，如果是右子树的左子树引起的，子树也要先旋转。（发现没？两者都是在旋转曲线的内侧） ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:7:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"旋转方式改进 从前面的内容可以总结出旋转有四种类型： 左子树的左子树引起的失衡，用 LL（Left-Left） 表示； 左子树的右子树引起的失衡，用 LR（Left-Right） 表示； 右子树的左子树引起的失衡，用 RL（Right-Left） 表示； 右子树的右子树引起的失衡，用 RR（Right-Right） 表示。 在 Insert 的时候，要区分这四种情况。 func Insert(root *TreeNode, value int) *TreeNode { // ... bf := GetBalanceFactor(root) if bf \u003c -1 { // 应该左旋 if value \u003c root.Right.Value { // 在右子树的左子树上 root = RLRotation(root) } else { // 在右子树的右子树上 root = RRRotation(root) } } else if bf \u003e 1 { // 应该右旋 if value \u003c root.Left.Value { // 在左子树的左子树上 root = LLRotation(root) } else { // 在左子树的右子树上 root = LRRotation(root) } } else { // do nothing } return root } func LLRotation(root *TreeNode) *TreeNode { return RotateRight(root) } func LRRotation(root *TreeNode) *TreeNode { root.Left = RotateLeft(root.Left) return RotateRight(root) } func RRRotation(root *TreeNode) *TreeNode { return RotateLeft(root) } func RLRotation(root *TreeNode) *TreeNode { root.Right = RotateRight(root.Right) return RotateLeft(root) } ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:8:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"结尾 以前一直认为二叉树旋转和平衡二叉树很难，现在认真看的时候却觉得其实也没什么。 接下去先用二叉树的打印水一篇，然后就是红黑树了。 以前也一直觉得红黑树很难，但最近看了资料（特别是那本《算法》），觉得挺容易理解的，就干脆一起写出来吧。 ","date":"2022-05-30","objectID":"/2022/05/30/balanced-binary-tree/:9:0","tags":["树","数据结构"],"title":"平衡二叉树","uri":"/2022/05/30/balanced-binary-tree/"},{"categories":null,"content":"我偶尔会思考“我的一生应该是什么样的”这个问题。当我偶然间思考出了一个足够明确的答案后，就自然而然地以最佳的状态去面对生活。 只是我没有及时地将这些内容记录下来，以至于因为一些事情而逐渐地忘记一部分内容，这种状态就消失了。如果能让那种时刻保持充分幸福感的状态一直持续下去，我也就心满意足了。所以我尝试现在将剩下的部分写出来，一是试图通过这种方式将遗忘的部分重新推导出来，二是为了以后重新进入状态的时候能快速补足内容。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:0:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"所为何求 是不是大多数人都知道自己追求的是什么？从我接触过的人、新闻或者影视剧上所认识到的是：有的人没有目标，只是纯粹被生活推着走；有的人的目标是别人定的；有的人只定了短期目标，在达成目标前没有及时设定下一个目标，导致陷入迷茫；有的人只设定了目标的质，没有设定量，导致无止尽的追求… 让我十几年保持不断进步的目标是我从小学就开始萌芽并逐渐在中学时期明确的一个想法：让自己成为一个程序员，然后成为一个能力处于第一梯队的程序员。 这个目标有两个阶段，第一个阶段容易实现，第二个阶段非常难实现。这就使得我既能看到已经实现目标的一部分，又能一直看到前进的路。无论遇到什么事情，处于何种状态，都能看到这个目标。 这是一个收益与风险并存的目标。它的收益之一已经说过，那么它的风险或者说问题在哪里呢？ 分为两个方面： 如何保证不会放弃这个目标？这是几乎所有目标的问题。属于共性问题。 这个目标的主语只有“我”，并且不能简单地扩大主语范围。属于特性问题。 第一个问题。人生有限，在某方面分配的时间多了，另一方面分配的时间就少。就像游戏里的技能点数的分配，给一个技能加等级，就意味着其他技能无法增加等级，而且在游戏的不同阶段也会有不同的加点方式。人生不是游戏，但资源的有限性却是相似的。如果我在写程序上点了太多等级，是否会因为其他技能不足而被锁住上限？或者迫于一些现实而无奈地改变目标？ 这就使得我在中学时期仍然以较为全面的学习为主，没有早早地走上写程序的道路。通过加入了校广播部锻炼自己的胆量，以及一定的随机应变能力。并且决定大学的时候当班长，提高沟通能力和做事能力。这些都是为了减少目标以外的因素对于目标本身的影响，但是牺牲了不少的目标推进进度。 第二个问题。当一个人的目标主体非常局限的时候，就会一直面临一个问题：如何对待目标以外的人？我的目标的问题比较小，因为它主要依赖于我自身的提高，而且可以通过良性合作加速达成。而有些人的目标则是主要依靠对抗达成，在某些领域甚至会走捷径达成目标。 除了在目标领域上的，还有非目标领域上的。 举个例子，有的人的目标是赚很多钱。多少钱呢？没有设限，多多益善。个人的欲望随着物质财富的增加而膨胀，在物质财富超过一定程度后，也没有调整。总是想着只有赚足够多的钱才能让家人幸福。而在赚足够的钱之前，不那么在乎家人的感受，甚至工作不顺利时回到家里还会迁怒其他人。言必称“赚钱是为了家庭”。这究竟是影视作品的夸张表现还是真实存在呢？ 同理，就算我的目标是非对抗的，也必须做出选择。 因此大学时期就增加与父母的聊天，更多地了解他们的想法，同时传递我的一些想法。 但是这还不够，第一目标仍然没有发生改变。直到我上一次领悟到一些东西后，结合价值观和世界观的更新，我的人生目标才正式确定。 创造可持续的幸福感。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:1:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"创造可持续的幸福感 每个人都有自己幸福的标准，因此要了解别人，在适当的方面提供帮助。 可持续的意思是不以牺牲未来的幸福来满足当前的幸福。 这个目标的对象范围是逐渐增加的，圈子中心是我自己，根据我自身的能力去覆盖对应范围的人。 幸福感 参考人的需求模型，其最顶层是自我价值的实现。这是对众多具体的目标的抽象。 但是自我价值的实现，要注意区分。例如有的人通过出卖自己的国家来“实现自己的价值”。这不是实现自我价值，因为它贱卖了这个国家的人民所生产的价值，以此获得了远超自身劳动所能生产的价值。 一个人或者集体通过自身的劳动，获得与劳动产生的等量价值，这些价值能够满足基本的衣食住行。这是一种幸福。 幸福是一种持续的积极向上的情感，它使得人们想要通过劳动创造价值，既维持了这种幸福的状态，又为他人带来幸福。这使人自然而然地消除了负面情绪（例如恐惧与愤怒）且不会受他人负面情绪的影响，总是处于放松的状态而基本不需要另外的娱乐（看剧或者玩游戏）来放松自己，总是积极地想办法解决发现的问题。 这种状态的好处还在于，它让人很想一直处于这种状态（上瘾？），并且只能通过提升自己以及帮助他人来获得，无法通过非劳动过程或者损人利己的方式获得。 当获得的价值高于自身劳动生产的价值时，得到的就不是幸福的感觉了。 我们平常所感受到的快乐和满足，还不足以称为幸福。如果把追求短暂的快乐、满足感、刺激感放在第一位，就会在痛苦、一般、快乐三种感觉不断地转换。 覆盖范围 资本主义有一种高脚杯模型理论，大意是将酒杯摆成金字塔形状，当顶层的杯子装满酒后，继续倒酒自然会溢出到其他杯子。这是献给少数强者而不是人民的理论。这里不展开讨论，只是用来与本节内容做出区别。 因此我用的是能力大小而不是自身的幸福度大小对应覆盖范围。 可持续 当我们不去考虑一件美好的事情的可持续性时，就可能走上牺牲未来以满足现在的道路，并且在时间交汇之时彻底失去美好的事物。牺牲未来，总会有耗尽的时候。 如果说人总是应该追求幸福，那么如何面对那些必须要经过痛苦的过程？是否与这一目标产生了冲突？ 当处于幸福的状态时，那些会让我们感到痛苦的过程，会被削弱。这样就只是暂时地取出一部分幸福与这些痛苦中和，然后生产出更多的幸福感。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:2:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"心态 基于上述的内容，可以自然地得出对待他人的心态。 别人取得成就，就算我没做出什么东西来，那么也祝贺他；别人赚了很多钱，而我只能赚点小钱，那么也恭喜他；别人有女朋友或者结婚了，而我只是一个人，那么也祝福他们。 我总是乐于见到他人通过自身努力获得想要的生活。当能力非常强的时候，想要帮助的人越多。当这个范围最大化时，就是希望全世界的人都能幸福。这是一个值得奋斗的目标。有人通过自身努力得到幸福，就算过得比我好，也是我希望看到的。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:3:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"选择 我们一生会面临无数的选择。有句话叫“选择比努力更重要”。有一定的道理，但也有问题。 有道理的地方在于，一个好的选择能使得自己的努力能产生更多的价值。有问题的地方在于，有些选择会让人获得超出自身所能创造的价值，最终被选择本身所限制住。 在获得远超出自身所能创造的价值的量时，是一个人所面临的最危险的时刻。他需要面对价值观的考验，稍有不慎，价值观将会被击溃。他会开始认为只有这些价值量能够匹配此刻付出的努力。然而获得超额价值量只是一时的，这使得下一次如果评估发现自己付出相同努力得到的东西远不如以前，就会放弃努力。 在家庭中也会面临这个问题。小孩子通常会因为家长的宠溺，从小就能通过非常少的劳动获得很高的价值量。一旦这种畸形比例关系没有及时纠正，固化在孩子思想中，随着年龄的增长，孩子的劳动创造的价值增加，家长需要付出更多的劳动来维持这种比例。当家长的劳动不足以维系这种比例时，从孩子的角度看，父母就像是资本家剥削工人那样，用远低于孩子的劳动付出的价值量与孩子交换，矛盾就爆发了。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:4:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"家庭 家庭对人的成长起到很关键的作用，同时也在很大程度上影响了长大后所面临的压力。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:5:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"历史的定位 过去我对于自己在家族的历史定位是承上启下。我所拥有的资源，以及我自身努力的上限，不足以使我做出多大的贡献。我们家是一代比一代强，因此我的主要使命是为我未来的孩子提供更好的资源，让ta拥有做出足够大贡献的能力。 至于现在，总体上不变，但我觉得自己又行了。是不是在我这代，也能做点事情？ ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:6:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"确定和不确定的人生 曾经见到一位同学在微博上抒发焦虑之苦，还提到人生若是确定了就无趣了。我当时的想法是： 焦虑感往往都来源于对未来人生不确定性的担忧，对未来人生不确定性的担忧来自于自以为对现实世界了解得足够多，但其实是对想象中的世界了解得多，而对现实世界了解得太少。对自己的了解也太少。 人生未来的可控性不是想可控就能可控的，人的自信应该建立在实力上。人生和电视剧不一样，就算剧本写好了，也会有各种各样的变化。普通人根本就做不到按照预设的剧本去生活。 个人通过主动提升对未来的可控能力，在一定的发展阶段，就会发现有能力选择更多的发展方向，可以根据需要选择，然后重新规划人生。人和机器不一样，不是设定了程序就不能主动改的。 提升个人对未来可控能力的同时主动创造不确定性，比起只能有一个选择而被迫任由不确定性宰割，所获取到的趣味和期待多得多。 ","date":"2022-05-24","objectID":"/2022/05/24/life-view/:7:0","tags":["life"],"title":"选择人生(草稿)","uri":"/2022/05/24/life-view/"},{"categories":null,"content":"事情要从某天晚上买夜宵说起。买了香肠拿着吃，想着多年来一直没搞懂的树旋转是不是应该看看，就点进某百科。 树旋转是在二叉树中的一种子树调整操作， 每一次旋转并不影响对该二叉树进行中序遍历的结果。 中序遍历！灵光一闪，好像很多东西都联系起来了！ ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:0:0","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"为什么是中序遍历 中序遍历是指按【左节点-\u003e父节点-\u003e右节点】的顺序遍历，这个内容能让我们想起什么？二叉排序树。 二叉排序树要求左节点的值小于父节点，右节点大于父节点。如果按照中序遍历二叉排序树，就能得到一个顺序结果。 知道这一点有什么用？在后续的旋转过程中，我们可以根据二叉排序树的父节点和子节点的大小关系来辅助理解旋转。 ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:1:0","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"举个例子 现在我们有一颗二叉排序树： 其中序遍历的结果是 1、2、3、4、5、6、7、8、9。 以下先不考虑子树的旋转。 ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:2:0","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"右旋 现在对根节点 5 执行右旋。右旋的时候，要选择根节点 5 的左节点 3 作为新的根节点，称为以节点 3 为转轴。 为讨论方便，把旋转前 3 的右子树称为 A，5 的右子树称为 B。如下图所示： 由于是右旋转，当节点 3 处于根节点的时候，其左子树的数必须仍然小于 3，又因为 A、5、B 都大于 3，所以 3 旋转前的左子树在旋转后保持原样，仍然是 3 的左子树。 现在有两个问题： A 放哪里？ 根节点 5 放在哪里？ 结合二叉排序树来理解。由于 3 即将成为根节点，A 和原先根节点都大于 3，因此两者都要放在旋转后 3 的右子树。那么问题就转化为：如何将 A、B、根节点 5 合并起来？ 首先考虑根节点 5，先忽略 A。在自然旋转后，节点 5 在 3 的右子树，B 和 5 的关系不变，且 5 的左子树必然为空。 现在考虑 A。由于旋转前它就在 5 的左子树里面，所以必然小于 5。旋转后 5 的左子树为空，就可以直接把 A 作为 5 的左子树。 旋转完毕。用中序遍历验证，其结果是 1、2、3、4、5、6、7、8、9，结果不变。 ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:2:1","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"右旋代码实现 先不考虑子树内旋转的情况，便于理解。 package main import \"fmt\" type TreeNode struct { Left *TreeNode Right *TreeNode Value int } // PutChild 用于简化初始化 func (n *TreeNode) PutChild(child *TreeNode) { if child.Value \u003c n.Value { n.Left = child } else if child.Value \u003e n.Value { n.Right = child } } // RotateRight 右旋，参数先不使用转轴 func RotateRight(root *TreeNode) { if root.Left == nil { return } // 把 A 备份出来 a := root.Left.Right root.Left.Right = nil // 旋转。3 原先在 5 左节点，现在让 5 变成 3 的右节点 newRoot := root.Left root.Left = nil newRoot.Right = root // 把 A 放回去 root.Left = a } // PrintInorderIteration 中序遍历迭代法 func PrintInorderIteration(root *TreeNode) { stack := make([]*TreeNode, 0) for len(stack) != 0 || root != nil { for root != nil { stack = append(stack, root) root = root.Left } root = stack[len(stack)-1] stack = stack[:len(stack)-1] fmt.Println(root.Value) root = root.Right } } func main() { // 为了与图对应，这里多申请一个位置，但只从 1 开始初始化。 nodes := make([]*TreeNode, 10) for i := 1; i \u003c 10; i++ { nodes[i] = \u0026TreeNode{Value: i} } nodes[5].PutChild(nodes[3]) nodes[5].PutChild(nodes[8]) nodes[3].PutChild(nodes[2]) nodes[3].PutChild(nodes[4]) nodes[2].PutChild(nodes[1]) nodes[8].PutChild(nodes[7]) nodes[8].PutChild(nodes[9]) nodes[7].PutChild(nodes[6]) fmt.Println(\"BEGIN\") PrintInorderIteration(nodes[5]) fmt.Println(\"END\") RotateRight(nodes[5]) fmt.Println(\"BEGIN\") PrintInorderIteration(nodes[3]) fmt.Println(\"END\") } ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:2:2","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"左旋 还是使用原始版本。这次根节点 5 的左子树为 B，右节点 8 为转轴，节点 8 的左子树为 A（转轴旋转方向的内侧）。 由于是左旋，转轴 8 的左子树 A 先不考虑。做自然旋转。 由于子树 A 之前是 8 的左子树，说明子树 A 上的节点都小于 8，因此旋转后子树 A 必然在新的根节点 8 的左侧。 又由于旋转后的节点 5 的右子树必然为空，而子树 A 在旋转前就在 5 的右子树，说明子树 A 上的节点必然大于 5，因此旋转后子树 A 应作为 5 的右子树。 旋转完毕。用中序遍历验证，其结果是 1、2、3、4、5、6、7、8、9，结果不变。 代码逻辑和右旋类似，不再重复。 ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:2:3","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"子树的旋转 还是回到原先树上。现在考虑以 3 为定点的子树，执行右旋，此时转轴为 2。 这里节点 2 没有右子树，因此用虚线表示 A 区域，便于识别 在旋转过程中，与原先不同的地方在哪？ 在于必须更新顶点的父节点。例如上图，需要更新节点 5 的左节点为 2。按照前面右旋的代码设计，当我们传入的是节点 3 的时候，没法更新节点 5。因此需要给节点引入一个新的字段 Parent，用于表示其父节点。 type TreeNode struct { Parent *TreeNode Left *TreeNode Right *TreeNode Value int } 在旋转的时候，基于前面右旋的代码，加上父节点的更新就可以了。有些步骤能合并，但为了容易理解，不予合并。 // PutChild 用于简化初始化 func (n *TreeNode) PutChild(child *TreeNode) { if child.Value \u003c n.Value { n.Left = child } else if child.Value \u003e n.Value { n.Right = child } else { return } child.Parent = n } // RotateRight 右旋，参数先不使用转轴 func RotateRight(root *TreeNode) { if root.Left == nil { return } // 把 A 备份出来，并取消双向连线 a := root.Left.Right root.Left.Right = nil if a != nil { a.Parent = nil } // 取到子树根节点的父节点，并取消双向连线 rootParent := root.Parent root.Parent = nil // 如果 root 是整颗树的根节点，无需调整 if rootParent != nil { if rootParent.Value \u003e root.Value { rootParent.Left = nil } else { rootParent.Right = nil } } // 旋转。2 原先在 3 左节点，现在让 3 变成 2 的右节点 newRoot := root.Left root.Left = nil newRoot.Parent = nil newRoot.Right = root root.Parent = newRoot // 设置根节点的父节点 if rootParent != nil { if rootParent.Value \u003e newRoot.Value { rootParent.Left = newRoot } else { rootParent.Right = newRoot } } // 把 A 放回去 root.Left = a } ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:3:0","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"结尾 上文使用了二叉排序树作为辅助理解的工具，顺便一提，在想到二叉排序树的时候，还应将其和二分搜索联系起来。 单看树的旋转，其实非常简单。树的旋转实际上是为平衡二叉树做铺垫，因此下一篇将会把普通的二叉树换成平衡二叉树。 ","date":"2022-05-11","objectID":"/2022/05/11/tree-rotation/:4:0","tags":["树","二叉树","平衡二叉树","树旋转"],"title":"因为一句话，秒懂二叉树旋转","uri":"/2022/05/11/tree-rotation/"},{"categories":null,"content":"证书验证 ","date":"2022-05-10","objectID":"/2022/05/10/cert-verify/:1:0","tags":["HTTPS","TLS"],"title":"手动验证 TLS 证书","uri":"/2022/05/10/cert-verify/"},{"categories":null,"content":"证书结构 我们现在使用的 TLS 证书的标准是 X.509，版本号为 V3。版本号可从证书的 Version 字段看到。 根据 RFC 3280 定义的证书结构，证书由三个部分组成： 证书主体（TBSCertificate，To Be Signed Certificate，待签名证书） 签名算法 签名值 证书主体包括版本、序列号、公钥等内容。签名值是对证书主体使用签名算法计算并经过证书签名机构私钥加密后的值。 证书的数据组织格式为 ASN.1 DER 格式（distinguished encoding rules）。这是一种 TLV 编码，其中的每个元素都包含 Tag、Length、Value。通常我们获得的证书是经由 Base64 编码后的 PEM 文件，为了节省不必要的麻烦，后续内容会提前将证书转回 DER 格式。 PEM 文件的格式是这样的： -----BEGIN CERTIFICATE----- 这里是 Base64 编码后的内容 -----END CERTIFICATE----- ","date":"2022-05-10","objectID":"/2022/05/10/cert-verify/:1:1","tags":["HTTPS","TLS"],"title":"手动验证 TLS 证书","uri":"/2022/05/10/cert-verify/"},{"categories":null,"content":"证书验证 为了验证证书，我们需要获取以下内容： 证书主体。用于通过签名算法得到其 hash 值 证书签名值。 CA 的公钥。用于解密签名，得到证书主体的 hash 值 如果 1 和 3 的 hash 值相等，则说明此次验证通过。 我们以博客园的证书为例。 证书下载 下载博客园证书 echo -n | openssl s_client -connect www.cnblogs.com:443 | openssl x509 -outform DER \u003e www.cnblogs.com.crt 证书原文是用 Base64 编码后的 PEM 格式文件，这里将其直接转换为 DER 格式。 查看博客园的 CA 证书地址 openssl x509 -inform DER -in www.cnblogs.com.crt -noout -text | grep \"CA Issuers\" 得到： CA Issuers - URI:http://cacerts.digicert.com/EncryptionEverywhereDVTLSCA-G1.crt 下载 CA 证书 curl -O http://cacerts.digicert.com/EncryptionEverywhereDVTLSCA-G1.crt 这个证书已经是 DER 格式，无需转换。 现在，我们得到了： 博客园证书 给博客园证书签名的 CA 的证书 DER 证书解析 在开始验证之前，再次了解证书的格式，便于理解后续的获取操作。 证书由证书主体、签名算法和签名值三个部分组成。DER 格式每个元素以 SEQUENCE 作为开头。由于完整的证书是一个 SEQUENCE，因此证书最外层有个 SEQUENCE。里面三个部分都各自有一个 SEQUENCE。 执行命令查看内容： openssl asn1parse -i -inform DER -in www.cnblogs.com.crt 解析的格式为：开头数字是偏移量，d 表示深度（depth），d 相同的表示是同一个层级。hl 表示头部长度（header length），l 表示不包含头部的数据长度（lenght）。 0:d=0 hl=4 l=1534 cons: SEQUENCE 4:d=1 hl=4 l=1254 cons: SEQUENCE 8:d=2 hl=2 l= 3 cons: cont [ 0 ] 10:d=3 hl=2 l= 1 prim: INTEGER :02 13:d=2 hl=2 l= 16 prim: INTEGER :03E3AC5347CD4A3862C2855A71A6F94F 31:d=2 hl=2 l= 13 cons: SEQUENCE 33:d=3 hl=2 l= 9 prim: OBJECT :sha256WithRSAEncryption 44:d=3 hl=2 l= 0 prim: NULL 46:d=2 hl=2 l= 110 cons: SEQUENCE 48:d=3 hl=2 l= 11 cons: SET 50:d=4 hl=2 l= 9 cons: SEQUENCE 52:d=5 hl=2 l= 3 prim: OBJECT :countryName 57:d=5 hl=2 l= 2 prim: PRINTABLESTRING :US 61:d=3 hl=2 l= 21 cons: SET 63:d=4 hl=2 l= 19 cons: SEQUENCE 65:d=5 hl=2 l= 3 prim: OBJECT :organizationName 70:d=5 hl=2 l= 12 prim: PRINTABLESTRING :DigiCert Inc 84:d=3 hl=2 l= 25 cons: SET 86:d=4 hl=2 l= 23 cons: SEQUENCE 88:d=5 hl=2 l= 3 prim: OBJECT :organizationalUnitName 93:d=5 hl=2 l= 16 prim: PRINTABLESTRING :www.digicert.com 111:d=3 hl=2 l= 45 cons: SET 113:d=4 hl=2 l= 43 cons: SEQUENCE 115:d=5 hl=2 l= 3 prim: OBJECT :commonName 120:d=5 hl=2 l= 36 prim: PRINTABLESTRING :Encryption Everywhere DV TLS CA - G1 158:d=2 hl=2 l= 30 cons: SEQUENCE 160:d=3 hl=2 l= 13 prim: UTCTIME :220228000000Z 175:d=3 hl=2 l= 13 prim: UTCTIME :230301235959Z 190:d=2 hl=2 l= 24 cons: SEQUENCE 192:d=3 hl=2 l= 22 cons: SET 194:d=4 hl=2 l= 20 cons: SEQUENCE 196:d=5 hl=2 l= 3 prim: OBJECT :commonName 201:d=5 hl=2 l= 13 prim: UTF8STRING :*.cnblogs.com 216:d=2 hl=4 l= 290 cons: SEQUENCE 220:d=3 hl=2 l= 13 cons: SEQUENCE 222:d=4 hl=2 l= 9 prim: OBJECT :rsaEncryption 233:d=4 hl=2 l= 0 prim: NULL 235:d=3 hl=4 l= 271 prim: BIT STRING 510:d=2 hl=4 l= 748 cons: cont [ 3 ] 514:d=3 hl=4 l= 744 cons: SEQUENCE 518:d=4 hl=2 l= 31 cons: SEQUENCE 520:d=5 hl=2 l= 3 prim: OBJECT :X509v3 Authority Key Identifier 525:d=5 hl=2 l= 24 prim: OCTET STRING [HEX DUMP]:3016801455744FB2724FF560BA50D1D7E6515C9A01871AD7 551:d=4 hl=2 l= 29 cons: SEQUENCE 553:d=5 hl=2 l= 3 prim: OBJECT :X509v3 Subject Key Identifier 558:d=5 hl=2 l= 22 prim: OCTET STRING [HEX DUMP]:04149DE559358AA4C25E1A52193A745F007F101B1C17 582:d=4 hl=2 l= 37 cons: SEQUENCE 584:d=5 hl=2 l= 3 prim: OBJECT :X509v3 Subject Alternative Name 589:d=5 hl=2 l= 30 prim: OCTET STRING [HEX DUMP]:301C820D2A2E636E626C6F67732E636F6D820B636E626C6F67732E636F6D 621:d=4 hl=2 l= 14 cons: SEQUENCE 623:d=5 hl=2 l= 3 prim: OBJECT :X509v3 Key Usage 628:d=5 hl=2 l= 1 prim: BOOLEAN :255 631:d=5 hl=2 l= 4 prim: OCTET STRING [HEX DUMP]:030205A0 637:d=4 hl=2 l= 29 cons: SEQUENCE 639:d=5 hl=2 l= 3 prim: OBJECT :X509v3 Extended Key Usage 644:d=5 hl=2 l= 22 prim: OCTET STRING [HEX DUMP]:301406082B0601050507030106082B06010505070302 668:d=4 hl=2 l= 62 cons: SEQUENCE 670:d=5 hl=2 l= 3 prim: OBJECT :X509v3 Certificate Policies 675:d=5 hl=2 l= 55 prim: OCTET STRING [HEX DUMP]:30353033060667810C0102013029302706082B06010505070201161B687474703A2F2F7777772E64696769636572742E636F6D2F435053 732:d=4 hl=3 l= 128 cons: SEQUENCE 735:d=5 hl=2 l= 8 prim: OBJECT :Authority Information Access 745:d=5 hl=2 l= 116 prim: OCTET STRING [HEX DUMP]:3072302406082B060105050730018618687474703A2F2F6F6373702E64696769636572742E636F6D304A06082B06010505073002863E687474703A2F2F63616365727","date":"2022-05-10","objectID":"/2022/05/10/cert-verify/:1:2","tags":["HTTPS","TLS"],"title":"手动验证 TLS 证书","uri":"/2022/05/10/cert-verify/"},{"categories":null,"content":"证书链 上面只验证了第一步。从浏览器中可以看出，上述 CA 证书不是根证书，还需要验证 CA 证书的可信。步骤与上述差不多，只是根证书是安装在设备的操作系统上或者浏览器上。 ","date":"2022-05-10","objectID":"/2022/05/10/cert-verify/:1:3","tags":["HTTPS","TLS"],"title":"手动验证 TLS 证书","uri":"/2022/05/10/cert-verify/"},{"categories":null,"content":"参考 [1]: https://blog.yuantops.com/tech/validate_a_digital_certificate_step_by_step/ (手工验证一张数字证书的有效性) [2]: https://yongbingchen.github.io/blog/2015/04/09/verify-the-signature-of-a-x-dot-509-certificate/ (https://yongbingchen.github.io/blog/2015/04/09/verify-the-signature-of-a-x-dot-509-certificate/) [3]: https://datatracker.ietf.org/doc/html/rfc3279 (RFC 3279) [4]: https://datatracker.ietf.org/doc/html/rfc2313 (RFC 2313) [5]: https://datatracker.ietf.org/doc/html/rfc3280 (RFC 3280) [6]: https://docs.microsoft.com/en-us/windows/win32/seccertenroll/about-encoded-length-and-value-bytes (Encoded Length and Value Bytes) [7]: https://linux.die.net/man/1/asn1parse (asn1parse(1) - Linux man page) [8]: https://stackoverflow.com/questions/2614764/how-to-create-a-hex-dump-of-file-containing-only-the-hex-characters-without-spac (How to create a hex dump of file containing only the hex characters without spaces in bash?) ","date":"2022-05-10","objectID":"/2022/05/10/cert-verify/:2:0","tags":["HTTPS","TLS"],"title":"手动验证 TLS 证书","uri":"/2022/05/10/cert-verify/"},{"categories":null,"content":"去年再次体会了状态从高峰跌落的感觉，只不过这次非常平静，因为知道以后还有机会进入这种状态。这次需要尝试的是缩短进入这种状态的间隔。和前一次不同，这次是因为身体不舒服而被强制削弱，而且持续时间不足一年，比较容易再次进入状态。 这次就先不按原先的顺序来了，主要是想先说说身体的状况。毕竟是这次状态下滑的罪魁祸首。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:0:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"哪里出了问题？ 我在 2021 年总结篇里说到身体不舒服，包含几个症状： 胸口难受，以至于我觉得心脏可能有问题 喉咙难受，除此之外还感觉那个位置的各种管相互挤压 我到 2022 年一月底才去医院查问题。由于感觉喉咙的问题影响不大，所以先查肺部和心脏。毕竟我一直在跑步，可能影响到了这两部分。医保刷了将近一千，检查结果是非常健康。 既然现在查不出来肺部和心脏的问题，就先查查咽喉部。做了喉部 CT 平扫没有问题，医生说只剩喉镜了。做了喉镜的结果是咽喉炎。医生开了些药，有治咽喉炎的，也有治气管问题的，吃了一阵后就缓解了，同时肺部和心脏的位置也不难受了。过了一阵子，喉咙部位的挤压感也消失了。 所以应该是咽喉炎影响到了肺部。但引起咽喉炎的原因仍不清楚。就算是几个月后的现在也会偶尔出现，但只有轻微的感觉，而且很快就恢复。 这次是出了问题经过三个月才去做检查，暴露出的问题是碰到身体问题没有及时去医院。我以为跟平时的一些习惯有关系，想通过一些改变看是否能够解决这些出现的身体问题，但习惯的改变所能带来的改善只有长期才能出现效果。因此如果身体问题对生活的影响较大，还是先去医院做检查比较好。 当咽喉炎的问题解决了，立马出现了其他问题。我发现上半年就是跟各种身体问题作斗争。这次是眼睛。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:1:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"眼睛的问题 主要体现在我看书的时候，聚焦看一行字会被这一行的上下几行干扰，但又不是散光的那种光影延申的感觉。去医院做了眼底照相，没有发现问题。测试视力显示度数没有变化。 这个问题的影响倒不是很大，除了看长篇文字的效率下降一点外，其他没有什么影响。 可能跟上班的时候长时间盯着显示器有关，医生给我开了滴眼液和眼药，然后推荐我晚上睡觉前滴散瞳液。 在这之后出现的问题是胃胀。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:1:1","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"胃胀的问题 有个周末吃了不少饼干后，又和舍友去吃大餐，结果吃太胀。从那时候开始，胃就一直胀着，吃消食片也没多大用。由于前面的经历让我在影响比较大的情况下，选择尽早去医院检查。并且以前也出现过类似情况，当时检查结果也显示是出现了个问题，吃了药就好了。这次不确定是不是同一个问题。 不过身经百战的同事建议我胃相关的问题是慢性问题，不用太早检查，我就先观察一阵子。前一段时间才去医院，预约了胃镜。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:1:2","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"状态的恢复 虽然我说的是恢复，但其实这里指的状态不是平常的状态，而是各个方面都远高于平常的那种状态。之前一段时间不是从平常的状态往下滑，而是从全面提升后的状态往下滑。 上一次开始往下滑的时间点是十月底咽喉炎出现。咽喉炎问题解决后，就有了恢复的基础。五月中旬有了一些发现，有些时候状态不好，跟我的头经常偏低看显示器或者手机有关，就像是拉扯头皮给头施加压力。于是我尝试放松，调整姿势，果然好多了。此外也减少肩膀因过于放松时的拉扯。 经过这两个改善，偶尔在午休之后能够短暂地提升到最佳状态。就算是平时，状态也会比前一段时间好不少。把它当成是中间态好了。 最近在刷牙的时候发现了让呼吸非常舒畅的方法，跟多喝水的原理有一点点相同。 条件准备得差不多，也该是时候完全恢复了。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:2:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"运动 一月到四月底，只跑了十次。没啥好说的，就是逐渐长胖。从去年十月到春节，胖了十二斤，之后逐渐减下来。现在和去年十月比，还胖了四斤。继续运动，降回去。 日期 公里数 配速（m.s) 2022/2/16 2 4.34 2022/2/21 3 4.36 2022/2/23 3 4.36 2022/3/28 2 4.36 2022/3/30 3 4.36 2022/4/11 2 4.36 2022/4/18 3 4.36 2022/4/22 3 4.36 2022/4/24 3 4.36 2022/4/26 3 4.36 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:3:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"输入 去年总结的时候，说接下去要阅读以下几本书： 《领域驱动设计》 《实现领域驱动设计》 《Go语言底层原理剖析》 《深入理解Kafka：核心设计与实践》 《物理学史》 弗·卡约里 某东上这本书经常没货，但还是被我买到了 《毛泽东文集（第一卷）》 结果没有多大进展，也没什么办法。近期偶尔状态好，才有去看书。只不过看的也不是技术类型的。零零散散，有时继续看《资本论》，有时看《毛泽东文集》。我之前找了一遍，发现《实践论》、《矛盾论》这俩没有收录到《毛泽东文集》，只有《毛泽东选集》里才有，就买了套选集。 除此之外，关注的公众号的文章倒是有看，算是勉强保持了一定的阅读量。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:4:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"输出 自从去年的文章发表后，就断断续续地开始写世界观的部分，总共有 25 个编辑记录。写完总体上感觉一般，毕竟也不像最佳状态时那样思考出很多东西，因此主要的目的是把最核心的东西写出来。至于如何把它提升到让人一看就能 get 到这种思想并能够以此为指导来分析和实践，就等后续再优化吧。 接下去的计划主要是把人生观的部分写出来。这部分跟价值观和世界观有比较大的关系。三观是互相影响的，因此人生观的很多判断都将是基于价值观和世界观的思考结果。 在这之后，将会把平常的一些零碎思考写出来。因为这些思考的内容比较零碎，而且是基于三观的扩展，所以我才会优先把三观总结出来。这样本来难以理解的内容，读者在看过三观文章后也会更容易理解一些。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:5:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"公众号 之前在发布《\u003c资本论\u003e中的“价值”》后，有位政治经济学在读硕士关注了这个公众号。这是一个惊喜。虽然我认为《资本论》对于价值的基本观点已经非常好了，但我不清楚这些观点在这一百多年内的发展情况，以及与现代一些经济学观点上的差异。因此如果能和其他人——特别是专业的人——聊聊的话，我对于价值的理解就能更加深刻了。 在日常生活和工作中，一般也没什么人愿意谈这方面的思考。不过也不是没有，我就和一个大我大概十岁的同事聊过一点关于“价值”的内容，但毕竟都不是相关专业的，能了解到的东西有限。 其实最好的交流时期应该是我正在读《资本论》第一部分的时候，当时思考的内容非常多。到后来由于状态不行，逐渐就减少了。我想我可能得从头再读一遍，或者整理出当时的阅读笔记。 这也让我有个想法，以后阅读什么内容的时候，最好及时把思考的东西写下来发布。 还有一个问题是，公众号限制只有关注的人在 48 小时内发送了私信，我才能发送消息，否则不能主动发。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:6:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"日记 有人提醒的情况下会写写，但是过了一阵就没写了。这几天逐渐增加了写日记的动力，试试看吧。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:7:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"娱乐 原神还在继续玩。呼吸锻炼没有多大效果，不过也没啥问题。我后面试试新的方式。在放松头部的时候，我发现把注意力集中到某个部位后，会自然地增加呼吸。我在跑步的时候试过，这种方式会让我在跑步的时候轻松得多。可能也会对其他方面有积极影响。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:8:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"吃吃吃 相比于以前，这几个月出去和同事吃饭的次数变多了，尝试了各种美食。 在大龙火锅吃的时候，发现贡菜特好吃。此后每次去都必点贡菜。过年的时候还特地买了两包回去煮火锅用，我妈吃了也很喜欢。 除了火锅，湘菜也吃了不少。我在农耕记吃了次腊八豆炒鸡蛋就爱上了。 新疆椒麻鸡也挺好吃，还有拿着馕沾一沾椒麻鸡的汁再吃也很好吃，绝了。还有大盘鸡也不错，特别是里面的土豆，香! 跟一个同事去吃客家菜，两个人点了个四人套餐，关键还吃完了。讲道理，我长胖可以分一半锅给他。水晶咕咾肉和脆皮盐焗鸡特好吃，还有梅子风味薯条也不错。 去渔舶汇吃了次海鲜自助，贵得不行，如果不是同事请客，我自己一个人大概不会去吃。但确实是好吃! 和舍友去京味张吃烤鸭，发现他们的饼很好吃。这饼本来是中间切开放肉的，结果我发现饼本身好吃得不行。他家酸奶也很不错，必点。以前和同事来吃，也有不错的，但忘记都有哪些了。只记得有道鱼很好吃，好像腰花也不错。 在先启半步颠里面吃到了炸汤圆，意外地好吃。 让同事这样带着，我也有了偶尔去探探店的想法。这是一大收获! 过年前，和几个同学约好找个时间去我家吃烧烤，就买了烧烤架和烧烤食材。结果全鸽了。不过我们之间约聚餐连续好多次都鸽了，只是这次轮到我了。过年期间亲朋好友过来我们这，可以吃吃烧烤，感觉很不错。有的还说这活动是最有过年感觉的。而且我爸还烤得很开心。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:9:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"咖啡 偶尔会和同事到楼下咖啡店点咖啡喝，边喝边聊天。会聊生活上的事情，也会聊工作上的。有些事情能够得到他们的建议，做起来也会方便很多。还能打开思路。 最近去星巴克喝了杯抹茶可可碎片星冰乐就爱上了，后来再去喝两次，就没有去了。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:10:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"没有完成的项目 教师资格证 我这次终于用学历报了名，但是后来发现还需要填写补充信息，而我看到需要填补充信息的时候已经过了截止时间。还是没重视起来。 单身 去年的尝试失败了，只好重新来过。这怕是自然界对我的考验，想让我经历这种失败来获得其他能力的成长。也好，就借着这个多多成长。 而且父母那边的思想工作早就已经做好了，每次回去也会巩固一番，另外他们的安排我也积极配合，所以也不会有什么压力。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:11:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":null,"content":"结尾 这四个月比较平静。在我把这些内容写出来之前，觉得这段时间很空白，似乎没有什么值得一说的。但写完后一看，似乎还可以。 生活上的经验累积了一些，世界观的篇章也写出来了。 这段时间本来想在技术上有比较多的成长，结果是在生活方面得到比较多的成长。这大概就是计划赶不上变化吧。 机会总是会有的。 ","date":"2022-05-08","objectID":"/2022/05/08/2022-life-part1/:12:0","tags":["life"],"title":"风暴过去了，但有新的挑战 —— 2022 的三分之一","uri":"/2022/05/08/2022-life-part1/"},{"categories":["三观"],"content":"为什么宇宙大爆炸后，太阳系上所有星球没有在一瞬间形成？为什么地球形成后要经过很长的时间才出现生物？为什么拥有像人类这样智慧的生物不是第一种出现的生物？为什么 3000 年前的人们不制造手机、电脑、火箭？为什么世界上第一台计算机没法做到像今天这么小？为什么新冠病毒的疫苗不能在一天之内研发出来？为什么我们学习要按照幼儿园、小学、中学、大学这样的顺序，而不是一开始就去大学学习？ 这些问题看起来似乎不合理。但是如果不带情绪，严肃地思考，认真地回答，那么这些问题的答案在本质上是相同的。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:0:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"发展的轨迹 如果 3000 多年前的人们掌握了制造手机的一切条件，那么他们能否生产出手机？可以。而如果他们想掌握制造手机的一切条件，需要多久呢？从我们的历史来看，需要 3000 多年。这就进入了一个循环，其问题在于这 3000 多年的发展无法缩短成一瞬间。生产手机的所有条件都必须一步步地探索和改进。某一个工具只有在另一些工具被制造出来后才能制造，而这一些前置的工具则以另一些工具为条件。 如此向前追溯，就能一直追溯到特定时代的人所拥有的条件。也就能知道那个时代的人如果想制造出某个物，需要经过多少个步骤。但仅仅只是回溯过程中的这些步骤么？ 我们站在结果往前看，往往看到的是一些已经明确了的分支，链接着结果及其条件。但无论在哪个时代，当代人所看到的不是这种强因果关系，而是链条上的每个阶段节点的下一个节点都有很多甚至是无限的可能性。人们每次都需要在这近乎无限的可能性中找到仅有的几条可行的路。在探索之前，一切都是未知的，充满着不确定性。不确定性越高，我们称为信息熵越大，反之亦然。人类对自然规律的探索过程，是经验积累的过程，是一个降低信息熵的过程。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:1:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"探索 人类对自然规律的探索，是一个长期的过程。为了解释这一认识，我以开宝箱作为例子。把自然规律看作是锁在密码箱里的宝物，人类的探索就是在尝试各种可能的密码。 假设有一个十进制的 8 位密码锁，总共有 100000000 种可能性。为了认识宝箱里的自然规律，需要逐个尝试每一种组合（例如 00000000 或者 01234567 等等）。大自然没有限制整个人类群体每次只能由一个人尝试一种可能性，因此遍布全球的人可能尝试的是不同的密码，也可能是相同的密码。特别是在通信不发达的古代社会，人类总体上无法避免地因为无法及时共享信息而造成重复探索。由于一系列客观因素的影响而尝试不同的密码的人们，各自形成了具有特色的文化的群体。 人们通过探索和实践积累经验，再从经验中总结出自然规律。 但一个人在有限的生命中，能尝试的密码组合有上限。再加上其所处时代的危险性——例如古代人需要面对凶猛的野兽，往往远在达到密码组合的上限之前就失去了生命。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:2:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"探索（经验）的延续 在一个探索者失去生命后会有两种情况： 有其他人知道这个人已经尝试的组合。通常是其后代或者没有血缘关系的徒弟。 没有人知道这个人已经尝试的组合。通常将这种情况称为“失传”。 如果情况是 1，后来者节省了大量的时间。如果情况是 2，那只能从头再来。 我们继续探讨情况 1，假设前一个人尝试了 100 种可能，那么继承者只需从第 101 种可能开始尝试。如果总是情况 1，那么解开密码锁认识到自然规律只是时间问题。但想要达到情况 1，需要以一种其他人能理解的方式传递信息，并且能让其他人接受到。 但是时间问题也是一个很重要的问题。如果一个问题需要一千年的探索才能解开，那么人们会持续探索一千年么？还是仅仅探索几十年就认为这是不可能解开的问题而放弃？ 信息载体主要是有规则的声音或者有规则的图案。其具体形式影响了信息的传递效率，在载体的具体形式本身得到关键的发展前，由于记录方式复杂、传播难度高，整体的发展速度非常缓慢。远古时期的人们想要记录 100000000 种可能性，要耗费很多精力在信息载体本身。用前述例子说就是因为纸的制作方法、复杂的语言和文字、印刷术也都各自存放在一个宝箱里，而不是人类天生就会的。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:3:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"探索的复杂性 既然大自然没有限制各地的人的尝试，如果 3000 年前的人发动所有人去寻找宝箱并解锁宝箱，那么是不是能碰巧解开制造手机的宝箱？ 这是不能的。 存放自然规律的宝箱，不是每个都单独展现在我们面前。有些宝箱放在其他宝箱里面，只有解开外面的宝箱，才能看到里面的宝箱。这意味着，无法跳过发展的过程直接进入下一个阶段。例如古人无法研究原子的特性。 这也就是为什么我们要站在巨人的肩膀上。如果不站在巨人的肩膀上，那么探索出来的东西大概率是已经被前人探索过的了，这样只能自我满足，对于其他人来说没有什么价值。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:4:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"探索之间的互相促进 对自然规律的探索，有的最终被转化为工具，有的成为另一种产品的组成部分。 对于工具类的探索，例如语言、文字、纸、交通工具，会加快信息的传播，从而减少其他人在已经验证过的信息耗费时间，促进其他探索的发展。或者例如显微镜，增加了人们观察世界的视角。 有些探索是其他几个已经完成的探索结合后才能有结果。就好像一个自然规律放到一个宝箱，开启这个宝箱需要好几把钥匙，而这些钥匙在其他宝箱里。但我们不知道总共需要多少把钥匙，很可能是获取其中某一把钥匙，才能知道下一把需要什么样的钥匙。 我们希望自然规律简单得只是一条直线，沿着一条路往前走就行。但实际上我们面临着众多不确定性，甚至会产生怀疑“这条路继续走下去能行吗？是不是缺少了另一个关键规律的探索？”。 我们说“人类的发展受到时代的限制”，是因为就算我们知道需要某一种特性的材料作为最终产品的组成部分才能造出产品，但符合条件的材料本身需要经过很长时间的实践才能从众多发展路径中找到正确的一条。如果人们在某个阶段看到了这一点，其中一部分人会因此感受到无力与绝望。而当经过漫长的时间找到这条路时，当代的人们就能创造出前人无能为力的作品。如果我们以取得这个成就的人们作为主角写一篇小说，就像是有个预言家说有件事几百年后才能做到。如果不考虑到前人的付出，从表面上来看就会有一种宿命感，或者说主角光环。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:5:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"探索的多种路径 尽管我们在某一个节点上面临着无穷无尽的分支，但并不是只有一个分支能够得到满足我们需求的结果。站在同一个节点的不同有效分支的选择，形成了不同的流派。 这些流派各有优缺点，这些优缺点主要体现在使用场景的不同。这些不同会引起对于一些通用实践的发展速度的不同。 各流派可以在核心不变的情况下，吸收其他流派的实践。这样就在核心上保持了多样性，加快对通用实践的发展，又可以通过吸收其他流派的实践获得自身的发展。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:6:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"客观的视角 上文讲了人类对于自然规律的认识过程。我们习惯于从人类的视角去认识自然，但是在寻求更深刻的认识的时候，就需要使用更高的视角：把人类放到自然界中，作为自然界的一部分来探讨。 上文是描述了人类逐渐降低信息熵逐渐认识自然的过程，如果按照这种持续积累的角度去看待整个自然界呢？ ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:7:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"人是自然界的一部分 上文在描述探索过程的时候隐含了一个前提，即人们已经开始主动地去探索。但是最初的人们不是天生就会有意识地主动地探索。 他们首先是被动地接触事物，从而直观地感受到因果关系。这种被动的感知逐渐积累起来，在突破一个界限后进入到了下一个层级。要达到这样的效果，前提是有一个容量足够的大脑。如果当初的人们脑容量只能记忆几天的信息，就很难把长时间段发生的事情关联起来，进而感知到一些复杂规律的存在。并且由于这个限制，语言和文字也难以发展出来（因为两者的发展也要时间），也就没办法通过语言或文字增加信息量。这样就使得更加难以感知到复杂的规律。 界限不一定是积累的经验越多就越先突破，而是要有特定经验的组合。这样使得突破界限的难度又增加了许多。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:8:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"人外之物 无论是生物还是非生物，都是自然界的一部分，都会随着时间发生变化。生物的优势在于变化的速度比非生物快，这种快不是表示一天两天，而是相对于几百年、几千年来说的。 处于运动之中时，物体就容易发生改变。对于生物来说，有生物内部的运动，也有生物由于在外界的运动带来的改变。 动物的 DNA 在繁衍的时候发生一些意外，致使其子代获得与父代相差较大的表现。但一块石头因为没有这种繁衍的机制，很难有这样的机会。 动物会在一定的范围内活动。在与周围环境的交互过程中，例如吃了不同的东西，也会出现一些变化。特别是在一些条件下会改变其活动范围，使得这种变化的可能性变大。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:9:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"遥远的过去 从当前已掌握的资料看，太阳系一开始没有行星，只有大量灰尘和气体。这些灰尘聚集形成小石块，小石块再聚集形成更大的石块。在不断重复聚集的过程后，形成了我们现在看到的行星。 最初一切都是无序的，但不是静止的。一旦发生了运动，就出现了无数的可能性。样本数量足够大，无数的样本被动地做着无数的“尝试”。直到一些样本偶然地满足某一个规则，因此聚集在一起。无序的程度降低了。但这是一个极其漫长的过程。 我们不知道为什么最初的物质能够运动，也不知道为什么会有一个规则让它们聚集在一起。这是我们发展到当前的阶段无法获知的。或许就像我们无法自己把自己举起来一样，由于我们由这些物质构成而无法知道这些物质最初是如何产生的，以及为何有这样的特性。但正如我们面对的可能总是无限的，这无限之中也可能会有我们想要的答案。 由于有以亿年为单位计算的时间，加上随机事件的样本数足够多，最终形成了现在的太阳系。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:10:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"生物 生物本身也是客观物质通过大量的随机碰撞后组成的，起初是很简单的生物。随着更多随机事件的发生，生物逐渐变得复杂。 这种复杂性是建立在不断地产生异常的基础上。这种异常有好有坏。好的一面是能更加适应其所处环境，坏的一面是更加难以适应环境。于是有些生物难以与外界持续循环，因而自然消亡了，我们说这是“被大自然淘汰了”。 或许在这种随机事件产生一个新的器官时，会有无数次的失败。其中有一些致命失败，使得这部分生物很快消亡；另一些失败则不致命，而是会在下一个阶段中，以当前的失败为基础，借助随机事件得到完善。每一个阶段都会产生出新的失败，只有那些不致命的且不断适应环境的失败，最终能得到不断地完善。 由于每个节点的分支都有无限的可能性，就为多个物种的产生提供了条件。在一个客观环境中是优势的“异常”，在另一个环境中无法生存。反之，在一个客观环境中无法生存的“异常”，换一种环境就会成为优势。这样就使得不同环境下存活的生物会有不同的表现。 生物要靠自身的变异来产生出适应环境的下一代。直到一部分生物在随机行为的过程中发现，尽管自身条件无法适应环境，但通过改变客观环境，能够使得一些原先无法生存的个体获得生存条件。这些大量的原先无法生存的个体，通过改变客观环境生存下来，并将这些生存知识传递给下一代。 于是它们的劣势转化成了一种优势，从依赖自身的变异获得生存过渡到通过积累改造外部环境的知识获得生存。变异是一种缓慢的过程，而相对这一过程来说，知识的积累以及对外部环境的改造就快多了，更容易适应复杂的环境。 但是知识本身也是一种客观事物在大脑里的反应，最终还是需要记录在大脑里面。大脑本身就是无序的物质随机组合而成的。它在形成时的大小，影响了能够记录在大脑里的信息的量。结果是有些新的信息以覆盖的形式将原先的信息覆盖掉了。信息的覆盖是很危险的，一旦覆盖了重要信息，可能会导致个体的灭亡。 于是有些变异体获得了不会覆盖重要信息的能力，比其他个体更容易生存。这些信息成为了它们与生俱来的能力，能让它们用剩余的脑容量做更多的探索，尽管这种探索的上限被大脑的容量限制住了。 值得注意的是，它们能做到这些并不是因为它们在脑中主动地构思，而是这些内容在它们出生之时已经被固定下来。之所以做出这种区别，是因为在随机的碰撞中，一种生物不断地生存下来并不断地进化，最终具有预先构思并将构思转化为对外部环境改造的能力。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:11:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"人类 从生物本身的强度来说，人类算不上强。如果仅依靠自身的强壮生存，那么经过漫长的时间直到现在这个宇宙时间点，人类可能只不过是强壮一点的猩猩罢了。 在众多生物产生的随机事件中，总会有一些获得了能够存储更多信息的大脑的物种。但这些物种的躯体不相同，从而影响了它们的进一步发展。 人类最初和其他物种没有多少区别。直到一些偶然的事件发生，让它们在脑中记录下了可以通过使用某些工具使自己更好地生存下去的信息。但是问题是它们除了将这些事件在其他人面前重现后进行传播外，没有其他方法。 直到其他一些偶然的事件发生，使得其中一部分人，甚至有可能是一个人，用某一种声音指代一种客观的事物。其他人则通过模仿，建立起这样的共识。这种声音起初是单音节，因为在单音节够用的情况下，不会主动地去使用复杂的多音节。直到音的变化种类无法满足事物的表达，才产生了多音节表示一个事物。 但是由于这是一种偶然，并且是会在多处出现的偶然，使得不同地方的音和物的对应关系不一样。不止如此，由于不同环境的物不完全一样，存在着两者之间没有交集的部分。当两个集体因各种原因融合时，这些没有交集的部分就可能沿用原先的音，或者用双方独特的发音方法近似地发出一个物在另一方的发音。 尽管信息的传递效率增高了，但其保存效率很低。人们需要依赖生物自身的记忆能力，通过口口相传记录这些知识。 在经过漫长的发展，人们发现可以用工具将这些知识记录在人体外部。人们只需要少量的记忆建立起外部必要知识和其含义就行了。用什么记录呢？起初是很简单的不同的形状组合成的图画。这些形状是对于外部事物的简单再现，通常是这些事物的主要特征。由此，人们再次减少对自身大脑记忆的依赖，让更多人可以基于前人的事件继续探索。 随着更多随机事件的发生，人类不断地发展。记录知识的方式越来越简便，记录的载体越来越容易传播，使得不同地理位置的人类群体可以共享各自的实践，互相吸收知识。这样，人们减少了很多重复探索的时间，把节省出来的时间用于探索对于双方都是未知的事物上来，加速了人类的发展。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:12:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"为什么世界一定会不断发展 自然界没有感知，它不会因为人类发展到某一个阶段就停止运动。而只要物质还在运动，就必定会有随机事件的发生。这种随机事件产生的结果会在各种物质上逐渐积累。 对于人类的认知发展来说，当出现了一种随机事件产生的结果，就意味着这种结果有被再现的可能。每一个随机事件的产生，必然有其前置条件和将前置条件转化为特定结果的中间过程。它不会凭空产生，也不会凭空消失。问题只在于我们多久才能具备观测其前置条件、或者多久才能通过尝试找到其前置条件，以及通过什么样的方式组合这些条件。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:13:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"难以意识到的发展 我们并不总是离真相只有一步。一个事件的发生是其直接的前置条件的组合结果，但其前置条件自身也是一些事件的组合结果。对于一个事件，我们目前的发展程度可能连其前置条件的前置条件都无法观察到。 我们认为不可能的事情，只是在当前的条件下无法直接达到。但世界不是静止的，一切事物都是有联系的，当前的条件不会永远不发生变化。只不过会发生变化不代表这种变化立马就会发生，也不代表只要一个事件发生就能达到效果。 在认知不足的情况下，我们很可能认为如果一个事件的发生不能引起我们所想看到的变化，那么这个事件是无效的。以至于我们更重视那最后一步，而忽略了前面的所有积累。甚至整个过程中，我们都在不断地否认各种变化所带来的积累对于最终结果产生的必要性，某个变化出现了一点负面影响就想要否定。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:14:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"合作 在认识简单规律或者创造简单产品时，可能一个人就能完成。或者基于前人的经验，即在历史垂直的维度上跨时间合作。 但随着我们对自然规律的深入认识，面临的问题越来越复杂，需要越来越多的人参与其中，即在历史水平的维度上跨空间合作。在未来面对重大问题的时候，需要的是领导者的强大组织协调能力，以及每个人为解决问题出一份力的自觉。 未来一定是属于能够消除矛盾，团结更多人的一方。在更遥远的未来，全人类必将走向团结。幸运的是，我们恰好站在这一过程的关键节点，在剩下的几十年里，可以观察到一些非常有意义的变化。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:15:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"使命 我们对世界的认识会不断地加深，我们的科技和社会将不断地发展。那么最终会发展成什么样子呢？人类是否肩负着一个使命，正是由于这样的使命而被设计出来？ 如果真的存在一个使命，我们现在也无法清楚地知道。但如果从发展的角度上讲，应该是所有的生物都是带着使命的，只是由于一些巧合，人类率先进化成了最有可能完成这个使命的一类生命。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:16:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"启示 当以这样的视角去看待事物的发展和人的生活时，会自然地解决很多问题。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"如何阅读历史 从发展的角度去阅读历史，不再把重点放到历史人物和朝代变更，而是把重点放在人类社会在历史进程中积累了哪些经验。 不只是关注那些王侯将相，还要关注普通人，甚至是那些连名字都没有记载的人们；不只是关注朝代名称、行政体系的变化，还要关注普通人对生活经验的积累、对工具的改进；不只是关注历史上的人，还要关注历史上的客观环境的变化。 我们看到的不再是历史的轮回，而是历史的前进。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:1","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"如何面对失败 当我们想要取得一个特定的结果时，面对的是众多可能性。在实践过程中，我们通过各种行为把熵降低，将多种可能性逐渐收束到其中一种可能性上。在有限的可能性集合中，一旦我们经历了一种失败，通常就代表着我们将可能性集合的无效元素剔除掉。经历的失败越多，可能性集合中剩余的元素就越少。最坏的情况是尝遍所有失败，集合中只剩下一种可能，那就是成功。这时，成功就成为了必然。从这种意义上来说，失败就是成功地证明了集合中的某一条路不可通，这在科学上尤为重要，也就是为什么有的人会说“我不是失败了xxx次，而是成功地证明xxx种情况的不可能”。这是一种自然而然的结论，不是什么自我安慰的心灵鸡汤，也不是什么需要学习的乐观精神。 我们通常认为这种成功地证明某一条路不可通的价值远不如成功地证明某一条路成功，这是“成功学”吸引人的地方。但是成功之路可以有很多条，每条路所要求的前提条件不同，人们忽略了特定一条成功之路所要求的前提条件，在不具备相关条件的前提下跟着走，极有可能失败。这也是对于“为什么懂得了很多道理却过不好人生”的回答。每个道理都有其前提条件，在具备前提条件的情况下，应用这个道理，得到预期的结果。但是这些“道理”在输出的时候，并不会告诉你前提条件是什么。甚至普遍的情况是，获得成功的人们并不能清晰地分析出哪些条件成就了他们，以至于在输出其经验的时候，选择了主观上认为是决定性因素但客观上只是次要因素的实践。 上面对于可能性集合的缩减，也隐含了“不二过”的重要性。如果不尽可能地避免犯相同的错误，那么可能性集合就不会减小。之所以用“尽可能避免”而不是“绝不能”，是因为不是所有人都能在犯错后都能准确地找到问题的根源，一旦判断错误，就有可能犯相同的错误。在多次犯错后，通过共同条件找到根源。不同人的差异体现在定位问题的准确性，以及对错误的反思次数和深度。 为什么不应该害怕犯错？在生活和工作中难免会犯错，特别是面对未知的事物。在大脑中预演的时候，难以预测到现实中种种复杂的因素的影响，再谨慎也可能出错，因此不必每次犯错就给自己巨大的压力。重要的是从错误中看到了什么？通过每次犯错后的反思，补充对于事物的认识，可以减少下次犯相同错误的可能性。如果害怕犯错而不去实践，或者犯错了不去想如何避免犯相同错误，那么就会导致将自己锁定在一个层次，停止成长。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:2","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"读书的意义 读书的过程就是将前人在排除了众多无效的选项后得到的一些结论以一种高效率的方式存入大脑，进而给予读书的人们在前人的基础上更进一步探索的机会，避免了重复探索的了浪费。同时也提醒了我们，要大胆地站在巨人的肩膀上，多去了解前人在某一个领域的探索成果，不要试图通过自己独立发现前人已探索过的规律来满足自己。 当然，读书的意义不仅在于让自己有更多探索的可能，在“价值”层面也有其意义。如果通过读书提高了自己的“减熵”能力，就等于提升了自己的价值。当我们发挥出自己的能力能够比以前减更多的熵，意味着创造的价值就越高，就能够通过交换获取更多的他人生产的产品。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:3","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"分享的重要性 我们将自己对某件事物的实践通过各种方式分享给别人，可以避免别人再花很多时间尝试无效的选项，或者踩到坑。这样双方都减少了时间的浪费，提高了整体的效率。 当今最新的成果，往往以论文的形式分享出来。所以做研究的时候，需要阅读相关的论文，了解该研究方向的发展阶段。了解了当前阶段解决了哪些问题，可以避免重复探索；了解了当前阶段没有解决哪些问题，可以以此为目标。 在平常的生活和工作中，碰到的不是那种非常困难的问题，不需要以论文的形式发表。更多的是个人的生活经验以博客、微博、短视频等形式分享出来，工作上的经验总结到博客或者项目的文档共享到公司内部文档系统。 这些行为是为了能够让其他人低成本地获得经验，使得所有相关人员能够尽快同步信息，在相同的基础上开始合作。而在输出文档的过程中，需要将大脑中复杂的信息的熵降低，去掉低价值内容，突出高价值内容，用逻辑连接上下文。这是一种非常重要的能力，特别是对于想要长期持续地发展的实体来说，非常宝贵。 我们通常在和其他人讨论一件事的时候，不会先同步已有信息，总是假设对方已经清楚了我们所了解的所有内容，这就导致沟通的时候互相不理解。所以想要达到高效的沟通，需要先了解对方掌握哪些信息，以及哪些信息没掌握，然后主动将对方没有掌握的信息同步过去，才能开始有效地沟通。在这个过程中，不仅要耐心，还要控制住情绪，这对于人来说是一个非常大的考验。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:4","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"接受不完美 我们对于不完美的恐惧在于，认为它会让别人降低对于我们的评价。 本篇的所有内容其实只能算是笔者认识发展的一个阶段性总结，它的出现并不意味着笔者对于世界的认识达到了一个终点，以后不再深入认识。正相反，它本身的内容就证明了笔者的认识还需要进一步发展。包括本篇的内容也不是终点，后续需要不断地补充和完善。为什么不等完美了再发出来呢？因为它永远无法达到完美的状态。 在适当的时候总结并分享出来，一方面有助于我巩固这些想法，避免未来由于各种原因忘记了以前想通了的事情；另一方面可以让其他人了解我的想法，使得有兴趣的人可以与我交流，促使我对世界的认识更加完善，更加符合实际。无论是哪一种，都对我未来的认识的发展有益。如果不总结和分享，那么我只能依靠自己继续探索，甚至是盲目地探索。 在总结和继续发展这两步中，我已经走出了非常关键的第一步，而第二步的继续发展和完善就容易得多。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:5","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"拥抱变化 变化是世界发展的基础，只有完全封闭的孤立的系统，才能尽可能地减少变化。一成不变的生活持续越久，就意味着脱离世界的发展越久，就越危险。 人追求稳定的想法是非常自然的。但是自然界它没有情感，也不会因为任何人停止变化。缺少应对变化的训练，会使得变化产生时，没有办法减少变化产生的负面影响。有的人会在稳定的时候，趁机做多种准备；而有的人会在稳定的时候，尽情享受，在变化到来的时候想尽一切办法阻止变化的发生。 只是历史的车轮滚滚向前，没有办法跟上历史发展的人，需要承受因沉浸于一时稳定的享受所带来的代价。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:17:6","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":["三观"],"content":"尾声 其实每个人都有一套对世界的认识方式，只是没有加以总结，并无意识地使用这种认识方式来对待人和事。 我的这些认识，也算不上什么高大上的东西，不过是对过去经验的总结。当然我也不担心别人会觉得我的这种认识很幼稚。因为基于这些认识所产生的推论是行之有效的，并且在很多地方被一些人使用着，只是他们没有探索这些方法的基础支撑逻辑的兴趣。 我们不需要做出轰轰烈烈的大成就，我们的每一份经验总结和分享，都为世界的发展做出了贡献。 历史是由人民群众创造的。 ","date":"2022-01-17","objectID":"/2022/01/17/world-outlook/:18:0","tags":["世界观"],"title":"发展的世界观","uri":"/2022/01/17/world-outlook/"},{"categories":null,"content":"正如我在标题所总结的，2021 年对于我本人来说是高开低走的一年。高，在于 9 月之前状态很好，思维非常活跃；低，在于 9 月之后状态下滑，十月底开始几乎停止了跑步。尽管如此，总体上还是不断进步的。 接下来首先回顾做了哪些事情以及哪些事情没有做好，接着分析为什么状态下滑以及如何尝试恢复。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:0:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"做了些什么 主要是老三样：运动、阅读、写作。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:1:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"运动 主要是跑步。这次我做了个图，省去一堆描述。其中有 6 次在户外跑，其余 85 次在室内跑步机跑。 4 月有几次尝试五公里，但直到 6 月才把速度从 14 km/h 降低为 13 km/h，同时从 4 公里稳定提高到 5 公里。一段时间后，因为天气实在太热，就降下来了。再后来状态下滑，从 5 公里逐渐降到 3 公里。 6 月降速有一方面是我去体检时发现心率太低（运动员水平），想着通过降速避免心率保持在低水平。10 月底由于身体不舒服，休息了一个月，从此逐渐变为不再跑步。从 2020 年 6 月开始保持的跑步，算是在 2021 年 10 月底中断了。 对 2021 年跑步状态分析后，总结出可能导致跑步状态下滑的因素： 天气太热 在 7 月下旬之前，我会开空调把温度降到 25 度左右。后来为了避免空调对身体造成影响，不再开空调。但跑的时候明显感觉吃力。这点我在跑步打卡的时候也有说到。 解决方法： 天气过热时降低跑步强度，少跑一两公里； 尽量去户外跑，只有在下雨天时到室内跑。 水分补充不足 有时候跑步之前半个小时忘记喝水，导致跑步的时候感到口渴。这个是临时的问题。 长期的问题是每日的饮水量不足。这是我在 11 月身体不舒服的那段时间发现的。在我喝水或者吃东西的时候，身体就会缓解很多。所以从那个时候开始有意识地加大饮水量，同时减少熬夜（下面会说），身体状况就恢复了。 在自我监测饮水量的期间，经常发现很多情况下自己处于口渴但没有及时补充水分。这不仅是这段时间的问题，以前也存在。 一开始是通过在办公室泡茶来达到多喝水的目的，但后来看到饭前饭后半小时内不宜喝茶，这段时间就需要调整。 解决方法： 工作日每天购买一瓶 550ml 的矿泉水。加上喝茶的部分，差不多 1200ml。如有跑步则另加； 跑步的日期设置 19:50 的闹钟提醒喝水，直到不需要闹钟也能想起来。 熬夜 据我对自己的观察，如果熬夜到两点偶尔到三点，保持七个半小时的睡眠不会有影响。但是如果经常三点，甚至四点，那就会有很大的影响。因为睡眠总时长只有五个半小时或者六个小时。 很有可能是熬夜太晚导致了我那段时间的身体问题，所以后来把睡觉时间调整到十二点。搭配上多补充水分，就好了。不过有时候也会复发。 最近和同事聊起这事，他说有可能是肺气泡。如果是的话，跟我跑步的呼吸调整应该有一些关系。近期会去做个检查。 解决方法： 尽快去做检查 控制在十二点前睡觉 没有及时补充睡眠 这个影响的不只是跑步的状态。在状态极好的那段时间，我就发现消耗的能量比以前大。想了想，毕竟要维持长时间的最佳精神状态，以及保持思维的高度活跃，需要消耗大量能量是必然的。 我通常是在晚上八点半到公司的健身房跑步。如果我在跑步之前，躺个半小时，跑步的状态有明显的提升。所以在相当一段时间内，我跑步前一旦发现精力不太足够，就会先去躺一会儿。只是后面就没有这么做了。 解决方法： 提前规划好工作内容，留出半个小时的时间先躺一会儿。休息完等待半小时再跑步。 其他 在身体恢复之后，跑步的次数就减少了。其中一个原因是经常（每周至少一次）和同事出去吃大餐，而且约的时候很经常在周一、周三、周五。而这三个时间点恰好是我跑步的时间。这个我已经有提前做好铺垫了，多次跟他们提起我的运动规划。现在有两种方案，一种是和他们说好只在周二或周四去吃；另一种是我把周五的运动调整到周六。两者也可结合起来。 总的来说，基本符合原先的规划。 2022 年会开启新一轮的跑步。不过会在弄清楚是什么导致我身体的问题之后。做了体检没有检查出什么问题来，需要做专项检查。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:1:1","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"阅读 原先的计划是阅读以下几本书： 吕思勉的《极简中国史》 九边的《西方博弈往事》 马克思的《资本论》 《领域驱动设计》 《实现领域驱动设计》 九边的《西方博弈往事》有看一些，不过不多。主要看的还是马克思的《资本论》。 我在 3 月 1 日阅读完第一卷的第一篇\\【商品和货币】和第二篇\\【货币转化为资本】，之后又重看了一遍。第一遍看个大概，不求甚解，主要是想看重点在哪里。第二遍细读，认真理解，并且把重要的内容划起来。 除了这两篇，后面的内容也有看一些，不过没有完整地阅读完一整篇。因为我发现后面的内容是这两篇的延伸，就把重点放到这两篇来。 为了避免像现在这样状态下滑，我提前把总结和看书的思考写了出来。就是我在 12 月 20 日发布的《\u003c资本论\u003e中的价值》，全篇两万个字。这篇还有很多不足的地方，但我发现修改起来太耗费时间了。如果不赶紧发布，我最想写的世界观就没法开启（尽管现在很难写出像 2021 年上半年的那种水准了）。 从提交记录看，《\u003c资本论\u003e中的价值》这篇共有 52 次新增内容或者修改。五一的时候写了一千字，不过直到 6 月 28 日才算正式开始写。经过 32 次新增和修改，完成了初稿。剩下的都是在优化文章结构和上下文逻辑上的平稳过渡。优化是最头疼的，有时甚至要一整块重写。被我删掉的或者重写的内容也有几千字，估计能上万。后来想着一定要在 2021 年内发布，就先直接发了，后续再优化吧。 除此之外，还看了： 《鲁迅全集》中的《狂人日记》、《孔乙己》、《论“他妈的!”》、《估“学衡”》、《伪自由书》等。 读《鲁迅全集》是因为 6 月的时候看了《觉醒年代》。这部剧非常好看。本来是冲着迅儿哥去的，因为去年看了 bilibili up 主“智能路障”的鲁迅系列视频。看完后最吸引我的是李大钊，他和工人们打成一片，教他们读书写字。仅这点，就已经很伟大。特别是将这件事和后来红军士兵仍然保持学习联系起来。 《毛泽东文集（第一卷）》 我是从头开始一篇篇看的，现在看到《寻乌调查》。其中《中国共产党红军第四军第九次代表大会决议案》和《寻乌调查》两篇让我印象最为深刻。 决议案对于问题的分析很深刻，不仅找出问题，而且列出了纠正方法。这种找出问题的本质，并列出务实解决方案的能力正是我已经入门并且不断提高的。在这篇，你可以看到对以下内容的讨论以及纠正方法：1. 关于绝对平均主义；2. 关于主观主义（关于党内批评问题）；3. 关于个人主义。 还有其他很多内容。例如工作中有些同事碰到的开会时效率非常低，执行不到位的问题。我相信很多人应该都会对此很感兴趣。 如果说《决议案》让我觉得自己已经入门了这种能力，那么《寻乌调查》的内容和对于各种对象的划分的细致程度则让我感受到了自己与教员之间的巨大差距。跟这些相比，我对于我所在地的了解程度几乎为零。如果想要达到我理想中的下一个能力层级，那么对于所在地详细认真地调查是必不可少的，这是今后的其中一个工作项。 《费曼物理学讲义》看到第3章：物理学与其他科学的关系 因为决定了考教师资格证的时候选择高中物理，所以想着学习费曼讲解知识点的方式。 《乌合之众》的正文全文 照理说不需要看这本书，但网上经常有人喜欢高高在上地用“乌合之众”一词形容广大的人民群众，于是我认为有必要了解他们究竟看到了什么内容。 但在看这本书的时候，就能明显感觉到作者在这本书中所体现出的思想水平比较一般。只有让自己同样高高在上鄙视文化水平和经济水平不高的人民群众，才能够比较认同作者的看法。如果不是，那么就能看到作者大体上是在借部分现象在贬低人民群众，以此获得让自己或者读者获得优越感。 但使用“乌合之众”的网友们估计没有想到，尽管作者在这本书里面大体上是默认经济文化水平低的人民群众，但他也在书中指出任何群体都是“乌合之众”，这也包括使用“乌合之众”的网友群体。 实际上群体的复杂性不是一句“乌合之众”就能概括的，需要根据具体的客观条件具体分析。整个大的群体实际上是在不断进步，而且这种进步从客观上来说无法阻挡。 总体看，技术书籍没有看，基本都是从公众号、官方文档、源码学习；其他的阅读量还算过得去。2022 年会主要把精力放在技术能力的提升上，所以比例会有所调整。现在先预选几本： 《领域驱动设计》 《实现领域驱动设计》 《Go语言底层原理剖析》 《深入理解Kafka：核心设计与实践》 《物理学史》 弗·卡约里 某东上这本书经常没货，但还是被我买到了 《毛泽东文集（第一卷）》 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:1:2","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"写作 1 月发布 1 篇跟时间戳相关的技术博客 2 月发布 2020 的总结 3 月发布 1 篇终端的发展史，以及 6 篇跟硬件开发相关的技术博客 4 月发布了两篇跟软件工程相关的博客 5 月发布《人生工程——2021 三分之一》 6 月发布《理性思考，感性选择》和《价值观简述》 7 月发布 1 篇跟数据库相关的技术博客 8~12 月合起来发布《\u003c资本论\u003e中的价值》 由于最后一篇内容多难度大，需要更多时间，就勉强算符合预期了。 在微信的文件传输助手里有记录平时的思考，然后按月份整理到微信收藏里。这些思考会在合适的时候整理为完整的文章。 在微博上也有转发一些内容，不过都设置为仅自己可见。以前设置为朋友圈可见，不过小伙伴说转发这种没人看，就全部改为仅自己可见了。 计划的季度维度的总结没有按照预期执行，但走出了第一步。当我开始写《\u003c资本论\u003e中的价值》时，就想着优先完成它，把其他事情的优先级都调低了，也因此这一篇的影响贯穿了整个下半年。由于那篇已经发布，因此压力会小一些，写下一篇的时候可以不用像这样持续地把其他事情调低。2022 年会再次尝试季度级别的总结。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:1:3","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"没有做好的部分 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"日记 没有写。这个没有什么要求，比较自由。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:1","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"心态 心态跟身体健康状况有一定的关系。由于基础打得足够稳固，因此尽管有段时间身体不舒服，心态仍然不会有大的波动。 除了身体状况之外，还有其他因素。一个是隔壁部门因为领导的管理能力不太成熟，整个部门的人都非常痛苦。我经常和他们出去吃饭，就会听到他们的吐槽。 从客观的角度看，他们的这些吐槽对我有帮助。特别是如果我以后必须当一个小领导，这些吐槽可以帮助我提前避开一些管理上的坑。还可以了解部门之间协作的问题。风险在于整个过程中我是在不断地接收负能量。 当多次经过这种过程的积累，而我恰好处于状态不好的阶段，会出现短暂的（分钟级别）的心态波动。这是在十二月底出现的。不过就像是触发了告警机制一样，我突然意识到不对劲，并且做出调整，禁止自己在恢复前对较为重要的事情做决定。 为了形成更稳固的心态，保持像 2021 年上半年的状态，需要尽快把接下来的两篇内容输出出来。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:2","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"体重 近期运动量减少，但却没有控制饮食，结果体重上涨了。现在已经在控制脂肪和糖分的摄入，会和运动一起做出调整，把体重降回来。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:3","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"作息 有段时间作息控制得还不错，但后来又出现问题。原因是引入了新的不确定因素，把游戏从《王者荣耀》切换到《原神》。 一开始玩的时候会被吸引住，花大量时间玩。不过后来在这上面花费的时间已经减少很多。根据手机系统自带的时间统计估算，现在平均每天花一个半小时的时间玩《原神》，大致是四局《王者荣耀》的时间。我正在给自己做思想工作，把时间压下来，把精力转移到能够获取更多满足感的地方。 经过了一年，这个问题没有得到彻底的解决。接下来要提高的文字输出可能会对此有所帮助。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:4","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"投资 计划是 2021 年下半年开始学习，然后发现投资环境变得不好。虽然跟学习投资没有多大关系，但也借着这个环境延后了这个计划。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:5","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"教师资格证 2021 年两次报名都是因为没有获取到居住证。上半年是因为还没有到居住年限，下半年发现虽然满足居住年限，但居住时间不是自动计算的，需要申请，意味着要再等一年。但同时发现可以用学历申请，成功通过。只是没赶上报名时间。只能报考 2022 年的考试了。 之所以有这样的结果，是因为我对此不够重视，因此没有提前做好充足的准备。不过如果没有去考，还一直说这个事情就不合适了。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:6","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"单身 一年过去了，还剩一年。 这一年内做了很多尝试，主要是在观念上得到了突破。不过一些选择没有得到理想的结果，但这是早就已经预知到了的，并且做出选择之前已经接受。 2022 年会重新规划。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:7","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"技术上的提升 之前计划的 Linux 内核 TCP 相关流程没有整理。LeetCode 也没刷。 有提升的部分： Python 的 GIL 守护线程、用户线程、守护进程、后台进程、终端 CDN 的 DNS 解析、与调度的关系 Linux LVM 锁的硬件层支持 阅读 InfluxDB 源码 WEB 服务热重启（热更新）原理 这些大部分内容都只是写了个草稿，需要花一些时间整理出来。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:2:8","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"娱乐 我在 9 月初开始把原先玩王者荣耀的时间切换为玩原神。由于一开始就抽到了很不错的角色，游戏体验一直很好。经过 125 天，仍然没有充值。后来和我一起玩原神的朋友倒是有充一些。 吸引我的点有： 不充值也能获得良好的游戏体验 没有玩家之间的对战 不用担心活动没做或者上线少而追不上别人 拥有属于自己的世界，世界里只有自己一个玩家 但有联机模式，可以跟朋友一起探索或者打 Boss 已经开放的地图非常大 可以探索很久 场景非常好看 我甚至单独花一些时间欣赏风景 音乐很好听 我最喜欢的是《Against All Odds》，似乎是产生了情感上的共鸣。 剧情还不错，深挖更有意思 几个角色演示： 钟离：听书人 https://www.bilibili.com/video/BV1hD4y1X7Rm 播放量最高。全球原神玩家最喜欢的一个视频。当角色说出“天动万象”时，无数个玩家的钱包就保不住了。这个词的中文版配音的语气符合一个思想水平很高的男性的形象，其他版本的语气就没有这种感觉。 雷电将军：净土裁断 https://www.bilibili.com/video/BV1kb4y1m7e7 我一开始就抽到的。大招的一刀，伤害极高而且帅。该视频中的第 54 秒左右的效果和音乐转换搭配角色说出“唯有永恒，才最接近天理”，飒得很。 说出“唯有永恒，才最接近天理”是由游戏背景决定的，“天理”有着其他含义，不过她最后也发生了改变。对于现实世界来说，“唯有发展，才最接近天理”。 可莉：哒哒哒 https://www.bilibili.com/video/BV18D4y1d7Uz 众多玩家的“女儿”。唯一一个在讨论角色强度时拥有豁免权的角色。如果有人拿这个角色和其他角色比较强度，大概率会被批评。 最开始玩这个游戏的时候，很容易一不小心就熬夜到很晚，需要特别注意。大概玩一两个月后，就基本是做个每日任务了，等米哈游开发新地图。 我以前有个问题是精神集中到一定程度时，会减少呼吸量（有点像在憋气）。我在玩原神的时候也会感觉到，所以一开始也有意识地锻炼看能否解决这个问题，但似乎没有起到什么效果。而且我后来身体感到不舒服，可能跟玩原神时长期呼吸变小或者饮水量降低有关。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:3:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"风险识别和预先排除 2021 年可以说是最近几年让我最受震撼的一年了。几个亲戚的生活突然掉到谷底，甚至其中有一两个目前很难看到翻身的点。另外还有一两个有潜在隐患，但因为没有到时间点，问题不会爆发出来。 我在 2021 年的年初才跟我爸说在个人成长这块要努力让自己以后能够帮助家族中其他人提高生活水平，结果下半年发生的事情直接给我调高了好几个困难等级。现在得先确保我们这边不会受到较大影响，否则他们就更加没有希望了。 我们家有给几个亲戚提供了很多关键的精神、思路、物质上帮助。有些事情上我有提醒我父母，如果当事人没有把事情想清楚，那就只能提供有限的经济上的帮助，等他们自己想清楚或者我们帮助他们想清楚，再重新评估，否则提供再多的帮助也起不到效果。他们也清楚这点。在分析完所有对我们的潜在风险后，得出的结论是对于我们的影响不大，可以接受。另外有些潜在风险我也会尽量帮忙他们排除掉，但比较有限。 他们的问题总体上都出现在想要享受远远超过自身能力的生活，只考虑短期内的生活，而不去考虑这种生活的可持续性。就像部分长辈催婚一样，只想着抱孩子，以为这就是幸福，结果带来了更大的痛苦。 早在这些事情发生之前，我就已经跟我父母聊过几次，他们当时都已经理解并认同我的想法。而在这些事情发生后，他们应该已经体会到了我的担心并不是杞人忧天。 现在家里如果有数额较大的投资，都会找我商量，我会帮忙评估风险。但是有时候得吃亏了才会谨慎，所以有一条风险较大但可控的投资我没有制止，并且提前给他们做好心理准备。当时没过多久就都亏完了，但是没有产生多大影响。反而我爸在工作上通过努力赚了比这次亏的多得多。 通过这种机制，我为父母引入了一个确定性。即一旦我没有制止某项投资，就说明它通常不会带来无法接受的影响，并且当影响出现时，我不会有怨言。这样他们就会减少很多压力，进一步降低冲动的可能性。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:4:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"舆论 2021 年花在关注舆论的时间也有不少。每次看到一些消息或者评论，就想着写一篇文章总结一下各种舆论场上的用语及其问题。 起初关注微博上的信息时，就意识到我在更早之前忽略的点。即微博上发出声音的人仅占全国人数很小的一部分。例如一条微博就算转发或者评论是一万，跟全国人数比起来不过是极少数人的意见。我在最近阅读的文章中看到一句很好的总结“舆论不等于民意”。大多数人都是不发声的，而舆论场上声音最大的点，会让人误以为大多数人都这么想。 另外网友的年龄分布也值得注意。根据微博官方发布的《微博2020用户发展报告》，90后占比 48% ，00后占比 30%，两者总占比将近 80%。这两个群体总体上（例外人数肯定一大把，这里只看总体）知识储备还不够多，思想还不够成熟，容易受到情绪的影响，从而无法识别出一些他们本就能够识别出的逻辑漏洞。 https://data.weibo.com/report/reportDetail?id=456 最近让我印象深刻的一个话术是： 抛开事实不谈，我认为…. “事实”可以替换成任意的问题的主要因素。这个话术试图将次要因素取代主要因素，用次要因素作为判断某人某方面的评价因素，以达到或美化主体或贬低主体。例如在网上可以看到一句评论，大意是： 抛开拜登没有做好总统这件事不谈，我认为他是个好父亲。 通常这种句式应该用于抛开次要因素，突出主要因素。但现在却被反过来使用，即主次颠倒。 2021 年疫情仍未消除，人们却开始争论是否应该全面开放。那些疫情不应该这样控制，认为应该全面开放的声音，在我看来是很矛盾的。一方面他们看到了美国没有足够的管控而有 86 万人死于新冠，并且他们也认为我国的医疗比美国还有很大的差距，以及中国人口是美国人口的 4 倍多；另一方面他们认为应该像美国那样开放，因为这样对人们的影响比管控还来得小。 那么，古尔丹，代价是什么呢？ 不知道支持全面开放的人预期用几百万同胞的性命换来他们这一小批人的“美好生活”呢？ 在网络中，有个词叫“二极管”，意思是思维只有两个极端，要么极好要么极坏。为了避免这种非黑即白的极端，大多数人会加入灰。但是要注意的是，“灰”不是只有一级，它本身也有很多种。在图像识别中，灰度有 256 个等级，灰度为 0 是黑，灰度为 255 是白，要知道还有其他 254 种类型的灰。因此在对舆论做出判断时，一定要注意区分各种声音的不同灰度等级，而不是说什么“社会不能只有一种声音”，因为这句话把灰度 256 个等级中的 255 种声音归纳到一种声音，这也是一种“二极管”思维。对于各种事件，在我头脑中时常会出现一维座标系，0 表示绝对中立，然后把各种声音根据程度标记在座标上。 其他内容太多，应单独写。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:5:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"分类讨论思想（细致化分析） 尽管分类讨论思想广为人知，但大多数都仅用于数学。其实这也应该用于生活中的思考。 一年来，有一个分析方法逐渐在我脑中成型。方法大致内容为：取事件尽可能多的相关影响因素，尽可能列出每个因素的不同可能性，不同因素每次取一种可能性组合。每一种组合都是一类，每一类单独思考其可能性、影响、处理方法。 这是我在之前文章中提到过的，“理性思考，感性选择”中的前半部分。 思考的时候，脑中就像有一张表。 假设 A 因素有三种可能，B 和 C 因素各有两种可能，则如下图： A B C A1 B1 C1 A1 B1 C2 A1 B2 C1 A1 B2 C2 A2 B1 C1 A2 B1 C2 A2 B2 C1 A2 B2 C2 A3 B1 C1 A3 B1 C2 A3 B2 C1 A3 B2 C2 这些可能性包含了客观和主观的可能。对于主观和客观上的可能场景的组合，在以特定的人的视角思考时，就是我们平常所说的“换位思考”。换位思考的成功率与对换位目标人物的熟悉程度有很强关联，它需要将目标的各种条件把自身的条件替换掉。如果没有把自身的条件去除掉，就容易得出“何不食肉糜”的结论。 各种情况都思考，是为了更加接近真理。 而这也是我在判断舆论的其中一个标准。对于一篇文章需要有一个基本的判断，即它是（或者说“主要是”）想要追求真理，还是想要表达情绪。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:6:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"状态能否回升？ 导致状态下滑的因素有很多，我印象中的一个明显的时间点是我试图变更在非工作时间的文字输出环境而导致停止了文字输出节奏。但其他因素也要考虑，例如是否是因为阅读量减少了，或者是运动的节奏发生了变化，或者是运动时的空气质量不好，或者是玩原神时引发了诸如精神集中导致呼吸节奏的改变，或者是熬夜的持续时间达到一个足以影响身体状态的界限，或者是阅读信息但没有及时输出导致反复思考同一个内容，或者是接收了太多负面的情绪，或者是工作上的进度缓慢，或者是大餐吃太多了，或者是咖啡喝太多了，或者是没有像 2020 年那样做出许多的变化… 我强烈地感觉到第一步应该是把我所思考的东西都输出出来。最近在阅读各种信息的时候，发现各种信息源都在不断重复着有问题的逻辑，因此有着越来越强烈的将这些错误逻辑归纳起来找出问题所在的想法。之所以没有及时输出，是因为我担心会影响主线，例如会影响我之前发布的关于价值的文章。 阅读量可以通过像以前那样在地铁上看书解决，最近是经常在地铁上浏览一些娱乐内容，这个容易解决。“管住嘴，迈开腿”比较容易，运动环境也容易变更。现在也满足了恢复到以前的“非必要不喝咖啡”的限制的条件。接收负能量是社交的一部分，我不打算通过减少社交来逃避这个问题，因为在我使用完整世界观思考时是不会受到影响的，我所需要做的就是避免遗忘我曾经的思考结果，方法就是把这些思考输出为文字，常看。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:7:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"最后 我对 2021 年的上半年很满意，但对下半年的感觉很一般。下半年唯一让我感觉比较满意的就是把《\u003c资本论\u003e中的价值》这一篇发表出来，尽管这一篇还有不少问题。 2021 年以内外部环境为基准，增加了对软实力的投入，调低了对硬实力的投入。2022 年的内外部条件已经发生了变化，需要重新调整，以硬实力的提升为主，软实力的提升为辅。 似乎还有很多想说的，但篇幅已经太大，最多也只能说个大概，就先这样吧。 ","date":"2022-01-08","objectID":"/2022/01/08/2021-life/:8:0","tags":["Life"],"title":"高开低走，仍有进步 —— 我的 2021","uri":"/2022/01/08/2021-life/"},{"categories":null,"content":"使用 Pre-request Script 修改 POST Body 内容 在 body 里面添加变量： { \"name\": \"{{name}}\", \"value\": {{value}} } 然后在脚本里设置变量的值： pm.variables.set(\"name\", \"timestamp\") pm.variables.set(\"value\", Math.round(new Date()/1000)) 设置的值会被简单地填充到 Body 中，不会根据值的类型格式化。例如 name 的值为字符串，在添加变量的之后就需要在花括号外面加上双引号。 ","date":"2021-11-21","objectID":"/2021/11/21/postman/:1:0","tags":["Postman"],"title":"Postman","uri":"/2021/11/21/postman/"},{"categories":null,"content":"测试 pm.test(\"length greater than 0\", function () { var jsonData = pm.response.json(); pm.expect(jsonData.names.length).to.gte(0); }); ","date":"2021-11-21","objectID":"/2021/11/21/postman/:2:0","tags":["Postman"],"title":"Postman","uri":"/2021/11/21/postman/"},{"categories":null,"content":"批量请求测试 在搜索框中搜索 runner 或者在右下角点击 Runner 都可。 之后拖动请求到空白处，会一次性添加整个 Collection 的请求。把不必要的请求勾选掉即可。 ","date":"2021-11-21","objectID":"/2021/11/21/postman/:3:0","tags":["Postman"],"title":"Postman","uri":"/2021/11/21/postman/"},{"categories":null,"content":"https://github.com/openhardwaremonitor/openhardwaremonitor https://stackoverflow.com/questions/3262603/accessing-cpu-temperature-in-python collector/coretemp.go // https://github.com/prometheus-community/windows_exporter/pull/727 package collector import ( \"bytes\" \"fmt\" \"strings\" \"syscall\" \"unsafe\" \"github.com/prometheus/client_golang/prometheus\" \"github.com/prometheus/common/log\" \"golang.org/x/sys/windows\" ) // technical documentation for the shared memory structure: https://www.alcpu.com/CoreTemp/developers.html var ( kernel32 = syscall.NewLazyDLL(\"KERNEL32.dll\") msvcrt = syscall.NewLazyDLL(\"msvcrt.dll\") createMutexW = kernel32.NewProc(\"CreateMutexW\") releaseMutex = kernel32.NewProc(\"ReleaseMutex\") openFileMappingW = kernel32.NewProc(\"OpenFileMappingW\") closeHandle = kernel32.NewProc(\"CloseHandle\") mapViewOfFile = kernel32.NewProc(\"MapViewOfFile\") unmapViewOfFile = kernel32.NewProc(\"UnmapViewOfFile\") memcpy_s = msvcrt.NewProc(\"memcpy_s\") ) func init() { registerCollector(\"coretemp\", NewCoreTempCollector) } // A coreTempCollector is a Prometheus collector for CoreTemp shared data metrics type coreTempCollector struct { Temperature *prometheus.Desc Load *prometheus.Desc } // NewCoreTempCollector ... func NewCoreTempCollector() (Collector, error) { const subsystem = \"coretemp\" return \u0026coreTempCollector{ Temperature: prometheus.NewDesc( prometheus.BuildFQName(Namespace, subsystem, \"temperature_celsius\"), \"(Temperature)\", []string{\"name\", \"core\"}, nil, ), Load: prometheus.NewDesc( prometheus.BuildFQName(Namespace, subsystem, \"load\"), \"(Load)\", []string{\"name\", \"core\"}, nil, ), }, nil } // Collect sends the metric values for each metric // to the provided prometheus Metric channel. func (c *coreTempCollector) Collect(ctx *ScrapeContext, ch chan\u003c- prometheus.Metric) error { if desc, err := c.collect(ch); err != nil { log.Error(\"failed collecting coretemp metrics:\", desc, err) return err } return nil } func (c *coreTempCollector) collect(ch chan\u003c- prometheus.Metric) (*prometheus.Desc, error) { s, err := readCoreTempSharedData() if err != nil { return nil, err } cpuName := s.GetCPUName() for i := uint32(0); i \u003c s.CoreCount; i++ { // convert to celsius if internal unit is fahrenheit temp := float64(s.Temp[i]) if s.IsFahrenheit { temp = (temp - 32) * 5 / 9 } ch \u003c- prometheus.MustNewConstMetric( c.Temperature, prometheus.GaugeValue, temp, cpuName, fmt.Sprintf(\"%d\", i), ) ch \u003c- prometheus.MustNewConstMetric( c.Load, prometheus.GaugeValue, float64(s.Load[i]), cpuName, fmt.Sprintf(\"%d\", i), ) } return nil, nil } // read memory from CoreTemp to fetch cpu data func readCoreTempSharedData() (*coreTempSharedData, error) { mutexName, _ := windows.UTF16PtrFromString(\"CoreTempMutexObject\") mutexObject, _, err := createMutexW.Call(0, 0, uintptr(unsafe.Pointer(mutexName))) if err == nil { return nil, fmt.Errorf(\"CoreTempMutexObject not found. make sure Core Temp is running\") } defer releaseMutex.Call(mutexObject) mappingName, _ := windows.UTF16PtrFromString(\"CoreTempMappingObject\") mappingObject, _, err := CoreTempMappingObject.Call(4, 1, uintptr(unsafe.Pointer(mappingName))) if mappingObject == uintptr(0) { return nil, err } defer closeHandle.Call(mappingObject) mapView, _, err := mapViewOfFile.Call(mappingObject, 4, 0, 0, 0) if mapView == uintptr(0) { return nil, err } defer unmapViewOfFile.Call(mapView) data := coreTempSharedData{} _, _, _ = memcpy_s.Call(uintptr(unsafe.Pointer(\u0026data)), 0xa80, mapView, 0xa80) return \u0026data, nil } type coreTempSharedData struct { Load [256]uint32 TjMax [128]uint32 CoreCount uint32 CPUCount uint32 Temp [256]float32 VID float32 CPUSpeed float32 FSBSpeed float32 Multipier float32 CPUName [100]byte IsFahrenheit bool // if true, true, the temperature is reported in Fahrenheit IsDeltaToTjMax bool // if true, the temperature reported represents the distance from TjMax } func (s *coreTempSharedData) GetCPUName() string { n := bytes.IndexByte(s.CPUName[:], 0) return strings.TrimSpace(string(s.CPUName[:n])) } collectors: en","date":"2021-11-17","objectID":"/2021/11/17/promethues-windows-temp/:0:0","tags":["Promethues"],"title":"Promethues Windows 温度监控","uri":"/2021/11/17/promethues-windows-temp/"},{"categories":null,"content":"过载保护 http://sharecore.net/post/%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4%E7%AE%97%E6%B3%95%E6%B5%85%E6%9E%90/ ","date":"2021-11-03","objectID":"/2021/11/03/distributed-system/:0:0","tags":["分布式"],"title":"分布式的几个要点","uri":"/2021/11/03/distributed-system/"},{"categories":null,"content":"令牌桶算法 按一定速率往桶里放令牌，收到请求的时候从桶里取出。如果取不到令牌，则拒绝处理请求。 import ( time sync ) var tokens = 0 var capacity = 100 var rate = 10 var latestPutTime = time.Now().Unix() var lock sync.Mutex func min(a,b int) int64 { if a \u003e b { return a } return b } func getToken() bool { lock.Lock() defer lock.Unlock() // 放入令牌 now := time.Now().Unix() tokens = (now-latestPutTime)*rate latestPutTime = now // 桶如果溢出，使用固定限制 tokens = min(capacity, tokens) if tokens \u003e 0 { token-- return true } return false } ","date":"2021-11-03","objectID":"/2021/11/03/distributed-system/:1:0","tags":["分布式"],"title":"分布式的几个要点","uri":"/2021/11/03/distributed-system/"},{"categories":null,"content":"漏桶算法 与桶令牌算法相似。不再是放入令牌取，而是往里面塞令牌。桶定时漏掉一些令牌。 ","date":"2021-11-03","objectID":"/2021/11/03/distributed-system/:2:0","tags":["分布式"],"title":"分布式的几个要点","uri":"/2021/11/03/distributed-system/"},{"categories":null,"content":"定期刷新可用数 设定每个间隔可以处理的请求数，定时刷新数量。 无法处理在刷新的前后两秒暴涨的问题。 柔性可用 ","date":"2021-11-03","objectID":"/2021/11/03/distributed-system/:3:0","tags":["分布式"],"title":"分布式的几个要点","uri":"/2021/11/03/distributed-system/"},{"categories":null,"content":"/etc/ansible/hosts 命令行工具：https://docs.ansible.com/ansible/latest/cli/ansible.html ansible -i hosts -u root -m command -a \"/bin/echo hello\" m 表示模块。a 表示 args，是模块的参数。 command 模块是默认的，可省略。 ansible -i hosts myall -u root -a \"/bin/echo hello\" ansible-playbook -i hosts deploy.yml - name: 0. BUILD ansible.builtin.shell: cmd: \"/bin/bash control.sh build\" chdir: \"{{ local_dir }}\" delegate_to: 127.0.0.1 run_once: True 在本地执行，在原来的基础上加 delegate_to 就行。如果只需执行一次，则加上 run_once。 脚本 #!/bin/bash # filename: run.sh cmd=$1 ansible -i hosts target_group -u root -a \"$cmd\" bash run.sh ’echo “test”' 构建和获取 --- - name: \"build and fetch\" hosts: \"build-host\" remote_user: root become: True gather_facts: False tasks: - name: 1. PULL AND BUILD ansible.builtin.shell: cmd: git pull; source ~/.profile; bash control build chdir: \"/usr/local/app-local-dir\" - name: 2. SLEEP wait_for: delay: 10 timeout: 0 - name: 3. FETCH fetch: src={{ item.src }} dest={{ item.dest }} mode=0775 flat=true with_items: - {src: \"/usr/local/app-remote-dir\", dest: \"deploy/local-dir\"} 部署 - name: \"deploy\" hosts: \"deploy-hosts\" remote_user: root become: False gather_facts: False vars: remote_dir: /usr/local/app-remote-dir tasks: - name: 1. UPLOAD copy: src={{ item.src }} dest={{ item.dest }} mode=0775 with_items: - {src: \"deploy/app\", dest: \"{{ remote_dir }}\"} - name: 2. RESTART shell: \"cd {{ remote_dir }}; ./control restart\" - name: 3. SLEEP wait_for: delay: 3 timeout: 0 - name: 4. STATUS ansible.builtin.shell: cmd: ps aux | grep app | grep -v grep register: out - debug: var=out ","date":"2021-10-12","objectID":"/2021/10/12/ansible/:0:0","tags":["Ansible"],"title":"Ansible 使用","uri":"/2021/10/12/ansible/"},{"categories":null,"content":"部署 --- - name: \"deploy\" hosts: \"app\" remote_user: root become: False gather_facts: False vars: src_dir: /usr/local/app-remote-dir remote_dir: /usr/local/app-remote-dir remote_log_dir: /usr/local/app-remote-dir/log backup_dir: files/backup tasks: - name: 0. BACKUP BIN fetch: src: \"{{ remote_dir }}/app\" dest: \"{{ backup_dir }}/app\" mode: 0775 flat: True run_once: True - name: 1. BACKUP CONFIG fetch: src: \"{{ remote_dir }}/config.json\" dest: \"{{ backup_dir }}/config.json.{{ inventory_hostname }}\" mode: 0775 flat: True - name: 2. PULL AND BUILD ansible.builtin.shell: cmd: \"git pull; /bin/bash scripts/control build\" chdir: \"{{ src_dir }}\" delegate_to: 127.0.0.1 run_once: True register: build_result - debug: var: build_result run_once: True - name: 3. ENSURE_REMOTE_DIR file: path: \"{{ item }}\" state: directory mode: 0740 loop: - \"{{ remote_dir }}\" - \"{{ remote_log_dir }}\" - name: 4. UPLOAD copy: src={{ item.src }} dest={{ item.dest }} mode=0775 with_items: - {src: \"{{ src_dir }}/app\", dest: \"{{ remote_dir }}\"} - {src: \"{{ src_dir }}/scripts/control\", dest: \"{{ remote_dir }}\"} - {src: \"config.json\", dest: \"{{ remote_dir }}\"} - name: 5. CONFIG replace: path: \"{{ remote_dir }}/config.json\" regexp: \"ADDR:8087\" replace: \"{{ inventory_hostname }}:8087\" - name: 6. RESTART ansible.builtin.shell: cmd: ./control upgrade chdir: \"{{ remote_dir }}\" register: upgrade_status - debug: var=upgrade_status - name: 7. SLEEP wait_for: delay: 3 timeout: 0 - name: 8. STATUS ansible.builtin.shell: cmd: ./control status chdir: \"{{ remote_dir }}\" register: out - debug: var=out ","date":"2021-10-12","objectID":"/2021/10/12/ansible/:0:1","tags":["Ansible"],"title":"Ansible 使用","uri":"/2021/10/12/ansible/"},{"categories":null,"content":"CPU CPU 分两大类：Intel 和 AMD。 Intel 和 AMD 处理器都有一个面向中高端的主系列，Intel 是酷睿（Core，CPU 型号首字母为 i），AMD 是锐龙（Ryzen，CPU 型号首字母为 R）。Intel 的 i3、i5、i7、i9 和 AMD 的 R3、R5、R7、R9 对应。 在低端 CPU 市场中，Intel 是赛扬，AMD 是速龙。 参考： https://www.zhihu.com/question/395050447/answer/1225349132 (超威半导体（AMD）处理器怎么分类？) https://www.zhihu.com/question/32669957/answer/1531698791 (怎么让小白理解intel处理器（CPU）的分类？) ","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:0","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"核显 选购 CPU 时，根据自己是否会购买显卡来判断要不要买集成显卡的 CPU（核显）。如果要买显卡，那就没有买带核显的 CPU 的必要。如果不想另外花钱买显卡，不会用到高性能的独立显卡，那么买带核显的 CPU 就行了。 Intel 默认 CPU 集成显卡，没有集成显卡的 CPU 会加上 F 作为型号的后缀，例如 i9-11900F。 AMD 默认 CPU 不集成显卡，集成显卡的 CPU 会加上 G（Graphic），例如 R7-5700G。 还有其他种类的后缀，这里以笔记本用的 CPU 为例。 参考： https://rank.kkj.cn/dcpu3.shtml (桌面级CPU性能排行榜) ","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:1","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"笔记本 CPU 考虑到笔记本电脑的轻薄程度、散热压力和电源压力， CPU 厂商提供了降低了电压的 CPU。 Intel 和 AMD 都在 CPU 型号末尾加上 U 后缀表示该 CPU 是低电压版 CPU。例如 Intel 的 i7-10710U，AMD 的 R7-5800U。 游戏本则使用标准电压版的 CPU，因此需要更厚的机身加强散热，更大的电源保障使用时间。 Intel 和 AMD 都在 CPU 型号末尾加上 H 后缀表示该 CPU 是标准电压版 CPU。例如 Intel 的 i9-11900H，AMD 的 R7-5800H。 参考： https://rank.kkj.cn/mcpu3.shtml (笔记本CPU性能排行榜) https://www.hack520.com/643.html (CPU 标准电压版与低电压版的区别是什么？睿频又是什么？) ","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:2","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"硬件虚拟化（Virtualization） 虚拟化用于开虚拟机（VMWare 或者 VirtualBox）。如果不支持硬件虚拟化，使用软件虚拟化也行，但性能会比硬件虚拟化低。目前的 CPU 基本支持。 主板 主板有三个基础项： CPU 支持 设备接口 大小 在选定了这三者以后，再去考虑稳定性的问题。 主板最基础的那块版，只是作为连接各个电子元件用。板本身的作用是导线，没有特别特殊的地方。 ","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:3","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"CPU 支持 主板只有两种大类：Intel 主板，AMD 主板。如果买了 Intel 处理器和 AMD 主板，就用不了。 如何区分主板类型？决定主板类型的是主板芯片组。不同厂家会使用同一款主板芯片组做主板，并在主板名称加上主板芯片组型号。例如 AMD 主板名称上有 B450，或者 B450M（M 表示板的大小）。 两种大类的主板芯片组的命名规则都是 单字母+数字，字母用于表示不同的等级。 等级 Intel AMD 入门级 H A 主流级 B（不支持超频） B（支持超频） 发烧级 Z（支持超频） 超高级 X X 其中两者用了相同的 X、B 字母，但是会用不同的数字区分开，因此不用担心识别错。 作为主流的 B 型号主板最需要一般消费者关注，这里以 B 型号主板作为数字区分的例子。Intel 用 6x 结尾（例如 B460），而 AMD 使用 5x 结尾（例如 B450），其中 4 表示 4xx 系列。 不同等级的主板芯片组要搭配合适的 CPU 型号。例如入门级的主板不支持 CPU 超频，如果买了 CPU 想超频，就不能买入门级的主板。 现在在售的 AMD 芯片至少是速龙系列，搭配 B 型号的主板。 需要注意的是，一块 Intel 主板未必能适用于所有的 Intel CPU，芯片组和 CPU 之间的接口要对得上才能连接。因此要先选好 CPU，再选主板。Intel 的接口经常变化，AMD 的接口较为统一（AM4 接口）。 参考： https://zhuanlan.zhihu.com/p/344994838 (一图看懂AMD和Intel的主板型号区别) https://cpu.zol.com.cn/55/558144_all.html (K10和K8微架构处理器之间的对应关系) ","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:4","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"设备接口 设备接口是指对 USB、SATA、SDD、独立显卡等的支持。主板对这些接口的支持，是基于芯片组的支持。 一块主板最基础的作用，就是用板上的导线将各种接口和芯片组连接起来。但是这并不意味着每一块主板都会为芯片组的所有功能都提供接口，因此使用同一款芯片组的不同主板也会有不同的等级。 PCIe 总线（用于高速设备） CPU、芯片组、高速设备之间通过 PCIe 这一高速总线进行数据交换。PCIe 有标准的接口，但其他接口也会接入到 PCIe 总线，例如固态硬盘的 M.2 接口。 PCIe 根据通道数的不同区分了四种类型：x1（1 通道）、x4（4 通道）、x8（8 通道）、x16（16 通道）。4 通道是 1 通道速度的 4 倍，以此类推。PCIe 协议有不同版本，每个版本单通道的速度是前一个版本的 2 倍，例如 PCIe 3.0 的速度是 PCIe 2.0 的 2 倍，以此类推。 以固态硬盘为例。现在的固态硬盘大多是 PCIe 3.0 ，PCIe 3.0 x1 的速度大约是 1GB/s，因此 x4 是 4GB/s，以此类推。现在同时还有 PCIe 4.0 的固态硬盘，可以推出，PCIe 4.0 x1 的速度大约是 2GB/s。还可以推出 PCIe 3.0 x8 和 PCIe 4.0 x4 的速度处于同一个级别。 主板会把 CPU 的 PCIe 分别连接到两个不同的模块，分别是直连 CPU 的插槽和主板的芯片组（系统 I/O 芯片）。主板会通过芯片组把连接到该芯片的 PCIe 通道再划分出更多的 PCIe 接口，这些分出来的接口可以是满速的 PCIe3.0 或者调到 PCIe 2.0 获取双倍数量的接口，但所有接口速度总和不超过 CPU 与芯片组的 PCIe 通道的速度。因此主板总体的 PCIe 最高性能受限于 CPU 支持的 PCIe 通道数量。 主流消费级的 CPU 有 16 个直连通道，通常用于插显卡。主板可将其分成两个 x8，也可以一个 x8 加上两个 x4。因为就算是顶级的显卡，也用不满 x16 通道，而是只使用了 x8 通道级别的速度。 低端的 CPU 则没有那么多 PCIe 直连通道，例如 AMD Athlon 3000G 只有 6 个。很难看到有直接使用 x6 的设备，我们可以看看主板是怎么使用这 6 个 PCIe 接口的。 以微星 B450M MORTAR 为例，从手册中的【规格】一节中可以看到，扩展插槽的 PCIe 3.0 一栏写了 AMD® Athlon™ 处理器支持 x4 速率，也就是占用了 4 个 PCIe 通道。接着在下面的存储中的固态硬盘 M.2 接口写了 PCIe 3.0 x2，也就是占用了 2 个 PCIe 通道。加起来刚好 6 个通道。 手册中还有 PCIe 2.0 的插槽，这就是前面说的从芯片组分出来的 PCIe 接口。主板芯片组和 CPU 通过 4 个 PCIe 3.0 通道连接，总速度为 4GB/s。芯片组扩展出其他接口，例如 M.2、SATA、USB 等等，共用这个速度。 扩展出的这些接口，在实际运行时的总速度不超过 4 个 PCIe 3.0 通道的总速度，这意味着某个接口可能无法达到满速。但在实际使用中，通常所有用到的接口满速加起来不会超过 4GB/s。 设备 理论最高速度(GB/s) M.2 SSD (PCIe 2.0 x4) 2.0 SATA3 0.75 USB 2.0 0.06 USB 3.0 (USB 3.2 Gen1) 0.625 USB 3.1 (USB 3.2 Gen2) 1.25 USB 3.2 (USB 3.2 Gen2x2) 2.5 微星 B450M MORTAR 提供了 4 个 SATA3 接口，两个 M.2（其中一个直连 CPU）。当非直连的 M.2 固态硬盘满速，再加上 3 个机械硬盘满速的时候，才会超出 4GB/s 的限制。想想自己是否会遇到这种场景？ 另外主板提供的一些接口是只能二选一的，毕竟总速度限制就在那。比如微星 B450M MORTAR 提供了 4 个 PCIe 2.0 通道给 PCI_E4 和 M2_2 接口，如果 M2_2 使用了，PCI_E4 将会失效。 虽然 PCIe 有四种通道数类型，但主板未必提供四种插槽，而是选择全部统一为 x16 的插槽。无论是 x1、x4、x8 还是 x16，都能插入到 x16 的插槽中。区分这些插槽的方式是看有多少个金属引脚。例如微星 B450M MORTAR 的 PCI_E4 的接口是 x16 的插槽，但实际只有 x4 的 PCIe 接口。 参考： https://zhuanlan.zhihu.com/p/62426408 (【PCI-E通道是个什么东西？他是干啥的？】) https://zhuanlan.zhihu.com/p/398325468 (amd芯片组（全网最详细介绍）) https://gadgetversus.com/processor/amd-ryzen-9-3950x-vs-amd-athlon-3000g/ (AMD Ryzen 9 3950X vs AMD Athlon 3000G) https://www.sohu.com/a/337657618_641165 (小科普 | PCIe通道到底怎么算？ ) https://download.msi.com/archive/mnu_exe/mb/M7B89v1.4_SC.pdf (微星 B450M MORTAR 手册) https://blog.csdn.net/hunter___/article/details/88081092 (扫盲：SATA、mSATA 、PCIe和M.2——SSD硬盘的接口) https://mb.zol.com.cn/348/3486970_all.html (纠结用哪个PCIE? 实测x16/x8/x4解疑惑) 其他 I/O 设备 其他要关注的主要是 USB 和 SATA 接口。 USB 的标准比较混乱，在下表中同一个标准的不同写法放一起了。 USB 速度(MB/s) USB 2.0 60 USB 3.0 (USB 3.2 Gen1、USB 3.1 Gen1) 625 USB 3.1 (USB 3.2 Gen2、USB 3.1 Gen2) 1250 USB 3.2 (USB 3.2 Gen2x2) 2500 因此你会在主板芯片组官网看到 USB 3.2 Gen2，在主板手册上看到 USB 3.1 Gen2，它们俩是同一个东西。 以下是 AMD 芯片组官网的一些信息： 型号 USB 最大 SATA 接口数 直连处理器 PCIe PCIe 规格 PCIe 通道（总数/可用） 支持超频 USB 总数 USB 3.2 10Gb/s 显卡 NVMe B550 14 6 8 1x16 / 2x8 1x4 PCIe 4.0 38/30 是 B450 14 2 6 1x16 1x4 PCIe 3.0 36/28 是 B350 14 2 6 1x16 1x4 PCIe 3.0 36/28 是 接下来以 B450 为例。 B450 芯片组支持 14 个 USB 接口，其中有 2 个是 USB 3.2 Gen2 接口。在微星 B450M MORTAR 手册上可以看到以下 USB 接口： 1 个 USB 3.1 Gen2 (SuperSpeed USB 10Gbps) Type-C 后置面板端口 1 个 USB 3.1 Gen2 (SuperSpeed USB 10Gbps) Type-A 后置面板端口 2 个 USB 3.1 Gen1 (SuperSpeed USB) 端口通过内部 USB 3.1 Gen1 接口可使用 6 个 USB 2.0 (High-speed USB) 端口 (2 个 Type-A 后置面板端口,通过内部 USB 2.0 接口可使用 4 个端口) 4 个 USB 3.1 Gen1 (SuperSpeed USB) Type-A 后置面板端口 加起来总共是 14 个 USB 接口，与芯片组的数量一致。 B450 芯片组最大 SATA 接口数是 6 个。在微星 B450M MORTAR 手册上可以看到提供了 4 个 SATA 接口，还有一个 M.2 接口上支持 SATA。剩下一个没找到，应该是主板不打算用满（也没必要用满）。 通常家用或者办公电脑不需要使用太多 SATA 接口，一个固态硬盘接 M.2 加上一个机械硬盘接 SATA 就够了。不过如果是组装 NAS，那么就可能要求比较多的 SATA 接口。 参考： https://www.amd.com/zh-hans/products/chipsets-am4 (AMD AM4 平台芯片组) https://download.msi.com/archive/mnu_exe/mb/M7B89v1.4_SC.pdf (微星 B450M MORTAR 手册) 固态硬盘 固态硬盘有多种接口：SATA、mSATA、PCIe、M.2。现在主板会专门提供 M.2 接口给固态硬盘使用，而有些厂商会提供 PCIe 接口的固态硬盘。 M.2 接口支持多种协议的固态硬盘，目前常见的是 NVMe 和 SATA，这点在买固态硬盘的时候要注意区分。 M.2 接口使用 PCIe x4 通道，而 PCIe 接口的固态硬盘最高可使用 PCIe x16 通道。M.2 固态硬盘的优势是体积小，这是 PCIe 固态硬盘的缺点。PCIe 固态硬盘虽然可以使用 x16 通道，但实际上由于固态硬盘本身速度的限制，不会用满 x1","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:5","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"网络唤醒（Wake On LAN，WOL） 参考： https://new.qq.com/omn/20191225/20191225A0SZI200.html (第567期：芯片组和主板有啥关系？主板型号怎么看？) ","date":"2021-10-04","objectID":"/2021/10/04/computer/:0:6","tags":null,"title":"自组装电脑选购基础知识","uri":"/2021/10/04/computer/"},{"categories":null,"content":"awk 是文本处理工具。 awk 命令接收的参数分为四个部分： 选项 脚本代码 变量 待处理文件 大多数 awk 命令只用到选项、脚本代码、待处理文件。 ","date":"2021-09-03","objectID":"/2021/09/03/awk/:0:0","tags":["Linux"],"title":"AWK 命令","uri":"/2021/09/03/awk/"},{"categories":null,"content":"示例文本 test.txt name,age xiaoming,20 xiaohong,18 ","date":"2021-09-03","objectID":"/2021/09/03/awk/:1:0","tags":["Linux"],"title":"AWK 命令","uri":"/2021/09/03/awk/"},{"categories":null,"content":"选项 最常用的选项是 -F field-separator，用来选择以什么字符串作为分割字段的依据。例如 -F ',' 表示按照 , 分割行。 另一个常用选项是 -f script-file，用来选择分析脚本文件。如果脚本代码太多，不适合写在命令参数里面，选择写在文件里面，则使用这个参数加载脚本。 ","date":"2021-09-03","objectID":"/2021/09/03/awk/:2:0","tags":["Linux"],"title":"AWK 命令","uri":"/2021/09/03/awk/"},{"categories":null,"content":"脚本代码 如果没有使用 -f script-file 参数，则需要在命令参数里写脚本代码。 脚本代码由单引号括起来，例如 '{print $1}'。这是因为在 shell 里写命令时，$ 符号用来表示使用一个变量。如果用双引号，则会被解释为使用某个变量。如果用单引号，则不会被解释，而是原样传入 awk 命令。 脚本代码使用 $数字 的方式表示第几个字段，例如 $1 表示分割后的第一个字段。 脚本代码由模式+动作组成，动作的代码由花括号包裹，组合起来的为pattern{action}。可以有多个这样的组合，组合之间不需要间隔符（可自己加空格便于阅读），每个组合都会对一行分别执行一次。满足模式的时候，就会执行相应的动作。 例如： awk -F ',' '$2 \u003e= 18 {print $1,$2} $2 == 20 {print $1,\"is\",$2}' test.txt 结果是： name age xiaoming 20 xiaoming is 20 xiaohong 18 模式可以省略，表示对于每行都执行动作。 例如： awk -F ',' '{print $1,$2}' test.txt 结果是： name age xiaoming 20 xiaohong 18 模式有很多种，常用的有： BEGIN END 条件表达式 正则表达式 BEGIN 表示读取文件之前，通常用于初始化变量。END 表示不再读取文件之后，通常用于打印最终的结果。 例如： awk -F ',' 'BEGIN{sum=0} {sum += $2} END{print sum}' test.txt 结果是： 38 ","date":"2021-09-03","objectID":"/2021/09/03/awk/:3:0","tags":["Linux"],"title":"AWK 命令","uri":"/2021/09/03/awk/"},{"categories":null,"content":"内建变量 有时候会对处理的行数有要求，例如前面的结果中把第一行也打印出来了，如果要把某一行去掉，可以自己声明一个变量记录行数： awk -F ',' 'BEGIN{line=0} {line++} line != 1 {print $1,$2}' test.txt 结果： xiaoming 20 xiaohong 18 每次都这样写麻烦，因此 awk 内置了一个变量 NR 用于表示当前行数。awk 默认把每行看作一条记录，NR 是 Number of records 的缩写。 于是上面的命令就简化为： awk -F ',' 'NR != 1 {print $1,$2}' test.txt awk 默认把每行看作一条记录，是因为其记录分隔符被默认为换行符 \\n。可以通过给内置变量 RS （Record separator）赋值改变记录分隔符。 例如： awk 'BEGIN{RS=\",\"} {print $1}' test.txt 结果是： name age 20 18 这里的 “xiaoming” 和 “xiaohong” 丢失是因为每次按照 , 分割，得到的是 age\\nxiaoming 这样的结果。然后默认的 FS（Field separator），也就是 -F 参数，是空白字符。空白字符包括空格和换行，因此 $1 就只包含 age。 如果要展示完整，则使用 $0。它表示一个 record 里的所有 field。 awk 'BEGIN{RS=\",\"} {print $0}' test.txt 结果是： name age xiaoming 20 xiaohong 18 如果一行中间的字段数不同，但要打印最后一个字段呢？ 有一个内置变量表示当前 Record 的字段数，NF（Number of field）。 awk -F ',' '{print $NF}' test.txt 结果是： age 20 18 同理，倒数第二个字段为： awk -F ',' '{print $(NF-1)}' test.txt 计算五分钟带宽 '{sum+=$6} END {print \"sum=\",sum*8/300/1000/1000}' ","date":"2021-09-03","objectID":"/2021/09/03/awk/:4:0","tags":["Linux"],"title":"AWK 命令","uri":"/2021/09/03/awk/"},{"categories":null,"content":"https://www.digitalocean.com/community/tutorials/how-to-list-and-delete-iptables-firewall-rules#:~:text=Deleting%20Rules%20by%20Specification.%20One%20of%20the%20ways,the%20rules%20list%2C%20iptables%20-S%2C%20for%20some%20help. ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:0:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"禁止所有网卡访问 9093 端口 iptables -I INPUT -p tcp –dport 9093 -j DROP –dport 一定要加在 -p tcp 后面，否则会提示没有该选项。 ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:1:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"放行所有访问本机某个网卡的 9093 端口 iptables -I INPUT -d 192.168.1.101 -p tcp –dport 9093 -j ACCEPT ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:2:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"列出所有规则 iptables -L ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:3:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"列出 INPUT 链的所有规则 iptables -L INPUT ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:4:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"列出 INPUT 链的所有规则，带上序号 iptables -L INPUT –line-numbers ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:5:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"删除 INPUT 链的指定序号的规则 iptables -D INPUT 2 ","date":"2021-08-31","objectID":"/2021/08/31/iptables/:6:0","tags":["Linux","网络"],"title":"iptables","uri":"/2021/08/31/iptables/"},{"categories":null,"content":"要求返回接口，但创建对象需要更多参数。 Golang 的 gin 的 Router 的参数： func (group *RouterGroup) GET(relativePath string, handlers ...HandlerFunc) IRoutes { return group.handle(http.MethodGet, relativePath, handlers) } type HandlerFunc func(*Context) 要求传入以 *Context 为参数且无返回值的函数。 正常情况： func example(c *gin.Context) { // ... c.JSON(200, \"\") return } func InitRouter() { r.GET(\"/test\", example) } 如果要用到其他 service，由于函数参数无法传入，通常只能用全局。 但是现在可以： func exampleWithService(service Service) func(c *gin.Context) { return func(c *gin.Context) { service.CallMethod() c.JSON(200, \"\") return } } func InitRouter(service Service) { r.GET(\"/test\", exampleWithService(service)) } ","date":"2021-08-31","objectID":"/2021/08/31/function-is-first-class/:0:0","tags":["数据库"],"title":"编程语言把函数作为 First Class","uri":"/2021/08/31/function-is-first-class/"},{"categories":null,"content":"已知： 在一个进程启动子进程的时候，可以将父进程文件描述符复制给子进程 在 Socket 层面，当进程监听某个端口时，会为其创建一个对应的 TCP_LISTEN Socket，用于监听来自客户端的请求。 TCP_LISTEN Socket 会绑定一个 Socket 文件，该文件位于系统内核空间。文件可以独立于这个 TCP_LISTEN Socket 存在。 做法： 父进程启动子进程，将父进程的标准输入、标准输出、标准错误、Web 服务的 TCP_LISTEN Socket 的文件描述符复制给子进程，忽略其他的 REQUEST Socket 文件描述符。 子进程根据继承的 TCP_LISTEN Socket 文件描述符创建对应 Listener。启动服务，调用 Accept() 接收请求。 此时父进程和子进程都在接收请求，父进程还未关闭它的 Listener。 操作系统允许多个进程监听同一个地址和端口，负载均衡。 https://lwn.net/Articles/542629/ 子进程发送信号给父进程，要求其关闭服务。 父进程关闭 TCP_LISTEN Socket 的 Listener，不再接收新请求。 关闭 Listener 不会关闭 Socket 文件，不影响子进程继续接收请求。 父进程处理完子进程监听请求之前的所有已接收的请求，关闭进程。 ","date":"2021-08-17","objectID":"/2021/08/17/web-service-hot-upgrade/:0:0","tags":["web"],"title":"WEB 服务热更新","uri":"/2021/08/17/web-service-hot-upgrade/"},{"categories":null,"content":"启动子进程 父进程： func (a *app) signalHandler(wg *sync.WaitGroup) { ch := make(chan os.Signal, 10) signal.Notify(ch, syscall.SIGINT, syscall.SIGTERM, syscall.SIGUSR2) for { sig := \u003c-ch switch sig { // ... case syscall.SIGUSR2: // ... if _, err := a.net.StartProcess(); err != nil { a.errors \u003c- err } } } } ","date":"2021-08-17","objectID":"/2021/08/17/web-service-hot-upgrade/:1:0","tags":["web"],"title":"WEB 服务热更新","uri":"/2021/08/17/web-service-hot-upgrade/"},{"categories":null,"content":"继承文件描述符 gracenet/net.go func (n *Net) StartProcess() (int, error) { listeners, err := n.activeListeners() if err != nil { return 0, err } // Extract the fds from the listeners. files := make([]*os.File, len(listeners)) for i, l := range listeners { files[i], err = l.(filer).File() // \u003c-- 这里的 File() 是 TCPListener 类的方法。 if err != nil { return 0, err } defer files[i].Close() } // ... } net/tcpsock.go func (l *TCPListener) File() (f *os.File, err error) { if !l.ok() { return nil, syscall.EINVAL } f, err = l.file() if err != nil { return nil, \u0026OpError{Op: \"file\", Net: l.fd.net, Source: nil, Addr: l.fd.laddr, Err: err} } return } ","date":"2021-08-17","objectID":"/2021/08/17/web-service-hot-upgrade/:2:0","tags":["web"],"title":"WEB 服务热更新","uri":"/2021/08/17/web-service-hot-upgrade/"},{"categories":null,"content":"关闭父进程 gracehttp/http.go 子进程： func (a *app) run() error { // ... // 开始提供服务 a.serve() if didInherit \u0026\u0026 ppid != 1 { // 关闭父进程 if err := syscall.Kill(ppid, syscall.SIGTERM); err != nil { return fmt.Errorf(\"failed to close parent: %s\", err) } } // ... } gracehttp/http.go 父进程： func (a *app) signalHandler(wg *sync.WaitGroup) { ch := make(chan os.Signal, 10) signal.Notify(ch, syscall.SIGINT, syscall.SIGTERM, syscall.SIGUSR2) for { sig := \u003c-ch switch sig { case syscall.SIGINT, syscall.SIGTERM: signal.Stop(ch) a.term(wg) // \u003c-- 会调用 server.Stop() return // ... } } } ","date":"2021-08-17","objectID":"/2021/08/17/web-service-hot-upgrade/:3:0","tags":["web"],"title":"WEB 服务热更新","uri":"/2021/08/17/web-service-hot-upgrade/"},{"categories":null,"content":"父进程等待当前所有请求处理完毕 httpdown/httpdown.go: func (s *server) Stop() error { s.stopOnce.Do(func() { // ... stopDone := make(chan struct{}) s.stop \u003c- stopDone select { case \u003c-stopDone: case \u003c-s.clock.After(s.stopTimeout): // ... } // ... } return s.stopErr } httpdown/httpdown.go: func (s *server) manage() { // ... var stopDone chan struct{} // ... for { select { // ... case c := \u003c-s.closed: decConn(c) delete(conns, c) if stopDone != nil \u0026\u0026 len(conns) == 0 { close(stopDone) return } case stopDone = \u003c-s.stop: if len(conns) == 0 { close(stopDone) return } for c, cs := range conns { if cs == http.StateIdle { c.Close() } } // ... } } // ... } 当发起关闭的时候，向 s.stop 中发送一个消息。在 manage() 中接收后，判断是否所有请求都已处理，如果处理完了就关闭 stopDone，让 Close() 继续执行。 如果没有处理完毕，那么就让循环继续执行。每次有一个请求处理完毕，都检查是否所有连接都已处理完毕，如果是，就关闭 stopDone，让 Close() 继续执行。 ","date":"2021-08-17","objectID":"/2021/08/17/web-service-hot-upgrade/:4:0","tags":["web"],"title":"WEB 服务热更新","uri":"/2021/08/17/web-service-hot-upgrade/"},{"categories":null,"content":"DNS 缓存 https://segmentfault.com/a/1190000017962411 chrome://net-internals/#dns可以查看chrome浏览器的dns缓存信息 https://www.zhihu.com/question/23042131/answer/66571369 DNS 让域名解析的时候，引入自己的 DNS 服务器 DNS 解析请求会得到多种类型的响应，例如 A、CNAME、NS。 当 DNS 服务器得到 CNAME 时，它会发起解析 CNAME 的请求。 将一个域名 CNAME 到 CDN 厂商的 CDN 域名。 CNAME 请求的结果是 CDN 厂商的 DNS 服务器 IP，响应类型为 NS。 于是 DNS 服务器向厂商的 DNS 服务器发起查询请求，查询的域名是 CDN 厂商域名。 CDN 厂商的 DNS 服务器收到请求，根据请求第一跳的用户 IP 去 IP 库查询 IP 的地址。根据该地址选择最近的机房，返回机房里的 IP。 ","date":"2021-07-14","objectID":"/2021/07/14/dns/:0:0","tags":["网络"],"title":"DNS","uri":"/2021/07/14/dns/"},{"categories":null,"content":"查询原理 执行查询 gorm.io/gorm/callbacks/query.go::Query(db *gorm.DB) 扫描结果集 gorm.io/gorm/scan.go::Scan(rows *sql.Rows, db *DB, initialized bool) 扫描结果集的过程 取出结果集的所有字段 根据 Find() 时传入的引用判断存放的容器。比如用 map[string]interface{}，或者结构体切片 []User，其他类型不一一列出。以下描述的是结构体切片的流程。 根据结果集字段的个数，生成与其长度相等的 interface{} 类型的 values 切片，以及长度相等的 Field 类型的 fields 切片。 values 切片将会被重复使用 找出结构体内与结果集列字段匹配的字段，并且在 fields 切片与列字段相同下标的槽里面放入结构体字段的信息。 生成结构体变量 在 values 切片的每个槽都设置与 fields 切片相同位置上的字段类型。 values 的每个 item 是 interface{}，如果不在每次扫描之前都重新赋值，就会影响到已赋值的数据。 调用原生库扫描数据行到 value 切片 取出 fields 里有效的 field，在 values 里面取出相同位置的数据，并使用 field 将得到的数据赋值到结构体变量里面。 结构体变量加入 ReflectValue 回到 5 继续扫描下一行 ","date":"2021-07-14","objectID":"/2021/07/14/gorm/:1:0","tags":["Goland"],"title":"GORM 底层实现","uri":"/2021/07/14/gorm/"},{"categories":null,"content":"Eager Load 执行驱动（driving）表的查询，将结果按照 foreignKey 的值分组 取所有 foreignKey ，到被驱动（driven）表查询 对于 driven 表的查询结果，逐个根据 referenceKey 找到 driving 表的结果分组，给分组里每个对象的相应 field 赋值 ","date":"2021-07-14","objectID":"/2021/07/14/gorm/:2:0","tags":["Goland"],"title":"GORM 底层实现","uri":"/2021/07/14/gorm/"},{"categories":null,"content":"给结构体变量和结构体指针赋值有区别么？ 如果是结构体指针，那么当 foreignKey 存在相同的数据时，同一个结构体指针会赋值给不同的 driving 表结果。一旦其中一个修改，另一个也会被修改。 ","date":"2021-07-14","objectID":"/2021/07/14/gorm/:3:0","tags":["Goland"],"title":"GORM 底层实现","uri":"/2021/07/14/gorm/"},{"categories":null,"content":"可以在同一个 gorm.DB 指针并发查询么？ 每次调用条件或者 Find 都会获取新的数据库实例，因此不会出现共用一个数据库实例的情况。 ","date":"2021-07-14","objectID":"/2021/07/14/gorm/:4:0","tags":["Goland"],"title":"GORM 底层实现","uri":"/2021/07/14/gorm/"},{"categories":null,"content":"LVM（Logical Volume Manager） 快照机制 快照指的是在创建完快照后，快照里的数据不再发生改变。就算原始数据发生变化，读取快照的数据仍然是创建快照的时间点的数据。 创建快照的时候，仅创建了指向实际数据的 inode 的硬连接，所以速度很快。 快照是一个逻辑卷（类似于硬盘），需要挂载后使用。 现在我们有两个卷。一个是原始卷，存储了原始数据。另一个是快照卷。 那么要更新原始卷的数据的时候是怎么处理的？ 数据最终会更新到原始卷里面。为了保持读取快照数据的时候不发生变化，系统会在更新原始卷之前，将原始卷将要改变的数据块复制到快照卷的存储空间里面，然后修改原始卷的数据块。 创建快照的时候申请了一个存储空间。如果存储空间满了，则快照失效。因为原始卷如果继续更新数据，系统无法把原始数据块复制到快照卷的存储空间，读取快照的时候就没法读取到修改前的数据。 https://www.cnblogs.com/sparkdev/p/10232567.html 原始数据卷还原到快照状态的方式有两种： 全量备份快照的数据，删除原始卷，把快照的备份数据还原到原始卷 使用 lvconvert –merge 合并快照卷 全量备份快照的数据的时候，会从原始卷中复制没有被修改的块，以及从快照卷中复制备份的数据块。 使用快照创建测试环境。即把快照作为可读写卷。如果把数据直接写入到快照，目标数据块会被标记为使用，不会从原始数据卷中复制这个数据块的内容。 即使仅修改数据块的一小部分，也会因读写单位是数据块而从原始数据卷中读取整个数据块，并在写入快照时写入整个数据块，所以没必要再从原始数据卷中复制。 https://www.linuxidc.com/Linux/2016-09/135484.html 快照针对的是整个磁盘，不是文件系统中的某个文件夹。 ","date":"2021-07-14","objectID":"/2021/07/14/lvm/:0:0","tags":["Linux"],"title":"Linux 文件系统快速备份","uri":"/2021/07/14/lvm/"},{"categories":null,"content":"比较通用的一种方式 { \"date\" : \"2020-11-05 00:00:00.000\", \"domain\": \"test.com\", \"data\": { \"00:00\" : { \"ts\": 1604505600, \"bandwidth\": 111.222, \"cost\": 333.444 }, \"00:01\" : { \"ts\": 1604505660, \"bandwidth\": 555.666, \"cost\": 777.888 } } } 查询支持按五分钟合并： db.oc_domain.aggregate([ { \"$match\": { \"domain\": \"test.com\" } }, { $project: { \"domain\": 1, \"data\": { $objectToArray: \"$data\" } } }, { $unwind: \"$data\" }, { $group: { \"_id\": { \"domain\": \"$domain\", \"timestamp\": { \"$subtract\": [ \"$data.v.time\", { \"$mod\": [ \"$data.v.time\", 5 * 60 ] } ] } }, \"bandwidth\": { $sum: \"$data.v.bandwidth\" } } }, { $project: { \"_id\": 0, \"timestamp\": \"$_id.timestamp\", \"domain\": \"$_id.domain\", \"bandwidth\": \"$bandwidth\" } } ]) ","date":"2021-07-14","objectID":"/2021/07/14/mongodb-store-timestamp/:1:0","tags":["MongoDB","Database"],"title":"MongoDB存储时间点数据","uri":"/2021/07/14/mongodb-store-timestamp/"},{"categories":null,"content":"不需要直观地看某个时间点的数据 { \"date\" : \"2020-11-05 00:00:00.000\", \"domain\": \"test.com\", \"data\": [ { \"ts\": 1604505600, \"bandwidth\": 111.222, \"cost\": 333.444 }, { \"ts\": 1604505660, \"bandwidth\": 555.666, \"cost\": 777.888 } ] } 查询支持按五分钟合并： db.oc_domain.aggregate([ { \"$match\": { \"domain\": \"test.com\" } }, { $unwind: \"$data\" }, { $group: { \"_id\": { \"domain\": \"$domain\", \"timestamp\": { \"$subtract\": [ \"$time\", { \"$mod\": [ \"$time\", 5 * 60 ] } ] } }, \"bandwidth\": { $sum: \"$bandwidth\" } } }, { $project: { \"_id\": 0, \"timestamp\": \"$_id.timestamp\", \"domain\": \"$_id.domain\", \"bandwidth\": \"$bandwidth\" } } ]) ","date":"2021-07-14","objectID":"/2021/07/14/mongodb-store-timestamp/:2:0","tags":["MongoDB","Database"],"title":"MongoDB存储时间点数据","uri":"/2021/07/14/mongodb-store-timestamp/"},{"categories":null,"content":"不使用时间戳 { \"date\" : \"2020-11-05 00:00:00.000\", \"domain\": \"test.com\", \"data\": { \"00:00\" : { \"time\": \"2020-11-05 00:00:00.000\", \"bandwidth\": 111.222 }, \"00:01\" : { \"time\": \"2020-11-05 00:01:00.000\", \"bandwidth\": 333.444 } } } 查询支持按五分钟合并： db.oc_domain.aggregate([ { \"$match\": { \"domain\": \"test.com\" } }, { $project: { \"domain\": 1, \"data\": { $objectToArray: \"$data\" } } }, { $unwind: \"$data\" }, { $group: { \"_id\": { \"domain\": \"$domain\", \"date\": { \"$subtract\": [ { \"$subtract\": [\"$data.v.time\", new Date(\"1970-01-01\")] }, { \"$mod\": [ { \"$subtract\": [\"$data.v.time\", new Date(\"1970-01-01\")] }, 5 * 60 * 1000 ] } ] } }, \"bandwidth\": { $sum: \"$data.v.bandwidth\" } } }, { $project: { \"_id\": 0, \"date\": { $toDate: \"$_id.date\" }, \"domain\": \"$_id.domain\", \"bandwidth\": \"$bandwidth\" } } ]) 如果不合并： db.oc_domain.aggregate([ { \"$match\": { \"domain\": \"v29-dy.ixigua.com\" } }, { $project: { \"domain\": 1, \"data\": { $objectToArray: \"$data\" } } }, { $unwind: \"$data\" }, { $group: { \"_id\": { \"domain\": \"$domain\", \"date\": \"$data.v.time\" }, \"bandwidth\": { $sum: \"$data.v.bandwidth\" } } }, { $project: { \"_id\": 0, \"date\": \"$_id.date\", \"domain\": \"$_id.domain\", \"bandwidth\": \"$bandwidth\" } } ]) ","date":"2021-07-14","objectID":"/2021/07/14/mongodb-store-timestamp/:3:0","tags":["MongoDB","Database"],"title":"MongoDB存储时间点数据","uri":"/2021/07/14/mongodb-store-timestamp/"},{"categories":null,"content":"完全不需要合并的结构 { \"date\" : \"2020-11-05 00:00:00.000\", \"domain\": \"test.com\", \"bandwidth\": { \"00:00\" : 111.222, \"00:05\" : 333.444 } } { $project: { \"datetime\": { \"$dateFromString\": { dateString: { \"$dateToString\": { \"date\": \"$date\", \"format\": \"%Y-%m-%dT%H:%M:%S\" } }, format: \"%Y-%m-%dT%H:%M:%S\", timezone: { $concat: [\"-\", \"$data.k\"] } } } } } ","date":"2021-07-14","objectID":"/2021/07/14/mongodb-store-timestamp/:4:0","tags":["MongoDB","Database"],"title":"MongoDB存储时间点数据","uri":"/2021/07/14/mongodb-store-timestamp/"},{"categories":null,"content":"子进程 通过 Shell 执行： # -*- coding: utf-8 -*- from subprocess import Popen, PIPE p = Popen([\"echo '1+1' | bc\"], shell=True, stdout=PIPE, stderr=PIPE) stdout, stderr = p.communicate() print(stdout, stderr) 不通过 Shell 执行： # -*- coding: utf-8 -*- from subprocess import Popen, PIPE p1 = Popen([\"echo\", \"1+1\"], shell=False, stdout=PIPE, stderr=PIPE) # p1.stdout 作为 p2.stdin p2 = Popen([\"bc\"], shell=False, stdin=p1.stdout ,stdout=PIPE, stderr=PIPE) # 在 p2 先于 p1 退出时，p1 能够接收到 SIGPIPE 信号 p1.stdout.close() stdout, stderr = p2.communicate() print(stdout, stderr) ","date":"2021-07-14","objectID":"/2021/07/14/python/:1:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"时间转换 # -*- coding: utf-8 -*- from datetime import datetime, timezone, timedelta # 如果在生成 date 的时候，没有指定 tz，则应该在后续通过 replace 设置时区 # date: datetime = datetime.fromisoformat(\"2021-05-05 16:00:00\").replace(tzinfo=timezone.utc) date: datetime = datetime.now(tz=timezone.utc) # astimezone 默认把时区转换为本地时区 # 对于 Asia/Shanghai 时区，等同于 date.astimezone(timezone(timedelta(hours=8))) tz_aware_date = date.astimezone() # 通过设置 timespec=\"seconds\"，把毫秒去掉 iso8601format = tz_aware_date.isoformat(timespec=\"seconds\") print(iso8601format) ","date":"2021-07-14","objectID":"/2021/07/14/python/:2:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"时间计算 datetime.now() + timedelta(days=1) ","date":"2021-07-14","objectID":"/2021/07/14/python/:3:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"时间比较 if datetime.now() \u003c datetime.now(): return ","date":"2021-07-14","objectID":"/2021/07/14/python/:4:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"打印一天内的每一分钟 [print(\"%02d:%02d:00\" % (hour, minute)) for hour in range(0,24) for minute in range(0,60)] ","date":"2021-07-14","objectID":"/2021/07/14/python/:5:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"Excel xls # -*- coding: utf-8 -*- from xlwt.Workbook import Workbook from xlwt.Worksheet import Worksheet wb = Workbook() sheet = wb.add_sheet(\"test\") i = 1 for r in range(1, 100): for c in range(1, 100): sheet.write(r, c, i) i += 1 wb.save(\"test.xls\") ","date":"2021-07-14","objectID":"/2021/07/14/python/:6:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"Excel xlsx # -*- coding: utf-8 -*- from openpyxl import Workbook,utils from openpyxl.worksheet.worksheet import Worksheet wb = Workbook() wb.remove(wb.active) sheet = wb.create_sheet(\"test\") i = 1 for r in range(1, 100): for c in range(1, 100): sheet.cell(r, c, i) i += 1 wb.save(\"test.xlsx\") 自适应： def columns_best_fit(ws: Worksheet): column_letters = tuple(utils.get_column_letter(col_number + 1) for col_number in range(ws.max_column)) for column_letter in column_letters: ws.column_dimensions[column_letter].auto_size = True if column_letter == \"A\" or column_letter == \"B\": ws.column_dimensions[column_letter].width = 20 ","date":"2021-07-14","objectID":"/2021/07/14/python/:7:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"获取公网 IP # https://stackoverflow.com/a/28950776 # https://stackoverflow.com/a/166589 def get_local_ip(): s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) try: # doesn't even have to be reachable s.connect(('8.8.8.8', 80)) ip = s.getsockname()[0] except Exception, e: ip = '127.0.0.1' finally: s.close() return ip import re import subprocess # https://stackoverflow.com/a/28532296 def is_ip_private(ip): # https://en.wikipedia.org/wiki/Private_network priv_lo = re.compile(\"^127\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\") priv_24 = re.compile(\"^10\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\") priv_20 = re.compile(\"^192\\.168\\.\\d{1,3}.\\d{1,3}$\") priv_16 = re.compile(\"^172.(1[6-9]|2[0-9]|3[0-1]).[0-9]{1,3}.[0-9]{1,3}$\") res = priv_lo.match(ip) or priv_24.match(ip) or priv_20.match(ip) or priv_16.match(ip) return res is not None def get_local_ip_by_hostname(): process=subprocess.Popen(['hostname', '-I'], stdout=subprocess.PIPE) ips, error = process.communicate() if error != None: return \"127.0.0.1\" for ip in ips.split(\" \")[:-1]: if not is_ip_private(ip): return ip return \"127.0.0.1\" ","date":"2021-07-14","objectID":"/2021/07/14/python/:8:0","tags":["Python"],"title":"Python","uri":"/2021/07/14/python/"},{"categories":null,"content":"守护线程： 用于服务用户线程的线程。进程在终止时，不会考虑守护线程的状态。因为如果被服务的线程已经结束，则守护线程没有存在的意义。 用户线程： 进程终止前需要确保所有用户线程已经结束。如果有无限循环的用户线程，则进程会一直等待，无法退出。 守护进程： 守护进程是一类在后台运行的特殊进程。它是一个孤儿进程，其父进程在调用 fork 后退出。这样它就作为一个不受任何终端控制的后台服务运行。 后台进程： 后台进程与守护进程的区别在于，后台进程未完全脱离终端，仍然是终端进程的子进程。后台进程会往终端输出结果，并随着终端退出而结束。 终端： Terminal。物理终端设备统称为 ttyS，与物理设备成一对一的关系。虚拟终端设备为 tty。逻辑终端设备称为 pty。tty 与 pty 和物理终端设备没有直接相关。 ","date":"2021-07-14","objectID":"/2021/07/14/guard-thread-or-process/:0:0","tags":["线程","进程","操作系统"],"title":"进程和线程的二级概念","uri":"/2021/07/14/guard-thread-or-process/"},{"categories":null,"content":"https://shardingsphere.apache.org/document/current/cn/features/sharding/principle/merge/ ","date":"2021-07-14","objectID":"/2021/07/14/db-merge-engine/:0:0","tags":["数据库"],"title":"数据库的归并引擎","uri":"/2021/07/14/db-merge-engine/"},{"categories":null,"content":"游戏引擎： https://github.com/OpenDiablo2/OpenDiablo2 Go: https://github.com/golang/mock https://github.com/hashicorp/consul https://github.com/rancher/rancher https://github.com/micropython/micropython ","date":"2021-07-14","objectID":"/2021/07/14/opensource/:0:0","tags":["开源"],"title":"一些可以用到的开源项目","uri":"/2021/07/14/opensource/"},{"categories":null,"content":"找到板子 先搜索到：https://www.banggood.com/LOLIN32-Lite-V1_0_0-WiFi-and-bluetooth-Board-Based-ESP-32-Rev1-MicroPython-4MB-FLASH-Module-p-1237861.html?akmClientCountry=CN\u0026cur_warehouse=CN 再搜索到：https://smalldevices.com.au/products/wemos-lolin32-lite?variant=5091850682401 然后确定：https://www.wemos.cc/en/latest/ 从 Edit on GitHub 进入 GitHub。 搜索：LOLIN32 Lite pdf 找到： https://www.adrobotica.com/wp-content/uploads/2018/03/schematics_lolin32.pdf https://skule.sormo.no/index.php/eshop/eshop-download-pdf?product_id=274 http://blog.pagefault-limited.co.uk/wemos-lolin32-pinout-vs-wemos-lolin32-lite-pinout https://web.archive.org/web/20190105015433/https://wiki.wemos.cc/products:lolin32:lolin32_lite https://web.archive.org/web/20190105005432/https://wiki.wemos.cc/_media/products:lolin32:sch_lolin32_lite_v1.0.0.pdf ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:1:0","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"板上元器件 ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:2:0","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"Micro USB ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:2:1","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"CH340C （USB 转 TTL） http://www.wch.cn/download/CH340DS1_PDF.html CH340 芯片支持 5V 电源电压或者 3.3V 电源电压。当使用 5V 工作电压时，CH340 芯片的 VCC 引脚输入外部 5V 电源，并且 V3 引脚应该外接容量为 0.1uF 的电源退耦电容。当使用 3.3V 工作电压时，CH340 芯片的 V3 引脚应该与 VCC 引脚相连接，同时输入外部的 3.3V 电源，并且与 CH340 芯片相连接的其它电路的工作电压不能超过 3.3V。 图中 V3 和 Vcc 连接，因此该处使用 3.3V 电源。 ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:2:2","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"ME6211 （线性稳压器） https://pdf.ic37.com/icpdf_datasheet_6/ME6211_datasheet_6920387/ 从数据手册关于引脚的介绍来看，这里使用的是 SOT23-5。 对应选型应该是：ME6211C33MX。 不过我这块板上的是另一个线性稳压器 4B2X。 https://b2b.baidu.com/land?id=153794d074372de7ee5e9f947b68b0a610 ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:2:3","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"CJ2301 （PMOS 管） https://item.szlcsc.com/9042.html https://www.21ic.com/article/878884.html https://blog.csdn.net/AirCity123/article/details/102810255 ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:2:4","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"TP4054 （锂电池充电控制电路） 提供充电过充保护，充电电流设定。 http://www.tp-asic.com/te_product_a/2008-04-10/24.chtml http://www.tp-asic.com/res/tp-asic/pdres/201203/tp4054_42.pdf ","date":"2021-05-31","objectID":"/2021/05/31/micropython-esp32/:2:5","tags":["ESP32"],"title":"ESP32 MicroPython","uri":"/2021/05/31/micropython-esp32/"},{"categories":null,"content":"能力不同的参与者 软件工程里面有很大一部分内容都是在讲不同角色之间的沟通问题，这部份对作为 PM 的同学的要求比较高。PM 是软件工程中受益最大的一个角色。 总体来说， PM 要最终确定具体产品是什么，明确产品的边界，并且在工程中确保价值最大化。 PM 要通过各种方式，引导需求方或者总结需求方的描述，确保自己完全理解需求方的需求，并把产品中最具有价值的几个点列出来。 PM 要把最有价值的几个点准确地传达给程序员，并且确保程序员准确地理解了需求。 PM 要在程序员偏离这些点的时候，把他们拽回来。 一部分人会有一种完美情结，希望在任何一个细节上做到最好。一部分程序员也会有这种情结，这种行为往往导致他们在产品初级阶段创造出远低于预期的价值。 软件工程只是工程思想和方法在计算机软件领域的一个实践。我认为软工课程最重要的应该是把工程思想和方法教给学生，并且让他们通过软件来实践这些思想和方法。 那么在衡量是否价值最大化的时候，应该怎么做？ ","date":"2021-05-31","objectID":"/2021/05/31/how-to-software-engineering/:1:0","tags":["软件工程"],"title":"软件工程","uri":"/2021/05/31/how-to-software-engineering/"},{"categories":null,"content":"不知道从什么时候开始，“努力奋斗”这个概念深深地扎根在我的思想中。我能确定的是至少上初中之前就已经存在了。不管经历了多大的变化，最终都是由这个观念占据主导位置，并且越来越显著。 ","date":"2021-05-09","objectID":"/2021/05/09/value/:0:0","tags":["life"],"title":"价值观简述","uri":"/2021/05/09/value/"},{"categories":null,"content":"等价交换 我以前主要是不断地努力。在看《钢之炼金术师》的时候，“等价交换”这一概念给了我很大的震撼，这很像我一直以来坚持的通过努力换取回报的思想。因此在跟别人聊起动画片的时候，我经常会说《钢之炼金术师》是早期对我影响最大的一部动画。 高中语文老师在临近毕业前让我们班所有人逐个说自己的座右铭，我选择了“等价交换”这个词。老师当时并不能理解我的意思，理解成了我想去做生意。 尽管我通过《钢之炼金术师》了解了“等价交换”这一概念，但是它并没有对这个概念做足够深度的讨论。 在现实生活中，也会有很多看起来明显违反“等价交换”的例子。例如一个学生花了很多时间去读书，但是仍然没有取得相应的进步。 ","date":"2021-05-09","objectID":"/2021/05/09/value/:1:0","tags":["life"],"title":"价值观简述","uri":"/2021/05/09/value/"},{"categories":null,"content":"书本上的“等价交换” 在学习高中化学和物理的时候，得到了一次融合科学理论的机会。 化学和物理知识告诉我们，孤立系统的质量和能量是守恒的。但是从现象来看，一定的质量和能量的输入却往往得不到等量的“我们想要的”质量和能量的输出。这是因为过程中会有损耗，比如物体摩擦带来的能量损耗。换句话说，转化率不是百分百。 这个转化率指的是“我们想要的”输出同输入的比例。它在客观上是百分百的，但因为转化的结果中不是所有的部分都是我们需要的，所以它在我们的认识中不是百分百。 当时的我如果局限在课本中的知识，而没有尝试超越课本的话，那么我也就没有办法在那个时候加深了对努力的意义的理解。 现实中一件事情的结果有我们想要的部分，也有我们不想要的部分。如果一个人没有意识到这一点，并将其加入到日常的思考中，就会构建出一个想象中的世界。这个世界没有摩擦力，不会发生损耗，即所有事情的转化率都是百分百。但是现实中并不是百分百，这就会产生一种痛苦，并且在结果到来之时，去抱怨自己的转化率不是百分百是因为别人的阻碍，以及去谴责为什么别人不能做到百分百的转化率。 在这群人中，有一部分极端的人。他们只能在理想的环境下发挥价值，这使得在现实世界中，他们无法发挥出平均的价值，甚至会阻碍其他人发挥价值。 ","date":"2021-05-09","objectID":"/2021/05/09/value/:2:0","tags":["life"],"title":"价值观简述","uri":"/2021/05/09/value/"},{"categories":null,"content":"《资本论》中的“价值” 我在前面的总结文中提到过我在 1 月 3 日开始看《资本论》，并且在 3 月 1 日看完第一卷的第一篇【商品和货币】和第二篇【货币转化为资本】。《资本论》的第一章就讲到了劳动产品的价值和有用性的关系。 如果物没有用，那么其中包含的劳动也就没有用，不能算作劳动，因此不形成价值。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 54 页 等价交换的“价”指的是“价值”。劳动（体力劳动、脑力劳动）只有有用的那部分才会产生价值，并且通过交换得到相应的回报。所以劳动的转化率就非常重要。 我们读书、总结、实践，都是为了提高对客观世界的认识，从而让我们的劳动转化率不断地提高。用理论的语言来说，就是提升劳动力。 一个人若处在封闭环境，他就不知道自己的劳动有多少价值。但大多数人都生活在社会中，他就可以通过和其他人比较来认识自己的劳动的价值。 当我们从个人的维度提升到社会维度来考察时： 社会必要劳动时间是在现有的社会正常的生产条件下，在社会平均的劳动熟练程度和劳动强度下制造某种使用价值所需要的劳动时间……只是社会必要劳动量，或生产使用价值的社会必要劳动时间，决定该使用价值的价值量。 —— 《资本论》第一卷，人民出版社 ","date":"2021-05-09","objectID":"/2021/05/09/value/:3:0","tags":["life"],"title":"价值观简述","uri":"/2021/05/09/value/"},{"categories":null,"content":"上学的意义 如果要回答仍在读书的同学们关于上学的意义，那么从上述内容来回答，可能是一个比较能让人接受的答案。 ","date":"2021-05-09","objectID":"/2021/05/09/value/:4:0","tags":["life"],"title":"价值观简述","uri":"/2021/05/09/value/"},{"categories":null,"content":" 本文原是前一篇文章的一部分，后来觉得内容较多就拆开发布。本想第二天就发，但发布之前做检查润色时，对前面部分不满意，就重新写了。经过反复写和删，改了近三千字，仍然不满意。只好先把原先写的发出来，后续写出满意的开头时，再重新发布。 感性和理性的关系我思考过很多次，直到今年才得到一个明确的结论，也就是标题所写的“理性思考，感性选择”。但它并不是一个很精确的表达，因为理性和感性不是完全对立的。 接下来讨论的是思考的时候情感因素的占比对思考的影响。理性和感性只是作为情感因素浓度的一个简单划分。这里的“理性”指的是在思考中情感因素占比较小，“感性”指的是在思考中情感因素占比较大。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:0:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"理性人和感性人 根据解决问题的方式不同，大致可以判断一个人是比较感性的还是比较理性的。感性的人对人的关注会多一些，理性的人对事物的关注会多一些。 如果对周围人观察总结，会从外部表现中捕捉到“如果感性增强，那么理性就相应减弱，反之亦然”的结论这种特点。但是如果仅根据一个人表现出来的感性或者理性程度来判断这个人，很可能在不同的具体的事情中得到自相矛盾的结论。 我们在讨论问题的时候，总是会把过去的经验作为基础。经验会影响我们对事物的判断，进而做出理性或感性的行为。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:1:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"经验与生存 经验分为两部分，一个是个人经验，另一个是由众多个人经验的共同点总结而成的共识。 人类这个群体在远古时期就从被动地接触新事物转变为主动地探索新事物。这使得过去的经验会随着人对事物的进一步探索而暴露出其无法完全反映客观事实的一面，于是需要通过对旧的经验中的缺陷进行补充修复(带有一定的滞后性)而令旧的经验得到发展，形成新的经验。 自然界不喜欢极端的事物，过于主动探索新事物和强烈拒绝新事物的人类在过去都被自然界淘汰掉了。存活下来的人兼具探索和保守，并随着时间的推移不断淘汰掉两者比例不符合社会和自然发展状况的人。存活的人总体上展现出对客观世界了解越少，越拒绝新事物的倾向，这是刻在基因里的人类生存本能。 不受基础生存威胁的人，才有加速发展的可能性。但人类群体至今仍未完全摆脱基础的生存威胁，全球范围仍然有将近 8.9% 的人（6.9亿）处于饥饿状态[1]。 就算不受基础生存威胁，仍然会有一些人或主动或被动地将自己置于生存威胁之中。这其中的一部分人在解决自己创造的生存威胁之前，也难以得到发展。值得一提的是主动为自己创造生存威胁的人，一般是为了享受远远超过自身能力的资源。 大部分人只是想过普通的日子，也没有进一步发展的需求。于是只有少部分人会把一部分原来用于享受的时间抽出来，做一些其他事情以增加对客观世界的了解。 这样几种不同的客观现象综合起来的表现是在大部分人会下意识地拒绝接受与自身经验不一致的事物。 对于一个认为扩展自身经验是痛苦的人来说，他所处的环境必须是静止的，不再发展的。这就要求这个环境独立于外部世界，并且这个环境里的所有人都不再探索新的可能，一旦有人由于客观世界的随机性获得了新的可能，也必须隐藏起来。否则客观世界的随机性总是会让一些人获得新的可能，也总会有人想要让全人类得到发展，进而推动一系列变化，导致过去的经验不得不更新。这对于那些不愿意更新的人来说，是非常痛苦的，因为这可能意味着他们的生存面临着威胁（尽管很可能是自以为面临威胁）。 一个人如果想要持续地生存，就必须提前或者在必要的情况下，超越过去的经验。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:2:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"超越过去经验 一个人的经验会束缚自身，但仍然保留有一个向上突破的窗口。那就是为了满足自身的除了生存之外最重要的需求而不断追求进步。一个人在某一方面已有的经验决定了这个人通过努力所能获得的回报的上下限，使用过去的经验总是会达到上限，人们称之为“瓶颈”。想要达到更高的水准，就要求人必须超越过去的经验。 这让我想起了一篇讲《平衡球》这款游戏历史的文章[2]，里面有这么一段话： 平衡球的记录党主要分为两派。一派的主要打法是剑宗，破记录的方式主要是挖掘尝试新的走法捷径，去压缩路程，以达到降低苛求细节。一派的主要打法是气宗，破记录的方式主要是把细节执行到前所未有的干净程度，去破记录。 用这段话来解释我前面那句话就是：剑宗决定了上下限，气宗只能将水平提高到无限接近这个上限，无法突破。 想要超越过去的经验，有被动和主动两种类型。 被动就是仍然按照过去的经验努力，外部环境出现巨大的变化时，被迫随着外部环境的改变而变化。这种变化对个人来说可能带来巨大的好处，也可能带来巨大的坏处，而抽到下下签的人面临着生存的威胁。这种类型的人如果想要享受更好的生活，就很需要外部环境的动荡。但大部分人都是普通人，得到的往往是坏处，因此虽然有机会获得巨大的好处，仍然不愿意外部环境发生变化。但有一部分人可以确定地从动荡中获取巨大的好处，这部分人会基于过去的经验尝试打破外部环境的稳定，制造动荡。这不是本文讨论的重点，不继续展开。 主动还可以分为主动探索和获取他人经验。 主动探索是指不断地暂时屏蔽掉其中一小部分过去的经验，重新探索。这是人类这个群体不断发展的重要基础。 获取他人经验可以是和他人交流或者看书。一本书的内容是作者经验的总结，作者的经验包括自身的经验和从其他书本获取的经验。这种经验不断地累积，直到我们去看这本书，获取人类积累了几百甚至上千年的经验。 在这些经验中，有一些是由于世界的发展而出现严重偏差的，需要抛弃。有一些经验拥有很强的适应能力，允许不断地补充和发展，以反映真实的客观世界规律。这种拥有很强适应能力的经验，我们称之为科学。 人总是被过去的经验影响判断，但一个人就算穷极一生都无法将客观世界的所有可能性都完整地实践一遍，因此这些经验很可能随着世界的发展而产生偏差，这提醒着人们必须怀着虚心学习的态度面对已知或未知的事物。 对一个事物的认识是有多个层次的，但大多数人都只停留在较为表层的认识，在获取表面现象再更深入一层的认识后就满意，不愿意再更进一步。这样做的要求较低，只需搜集很小的一部分资料就可以得到另自己满意的结论，虽然这个结论往往是不可靠的。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:3:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"更上一层楼 我们被束缚住了呢？我找到了其中一个因素，即思考时的负面情感因素。 回想一下，当一个人做错了事情时，面对其他人的批评，他的第一反应是什么？只要素材够多，就能得到一个结论：大多数人都会下意识地否定别人的批评，并且伴随着强烈的情感（或者说情绪），情绪足够强烈时甚至说要赌上自己的性命。 情感因素本质上是生物求生本能的外在表现，这不是什么问题。问题在于人由于缺少对事物的全面认识，会误判自己的生存可能性，产生不必要的正面或负面的情感。 情感因素的存在本来是有利于人的延续，但缺少认识可能会使得情感因素对人的延续产生负面影响，其中一个负面影响就是阻止人对事物更进一步地认识。于是认识的缺少增强了情感因素，从而阻止认识的增加。如此反复，使得一个人对事物的认识被锁死在一个层次，只有当受到外部强力因素干扰时才有望突破。 有些人据此得出结论：那么完全地把情感消除掉就好了。但情感因素有利于人类的延续，把情感消除掉岂不是主动地毁灭人类？ 仔细想想，我们在做这个讨论的时候受到了什么经验的束缚？ 我们通常在讨论思考的时候，隐藏了一些默认的前提： 思考的时候只有一个阶段，只能在这个阶段内分配情感因素的比例。 一个人对于情感因素的比例的投入只能是一个长期处于稳定的状态。 由于去年年底就已经第二次进入“归元”的状态，在之后的一段时间就有了大量对比自己进入归元前后状态的机会。在对自己的观察和思考总结中，我发现了这些奇怪的前提。 为什么不是将理性和感性拆开到两个阶段呢？先通过接近于完全的理性思考，尽可能多地找出选项，然后再使用感性去选择。即理性思考，感性选择。 在第一个阶段即理性思考阶段，尽可能地剥去情感因素，保留最低限度用于辅助理性思考。因为有些事情需要逐个站在当事人的角度，用他们的情感去分析。 在这个阶段，由于没有多余的情感因素的参与，所以能够利用自己所有的知识和经验去尽可能多地找出选项。如果在第一阶段就加入过多的情感因素，那么很多选项都会被情感因素排除。总的说，这里的情感因素起到了挖掘更多选项的作用，决不能起到过滤选项的作用。 一旦在第一阶段结束之前，由于情感因素的影响，有意无意地把最符合客观事实的选项排除掉，那么就只能在错误的选项中选择。如果人总是这样做，那么他生命的绝大部分时间都在做错误的选择，即生活在自己想象的充满错误的世界中。 如果排除了多余的情感因素，那么在第一阶段结束后，面对的将是包含较为符合客观事实的选项。感性可能会把它排除掉，但是它至少出现在了选项列表中，感性会捕捉到这个因素。如果出现这种情况，只要做判断的人具备了搜集原始资料和解读能力，在通过搜集资料和解读得到接近事实的选项后，就会发现他的感性总是在第二个阶段排除掉符合客观事实的选项。那么他就会提高警惕，碰到类似选项的时候，不急着下定论。 我在得到“理性思考，感性选择”这个结论时，想起了之前一个新闻。我一时找不到这篇新闻，后续找到了再补上。新闻说是有个外国人大脑中负责情感的部分出现损伤，结果他就难以做出选择。 在与他人交流或者观察网络上不同人之间的交流时，一旦控制让自己在一段时间内不受到情感的影响，就能够一定程度上看穿对方一句话背后的真实意图。一方面，可以以此知道身边的人需要什么，然后去满足他们的需要，让他们更加幸福。另一方面，那些企图对社会造成破坏的人的意图就会显露出来。 因此在对事物认识到一定程度之前，要先抑制情感因素的表达，并在认识到一定程度之后加入情感的表达。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:4:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"作用之一：防止被洗脑 分阶段的作用还有很多，我再举出其中一个，那就是防止被洗脑。很多人分不清教育和洗脑的区别，所以我先说明我说的洗脑指的是利用他人负面情感制造出信息茧房并使得被洗脑者试图阻止社会发展甚至破坏已有的发展成果。 前文说了，人在思考的时候，会因为情感过早地参与而把事实排除掉，以至于只能在错误的选项中选择。现代洗脑的一个常用方式是调动起人的恐惧和愤怒的情感，让一个人事先将某个对象的真实意图过早地排除掉。一旦取得一定效果，无论这个人的学历有多高，都就会自觉地事先将符合事实的信息排除掉，让自己的信息茧房更加牢固。 阶段分离只是防止被洗脑的第一步。在理性思考阶段，要尽可能多地搜集一手信息，然后进行理解和判断。在全球化的背景下，我们往往会接触到其他国家的信息。有些人会利用信息接收对象不懂得外语，或者懂外语但不会检查原文的弱点，进行断章取义或者通过翻译扭曲原意。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:5:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"讨论问题的几个阶段 对于一件新的事物，怀疑和取证本应该是一体的。人们会下意识地怀疑。但大部分人会停留在怀疑阶段，不会主动地去搜集资料并做逻辑上的判断。他们会质疑权威，而不是科学地质疑权威。这使得容易相信基于谎言的结论。 对于没有经过专业训练的人来说，在即时讨论中很容易被对方的话术所击败。从我的观察得出谈论时大致有以下五个阶段： 情感占大部分的讨论（争论），通常是基于局部的客观事实互相反驳，没有看到全貌。这种是为了对抗，而不是为了接近真理。因此矛盾会扩大化。这也是微博热搜评论中最常见的。 在发现第一阶段会被对方的话术利用后，在讨论前有意识地提醒自己不要被对方负带情绪攻击的反驳影响到。 第二阶段的问题在于对方发现自己因为我方给出覆盖面广的事实以及较为全面的逻辑而完全处于下风时，会使用反串并极端化我方的表述和附带强烈的情绪来激怒我方。一旦被对方激怒或者急于解释，将会落入对方陷阱。 在面对具有极大恶意的对方时，有一句教员说过的非常实用的话可以拿来用，“你打你的，我打我的”。也就是掌握主动权，避免掉入对方的陷阱。在公共场合的讨论，不只是为了证明对方的观点不符合实际，更重要的是让观众从我方的观点中获得思考。真正为人民群众发展考虑的人，自然会从我方观点中获得启示，也会从我方观点中得到对方在事实和逻辑上的漏洞。 对方的应对策略之一是，嘲讽我方不敢正面回答问题。由于真理是具体的和有条件的，要把一件事情完整地说清楚很难，需要根据受众的理解水平做相应的调整，观众也不一定喜欢听。这时可以尝试分析对方问题背后的深意，然后解构，可以选择是否揭露对方的恶意，然后把问题重新表述为具体的发展的，最后对这个具体的发展的问题作出回答。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:6:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"可持续性 虽然我得到了把分阶段作为思考的工具，但是我不能确保在任何场景下都可以做到。在把无意识的分阶段思考转化为有意识的行为后，又需要把有意识的分阶段思考转化为无意识的行为。如果不重新固化为无意识的行为，很可能丢失掉。 固化的方式就是频繁地使用。这也能检验是否已经丢失了这一工具。 最简单的方法是看微博的热搜。我让自己每天至少看一次热搜及评论，检验是否仍然有使用分阶段思考工具的能力。 第二是和同事讨论问题。和同事出去吃午饭或者晚饭的时候，总是会谈一些话题。通常就是吹吹牛逼，吐槽一下新闻。偶尔会有和其中一两个同事看法不一致的时候，如何处理这种情况是一个问题。 根据上文可以得出，两个人看法不一致很可能是获取的信息不一样，或者思路不一样，那么首先就得同步信息或者思路。 于是当看法不一致的时候，我会首先尝试换一换思路，看能否找到一种思路能够得到与对方相同的结论。如果找不到，那就说明我正在基于不全面的思考方式得到结论。会有什么问题？一旦只满足于当前的思考方式而不是试图了解其他人的思考方式，就会把自己的思考锁死在当前的层级。如果找到了呢？那也不能说明这是能得出这个结论的唯一思路，还需要验证对方是不是用这个思路在思考。因为如果不这样，那就和“公知”们的“非此即彼”的思维方式是一样的了。 由于此时交流的目的不是试图否定对方，而是一种探讨，所以不需要带什么强烈的情感。因此就算双方意见非常不一致，也不会演变成争吵。同事会基于本能地认为我是想否定他们，所以有时候会带有强烈的情绪。但是只要我不带有强烈的情绪，几次下来他们就知道我是在认真地讨论问题，并且是真心想了解他们的想法。 不过在最初使用分阶段思考这一工具时，仍然要注意不能有一种认为能做到不带强烈情感地讨论就比对方强的居高临下的情感。因为这种傲慢会阻止人进一步发展，而且还会对与交流的参与者之间的关系有负面的影响。 第三是和那些带有极大恶意的人讨论问题。同事通常不会去学习话术，也不会故意刁难，但是他们就不一样了。这些人学习了很多针对人的话术，会把对事情的讨论引导到对交流参与者个人的讨论。他们掌握了激怒一个人的多种方式，使用情绪让对方失去讨论时的理智。 我之前被拉入到一个群，里面有几个这样的人。我观察了一段时间后，尝试和他们进行了几次交流。最初几次发出疑问没有得到回应，我就思考是不是因为没有怎么带情感。后来我尝试了带一些情感，很快就得到回应了。如果我带了更多情感，让他们以为我在反驳他们，那就会有几个平时不冒泡的人出来阴阳怪气，几个经常说话的也会有比较大的反应。在讨论问题的时候，他们会预设我的立场，然后当我想把问题具体化讨论实际时，要么得不到回应，要么被批评难以交流。我其实不想要这样的对抗式交流，所以后来主要是观察总结，有时会礼貌地向他们请教。我想了解他们的思路，以理解他们所想要的究竟是什么。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:7:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"结尾 “分阶段思考”这个说法听起来有点严肃，所以还是用比较容易理解和接受的“理性思考，感性选择”这个表达吧。 ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:8:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"参考 [1]: http://www.fao.org/3/ca9692en/CA9692EN.pdf (2020《世界粮食安全和营养状况》 by 联合国粮食及农业组织) [2]: https://www.zhihu.com/question/25512253/answer/1362582506 (如何评价《平衡球》(Ballance) 这款游戏？ —— 知乎) ","date":"2021-05-09","objectID":"/2021/05/09/think-reasoningly-but-choosing-emotionally/:9:0","tags":["thinking"],"title":"理性思考，感性选择","uri":"/2021/05/09/think-reasoningly-but-choosing-emotionally/"},{"categories":null,"content":"上一篇说到记录生活的四个粒度：按日记，按周记，按月记，按季度记和按年记。原定计划是按周记，但实际上没有实行，退到按季度记也没有实行。现在将今年的前三分之一（1~4月）补上。 我本来是想把一些思考总结到这篇记录里面，但是写着写着发现内容太多了。每个小结基本都能单独作为一篇文章，因此打算拆开分批发出。 这一篇我打算先检查原定计划的完成情况，并进行适当调整，然后再补充一些新的内容。 计划的完成情况和调整 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:0","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"运动 原定计划：保持每周三次运动，仍然以跑步为主。 完成情况：符合预期 2021 年 1 月至 4 月共运动 44 次，其中 43 次跑步，平均每周运动 2.7 次。原先预期的两个风险（春节、加班）都没有造成影响。 这段时间有 39 次是在跑步机上跑步，速度和之前说的一样都是 14km/h（配速 4'17\"）。 有 38 次跑步时的平均步频是 180 步/分钟，其他 5 次都在 170 步/分钟以上。 在 1 月底开始可以在跑步机上跑 4 公里。到 4 月结束，共有 29 次跑 4 公里以上。原定 4 月进入 5 公里的阶段，但没保持住。我在 3 月下旬尝试跑 5 公里，在成功之后仍然按计划跑 4 公里。直到 4 月才开始继续尝试 5 公里。其中 4 月中旬成功 1 次，下旬成功 2 次。四个月共跑了 4 次 5 公里。 4 个月期间，有时候状态差只跑 3 公里，有时甚至是只能跑 2 公里。从开始跑 4 公里后，少于 4 公里的共有 5 次。这倒不是什么问题，对我来说，只要有运动就行。不过进入 4 月，发现状态下降得比较多，不知道是否跟气温有关系。 如果在周一、周三、周五有聚餐（通常是周五），我会无压力地鸽掉运动，也不会去补。如果是周一有活动，我会在周日提前运动，不过到目前为止只有一次需要这么做。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:1","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"阅读 我的计划是看以下几本书： 吕思勉的《极简中国史》 九边的《西方博弈往事》 马克思的《资本论》 《领域驱动设计》 《实现领域驱动设计》 当前阶段主要看的是《资本论》。我是 1 月 3 日开始使用手机的 Kindle 软件以及 Kindle 设备上阅读这本书。在 3 月 1 日看完第一卷的第一篇\\【商品和货币】和第二篇\\【货币转化为资本】，然后回头再看一遍。 第一遍主要是总体有个印象，有一些不太理解的地方直接跳过去了。在第二篇结束后，我认为很有必要先回头再看一遍，而不是继续往下看。 在 3 月 1 日后，回头再看第二遍。这次看，会把一些有助于理解的句子标记起来，把一些理解后的笔记记录进去。现在看到第一篇第三章。 我打算近期发一篇总结。五一路上写了一小部分，一千多字。还需要不少时间完善。 有很多人对《资本论》有误解，说阅读《资本论》或者马克思的著作，会有危险的思想。我反倒觉得会散步这种论调的，主要是那些只想着剥削的资本家，或者为他们说话的人。另外还有相当一部分人是受到这些人话语的影响而跟着一起说的。认真阅读过和理解《资本论》的人，会认为读这本书无论是对自己还是对国家都是有好处的。 我本来在 B 站上关注了个讲马克思和《资本论》的 up 主。这个 up 主貌似是美国一所顶级大学的学生。但我看了视频之后，认为视频的价值有限。up 主跳过了最重要的资本论的第一篇和第二篇，大谈特谈后面的内容，句子绕来绕去。很容易造成观看视频的人一知半解，然后拿着模糊的结论当真理。不过也有可能是我的理解能力有问题，所以我打算在看完《资本论》后，再回头看这些视频。 等把总结写完后，我会先去看其他书。 当前读书这部分达到预期。虽然中间有几次觉得有中断，但现在基本每天都有看一些。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:2","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"作息 作息可以说是完全没有改善。不过最近起床时间提前了。睡觉时间有提前的趋势。 起床时间提前不知道跟我在睡觉时戴上眼罩有关系，需要花一段时间实践对比看看。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:3","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"考高中数学或物理的教师资格证 本来计划是上半年报名的。但是折腾半天发现没法报名这边的考试。因为报名需要居住证，而我刚过来才半年，居住证得一年才能拿。 老家那边的也不是不能报，但来回动车太久，顶不住。 报名下半年的吧。下半年的笔试报名时间是 9 月。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:4","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"开始学习投资，并做适当的实践 原先计划是下半年开始，所以现在没有什么准备。 最近经常听同事聊各种“币”的涨跌，我心里没有任何波动。毕竟买这种币不符合我的价值观，涨一万倍都跟我没关系。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:5","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"技术上的提升 哭了。技术上的提升现在基本都靠项目，不过也还算有提升。 其他的是了解了一些知识点，例如： Python 的 GIL 守护线程、用户线程、守护进程、后台进程、终端 CDN 的 DNS 解析、与调度的关系 Linux LVM 锁的硬件层支持 这些得花一部分时间整理出来放到博客上。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:6","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"博客 符合预期。 1 月发布了篇跟时间戳相关的博客 2 月就是 2020 的总结 3 月有一篇终端的发展史，以及 6 篇跟硬件开发相关的博客 4 月写了两篇跟软件工程相关的博客 终端那篇严格来说还没写完，还有一个跟后台进程相关的内容。 这篇发布的时候都六月了，但这是五月份写的，只是后来多了一些内容，没有先把这些发出来。我觉得应该算到 5 月去。 2021 的前三分之一 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:0:7","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"春节 这次春节和往年不太一样。主要是我刚经历了质变，刚好可以试试看在春节这种需要和很多人聊天的场景能发挥出多大的作用。现在回过头来看，已经超出我以前的预期了。 读大学以前，我反应慢、胆子小、不会说话，于是就选择尽量少说或者尽可能独自找个地方玩手机。不过在经过十年左右的安排后，现在已经达成了最初设定的目标。现在反而会去主动找别人聊。 虽然我还是单身，但七大姑八大姨并没有为难我，我和她们聊得很开心。在聊天过程中，已经让她们知道我现在成熟到可以自己做出合适的选择了。另外有的亲戚帮忙介绍女生，除非有其他事情安排不来，否则我肯定乐于多认识一个人。安排不过来的，我也当面对亲戚表达了感谢，希望有机会还帮我介绍。 原先计划春节期间写几篇技术博客，不过机会不多，就没写了。以往春节都是倾向于待在家里，但这次倾向于出去外面。光是找亲戚就花了绝大部分时间。反正我是挺享受这种和人交流的过程，特别是还能保持和亲戚关系。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:1:0","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"碰到了传教的人 三月份某个晚上，我因为在公司的健身房跑步，到九点才回去。路上碰到有个教徒向我传教，我听了大概一个小时，感觉 ta 还需要多练习。 本来是想一开始就走的，但是 ta 比较热情，而且我想了想，这正是一个了解他们想法的机会，就停下来听了。 我做了两个准备： 暂时屏蔽了负面情绪的生成 开足脑力使用逻辑思考 第一点是因为好像有些人碰到这种事情会自然而然地产生抗拒的情绪，可能觉得浪费时间，甚至会觉得这是一种试图洗脑的行为。洗脑一般是要让人产生负面情绪的，屏蔽负面情绪的生成可以在一定程度上防止被洗脑。屏蔽以后，可以从对话中察觉到一部分 ta 想激起负面情绪的话术。 第二点是为了了解 ta ，当然也有保护的作用。我很想了解所有人（无论是否信教）对一个问题的看法是什么。这样我在看问题的时候就可以增加一个角度，在必要的情况下可以照顾到他们的感受。 毕竟第一次碰到，谨慎一些。 在聊的过程中， ta 几次想要调动我的负面情绪。例如： 说我加班这么晚（后面那句是啥我忘记了） 说我压力应该很大 说到金钱的邪恶，物欲横流什么的 问我是不是有很多缺点困扰着自己 问我是否有想清楚想成为什么样的人 听到第一个问题，我就联想到 ta 在这个时间点传教客观上可以提高成功率。加班的年轻人确实可能会有点情绪。而且加班也累，思维可能一时理不清晰。我不确定 ta 是否有这个考虑，但我刚好是不是加班，而是跑步。跑步不仅有助于产生正面的情绪，而且还能让头脑清晰。我回答我是刚跑完步，不是加班，ta 估计愣了一下，这话不太好接。 我觉得我现在的压力反而变小了。我正想办法给自己找压力，不然过多地投入到对未来的投资而陷入短期危机，倒是反而会影响未来的发展。 我这个人绝大多数情况都是买适合我的东西。不会为了炫耀而多花钱买我用不到的东西。我给我爸妈买的东西也突出一个实在，满足他们的关键需要。如果我拥有了远超出我为社会创造的价值总和对应的金钱，大概率是存着或者捐给学校。 关于缺点这事，ta 举的例子是自己以前很自大（ta 是国内排名前几的某 985 大学的研究生），容易愤怒，压力很大，信教后好了很多。我想说我们俩刚好是相反的，以前我倒是会因为自己能力不行而感到自卑——比如觉得配不上女生而不敢去追，不过我已经自己解决自卑这个问题了。但感觉会伤到 ta，所以实际上没说。我现在的缺点其实是专业能力不够强，和我给自己定下的目标相差非常大，不过这得靠努力学习和工作去提高。不是信了教我就能写出非常牛逼的代码或者设计非常牛逼的系统的。 其实我是很好奇 ta 具体因为啥才有这些缺点，说不定我能帮 ta 想办法从根本上解决问题。不过想了想觉得不妥，这可能会反而给 ta 带来困扰。没有主动寻求帮助的人，是需要谨慎选择帮助的。要避免自以为是地认为只要是为了对方好，就能擅自去帮助对方。到时不但帮不上，反而还会扯后腿。 我不太清楚别人被问到有没有成为自己想成为的样子时，会有什么样的情绪反应。我之前可能会觉得自豪，毕竟我去年年底刚刚成为我小时候想成为的样子。现在虽然也觉得开心，但有更开心的事情，也就比较平淡了。我觉得在那时候问我这个问题，真是撞枪口上了。我大概回答的是“我十年前就已经思考清楚这个问题了，然后通过十几年的选择和努力，现在提前成为了我以前所设想的样子”。我不清楚这个回答对于一个信教的人来说，会有什么影响，只是看 ta 似乎没有情绪上的波动，还是希望不会影响到 ta。 ta 后来提到本来自己也不信教，但是第一次去感觉教堂很震撼。对于这个现象，九边之前的文章有专门写过。 后面 ta 还拿出一本仔细保护好的经书带着我看，我认真看认真听。同时思考让 ta 相信这些的条件是啥，但是因为没法关联到类似的体验，一时想不出什么东西来。以前在知乎上看到有答主说他大学舍友看了某本经书后，如获至宝。其实我一直蛮好奇的，只是我需要在确保自己有足够的分析能力和自我管理能力的时候，再去阅读。 前后大概一个小时，基本都是我在认真听，认真思考。中间有尝试问了一两个问题。如果是为了解决问题而提出问题，那么问题和回答通常是很具体的。但我发现对于这种问题， ta 不是正面回答，而是通过抽象地讨论回避深入思考和具体回答。我觉得我再问这种问题，可能会吓跑 ta，就没有继续提了。 最后我们互相道谢，结束这段令双方都感到愉快的对话。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:2:0","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"硬件模块 我一直想在硬件上做点东西出来，用硬件可以做出很多对生活有帮助的东西。以前想做一个用手机控制关灯的设备，但是往模块写入程序后愣是没有反应。后来也不需要了，就一直没碰。 这次因为我想减少使用线连接到电脑的设备，但是又不好把我这机械键盘换掉，于是就想着把有线机械键盘改装成蓝牙键盘。 我一开始是想买蓝牙模块做，因为这样总体比较便宜。但是比较麻烦，要学一些硬件知识，比如模块各个引脚的使用，以及通信协议。于是先用我手头上的树莓派试试效果，到 GitHub 上找到别人写好的代码跑跑看。用是可以用，但是不够完善。 想想还是狠下心来买蓝牙模块，顺便了解一波硬件知识。 我的思路是把蓝牙模块和 USB Host 模块连接在一起，机械键盘使用 USB 线连接，然后蓝牙连接到电脑。给模块配一个电池，就可以用了。 最开始查了很多别人写的相关文章，但总体上都是蓝牙模块 + Arduino + USB Host Shield Mini。我想如果使用蓝牙和 WiFi 一体的 ESP32，那是不是就可以去掉 Arduino 了？ 由于有上次失败的经历，这次直接购买开发板。关键是开发板也各不相同，最开始买的开发板没有详细资料，还得我自己看模块引脚和线路板的连接线，找到每个焊盘对应的含义。 在了解了各个模块的作用以及引脚的连接方式后，写入程序。结果发现手机和电脑无论如何都找不到蓝牙模块的蓝牙。重新买了一块 ESP32 的其他类型的开发板，刷进去还是没反应。 于是想着是不是得用 AT 模式开启，结果刷入 AT 也用不起来。 又卡住了。以后再尝试看看。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:3:0","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"家庭 四月份跟爸妈去景区玩了一趟。回忆了一下，以前去玩的时候，总是会和其他人一起。人一多，总是要多花些精力在和其他人的交流上。 和父母一起的三人游，额外增加了很多幸福感。以后还会增加一个人，然后再增加一到两个人。 个人的奋斗在多个方面或层次都有一定的意义。作为其中之一的个人方面，确保这种幸福生活的可持续性是不断奋斗和进步的很重要意义。 有的人在工作一段时间后，会过于注重事业而忽视家庭，或者过于注重家庭而忽视事业。而我要实践的是两者兼顾。这很难，但我有多少次是在人生道路上选择简单的路线呢？既然承受能力强，那就尽量选择艰难但能获得更大收益的路线。 我爸以前也是把大部分精力放在事业上。近几年通过我们的努力，现在我爸经常带着我妈去各种景区玩，酸到我了 555… 接下去就是通过规划和实践，达到两者兼顾。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:4:0","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":null,"content":"软件工程 四月份写了两篇软件工程相关的博客。起因是看到软工老师分享了一篇北航软工课程学生团队的软工博客。第一篇是写我对这篇博客的理解，以及加一点评论，没想到就有三千多字了。发表后得到了一定的肯定，说明我至少不是在误人子弟，还好还好。 北航的同学看完后提了个问题，我发现这个问题比较大，不太清楚应不应该回答。于是去找我大学时的软工老师征求意见。他看完我第一篇博客后，给予了对我来说很高的评价。我想既然如此，就专门写一篇博客回答这位同学的问题吧。 我一开始没想着大概写多少，但是列出大纲发现涉及的内容还蛮多的。不知不觉写了几千字，赶紧收住，不展开太多。最终写了将近八千字的博客作为回答。 由于写这些内容需要花的时间比较多，比如第二篇花了我将近一天的时间，于是决定今年还是少写软件工程的内容比较好。 软件工程是工程学在计算机软件领域的具体应用。我们非常幸运地可以通过软件工程去学习人类智慧的结晶，即工程思想。计算机软件领域的快速迭代，提供了一个很好的体验机会。我们可以从开源中直接接触那些最具有智慧的人的实践，了解一个伟大产品的发展过程。 我不太清楚其他工程学的具体应用（例如生物医学工程、土木工程、水利水电工程）是否也有类似的机会，但我可以肯定的是软件开发领域的机会非常多。 有的人会说，软件开发工程师只是一个业内发明出来用于抬高程序员逼格的称呼。这其实不是本意，只是可能被误用了。工程能力强（而不是写代码能力强或是算法能力强）的程序员，确实应该称为软件开发工程师。 在我的理解中，工程就是想把理论合理地与现实结合，提高现实资源的利用率，并把实践的结果反馈到理论以帮助理论发展，从而进一步提高现实资源的利用率。我当时在博客中写下我对工程的目标的理解： 工程的目标是：在当前社会环境中，工程实施者使用有限的资源，把能创造的价值最大化，并将其凝固在一个产品里面。 我不太喜欢把自己局限在一个狭窄的范围内思考，总想把事物的发展联系起来。在过去的思考中，我想到了社会的发展不也是一个工程么？ 人们对社会资源的理解程度不同，对社会资源的转化能力不同。我们由于自身能力水平的限制，以及信息量的限制，导致很容易误判。已经拥有一定量的资源时，有的人就想要开始享受，而不去管现有的资源是否能够长久地保持稳定或者增长，即可持续性。 现在网友们老说兔子摸着鹰酱过河，都快把鹰酱摸秃了。鹰酱在拥有丰富资源的时候没有为可持续性付出足够的行动就开始享用维护成本远高于时代的好处，以至于不断地走下坡路。我兔为了避免这种情况出现，就算是挨骂也要不断发展。 对于个人也是这样。在我的意识中，有一个“人生工程”的概念。人生工程是对在哪个阶段应该做哪些事情以及做到什么程度的探索和实践。我在软工博客里写到现在领导要是给我一个不怎么写代码的职位，就算是加薪我也不干。正是因为我认为我还没有在写代码上有足够的积累，一旦现在错误地高估我已有的资源，那么未来一定会碰到大问题。 工程学是一个很大的话题，不过要是想通了“为什么在客观上人类一定会不断进步”，就很好理解了。 ","date":"2021-05-09","objectID":"/2021/05/09/2021-life-part1/:5:0","tags":["life"],"title":"人生工程 —— 2021 三分之一","uri":"/2021/05/09/2021-life-part1/"},{"categories":["马克思"],"content":" 本文从 2021 年 5 月就开始写，8 月中旬写完初版，之后修改润色花了不少时间。一直拖到现在还没做好调整，例如上下文逻辑衔接还没全部做好，但我还是决定先发出来，以后再慢慢改吧。 自从开始阅读《资本论》后，我就认为一个人无论如何都应该阅读《资本论》的第一篇，因为阅读这部分内容会对价值观产生积极的影响。 第一篇是整本书的基础。如果没有理解第一篇就学习或讨论《资本论》后续的内容是没有多少意义的。这一篇里面共有三章，分别是「商品」、「交换过程」、「货币或商品流通」。只看标题会感觉很枯燥，特别是这些内容在中学学过。学校教材受到了篇幅的限制，需要用简洁严谨的文字表述，会显得枯燥。《资本论》则不受限制，可以详细解释各个概念，但也因此整本书厚到让人望而却步。读者可以把目标设定为阅读并理解第一篇就算读完《资本论》，以减少本能的抗拒心理。这也就是为什么我只会建议别人看《资本论》的第一篇，而不是整本《资本论》。 我写这一篇主要有三个目的： 解释“等价交换”的含义； 把我对于《资本论》的一些理解记录下来，增加自己对这些内容的理解； 帮读者做初期准备。 虽然《资本论》第一篇不是很难，但是有些内容需要结合后续内容才更好理解，因此第一篇阅读两遍以上比较合适。这篇文章会扫清一些书中没有解释清楚的内容，尝试帮助读者免去读第一遍。 能否记住这些内容不是最重要的，而是能否在理解之后，按照这些思想来约束自己的行为。例如买股票的时候不炒短线，尽量买与科技发展相关的股票；再也不去想通过赌博或者其他违法犯罪手段达到一夜暴富；按工作创造的价值而不是按工作年龄来衡量自己的薪资等等。 本文不一定和马克思想要表达的内容完全一致，读者很有必要在看完本篇后自行阅读原著相关内容。 读者可能会更关心“钱”意味着什么，但它作为商品形式发展的几个阶段中的最后一个阶段，悄悄地隐藏了很多内容，从而干扰我们对“钱”的本质的认识。想要认识它，就要去了解它是为了解决什么问题而出现，以及它所处的地位。因此对于钱的讨论会放在非常后面（以后可能会另外写一篇以“钱”为切入点的文章），请读者做好心理准备。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:0:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"《资本论》的核心 在接触与马克思相关的内容，特别是与经济相关的内容时，总是绕不开马克思对劳动的看法。马克思认为人的本质是自由的有意识的劳动，这是从人和动物的区别方面来谈的。马克思从多个方面讨论了人的本质，但对于本文来说不是重点。这里只是为了强调劳动在马克思心中的地位。 读者目前可能体会不到“劳动”二字对于人类社会发展有多大的意义，特别是在学习工作中碰到困难时，越发觉得自己的工作没有意义。在阅读《资本论》前三章的过程中，可以从商品交换看到人类社会的发展，从而看到人类社会未来大致的发展方向。这样，就会体会到自己的工作比想象中的有意义得多，而且知道怎么做会使自己的工作更有意义。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:1:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"价值与使用价值 《资本论》里“价值”这个词非常重要，但它的含义与我们日常生活中使用的“价值”不同。日常生活中的“价值”突出的是物的有用性，而有用性这个概念在《资本论》里对应“使用价值”。 既然我们日常生活中的“价值”的概念被《资本论》的“使用价值”所替代，那么《资本论》里的“价值”指的是什么？这里的 价值 全称是 商品价值，是凝结在商品中的无差别的人类劳动。 读者看到这个概念描述，可能会梦回高中，感受到一股枯燥的气息。为了避免枯燥，我举几个例子帮助读者大致理解这个概念： 人工种植的用于交易的树是价值 人工种植的但不用于交易的树不是价值 自然生长的还未与人产生联系的树不是价值 自然生长的被人砍伐后不用于交易的树不是价值 自然生长的被人砍伐后用于交易的树是价值 读者可能会奇怪，为啥是用“是价值”而不是用“有价值”来描述？这一点下文会解释。读者可以先把“是”改成“有”来理解。 从这些例子中可以总结出，如果要说一个东西是价值，需要同时满足以下条件： 这个东西是人的劳动产品 这个东西要用于交易 如果不同时满足这两个条件，就说这个东西不是价值，或者说没有价值。 当读者看到这里的时候，可能已经忘记“价值”的全称了。“商品价值”这个全称对于理解第二点的“交易”很重要，它缩小了“价值”这个词的讨论范畴。只有把物放到商品世界中时，才有讨论这个物的商品价值的必要。至于是否将范围扩大到非商品交易的领域，就需要我们自己去思考了。 为了避免让读者再次将两个概念混淆起来，下文涉及到该词的地方会尽量用“商品价值”代替“价值”来表述，除非是引用《资本论》的原文。 这里对后面要详细展开的两个重要概念做一个总结： 商品价值，指的是凝结在商品中的无差别的人类劳动（下文会展开解释）。商品价值限定于商品世界中讨论，是一种纯粹的社会的抽象的概念。商品价值没有实体，不包含一颗原子。 使用价值，是指具有有用性的物质。在讨论使用价值的时候，就是在讨论这个物的实体。 一个物的商品价值属于抽象范畴，而它的使用价值属于具体范畴。当我们关注的是抽象范畴时，我们说商品是商品价值；当我们关注的是具体范畴时，我们说商品是使用价值。由此，我们把一个物的复杂性拆分成了抽象和具体两个大的角度，并且接下来将分别从这两个角度分别分析。 但在此之前，必须先回答几个问题。我们不仅要知道商品价值和使用价值各自的含义，还要知道为什么会有这样的含义。有些内容书里没有说明，就按照自己的理解来尝试解答。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:2:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"为什么会产生出商品价值这个概念？ 注：这部份是个人理解，不是书里的内容 如果社会中的每一个人都可以在段时间内生产自己所想要的所有物品，这是一个高度发展的社会，甚至人和整个宇宙合为一体了。在当前时代这是一种妄想，有可能人类无论发展到哪一种程度都无法做到。基于这种毫无感情的且无法用意志力迅速改变的客观现实，社会中的每个人如果想要获得更多种类的物品，除了自己通过学习去制作（总体效率低）外，还可以把自己生产的东西与其他人交换（总体效率高）。当这种交换开始时，就会出现一个问题：不同种类的物品之间的比例怎么确定？ 比如为什么我现在用一个普通的用来吃的苹果去换一栋一线城市中心的房子会失败？你可能会觉得我傻了，“这不是明摆着嘛？房子肯定比苹果值钱”。这个回答其实是绕回原点了。因为“房子比苹果”值钱的这个结论，正是基于物品交换的比例确定后得到的。 那么换一种回答，“房子肯定比苹果有用”。对于这个回答，我们先忽略各种极端条件，例如全世界就剩下我这个苹果了，因为这时苹果可能比房子有用。但正常情况下确实是房子比苹果有用。这种观点是认为商品的交换比例是基于它们的使用价值，这也是我们日常生活中的直观感受。当我们觉得一个东西值，是因为我们认为它带给我们的满足感大于或等于我们的付出。 使用价值有其客观性和主观性，日常生活中人们通常默认强调人的主观意志，也就是使用价值的主观性。当把使用价值作为商品价值时，体现的是一个人觉得某个东西越有用，就越原意出高价购买。但问题是如果其他人能用更低的价格买到这个东西，是不是所有觉得这个东西很有用的人都原意付出比其他人更多的价格来买？ 例如有一套房子你觉得很有用，它的有用性对你来说值 500 万，结果一看房子的价格是 100 万。那么你最终会用 500 万购买房子么？如果是使用价值决定商品交换比例，那么回答就是“会”。如果回答是“不会”，那么就需要解释为什么房子的定价是 100 万，为什么认为这房子价值 500 万的人不愿意用 500 万去买这 100 万的房子？如果亏了，亏在哪？ 商品价值是社会确定商品之间交换比例的依据，把商品的哪一种属性作为商品价值，反映了一个群体对于社会和客观世界的认识。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:3:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"为什么要用凝结在商品中的无差别的人类劳动作为商品价值？ 注：这部份是个人理解，不是书里的内容 始终要注意的一点是，我们讨论的是商品，而不仅仅是劳动产品。商品包含着交换的含义，交换的参与者是人，商品是人的劳动产品，劳动产品最终是由人的劳动改造客观世界生产出来的。客观世界的物质不会自动组成人们想要的物品然后自己跳到需要的人的手上。商品的交换最终意味着人的劳动的交换。 这就需要面对一个问题：不同人生产商品所付出的劳动是平等的吗？或者说不同人付出同样的社会平均劳动生产出几种类型不一样的商品，如果这些不同类型的商品的使用价值相差巨大，那么它们的价值是一样的吗？ 回答为：是，所有人类生产商品所付出的社会平均劳动都是平等的。正因此，才把凝结在商品中的无差别的人类劳动作为商品价值。 上面内容的信息量其实不小，但如果不了解各种名词或者形容词的含义，难以察觉到信息量大。 例如为什么要把劳动限定为“生产商品所付出的劳动”？为什么要强调“社会平均劳动”？为什么用“无差别的人类劳动”的表述而不是“人类劳动”？无差别是什么意思？ 为了解答这些问题，首先需要明确几个已知的、在上文用到的概念的内涵和外延。从非常重要的使用价值开始说起。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:4:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"关键词一：使用价值 每一种物都有多种属性，这些属性中一部分或者全部对人有用，一部分属性对人无用。由于物的属性有多个，因此可以在不同的方面对人有用。这一点很重要，因为当人类的有用劳动作用在这些物上面的时候，会有两种结果：不改变原有使用价值，仅在原来的基础上添加使用价值；或者去掉物体已有的某些属性（进而去掉某些使用价值）然后替换为另一种使用价值。 之所以强调“对人有用\"而不是“对所有动物有用”，是因为商品的交换实质上是人与人之间的劳动产品的交换，体现着人与人之间的社会联系。至于将人作为动物这一概念外延的一部分，因而拥有的与其他动物之间的社会联系，不是本文所要讨论的。 之所以要强调人类劳动替换或者新增使用价值，是因为在计算一个商品的价值量时，我们关注的是这个商品的某种或某些使用价值在被生产出来时所消耗的人类抽象劳动量。 理解了上述内容，也就容易理解下面这句《资本论》里的关于使用价值和价值的关系的内容： 一个物可以是使用价值而不是价值，在这个物不是以劳动为中介而对人有用的情况下就是这样。例如，空气、处女地、天然草地、野生林等等。[1] 一个物可以是使用价值而不是价值，换句话说就是一个物的自然形态本身就对人有用，不需要其他人为需要该物的人生产，因而不是无差别的人类劳动的凝结。 当我们说一个物以劳动为中介而对人有用的时候，不是说这个物在以劳动为中介之前对人没有用，而是指在以劳动为中介后，出现了这个物之前所没有的使用价值。 接下来我们将视角从使用价值（有用性）转移到提供使用价值的物（物体本身）上面。但在此之前，需要把所有的物分为两类，一类是不以劳动为中介而对人有用的物，这部份就不多加讨论；另一类是以劳动为中介而被改变或者添加使用价值的物，也就是劳动产品，这是接下来要讨论的。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:5:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"关键词二：劳动产品 劳动产品指的是以人的劳动为中介作用在一个对象上，改变劳动对象的使用价值或者添加使用价值得到的产品。 这里的劳动指的是有用的劳动。一个以劳动为中介所生产出的物如果没有用，那么就意味着这个物相比原先的物没有产生新的使用价值。因此我们说这个物中包含的劳动没有用，不能算作劳动。 由于我们所讨论的是商品相关的内容，并且我们把无差别人类劳动作为衡量商品价值的依据，因此需要把物按照是否以劳动为中介区分开来，才不至于在讨论的时候产生混乱。 劳动产品这个词就体现了这种区分。每当我们使用“劳动产品”这个词的时候，都表示我们正在讨论的对象是一个以劳动为中介而对人有用的物，并且这个劳动指包含了所有劳动中有用的那部份。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:6:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"关键词三：无差别的人类劳动 当我们把商品体的自然的使用价值（例如木桌的木这一属性）撇开，商品体就只剩下劳动产品这个属性。这个劳动产品指的是起到了桌子这一使用价值的那部份。 我们再把使得劳动产品成为使用价值的具体的物体的组成部分和形式抽掉，于是这些劳动产品不再是桌子、房屋或者某种有用物。这些物本来是木匠劳动、瓦匠劳动或者其他某种一定的生产劳动的产品。由于这些具体的不同的有用劳动，使得各种劳动产品（在质上）也有所区别。 如果劳动产品的具体形式消失了，体现在劳动产品里的各种劳动的有用性质也消失了。各种劳动产品再也不会因为具体的劳动而在质上有所区别，它们现在只表示为没有任何区别的同一的人类劳动，即抽象人类劳动。 这里的 抽象人类劳动 等同于 无差别的人类劳动，等同于 同一的人类劳动，三者是同一个概念的不同表述。读者可选取让自己容易理解的表述，提高阅读效率。 同一的人类劳动力在商品的生产上只使用必要劳动量、平均必要劳动时间或社会必要劳动时间。 无差别的人类劳动凝结在劳动对象上，形成价值。 也就是说 无差别的人类劳动 是抽去了“有用劳动的具体形式”后剩下的相同的内容，是以有用劳动为前提的。所以如果一个劳动产品没有使用价值，那么其中就不包含具体的有用劳动，如果抽取这些劳动产品的具体形式，就什么都没有了。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:7:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"关键词四：商品 当一个人不是为自己，而是为社会（其他人）生产使用价值时，才有可能成为商品。 如果一个人通过劳动生产具有使用价值的劳动产品，但这个劳动产品是给自己使用的，那么这个劳动产品就不是商品，也就没有讨论商品价值的必要。 为什么为别人生产使用价值只是有可能成为商品？因为缺少了交换。 例如在欧洲封建社会中，农奴每周要以自己的劳动工具在封建领主的耕地上劳动若干天，以劳役支付地租。随着商品经济的发展，农奴可以缴纳实物代替劳役，形成代役租。此时农奴是为农奴主生产使用价值，但不是通过交换劳动产品的方式，因为农奴没有获得农奴主的劳动产品。因此农奴缴纳的这部份劳动产品虽然是为别人生产的使用价值，但不能称之为商品。 交换的另一层含义是社会。处于社会之中才能进行交换，如果一个人是孤立的，就谈不上交换劳动产品。换句话说，人只有处于社会中，其劳动产品才有机会成为商品，才有讨论它们的商品价值的意义。 商品的交换最终意味着人的劳动的交换。如果一个人交换出的商品所代表的劳动少于其所获得的商品所代表的劳动，就形成了一种劳动的不平等。 劳动产品与商品 劳动产品与商品不一样，商品是以人的劳动为中介作用在一个对象上，改变对象的使用价值或者添加使用价值，使得这些使用价值对其他人有用，并用于交换的产品。也就是说，商品在劳动产品的基础上添加了“对其他人有用”和“用于交换”两个限制，取得了缩小版的劳动产品外延。因此商品必定是劳动产品，但劳动产品未必是商品。如果劳动产品不是为别人生产的且用于与他人交换的，那么劳动产品就不是商品。 因此，当我们在一个表述中使用“商品”这个词的时候，已经表示正在讨论的是一个劳动产品，以及强调了这个劳动产品是为了满足他人的需要而生产，并且要用于交换。 同时，当我们在一个表述中使用“劳动产品”这个词的时候，表示我们讨论的范围不仅包括了商品，还包括了为自己生产的产品。换句话说，劳动产品的外延比商品大。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:8:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"商品的二重性（商品的两个讨论角度） 在讨论商品的时候，可以从两个角度单独讨论：仅表示商品的质的差别的“使用价值”和商品的价值量的差别的“交换价值”。 前文已明确了使用价值的表述，即“XX商品是使用价值”，表明XX商品是以人的劳动为中介而对人有用的物。当商品进入交换阶段的时候，就产生了一个问题：商品之间以什么为依据来确定交换时的量的比例？ 例如我们可以用以下等式来描述两种商品的交换： 3包纸 = 1瓶可乐 我们已知这两件商品的形状不一样、物质组成不一样、制作方式不一样，制作时间不一样，似乎无法将它们化为统一的单位，但是它们却能建立这样一种相对稳定的可交换的关系。这种相对稳定的可交换的关系必定基于某种标准，从而将不同的商品化为同一单位，才能进行量上的比较。 这种单位可能是商品自身的自然属性，也可能是其社会属性。 商品的自然属性包含了可以量化和难以量化的属性，例如形状属于难以量化的属性，而重量属于可以量化的属性。而在可以量化的属性中，又找不到可以共同衡量商品之间交换的共同单位。如果以重量作为单位，那么我想要一台最先进的手机，只需要去找到重量和手机差不多的石头就可以进行交换了，这显然不符合现实。 于是我们转而从商品的社会属性寻找这一同一单位。 一旦讨论到社会，那么就离不开人与人之间的联系。如果人与人之间没有联系，那么人是孤立的，是自给自足的，不成为社会的一部分。商品之所以区别于劳动产品，很重要的一点就在于商品的社会属性，即交换。 由于商品需要交换到把它当作使用价值的人那里，因此商品交换的双方在此时直接地建立了社会层面的联系。这个时候，交换的双方不仅包含了商品，还包含了商品的持有者。 A 商品持有者 + A 商品 \u003c=\u003e B 商品持有者 + B 商品 在这种关系中，商品的有用性是最明显的属性。商品对双方必须有直接的或间接的使用价值。使用价值自身有量的单位，例如包、瓶、台，但是这些单位无法在交换时转化为统一的单位。如何找到这个单位？ 我们再深入地思考一层：为何需要商品的交换？因为商品交换者缺少对方的劳动产品。这就引出了一个很关键的问题：为何不自己生产对方的劳动产品呢？对于这个问题的回答，会将我们引到一个商品的共同的单位。 如果所有人都必须而且能够自己生产所有他所需的当前时代能够生产的物，那么人和人之间就不需要有商品的交换。现实是至少在现在这个时代不可能，于是就需要商品交换。 为了避免对商品交换的讨论过于复杂，这里以一个简单的模型作为例子。 假设在很早很早以前，有一个原始部落遭受袭击，只有两个人幸存下来，A 和 B。他们都会摘水果和捕鱼，但是捕鱼的地点和水果的地点分别在住处的两个不同方向。如果他们既想吃水果又想吃鱼，那么可以自己分别到两个地方去获取食物；也可以各自去一个地方获取特定食物，然后通过交换获取对方的食物。 如果自己分别到两个地方去获取食物，那么想要获取相同量的食物，在路上花费的时间增加了，同时意味着劳动增加了。但如果各自获取一种食物，双方在获取（或者说生产）一种食物的时候，为了对方的需要 而获取（生产）了超出自己所需要的食物，之后交换。这样获取相同量的食物，就减少了路上的花费，等于是减少了能量的消耗。社会总体的使用价值不变，但总体劳动减少了；或者说同样的社会总体劳动，社会的总体使用价值相较之前提高了。 那么交换的依据是什么呢？比如说 A 去摘水果，B 去捕鱼。B 想用 1 条鱼换 10 个水果，A 就不愿意了。A 说我摘 10 个水果需要消耗的劳动比自己去捕鱼所消耗的劳动多很多，而且我们两个做这两件事在劳动的消耗是一样的。 劳动的多少不好衡量，于是假设相同时间消耗的劳动是一样的，以此简化模型。转化后摘一个水果平均需要 5 分钟，捕一条鱼平均需要 10 分钟。那么 A 摘两个水果，相当于 A 或者 B 捕一条鱼。按照这样算，10 个水果应该换 5 条鱼。如果 B 不按这样算，坚持 1 条鱼换 10 个水果，那么 A 可以选择自产，从而使得 B 生产的鱼无法交换 A 的水果，逼迫 B 也需要自己去生产水果，社会总体的生产效率倒退了。这对双方都没有好处。 据此可以总结出，两个人商品交换的依据是生产商品所需要的平均劳动的多少。 如果 B 认真总结经验，使得捕一条鱼所需的劳动比摘两个水果少，这时他仍然可以用一条鱼和两个水果交换。因为对于 A 来说，他捕一条鱼所需的劳动仍然是摘两个水果的时间。 从更深一层的角度来看，在两个人组成的社会中，其中一个人生产一个产品所需要的劳动量决定了他从另一个人手中交换获得这种产品的数量。也就是消费者自己生产商品所需的劳动量决定了从生产者手中交换获得这种商品的数量。 不过两个人之间的商品交换还不足以反映由众多人口共同组成的社会的实际。在扩大了的社会范围中，某一商品的生产者和消费者不是单一的，他们各自组成了一个群体，此时应该按照各自群体的平均值计算。 交换的比例是一个动态变化的过程，任意一方提高效率，都会改变这个比例。 例如虽然 B 把捕 1 条鱼所需的时间减少为 5 分钟。但是新来的一个人 C 把捕一条鱼所需的劳动减少为 7.5 分钟，并且把这种方法公布出来，使得 A 也能仅花 7.5 分钟就能捕一条鱼。那么 B 下次再拿鱼交换的时候，1 条鱼只能交换 1.5 个苹果，而不是 2 个苹果了。这是在一方（B）已经提高效率后，另一方（A）不是通过提高自己专注的劳动（摘苹果）的效率，而是提高其需要通过交换获取的物品由自己生产时的效率，使得自己交换获得的东西变多。 例如有一个 D 不愿意提高捕鱼效率，或者在 C 的方法基础上多了一大堆花里胡哨的动作，使得捕 1 条鱼所花的时间仍然为 10 分钟。那么他在和 A 交换的时候，1 条鱼也只能换到 1.5 个苹果。D 消耗高于平均水平的时间，代表花费了更多的劳动，但也只能换到平均时间对应的商品。这种表面现象的不公平，是追求本质上公平的结果。 例如 B 这次捕鱼已经把周围的鱼捕完了，A 再也无法通过自产的方式用和以前相同的劳动时间捕到鱼，需要更多的时间才能捕到。那么这个时候对于 A 来说，捕 1 条鱼所花的时间可能就是 10 个苹果。那么 B 就可能以 1 条鱼换 10 个苹果的要求与 A 交换。结果是 B 以少量的劳动换取了 A 的大量劳动。从这里可以看出，一个商品的价值在劳动过程中就逐渐形成，但在交换的时候受到此时社会平均劳动时间的影响。尽管如此，它们仍然具有共同点，无论价值量怎么变，价值都是人类抽象劳动的凝结。 少量劳动换取大量劳动，有些是自然影响（例如正常捕捞捕完了），也有些是认为制造的（例如恶意囤积急需的货物）。这说明了不是所有用少量劳动换取大量劳动的人就是恶的，要具体情况具体分析。 对于整个社会来说，个别人的生产耗时不会被作为商品交换时的数量依据，只有整个社会的平均生产耗时才是商品交换的数量依据。换句话说，社会的平均劳动时间才是商品之间交换的数量依据。社会平均劳动时间转化为严格的表述是“社会必要劳动时间”。 社会必要劳动时间是在现有的社会正常的生产条件下，在社会平均的劳动熟练程度和劳动强度下制造某种使用价值所需要的劳动时间。……只是社会必要劳动量，或生产使用价值的社会必要劳动时间，决定该使用价值的价值量。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 52 页 D 捕 1 条鱼用了摘 2 个苹果所需的劳动时间，但捕鱼的社会必要劳动时间与摘 1.5 个苹果的劳动时间相等，因此 D 花摘 2 个苹果所需的劳动时间，只能代表摘 1.5 个苹果所需的劳动时间。 严格地说，社会必要劳动时间在计算的时候，需要把不懂得捕鱼和摘苹果的人所需的劳动也算上。 人类劳动……是每个没有任何专长的普通人的有机体平均具有的简单劳动力的耗费。……比较复杂的劳动只是……多倍的简单劳动。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 58 页 计算一个人在特定领域的劳动力，相当于是计算一个没有任何专长的普通人成长为在这个领域能够以这个人的效率生产使用价值的人所需要耗费的社会必要劳动时间。 从一个没有任何专长的普通人，通过学习和练习获取一定的专长。这个学习和练习的过程，是一个提高劳动力的过程。学习和练习本身就是劳动力的消耗，即劳动。而这种劳动力的消耗，反过来提升了劳动力。这和基建类似，是一种劳动的不断积累，而不是纯粹的消耗。 需要特别提醒的是，劳动包括体力劳动和脑力劳动两大类。脑力劳动通常没有具体的形象化展示，因此在特定条件下，会被认为没有体力劳动重要。 从上述内容可以看出，分工是商品生产存在的条件。人们为了减少不必要的劳动力的耗费（同时提升整个社会的效率）而进行了分工，分工之后的人们把为别人生产的劳动产品与其他人的劳动产品交换以满足自己的需要，这种为满足他人的需要而生产的劳动产品就是商品。 各种使用价值或商品体的总和，表现了同样多种的、按照属、种、科、亚种、变种分类的有用劳动的总和，即表现了社会分工。这种分工是商品生产存在的条件，虽然不能反过来说商品生产是社会分工存在的条件。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 55 页 这里的分工指的是社会分工而不是单个商品生产的各个环节的分工。 前文已经讨论了社会环境中进行交换的劳动产品可以化为无差别的人类劳动（价值）。如果将等式两边的商品都转化为以价值为单位，那么就能建立相等的关系了。 假设 1 包纸包含 1 单位的价值，1 瓶可乐包含 3 单位的价值。那么使上述等式成立的基础是： 3 x 1 单位价值 = 1 x 3 单位价值 为了讨论方便，我们称 3包纸 = 1瓶可乐 这样的简单的等式为 价值形式，","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:9:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"劳动的二重性（劳动的两个讨论角度） 劳动的二重性是是在商品中体现出来的。同商品的二重性类似。（有用）劳动可分成两个角度讨论：仅表示有用劳动形成的使用价值（质）和劳动的多少（量）。 就具体的有用的人类劳动来说，它生产使用价值。而就抽象的人类劳动来说，它生产价值。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:10:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"价值的量 价值的量取决于社会必要劳动时间，不以物对于人的主观的有用性的程度来决定。 超市中的矿泉水和沙漠中的矿泉水的价值的大小差别，在于沙漠中的矿泉水运输所需的社会必要劳动时间。就矿泉水的生产而言，它所需的社会必要劳动时间是固定的。而由于将矿泉水运送到超市和运送到沙漠所需的社会必要劳动时间不一样，凝结在矿泉水中的抽象人类劳动就有量的差别。 沙漠中的矿泉水还可以至少区分两种情况： 矿泉水是自带的 有专门的人在沙漠中售卖矿泉水 对于第一种情况，他的矿泉水是给自己用的，因而是刚好的量。就算有多余的量，也是为了保险，而不是为了交换而携带比自身所需还多的矿泉水。如果要通过交换从这个人身上获取他多余的矿泉水，那么除了运送矿泉水的社会必要时间外，还需要考虑他所可能面临的潜在风险，一旦这种潜在风险发生，这些水就不是多余的矿泉水，因此售价相对会高一些。而如果要通过交换从这个人身上获取他自身所需的矿泉水，那么他很可能拒绝。想要将不为他人生产的产品转换为为他人生产的产品，就算是支付巨大的代价也未必能达成。例如两个人都在沙漠，A 带的水由于各种原因而提前用完，而 B 的水刚好够他自己用，那么 A 就算用很多钱购买 B 的水，B 也未必同意。毕竟就算有了很多钱，也无法确保 B 的存活，那么拥有这些钱就失去了意义。 如果 A 可以自己回去重新带水来，那么他需要评估这些劳动的价值和与 B 携带的多余的矿泉水的价值在量上的关系，如果交换可以使得 A 的总体劳动减少，那么就可以达成交换。 第二种情况则是携带了远超出自己所需的矿泉水，他是为了进行等价交换而携带这些水。当其他人购买矿泉水时，是用与矿泉水生产和运输相等社会必要时间的商品与售卖者交换。这种情况是大家都可以自己回去带矿泉水过来（自产），因而进行的平等的交换。 如果存在黑心的商家，则会把那些合理的矿泉水售卖点打击掉，这样沙漠里的人再次获得矿泉水所需的劳动就提升了。一旦出现问题，黑心商家已经使得这些人不得不花比正常更多的代价达成交易，这使得黑心商家使用包含少量价值的商品换取了包含大量价值的商品。由于价值是人类抽象劳动的凝结，这就相当于用自己少量的劳动换取了别人的大量劳动。在这个简单的例子中，比较容易理解，但是在社会环境中，这种行为可以非常隐蔽，使得人们无从发现。就算发现了，也可能没有办法。例如囤积了远超自身所需的新冠疫苗，再通过各种方式提高自产疫苗的成本（社会必要劳动时间），再抬高价格售卖这些先前囤积的疫苗，就可以用少量的劳动收割别人大量的劳动。 简单价值形式到货币形式的发展 谁都知道——即使他别的什么都不知道，——商品具有同它们使用价值的五光十色的自然形式成鲜明对照的、共同的价值形式，即货币形式。但是在这里，我们要做资产阶级经济学从来没有打算做的事情：指明这种货币形式的起源，就是说，探讨商品价值关系中包含的价值表现，怎样从最简单的最不显眼的样子一直发展到炫目的货币形式。这样，货币的谜就会随着消失。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 ？ 页 这一部分内容放在文章的不同位置，对读者理解这部份内容有不同程度的影响。理由如下： 当前时代的价值形式已经发展到成熟阶段，即货币形式。由于人们有通过当前社会状态解释事物的习惯，导致自觉或不自觉地在分析经济相关问题的时候使用货币形式的视角分析，从而主动地或被动地忽略了货币形式所隐含的商品物物交换的本质。 在汉语中，有一个“耦合”的词，在通信工程、软件工程和机械工程等领域都会用到。在简单价值形式中，商品的交换可以看做是耦合的。在涉及众多商品的交换时，需要经过多次简单交换达到预期结果，耗时耗力不灵活。想要达到灵活的效果来提高交换效率，需要添加一个中间层来解耦这个过程。货币形式是通过在两个待交换商品的中间引入货币这一中间层达到解耦的效果。对于了解解耦的人来说，很自然地就想到这种解耦能使过程更加灵活，但会增加理解成本。 因此我们需要回到货币形式的原始形式，探究其发展的过程，这样才能理解货币形式的本质。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:11:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"第一种形式：简单价值形式 举例： 3包纸 = 1瓶可乐 简单价值形式可以看作是原始社会之间最初的交换方式。这种简单的交换是偶然发生的，也是个别地发生的。 前文已经包含了足够多的简单价值形式的内容，不再赘述。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:12:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"第二种形式：扩大的价值形式 举例： = 1瓶可乐 = 2瓶矿泉水 3包纸 = 1个肉包 = .... 前文提到商品世界中的除了纸以外的商品都能拿来表现纸的价值。当我们把商品世界中除了纸以外的所有商品都作为等价物，都用来表现纸的价值时，就得到了扩大的价值形式。 这种形式的意义在于，尽管形成其他任何一种商品的劳动具有不相同的具体形式，但形成 3 包纸的价值的劳动表现为与其他任何一种劳动等同，3包纸的价值这才真正地表现为无差别的人类劳动的凝结。 在商品世界中，每一种商品的价值都需要得到表现，就造成了每种商品都需要列出这样的扩大的价值形式。一旦新增了一种商品，除了为这个商品建立扩大的价值形式之外，还需要为其他的所有商品的扩大的价值形式中把这新增的商品作为等价物添加进去。 在每一种商品的扩大的价值形式中，所有取得等价形式的商品都是作为特殊的等价物，在这种形式中表现特定商品的价值。 在 3 包纸的扩大的价值形式中，3 包纸可以与其他任意的商品交换。在交换的时候，从纸的持有者角度看，就得到了上面的价值形式。然而我们没有理由总是站在纸的持有者角度上看，交换商品的双方是平等的。如果从要通过交换获得纸的所有商品持有者的角度看，就得到了下面的一般价值形式。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:13:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"第三种形式：一般价值形式 举例： 1瓶可乐 2瓶矿泉水 = 3 包纸 1个肉包 .... 从形式上看，一般价值形式是扩大的价值形式倒过来的结果。但它的意义却和扩大的价值形式不同。 等式左边的商品的关注点是其价值，等式右边的商品的关注点是其物体本身作为等价物的作用。于是这个一般价值形式就表示除了纸以外的所有商品的价值都通过纸来表现。 等式左侧的所有商品的价值都能通过 3 包纸表现出来，表明这些商品的价值相等。这些商品在价值这一种质等同，并且与处于等价形式的商品的质不同。 在扩大的价值形式中，就不能这样表述等式一边的多种商品的价值相等。因为扩大的价值形式的等式右侧的商品是作为等价物，强调的是物这一特征。虽然它们本质上价值相等，但在等式右侧时，它们仅作为表现价值的物，而不是价值本身。 既然一般价值形式中的等式左侧各种商品在价值上相等，而价值相等正是建立价值形式的基础，那么在左侧任意选择两种商品就能组成一个简单价值形式。这样从整体来看，它们互为交换价值。 等式右侧的3 包纸成了一般等价物。但是和扩大的价值形式一样，一般等价形式是价值本身的一种形式，因此商品世界中的所有商品都可以取得一般等价形式。这个重要的信息让我们可以得出，在社会的发展过程中，不同的地区、不同的发展过程可能会有不同的商品作为一般等价物。 一个商品处于一般等价形式，是因为而且只是因为它被其他一切商品当作等价物排挤出来。这种排挤的结果最终只剩下一种独特的商品，从这个时候起，商品世界的统一的相对价值形式才获得客观的固定性和一般的社会效力。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 ？ 页 在历史过程中，有一个商品夺得了一般等价形式的独占权，这就是金。在它没有夺得一般等价形式的独占权时，它处于一般价值形式中的左侧： 1瓶可乐 2瓶矿泉水 = 3 包纸 1个肉包 0.00625克黄金 .... 当位于左侧的黄金和纸交换位置，一般价值形式发展成了货币形式。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:14:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"第四种形式：货币形式 1瓶可乐 2瓶矿泉水 = 0.00625克金 1个肉包 3 包纸 .... 一般等价形式发展到货币形式没有发生本质的变化，只是由金代替纸取得了一般等价形式。而简单价值形式发展到扩大的价值形式、扩大的价值形式发展到一般价值形式都发生了本质的变化。 之所以说转变为货币形式是一种发展，是因为原先多元的混乱的一般等价物，现在集中为金，仅由金这种商品来充当一般等价物。 之所以说转变为货币形式没有发生本质的变化，是因为它是一般价值形式的特殊形式，是一种在社会实践中被筛选出来的形式。货币形式仍然是一般价值形式。金也不会因为夺得了一般等价形式的独占权就发生了什么本质的变化。金仍然是商品，仍然是劳动产品，仍然是凝聚了无差别人类劳动的物。各种商品和被这些商品排挤出来的金交换时，仍然是以无差别人类劳动的量为依据。 为了将金这种特殊的商品同其他商品区分开来，称这种这种这种特殊的一般等价物为货币商品，简称货币。 在日常的生活中，通常接触不到货币形式这种完整的庞大的形式，而是基于货币形式的简单价值形式。这种简单价值形式被称之为价格形式。 一个商品在已经执行货币商品职能的商品上的简单的相对的价值表现，就是价格形式。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 ？ 页 在简单价值形式中，当等价物是已经执行货币商品职能的商品时，这种特殊的简单价值形式就被称之为价格形式。价格是和货币相关的。如果在简单价值形式中，等价物不是已经执行货币商品职能的商品，那么这个形式中就不存在“价格”这个概念。 价格的概念是基于简单价值形式，也就是交换价值的。前文在为交换价值举例子时，用到了“3包纸值1瓶可乐”这样的表述。现在我们把等价物替换为货币。世界上起到执行货币商品职能的商品有很多，例如人民币，美元，日元等等。因此一个商品可以有多个价格形式： 3包纸的价格是（值）3元人民币 3包纸的价格是（值）0.46美元 3包纸的价格是（值）51.10日元 按照上述的说法，价格应当如实反应价值关系。但在实际社会中，我们观察到价格似乎并不总是如实反应价值关系，例如商品会受到供求关系的影响。为了理解这种现象，我们需要复习价值的量的定义。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:15:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"商品价格的波动 商品的(商品)价值量由生产该使用价值的社会必要劳动量或社会必要劳动时间决定。社会必要劳动时间是在现有的社会正常的生产条件下，在社会平均的劳动熟练程度和劳动强度下制造某种使用价值所需要的劳动时间。 在不考虑劳动剥削的理想情况下，当商品的供应减少时，说明供应端出现了问题。我们拿原材料不可逆减少作为例子。在原材料不可逆减少的情况下，原材料更难寻找，需要更多的人类劳动才能找到足量的材料（寻找材料产出地也是劳动），这就说明整个社会制造同样的商品需要更多的劳动时间。生产商品的社会必要劳动时间提高了，使得商品的价值提高了。商品价值的提高反映在价值形式中，包括价值形式的特殊形式——价格形式——中，进而表现为商品的价格上涨了。 在上面的表述中，有一个很关键的词汇：社会。在一般理解中，社会是整个地球上的人类共同组成的社会，然而这并不能反映实际。 生存于一个与世隔绝的孤岛中的人们，也组成了一个社会。但这个社会的某个商品的必要劳动时间却不能以包括这个孤岛的整个社会的必要劳动时间来衡量。只有在这个社会与其他社会建立连接时，才会以所有建立连接的社会的必要劳动时间来衡量一个商品的价值。 但现实中并不这么简单。如果两个社会建立了连接，由于两者在连接之前的生产各种商品的社会必要劳动时间有一定的差距，就会使得在连接后，社会必要劳动时间少的一方对多的一方的冲击。双方需要商定一个协议，使得双方在各自整体上的劳动总和的交换处于相对平等的状态。 当两个局部社会（或者称之为实体）通过协议处于相对平等交易的状态时，他们就可以在一定程度上形成分工。根据前文的内容，分工的形成有助于两个实体一起减少成本，总体上提高了生产力。但这种分工的前提是各方共享自己在分工后的经验积累，使得参与分工的各方都能以相同的社会必要劳动时间生产商品，否则必然会出现其中一方由于商品的社会必要劳动时间缩短的空间比其他的还大，进而当其社会必要劳动时间缩短到一定程度后，可以用少量劳动换取其他各方的大量劳动。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:16:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"关于社会必要劳动时间的补充 一个商品的社会必要劳动时间不只是包含了当前生产者自身的劳动时间，而且还包含了其他不参与生产的人通过学习和实践做到生产同质量商品所需的时间。 商品的生产劳动是可以不断积累的。当不只是生产商品本身，而且不断积累技术时，这些劳动时间实际上是积累起来的。在商品实际生产过程中，工人生产商品所需的时间比其他人少，但是其价值却和其他人一样。其他人想要达到短时间的生产，必须像他那样经过长时间的技术积累。这种积累的缺失，使得生产同样的产品需要更多的时间。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:17:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"人类的财富 使用价值是财富。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:18:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"劳动与劳动力 前文多次提到了劳动与劳动力，并且有“劳动力的消耗“这样的说法。在不清楚概念的情况下，会比较难理解文章内容。 用一句话概括两者的关系，就是”人类的劳动就是在消耗自身的劳动力“。那么劳动力是什么？为什么它能被消耗？ 我们把劳动力或劳动能力，理解为一个人的身体即活的人体中存在的、每当他生产某种使用价值时就运用的体力和智力的总和。—— 《资本论》第一卷，人民出版社，马克思诞辰 200 周年纪念版，第 ？ 页 劳动力是一种商品，具有价值，可以与其他商品交换。劳动力的消耗，即劳动，创造价值。 当谈到消耗劳动力时，这里的“消耗”实际上是指 运用 体力和智力。在消耗劳动力的过程中有能量的消耗，但这消耗的能量的多少不是作为劳动的量的依据。 那么依据是什么？我的解释是：一个没有任何特长的人，通过学习知识达到能生产某种使用价值的过程中所消耗的能量，再加上劳动时所消耗的能量。在每次生产这种使用价值的时候，都算上获取这些知识时能量的消耗。 一个安装配件的流水线上的工人为学习安装配件知识所消耗的能量，必定比一个数学博士为了成为博士学习知识时所消耗的能量低。两种类型的人在运用各自学习的知识生产使用价值时，所计算的劳动的量有很大的差距。 如果一个数学博士去做安装配件的工作，需要学习的也是安装配件的知识。他在安装配件时，并不需要使用多于其他人的知识，因此得到的工资和其他人一样。 之所以说是“生产某种使用价值时就运用的体力和智力的总和”，就在于运用的体力和智力的总和与生产者生产哪一种使用价值有关系，而与这个人是否能够运用更多的体力和智力没有关系。 一份工作所需的劳动力会被工作本身限制住，如果想持续地提高自身的劳动力，可以选择一个知识积累上限很高的领域。在这样的领域内，才有机会通过积累知识极大地提高自己的劳动力。 由于上限足够高，随着时间地推移，不断地会有一些人减缓积累知识的速度。而那些不减缓速度的人，也因此容易脱颖而出。如果上限不够高，大多数人都达到这个上限，后来者的劳动力就难以得到完全的发挥。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:19:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"人的劳动力的完全发挥（人的解放） 一个人掌握的知识不仅仅只能让他生产唯一一种使用价值，但是他能生产的使用价值中，必定有一种能够让他最大程度地发挥他的劳动力。 尽管如此，社会并不能总是保证提供给他生产这种使用价值的机会。一种工作岗位无法无限度地增加，因为在这种工作岗位生产出来的使用价值必须被消费，但消费者不能无限度地增加。生产的使用价值最终会接近消费者的需求。 但是除了这种情况外，还有一种社会因素。例如为了维持一个家庭的生存和发展，需要处理很多事情。一个家庭可以通过自给自足来维持，在这个过程中，需要耗费时间自己生产食物。解决这些问题不需要大量的知识，也就代表了少量的劳动力。如果一个家庭用他们所积累的知识，在同样的时间内生产更多的使用价值，那么用其中一部分使用价值与其他家庭交换食物，剩下的使用价值则是同样时间内增加的部分。但是一个家庭的生存和发展不只是要解决生产食物的问题，还有其他的社会因素。因此需要有一个人来生产这些只需要少量劳动力就能生产的使用价值。这个时候，这个人没有生产能够让他最大程度发挥劳动力的使用价值，表示他的这种劳动力浪费了。 以上都是已有劳动力没有得到完全发挥的情况。还需要注意到劳动力的增长，也就是一个人的潜力。 如果一个人的劳动力能够提高，但被其他因素影响而无法提高，这也是劳动力的浪费。具体表现为小孩子没有上学的机会。 小孩子由于需要分担家庭的生存所需的劳动，或者由于家庭无法生产足够的价值去交换学习知识这一商品，使得小孩子的劳动力无法得到快速的提高。这样就相当于把家庭的总体劳动力锁死在一个较低的层级，难以充分发挥潜力以获取更多资源。如果整个社会都这样，那么这个社会的总体劳动力就被锁死在一个较低的层级，非常脆弱。 如何让每个人都充分地发挥他的劳动力，是整个社会所需要解决的问题。每个人的已有劳动力和潜在劳动力得到完全的发挥，就能够最大限度地创造价值，与其他人交换劳动产品，从而极大地提高自身的生活水平，提高幸福感。最终的表现就是整个社会的财富的巨大增长。 当一个人无法最大程度地发挥他的劳动力时，就需要去生产一种能发挥他第二大劳动力的使用价值。而如果一个人只掌握生产一种使用价值的知识，那么他在失去发挥对应劳动力的机会后，就和一个没有任何特长的人一样了。 从以上分析可以得到，通常学习的知识更加全面的人，客观上应对风险的能力就更强。这是由于其知识的全面性，使得他已经具备各种行业的预备知识。在生产某种使用价值的时候，无需再次学习一些基础知识，因此能够更快地开始生产使用价值。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:20:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"商品中的价值累积 一件简单的商品，完全由一个人生产时，商品的价值全部都是这个人无差别劳动的凝结。 但随着社会的发展，商品的生产越来越复杂。但是尽管复杂，仍然有一部分复杂的商品可以仅由一个人生产。只是当一个人想要独立地生产整个商品时，需要学习很多知识。 但是这样有多个问题： 一个商品的生产过程中，有一些部分的生产的社会必要时间少，另一些部分的生产的社会必要时间多（后面将该表述简化为“商品价值”）。一个可以生产高商品价值且具有生产条件的人，花费大量时间去生产低商品价值，浪费了劳动力。 复杂商品由掌握着众多知识的人负责独立生产，简单的商品无法提供足够的工作岗位。这就意味着社会中有大量的劳动力得不到解放。 因此一个复杂的商品会被分解为多个生产步骤。将原本一个人需要的所有知识切分成多份，切分到仅需少量前置知识就能够学习和掌握，使得社会中的劳动力尽可能多地得到发挥。 由于一个商品的生产被分为多个相对简单的步骤，原先一个商品的价值都由一个人的无差别劳动凝结而成，现在一个商品的价值由多个人的无差别人类劳动凝结而成。一个商品生产过程的步骤中，不同的人在商品体上凝结了他那一份无差别人类劳动。负责前一个步骤的人在商品体（半成品）凝结他那一份无差别人类劳动后，后一个步骤的人得到的是凝结了前面所有步骤的负责人的无差别人类劳动的商品体（半成品），并在上面凝结了自己的无差别人类劳动。 在创造价值的过程中又分为两种情况： 一个商品的生产都是在同一家工厂内，那么直接将半成品传递给下一个步骤的人即可。 一个商品的生产由不同的工厂完成，那么不同的工厂之间就会交易这个商品的半成品。当前步骤的工厂总是支付给上一个步骤的工厂与前面所有步骤所产生的价值相等量的货币。上一个工厂生产的虽然是一个完整商品的一部分，但对当前工厂来所，这个半成品具有使用价值，使得这个工厂可以通过劳动在它上面增加价值。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:21:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"机器的价值转移 严格来说，我们把用于生产商品（劳动过程）所需的所有物都称之为生产资料。机器是生产所需的工具，是生产资料的其中一部分。劳动过程还需要人的因素，即劳动力。 生产资料，例如机器，本身凝结着一定量的无差别人类劳动。在使用机器的过程中，这些无差别的人类劳动转移到了劳动产品上，就好像生产这些机器的人在直接地生产这个劳动产品一样。 因此在一条全机械化的生产流水线上生产产品时，仍然会增加产品的价值。只是这个价值早已存在于机器中，现在只是转移到了产品上。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:22:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"工人在什么时候生产自己的工资？ 西尼耳的“最后一小时”理论，说工厂的利润由工人最后一小时产生，前面所有工作时间都是在补偿工厂的资本。并且得出结论：如果工作时间缩短一个小时，总利润将会消失。 但根据前面的内容，我们知道当一个人开始生产商品的时候，就已经在凝结无差别人类劳动。当他生产完一个商品后，他的无差别人类劳动也就完全凝结在这个商品上了，商品的价值增加了。 如果资本家付给工人的工资，与工人在商品上通过自身劳动凝结的价值相等，那么资本家获得的利润就只有与他自己的无差别人类劳动在商品上的凝结所产生的价值的那部分。 但实际上却不是这样，资本家付给工人的工资，比工人在商品上通过自身劳动凝结的价值来得少。我们将这两者之间的差距称之为剩余价值。 假设工人一天在商品上凝结的价值为 100，但资本家只给了工人 50，那么剩余价值为 100 - 50 = 50。则剩余价值率为 50 / 50 = 100%，此时我们称劳动力的剥削程度为 100％。资本家通过剥削获得的量是工人实际获得的 100%。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:23:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"价值更为深入的含义 人类在通过劳动创造价值的时候，有各种各样的表现形式。小到整理房间，到理解需求、了解业务、设计方案，大到探寻宇宙的真理，都是在把无序的、不确定的东西转换为有序的、确定的东西。 一个人的能力有多强，就在于他能够把多么无序的、不确定的东西转换为有序的、确定的东西。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:24:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"自行阅读原著的注意要点 《资本论》三卷共 98 章 2989 页，我们只需阅读前 3 章，剩下的 95 章要不要看取决于看完前 3 章是不是产生了兴趣。 不给自己太多压力，就容易看完。 前 3 章的概念会反复出现在不同的例子中。如果碰到难以理解的地方，可以先跳过，后面总有一个例子能够让你理解。 因此第一遍可以快速看，从后面的例子中理解概念，再回头重新看一遍，就会理解得更深刻。看完两遍能理解就是血赚。 一次不需要阅读太多内容。最好在手机上看，在路上或者去洗手间的这段时间看就行。 哪怕是每天只看一页的内容都比隔几天一次看多页的效果好。 先不看现代西方经济学对《资本论》的解释，先自己去看一遍原著，避免被误导。特别是讲《资本论》却不包含前三章的那些资料。 我之前看到一个学习了现代西方经济学的人对《资本论》提出质疑，但从他写的相关内容可以看出他没有理解马克思在说啥。 《资本论》的前三章都是围绕着“劳动”以及“劳动创造价值”来展开的。需要特别注意的是，这里的“价值”与我们通常所理解的价值不一样。《资本论》中的“价值”究竟指的是什么？这是要在阅读过程中不断整理和思考的问题。 因此，在阅读《资本论》前三章的时候，要始终带着两个问题： 劳动为什么那么重要？ “价值”指的是什么？ ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:25:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":["马克思"],"content":"引用 本文对《资本论》的引用都来自于《资本论》第一卷，版本为人民出版社出版的马克思诞辰 200 周年纪念版。为了避免无意义的重复，以下不强调这个版本，仅说明原文在《资本论》第一卷的第几页。 ","date":"2021-05-04","objectID":"/2021/05/04/das-kapital/:26:0","tags":["资本论","价值"],"title":"《资本论》中的“价值”","uri":"/2021/05/04/das-kapital/"},{"categories":null,"content":"减少电脑耗电 在断电后，需要让电脑尽可能少用电。 有两种方式： 休眠 关机 关机最省电，但需要主板支持 WOL（Wake-on-LAN）。休眠可以由 WOL 唤醒，也可以通过定时任务唤醒。 WOL 方式 WOL 方式要求直接用网线将路由器和电脑的网卡连接起来。 开机时进入 BIOS 各种主板进入 BIOS 的按键不一样，大多是 Esc 和 F12 [1]。主要看开机时屏幕的提示。 找到跟 WOL 相关的选项，设置为 ON 或者 Enable[2] 选项可以是以下的任意一种： “Wake PCI Card” “Boot on LAN” “PME Event WakeUp” “Power On by PCI Card” “Power On By PCI Devices” “Wake-Up by PCI card” “WakeUp by Onborad LAN” “Resume by MAC LAN” “Resume By PCI or PCI-E Ddevice” 重启进入 Windows 系统 [Win 键 + r] 打开“运行”，输入 devmgmt.msc 并确定，这样进入了设备管理器。 在【网络适配器】里面找到有线网卡，通常名字包含 Realtek，对该网卡执行以下两个操作： [ 右键 | 属性 | 高级 ]，找到【关机 网络唤醒】和【魔术封包唤醒】，如果值不是“开启”，则应设置为“开启”。 [ 右键 | 属性 | 电源管理 ]，【允许计算机关闭此设备以节约电源】和【允许此设备唤醒计算机】的前面都打上勾。 [Win 键 + r] 打开“运行”，输入 control panel 并确定，这样进入控制面板。 把查看方式从【类别】改为【大图标】，进入 [ 电源选项 | 选择电源按钮的功能 | 更改当前不可用的设置 ]，把【启用快速启动（推荐）】前面的勾去掉，保存修改。 [Win 键 + r] 打开“运行”，输入 cmd 并确定，这样进入命令提示符。 执行命令 ipconfig /all，找到网卡的物理地址，大概是 08:BA:AD:F0:00:0D 这样的。复制下来。 关机 在局域网内另一台机器上面，对刚才的物理地址发送魔术封包[3]。需要将这个 UDP 包发送到 9 这个端口。 可以用 https://github.com/sabhiram/go-wol 这个工具。如果之前安装过 go，那么直接执行以下命令就可以安装了： go get github.com/sabhiram/go-wol/cmd/wol 然后执行唤醒命令： wol wake 08:BA:AD:F0:00:0D 此时可以看到刚才那台机器启动了。 （非必需）如果需要远程启动，则 需要有公网 IP 在路由器上，将目标机器的 IP 和 MAC 地址绑定起来 路由器将 9 这个端口映射到目标机器 IP 的端口 9 上面 定时任务方式 看下面这篇就够了： https://jingyan.baidu.com/article/4e5b3e1930bff091901e24d9.html (利用任务计划实现计算机定时开关机(休眠唤醒)) ","date":"2021-05-04","objectID":"/2021/05/04/ups/:1:0","tags":["UPS"],"title":"确保重要设备供电","uri":"/2021/05/04/ups/"},{"categories":null,"content":"前几天写了一篇跟软工课程相关的博客[1]，有一位能够促成具有建设性对话的同学问了我一个问题： 能展开讲讲你说的工程思想和方法么，以及从软工的哪些环节可以获得？ 这位同学问了一个很好的问题。好在哪里呢？好在他试图详细地了解和分析一个抽象的表面上看起来高大上而且美好的概念，从而提出了一个往具体方向上讨论的引导问题。 同学们在面对类似的抽象的表面上看起来高大上而且美好的概念（例如“自由”和“平等”）时，一定要向自己或者向别人提出类似的往具体和细节上引导的问题。 在正文开始前，我要向各位老师和助教道歉。很抱歉在没有事先跟你们沟通的情况下，就冲动地答应回答这个问题。希望我这个越俎代庖的行为能得到大家的原谅。 另外我想跟同学们澄清一件事，我大多是在纸上谈兵，老师和助教们是在实践。大家要谨记“实践是检验真理的唯一标准”，当理论和实践不一致的时候，要以实践为准。 以下是我个人的看法，不一定代表老师和助教们的看法，也不一定对。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:0:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"工程思想和方法 在正式展开我个人看法之前，我想再次重复我在 当前阶段 总结的工程的目标：在当前社会环境中，工程实施者使用有限的资源，把能创造的价值最大化，并将其凝固在一个商品里面。 我想请读者试着回忆我在上一篇博客中反复强调的一个跟工程相关的行为是什么？ 大家的回答应该都是差不多的，否则就是我的表达能力有问题。尽管回答可能差不多，但以我 对自己的要求和标准 来看，我分成了四个层级： 第一层是舍弃，第二层是取舍，第三层是为了创造最多的价值而取舍，第四层是在理解资源是有限的且了解了当前社会环境提供了哪些资源以及自己掌握了哪些有限的资源的情况下为了创造最多的价值而取舍。 对我来说，这就是工程思想的核心。 接下来我会列出其他思想中的 一部分。其中有些是前辈们在他们各自所处的时代中通过实践总结的，有些是我自己总结的（因为我没有系统地学过工程学）。 分类讨论思想 迭代思想 快速迭代思想（快速试错、快速同步） 可持续思想 分而治之思想 动态调整思想 折中思想 矛盾思想 可预期思想 接下来我尝试对这些思想做一些展开：这些思想是为了解决什么问题以及符合这些思想的方法有哪些。然后将这些思想结合到软件工程的教学中。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:1:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"分类讨论思想 对于同学们来说，分类讨论思想再熟悉不过了。数学考试中经常要用到分类讨论思想，而且通常是一道比较难的题目。但我们这里不讨论纯粹的数学，我希望能够立足于课本知识，但超越课本知识。另外我数学成绩也只能算是中等（逃 分类讨论思想的通俗化表达是“具体问题具体分析”，它的反面是“在任何条件下都使用同一种具体的解决方法”。 分类讨论思想所要解决的是将一个抽象的问题根据不同条件转化为多个具体的问题，再根据具体问题的不同特点分别用不同的方式解决。 举个生活中的例子： A 和 B 说头疼了，B 应该怎么回答？或者有人跟你说头疼了，你应该怎么回答？ 这是一个非常抽象的问题，需要分好几层的分类讨论。 首先要对双方的性别做一个分类，做这个分类在当前社会环境下具有一定风险。我在这里根据国内大部分人的共识将其分为两类，男和女。由于缩小了数据集，因此只有四种情况。 这里使用分类讨论思想的图表法，把四种情况列出来。 男 女 男 A男、B男 A男、B女 女 A女、B男 A女、B女 接着要对问题的主体双方的关系进行分类，可以有以下几种类型（我只列出部分）： 师生关系 情侣关系 普通朋友关系 损友关系 同事关系 其中这些类型有的是非对等关系（比如师生关系），换个顺序很可能得到不一样的结果；有的是对等关系（比如损友），换个顺序很可能得到一样的结果。 非对等关系通常使用排列的方式展开，对等关系通常使用组合的方式展开。这个可以根据实际情况调整。 通常我们看到一个问题，会下意识地采用社会共识所选择的类型加以判断。例如有人问 1+1 等于多少？根据社会共识，这个问题的完整表述是：在数学中，1+1 等于多少？于是我们回答，等于 2。 回到这个具体问题，社会的共识是，这个对话通常是在情侣关系中由女方发起的。我们可以先从最主要的这个分类开始讨论。 接下来我们对头疼这个现象进行分类。可以大致分为：真的头疼和假的头疼两类。然后再对女方的心理活动进行大致分类：想要解决头疼问题、想要男朋友的关心、两者都要。 现在我们使用分类讨论思想中的连线法进行组合： 再从男方的角度做一些分类： 想解决头疼问题 想关心女朋友 两者都想 还可以再做进一步分类，例如想解决头疼问题： 可以想出很多种方法 只能想出“多喝热水” 写得我好累。不想再展开了。 从这些分类中，根据不同的连线大致写出对应的解决方法。 比如男女双方是情侣，女方对男方说头疼，但不是真的头疼，她想要男朋友的关心，但男方想解决头疼问题，又没办法想出很多方法，于是回答“多喝热水”。 再比如男女双方是情侣，女方对男方说头疼，但不是真的头疼，她想要男朋友的关心，男方想关心女朋友，于是说了一些关心的话，还可以做一些她喜欢吃的东西，以及其他 N 中解决方法。 我们通常说一个男生是直男，有一部分原因是他们对于问题的理解停留在了表面，同时试图解决这个表面的问题，又找不到合适的解决方法。这是一下子犯了 N 个错误才导致了 “多喝热水” 这种答案。把问题分析到这种地步的时候，就会知道不是只有男性这个群体才会出现的问题，这是人类这个群体会出现的问题。 分类讨论思想不仅仅是用来解决工程上的问题，还可以用来解决思想上的问题。 我花这么长的篇幅来举这个例子，是为了以下内容不必过多展开。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:2:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"迭代思想 迭代思想指的是在原有的基础上有计划地不断改进以接近完美，与之相对的是一次性完美思想。 认为一个完美的东西才有价值，是人的缺点，需要通过后天的努力去改正。迭代思想要求人们接受不完美，看到不完美的东西的价值。 迭代的方法之一是把一个工程分成多个阶段，每个阶段设定一个可交付的标准，当前阶段的每个事项做到预设标准就停下来，不继续往下做。 不完美至少有两种情况： 同一个功能有速度快的粗糙实现方案（比如手工调整），也有速度慢的灵活实现方案（比如自动调整）。粗糙的方案在这种情况下不是完美的选择，但是可以在前面的迭代中实现，后续迭代中用灵活的实现方案替代。 在同一个方案中，有些重要的功能先实现，有些不那么重要的功能放到以后实现。同一个方案没有全部实现也不是完美的选择。 迭代思想通常与分而治之思想配合。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:3:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"分而治之思想 当我们明确了一个具体的问题之后，需要开始解决问题。但是如果这个问题非常大，无从下手，怎么办？分而治之。 分而治之思想还会和迭代思想以及矛盾思想进行配合。 详细地分析这个具体的问题，会发现这个大的具体的问题可以分成几个中等的具体问题。然后再去分析这些中等的具体问题，就会发现一个中等的具体问题还可以分成几个小的具体问题。接着继续一直划分下去，得到了一颗多个层级的树。如果要开始解决这个大的问题，那么就去解决叶子节点上的问题。 可是这个时候又发现叶子节点实在太多了，如果随便选一些完成，可能要到所有叶子节点的问题解决之后，才能影响到大的问题的解决。 这时我们需要挑选叶子节点中对大问题影响程度大的那些，每次都尽量挑选一批影响程度最大的叶子节点出来解决。挑选影响程度大的叶子体现了矛盾思想，分次挑选体现了迭代思想。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:4:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"矛盾思想 矛盾思想这里指的是主要矛盾和次要矛盾。 主要矛盾是当前阶段中对结果影响最大的一个点，其他的点都是次要矛盾。当解决了主要矛盾，会进入下一个阶段，此时在上一个阶段中的次要矛盾中会有一个对结果影响最大，此时该次要矛盾成为了当前阶段的主要矛盾。 区分主次矛盾不是为了忽略次要矛盾，而是把更多精力用来解决主要矛盾。比如 80% 精力用来解决主要矛盾， 20% 精力用来解决次要矛盾。在实践中，通常解决次要矛盾有助于解决主要矛盾。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:5:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"快速迭代思想（快速试错、快速同步） 快速迭代与迭代的区别在于迭代的期限很短。为啥要缩短迭代的期限？ 因为我们通常在迭代结束的时候，才会交付当前迭代的成果。但是由于对问题理解上存在偏差，很可能导致这个成果无法解决问题，需要重新做。 为了减少这种反复重做导致的资源浪费，可以缩短迭代的期限。这样可能在某个行动项上只完成了 20% 就发现这个行动项无法解决问题，此时只浪费了 20% 的资源。迭代的期限如果拉长，等到完成 80% 才发现无法解决问题，那么就浪费了 80% 的资源。 快速迭代思想还不止于此，在具体的实践中，会有一些优化方案。我会在结合软工时补充上这一点。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:6:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"可持续思想 同学们对可持续思想也比较熟悉，经常背到。不知道同学们对可持续的重要性有多少思考？ 可持续思想能够解决的是随着时间的推移，系统会自毁的问题。人就是一个系统，而且会不停地作死。 可持续思想是怎么解决这个问题的？通过限制效率。一个项目的推进有多种选择，其中必定有一种效率最高的推进方式。可持续思想就是要阻止你选择这个效率最高的推进方式。 比如有一个高效率获取快乐的方式（大家应该都懂，我怕文章被和谐就明确指出来了），为什么所有人都不使用这种方式去获取快乐呢？因为它不可持续。高效率带来的是快速的消灭。人类将其加入刑法，就是阻止人类高效率地作死。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:7:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"动态调整思想 动态调整思想能够解决因对有限资源的误判或者有限资源临时变动出现的问题。 比如在迭代前选定了一批要解决的具体问题，但是在迭代中工程的施工人员数量发生变化。如果按照原先安排，可能会导致人员数量继续发生变化。于是需要重新组合有限资源，并且对选定的问题做一些动态调整。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:8:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"折中思想 折中思想面对的是单一的完全的选择没有比多个的部分的选择的价值高的问题。形象的表达是 0.5 + 0.5 \u003e 1。 在计算机历史中，最不缺的可能就是折中了。同学们应该深有体会。 比如既然内存速度那么快，为啥还要用到硬盘？你可能说因为内存断电数据就丢失了呀。那我用一大堆设备确保它不断电不就行了？ 其实这总体上是一个性能和成本的折中问题（往深了讲还有发展水平问题）。只用内存的产品不是没有，但是很贵，难以推广到全世界大多数人使用。只用硬盘也可以，但是速度会很慢，如果大家都只用硬盘，那就没关系。问题就在于有人用少量内存和大容量硬盘结合起来，速度既可以很快，价格又能接受，于是更多人选择了这个折中的产品。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:9:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"预期管理思想 说实话没想好准确的词，凑合着用吧。 预期管理思想是让自己成为一个可预期的存在，以此管理其他人的行为。大致分为时间预期和行为预期。 时间预期就是完成一个事项需要多少时间。这样在工程迭代过程中，可以做出不同的调整。 例如根据事项的预期完成时间和迭代的时间限制来挑选事项； 或者发现所有事项的预期时间之和超过了所有迭代时间之和，那么就可以做不同的调整。 例如舍弃掉某些事项； 或者增加施工人数。 大家对行为预期比较熟悉的一个子类应该是底线思维。就是让对方知道，一旦跨过这个底线，我方一定会让对方付出巨大的代价，这个代价大概率是对方无法接受的，除非对方想同归于尽。这样对方就处在我方的管理之下，最多只能在底线之上蹦哒。 举个我自身的例子。我会让领导知道我在当前阶段主要想通过编程（包括学习技术、系统设计、编码）解决问题，如果领导在这个阶段强行给我一个没什么时间编程的岗位（例如产品经理或者管理岗），那么我会立刻提出离职；而如果让我把大部分时间花在编程上，那么我剩余的时间会帮助领导解决很多系统之外的问题。这样领导可以评估他的这个行为会对项目和他自己产生多大的影响，以及是否有消除影响的方法。如果有消除影响的方法，那么可以逼我去没时间编程的岗位，或者直接裁掉我。 为了避免误导大家，希望大家要注意，行为预期管理思想对个人的硬实力要求高。比如我自身的例子要求我能够创造足够多的价值，以及我能够承受离职带来的影响。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:10:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"工程思想和方法在软工课程的各个环节上的体现 这里的软工课程指的是参考《构建之法》开展的软工课程，因为不同的软工课程的开展方式不一样。 同学的原问题是“从软工的哪些环节可以获得（这些思想方法）”。为了严谨，我把思想和方法分开。方法可以从软工的环节中获得，思想则需要提醒和思考总结。 由于《构建之法》本身提供了很多方法，满本都写着两个字“方法”（皮一下，如果邹欣老师觉得这么说不妥，我再换种表达），而且肯定比我一篇博客所能讲的还来的详细，感觉没多大必要做这样的重复。不知道下面的回答能否算是提供了一个令人满意的答案。 课前 老师和助教通过可预期思想方法，让同学们知道自己大概会在课程结束后得多少分。 个人技术和流程 这里涉及了测试。测试是为了阻止开发者以最高的效率完成一个既定任务，让工程稳定可持续地推进，避免出现重大问题导致工程失败，体现可持续思想。 这个阶段对需求分析的要求还不是很高，放团队项目说明。 结对编程 制定代码规范、代码复审和结对编程过程是软件工程中为了可持续推进项目而要求实践的工程方法，都是为了阻止开发者以最高的效率完成一个具体任务，体现可持续思想。 可以在两人合作过程中使用分类讨论思想的方法以提高合作效率。 团队项目 老师和助教可以通过分类讨论思想的方法，大致确定各团队选题方案的合理性。可以通过可预期思想方法，让同学们再次对自己的最终得分有一个较之前更准确的认识。 以下是与同学们相关的内容。 可以实践分类讨论思想的方法提高合作效率。 需求分析的时候： 用分类讨论思想方法分析用户群体 用矛盾思想方法筛选出核心用户群体 找到具体的用户，通过分类讨论思想方法了解用户，交流需求 用分而治之思想拆解问题，和用户确认重要具体问题的细节，产出软件功能规格说明书 用快速迭代思想的方法快速绘制原型，尽快与用户确认，避免浪费开发资源 设计系统的时候： 用分而治之思想拆解具体问题 用折中思想的方法平衡完成时间和软件性能或者功能 做工程规划的时候： 用迭代思想、分而治之思想、矛盾思想、可预期思想的方法规划各阶段任务（Alpha 和 Beta 版本），避免系统无法按时交付 施工的时候： 结合可预期思想和动态调整思想的方法，调整迭代任务 同学们通过站立式会议，及时准确地同步项目进展。PM 通过当前进展调整迭代任务，并反馈给老师和助教。让老师和助教对团队的进展有一个清晰的认识，并及时提供指导意见。 Alpha 做完后快速交付给用户，得到及时反馈，尽快调整 Beta 迭代的任务。这个是快速迭代思想的方法 Alpha 和 Beta 做完后专门做了事后分析，阻止了同学们试图采取最高效率的方法继续推进项目，确保项目可持续 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:11:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"关于课程总结 我们当时在上张栋老师的课时，最后一次作业[2]中，有一个内容： 写下属于自己的人月神话——项目实践中的经验总结+实例/例证结合的分析 现在想起来，觉得很妙。且听我具体解释。 大多数理论都是不完美的，特别是工程类的理论。理论总是要结合实践，通过实践来验证理论的正确性，但仅仅这样还不够。 世界的复杂性决定了一个理论难以适用所有的情况。可以说任何人所具备的“有限的资源”都是不一样的。一旦条件不一样，那么在实践一个理论时，很容易就得到不同的结果。 理论只有指导作用，指导不是强制。理论不一定和实践一致，而且往往与实践不一致。如果理论和实践不一致，要以实践为准，并通过实践反过去补充和完善理论。正是在实践中得到的不同的结果，推动了理论的完善，进而推动人类的进步。 人们想要把具体实践对理论的影响固化下来，想出了一种方法：写论文。从上述内容看，一个实践可以通过验证在不同条件下理论仍然成立来补充理论；也可以验证在不同条件下理论不成立，以及如何调整理论来包括这种条件以补充理论。 同学们认为软工改革的时候，自己是“小白鼠”，这似乎是比较悲观的看法。在我看来，同学们以及老师和助教都是在通过自己“有限的资源”实践软件工程理论。在实践的过程中会碰到属于自己的问题。在解决各自问题的过程中，形成了属于自己的软件工程理论，最终通过总结把自己的“有限资源”以及软件工程理论在这些“有限资源”的条件下的实践情况表述出来。这样在客观上推动了软件工程理论的完善，进而为整体人类的进步做出自己的贡献。 你们还未察觉到你们正在做的事情有多么伟大。 这不是安慰，也没有其他乱七八糟的想法。我是通过上述的事实和逻辑（虽然很粗糙）得到这个结论的，也就是说我是真的这么认为的。 ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:12:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"写在最后 写完这篇博客后，除了工作上的一些总结之外，我在 2021 年不会再像这样大量输出跟软工课程相关的内容了。我现在最需要的是输入而不是输出，从这两篇博客的内容上大家就可以看得出来。 客观上，这两篇的博客会在一定程度上（虽然很低）提高我个人的影响力，这是我目前阶段应该尽量避免的，因为我必须承担由此带来的本不必要的风险和代价。 不过也正是因为之前的那篇博客，使我和恩师张栋老师的关系得到进一步加深，所以我认为承担这个风险和代价是值得的。 最后的最后，提前给大家拜个早年。祝大家身体健康，事业顺利，学业顺利！ ","date":"2021-04-24","objectID":"/2021/04/24/software-engineering-cource/:13:0","tags":["软件工程","软件工程课程","工程思想方法"],"title":"工程思想方法在软件工程分支的教学上的体现","uri":"/2021/04/24/software-engineering-cource/"},{"categories":null,"content":"今天看了北航软工课程学生写的一篇博客[1]，刚好可以借此机会把我这几年关于软工的思考整理出来。 我在 2016 年做过福大OO课的助教，以及在 2017 年做过集大的软工助教。当时我有很多地方都没有做好，并且这几年没怎么关注软工课程，与教学一线脱离较久，说的东西不一定对，大家随意看看就好。 为了确保我理解了上述博客的内容，我会先尝试对博客做个总结，然后选择标题“足够好”作为切入点说说我自己的一些粗浅的看法。 ","date":"2021-04-21","objectID":"/2021/04/21/software-engineering/:0:0","tags":["软件工程"],"title":"软件工程与“足够好”","uri":"/2021/04/21/software-engineering/"},{"categories":null,"content":"博客说了啥 博客的一个关键背景是团队项目选题，详细内容在： https://edu.cnblogs.com/campus/buaa/BUAA_SE_2021_LR/homework/11925 选题方式有三种：自选题、推荐选题、继承往届选题。不同选题方向对团队关于所选题目有不同的要求（这里课程组没有充分地把要求清晰地表达出来）。选题和需求分析的时间限制是 4 天（周末两天），同学们面临的来自时间压力比较大。由于本次选题后的需求答辩邀请了业界人士和大众评审团，需要事先提供各团队的相应资料给他们，助教面临的完成度压力也比较大。 在第四天，助教团队发现部分学生团队未提供上述需要提前给相关人员查看的内容，以通知的形式说了一段比较重的话，特别是三个“严重”。成为了同学们爆发不满情绪的直接原因。在此之后助教主动联系一位同学，交换了关于此事的意见，事后助教向同学们道歉。助教在这一点上做得很好，说明助教是想解决问题的。 现在无法通过博客内的连接查看到对话原文。从优化后的对话来看，WPB 同学对事件思考深刻，抓住了问题的根本原因，容易促成具有建设性的对话；说话有条理，清晰地表达了自己的想法，有利于助教快速准确地理解学生的看法。 总结一下，该事件的直接原因是助教的通知说了很重的话；主要原因是本学期时间更短但事情更多；根本原因是学生和老师助教对于软工的理解不一样。 WPB 同学敏锐地察觉到根本原因，不愧是助教选中并进行对话的同学。他有一句总结“这就是我们对软件工程理解的偏差了，我们希望关注技术，你们希望关注产品”，这是本篇想要重点讨论的内容。 接下来我会展开谈一谈对于软工和软工课程的 个人看法，不一定对，仅供参考。 ","date":"2021-04-21","objectID":"/2021/04/21/software-engineering/:1:0","tags":["软件工程"],"title":"软件工程与“足够好”","uri":"/2021/04/21/software-engineering/"},{"categories":null,"content":"软件 or 工程？ 软件工程课程的重点是软件还是工程？重点当然是工程。软工课的所有内容都在告诉我们实现软件工程的目标的方法有哪些。如果重点是软件，那么可以在OO课上完成这些事情，不需要放在软件工程课。 既然重点是工程，那么工程的目标是什么？助教反复强调有三点，我在这三点中挑最重要的一点，即【通过一定的软件流程，在预计的时间内发布“足够好”的软件】。我想把这句话在补充完整的同时不受限于这句话本身。 前方高能，请坐稳手扶好 工程的目标是：在当前社会环境中，工程实施者使用有限的资源，把能创造的价值最大化，并将其凝固在一个商品里面。 之所以说在当前社会环境中，是因为同一个商品的价值在不同社会发展阶段的价值不一样。例如三十年前做一个图书馆管理系统和现在做一个图书馆管理系统的价值相差特别大。 有限的资源包括了时间资源、能力不同的参与者（需求方、PM、程序员等等）、人脉、资金等。 我们经常说“足够好”，那么这个“足够好”应该用什么来衡量？抽象地说就是看创造了多少价值，具体说就是最终的产品（商品）在多大程度上满足了用户的需求。 在软件工程课程里面，学生团队与实际公司的团队的一个很重要的区别在于他们的资源更加有限，特别是时间资源。他们难以动用资金资源。对于人脉资源，有些同学有，有些则没有。那么同学们主要用于创造价值的资源是【能力不同的参与者】。 同学们容易陷入的误区是，只让技术能力强的同学得到充分发挥，也就是重技术过于产品。然而事实是，在错误的方向上，技术能力越强的同学越会让项目陷入危险。 作为程序员角色的同学，希望通过软件工程课程学到技术。有一部分原因是其他课程没有提供这个机会，但是软工课老师又不能要求其他课程提供这些机会。系统层面的问题这里不作过多讨论，因为解决这个问题需要很长的时间，所以重点讨论课程本身怎么样在短期做到最大化的改善。 软工课程主要提供了两个东西： 学习和实践工程思想和方法 多人合作做出一个软件，从中学习到技术 老师主要想让同学做的是第一点，同学想要做的是第二点。 不学习和实践工程思想和方法的团队可以做出比其他团队更好的软件，但是他们未必使用有限资源最大化地创造出价值，因此技术强的同学抱团的成绩未必比其他团队的成绩高。如前文所述，软件本身（尤其是所用到的技术是否高级）不能作为软工的主要评价依据。不过这在评分的时候，对助教的要求会高很多。 是否最大化地创造价值，有多个依据。我举一个比较容易看出来的依据：看团队主动舍弃了哪些需求，以及降低了哪些需求的完成标准。 老师需要明确地把第一点的要求传递给学生，主要是在宣传上做几件事情： 强调课程的目标是让学生学习和实践工程思想和方法，弱化软件本身的重要性 强调评分主要基于同学们对工程思想和方法的掌握和实践情况 让那些只想要做软件而不是学习工程思想和方法的同学，提前做好分数低的思想准备 对于只想学技术的同学来说，选择这门课是一个不理智的决定。这些同学需要在非常有限的时间内同时完成两件事，既满足老师的需要又满足自己的需要。就像既想从英语技术书中学到技术，又想学到英语。这种“我全都要”的想法，对个人的能力要求非常高，而人们容易高估自己的能力，结果是“全都要不到”。 如果我是这部份学生，我会舍弃掉软工课程的分数，仅仅做到及格。与此同时最大化我自身的需要，即多人合作做出一个想做的软件，学到想学的技术。这就是“足够好”。不过我要强调的是，这是在我意识不到工程思想和方法的重要性的情况下做出的选择。事实上，我当时上这门课的时候，是舍弃了其他课程的分数。 ","date":"2021-04-21","objectID":"/2021/04/21/software-engineering/:2:0","tags":["软件工程"],"title":"软件工程与“足够好”","uri":"/2021/04/21/software-engineering/"},{"categories":null,"content":"老师和助教是【教授软件工程课程】这一工程的实施者 对于老师和助教来说，他们也是在实践工程。这一工程课的具体目标是使用有限的资源让【软件工程课程】的学生从软工课中学到和实践的工程思想和方法的价值最大化。 在有限的资源发生变化时，要做出相应调整，适当舍弃一些内容。我看到助教团队和老师出题的时候，给了一些不同要求的题目供选择，在对话中也有谈到。 这正是在通过舍弃一些内容，让同学们学习和实践更具有价值的工程思想和方法。这是很好的实践，只是由于信息未能充分地传达给同学们，出现了一些问题。 改革是不可能不出现问题的，但是有勇于离开舒适圈的老师以及愿意解决问题的助教，这门课必定会越来越好。 ","date":"2021-04-21","objectID":"/2021/04/21/software-engineering/:3:0","tags":["软件工程"],"title":"软件工程与“足够好”","uri":"/2021/04/21/software-engineering/"},{"categories":null,"content":"ESP32 芯片是由乐鑫开发的芯片。下图是基于 ESP32 芯片的开发板： 图 1：ESP32 DevKitC V4 开发板 ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:0:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"ESP32 的多种应用开发方式 至少有四种方式可以选择： 乐鑫官方的 ESP-IDF[1] 乐鑫官方的 Arduino 开发工具包[2] MicroPython[3] TinyGo[4] TinyGo 目前（2021-03-14）还未支持 ESP32 的 WiFi 和蓝牙 为了方便开发，通常选择 Arduino 开发工具包和 MicroPython。下文选择介绍前者。 ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:1:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"Arduino core for the ESP32 官方提供 Arduino 开发工具包是为了使用 Arduino IDE 和 Arduino 的生态，并不是要求一定要再买一块 Arduino 板来连接。 其工具包在 GitHub 上： https://github.com/espressif/arduino-esp32 要使用这个工具包进行开发，需要做好两个准备： 下载和安装 Arduino IDE 把 ESP32 开发工具包导入到 Arduino IDE 第一步简单，到官方网站下载即可。以下简要说明第二步。 ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:2:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"安装和配置 Arduino IDE 的 ESP32 开发环境 导入方式有三种，根据情况选择。资料比较全，这里简单列举出来，不再过多重复。 最简单的配置和安装方式是使用 Arduino IDE 的开发板管理器： https://github.com/espressif/arduino-esp32/blob/master/docs/arduino-ide/boards_manager.md (Installation instructions using Arduino IDE Boards Manager) 前提是能访问 raw.githubusercontent.com 这个域名（被墙了）。 如果没法访问，那就自己用 Git 去 Clone GitHub 仓库： https://github.com/espressif/arduino-esp32/blob/master/docs/arduino-ide/windows.md (Steps to install Arduino ESP32 support on Windows) 不过这种方式比较慢，因为要 Clone 整个仓库，比较大。 下载仓库的压缩文件： https://www.cnblogs.com/codeit/p/14323836.html (Arduino配置ESP32开发环境) 不过由于不能执行 Git 方式的自动 Clone，需要自行下载 Git 子模块。即文章里已经有提到的 ESP32_AzureIoT_Arduino。这个是开发包的官方依赖，不用担心存在什么大问题，使用其他方式也得这样安装。 如果 GitHub 下载慢，试试 GitLab： https://gitlab.com/schaepher/arduino-esp32 (Arduino Esp32) https://gitlab.com/schaepher/ESP32_AzureIoT_Arduino (ESP32 AzureIoT Arduino) ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:3:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"选择哪块板 在 Arduino IDE 的 [ 工具 | 开发板 | ESP32 Arduino ] 菜单里面能够看到有多个选项，不同的选项对应着不同的配置。为了不影响后续开发，需要选择合适的板。 如果上面有和手头上相同名称的板，那么直接选择就可以了。如果没有相同名称或者不太确定，那么就需要根据一些内容来判断。 从已有信息中找到目标。 菜单的第一个选项是 ESP32 Dev Module。根据这个名称到工具包代码里搜索，找到 boards.txt 这个文件。里面就是各种板的配置。 大致对比了一下不同板的配置，基本都一样。部分区别列举如下： build.variant 选项 这个是跟引脚相关的配置。主要关注这个配置。 build.flash_freq 选项 这个是 Flash 存储器的频率。默认的值有的是 40m 有的是 80m。 menu.PSRAM.* 选项 有些板带了 PSRAM，就会多出这部分配置 其他选项在目前阶段看起来不太重要，因此这里主要关注 variant （变体）选项，确保选择的板的引脚对应正确。 以 build.variant=esp32 为例。在 variants 文件夹里面找到 esp32 文件夹，打开里面的 pins_arduino.h。再结合仓库里的 README.md 底下的 PINMAP，就能知道 pins_arduino.h 里面配置的含义了。 图 2：引脚配置文件里的数字与开发板引脚的对应关系 从上图看出，数字不是乱选的。这些数字分别对应了通用输入/输出引脚（GPIO，General Purpose Input Output），其中有一部分引脚只能作为通用输入（GPI）。 不同的开发板可能会在设计的时候就占用了不同的 GPIO 引脚，因此需要在开发板剩余的 GPIO 中选择合适的引脚配置到 pins_arduino.h 里面。 在配置的时候，需要参考开发板的引脚文档。如果缺少文档，可以直接看板上的引脚数字。这些引脚表示对应数字的 GPIO 引脚。具体的 GPIO 引脚都可以用作哪些功能，可以参考芯片的数据手册[5]。 在选择开发板选项的时候，着重注意一下引脚文件的配置就行了。不要选择那种配置了已被开发板元器件占用的引脚。 引脚配置文件里面还有开头的几个宏定义，这些是固定的。例如： #define NUM_DIGITAL_PINS 40 表示芯片共有 40 个数字管脚。ESP 32 共有 49 个管脚，其中 9 个引脚的类型为 P，即 Power。剩下的要么是通用 I/O ，要么是独用的 I 或者 O。 ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:4:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"SPI 通信 变体引脚配置文件里面跟 SPI 相关的引脚使用的是： SS = GPIO5 MOSI = GPIO23 MISO = GPIO19 SCK = GPIO18 查询芯片的数据手册可以看到这些引脚的功能。 图 3：ESP32 数据手册里的管脚描述[5] 一个引脚有多个功能，但在使用的时候只选择一种。 ESP32 共有 3 组 SPI（SPI、HSPI、VSPI），这里是用到其中的 VSPI，没啥特别。 ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:5:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"拿个例子过一遍开发和上传的流程 在官方的 Arduino 开发工具包里面找到 libraries/ESP32/examples，里面有例子。 在将开发板连接到电脑之后，挑选一个例子的代码复制到 Arduino IDE 的编辑器里面，编译并上传。 例如选择 examples/ChipID/GetChipID/GetChipID.ino 文件里的代码： uint32_t chipId = 0; void setup() { Serial.begin(115200); } void loop() { for(int i=0; i\u003c17; i=i+8) { chipId |= ((ESP.getEfuseMac() \u003e\u003e (40 - i)) \u0026 0xff) \u003c\u003c i; } Serial.printf(\"ESP32 Chip model = %s Rev %d\\n\", ESP.getChipModel(), ESP.getChipRevision()); Serial.printf(\"This chip has %d cores\\n\", ESP.getChipCores()); Serial.print(\"Chip ID: \"); Serial.println(chipId); delay(3000); } 然后编译，这个选项在 [ 项目 | 验证/编译 ]，或者用快捷键 Ctrl+R。 编译通过后上传，这个选项在 [ 项目 | 上传 ]，或者用快捷键 Ctrl+U。 以下是上传的日志： esptool.py v3.0-dev Serial port COM7 Connecting..... Chip is ESP32-D0WDQ6 (revision 1) Features: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None Crystal is 40MHz MAC: xx:xx:xx:xx:xx:xx Uploading stub... Running stub... Stub running... Changing baud rate to 921600 Changed. Configuring flash size... Auto-detected Flash size: 4MB Compressed 8192 bytes to 47... Writing at 0x0000e000... (100 %) Wrote 8192 bytes (47 compressed) at 0x0000e000 in 0.0 seconds (effective 16383.2 kbit/s)... Hash of data verified. Compressed 18656 bytes to 12053... Writing at 0x00001000... (100 %) Wrote 18656 bytes (12053 compressed) at 0x00001000 in 0.2 seconds (effective 938.7 kbit/s)... Hash of data verified. Compressed 207008 bytes to 108334... Writing at 0x00010000... (14 %) Writing at 0x00014000... (28 %) Writing at 0x00018000... (42 %) Writing at 0x0001c000... (57 %) Writing at 0x00020000... (71 %) Writing at 0x00024000... (85 %) Writing at 0x00028000... (100 %) Wrote 207008 bytes (108334 compressed) at 0x00010000 in 2.2 seconds (effective 767.1 kbit/s)... Hash of data verified. Compressed 3072 bytes to 128... Writing at 0x00008000... (100 %) Wrote 3072 bytes (128 compressed) at 0x00008000 in 0.0 seconds (effective 4915.3 kbit/s)... Hash of data verified. Leaving... Hard resetting via RTS pin... 这样就上传成功了。 接着验证结果。在 [ 工具 | 串口监视器 ] 里面，把波特率设置为 115200，就可以看到输出。例如： ESP32 Chip model = ESP32-D0WDQ6 Rev 1 This chip has 2 cores Chip ID: 5940616 接下来就可以发挥创造力了~ ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:6:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"参考 [1]: https://docs.espressif.com/projects/esp-idf/zh_CN/stable/esp32/ (ESP-IDF 编程指南) [2]: https://github.com/espressif/arduino-esp32 (Arduino core for the ESP32) [3]: http://docs.micropython.org/en/latest/esp32/quickref.html (Quick reference for the ESP32) [4]: https://tinygo.org/microcontrollers/esp32-coreboard-v2/ (ESP32 - CORE BOARD) [5]: https://www.espressif.com/sites/default/files/documentation/esp32_datasheet_cn.pdf (ESP32系列芯片——技术规格书) ","date":"2021-03-14","objectID":"/2021/03/14/esp32-and-arduino-core/:7:0","tags":["ESP32","Arduino"],"title":"ESP32 芯片和乐鑫官方的 Arduino 开发工具包","uri":"/2021/03/14/esp32-and-arduino-core/"},{"categories":null,"content":"设备间通信 两个设备或者多个设备之间通信时，需要有一份共同遵守的协议，避免鸡同鸭讲。 常见的通信协议有：SPI、USB、UART、I2C、CAN[1][2]。 图 1：不同的设备间通信协议 不同的协议是为了满足特定场景的要求而制定的，而不是为了好玩。因此应该分析特定应用的要求，并选择合适的协议。 ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:1:0","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"SPI 协议 使用 SPI 协议的场景是全双工、同步传输、一主多从、线少、高速率。 以下是使用 SPI 协议通信的设备群： 图 2：SPI 一主多从 从图中可以看出一对设备通过 SPI 协议进行通信需要四条线。SPI 四条线分别是： 片选信号线（CS，Chip Select）：选设备 主输出从输入数据线（MOSI）：传数据 从输出主输入数据线（MISO）：传数据 时钟信号线（SCK，Serial Clock）：提供采集数据的时机 ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:2:0","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"数据传输线 由于使用同步传输，所以传输一个字节的 8 位二进制不需要八条导线，只需要一条线分 8 个时钟发送就行了。下文的时钟信号线部分会再次提到。 另外由于要支持全双工，即一次传输过程中主设备传输数据到从设备的同时，从设备也可以传输数据到主设备，因此需要两条传输方向不同的线 MOSI(Master Out/Slave In) 和 MISO(Master In/Slave Out)。如下图： 图 3：全双工。蓝色线表示主设备输出，红色线表示从设备输出。 从图 3 可以看出，单向传输的线连接了多个从设备。 注意这里的接线方式：主设备的 MOSI 接从设备的 MOSI，主设备的 MISO 接从设备的 MISO。 此时我们有了疑问：这些从设备都会同时接收数据吗？多个从设备同时发送数据给主设备岂不是冲突了？ ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:2:1","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"片选信号线 主设备会通过控制从设备的 CS 引脚来激活从设备。上面说过从设备的 CS 引脚为低电平的时候，从设备被激活。 图 4：片选信号线 下图表示从设备被激活后传输数据（NSS 是 CS 引脚的另一个称呼）： 图 5：拉低片选线后才开始传输数据 红框部分表示持续激活从设备，以及采集数据。 注意，主设备用 I/O 线连接到从设备的片选信号线上。 ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:2:2","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"时钟信号线 时钟信号线提供读取数据信号的时机。时钟信号由主设备提供给从设备。 图 6： 时钟信号（中间波浪线表示省略 N 个信号） 时钟线的作用是提供高低电平的变化作为数据采样的信号。根据不同的模式，可以是从高到低的时候采样，也可以是从低到高的时候采样。 SPI 协议有四种通信模式，分别由 CPOL（时钟极性）和 CPHA（时钟相位）来控制。也就是选择采样的时机。 CPOL(Clock POLarity) 表示时钟空闲时的电平。0 为低电平，1 为高电平。如下图表示 CPOL = 0。 图 7：空闲的 SCK 为低电平，即 CPOL 为 0 的情况 CHPA(Clock PHAse) 表示数据有效的时刻的相位，或者说边沿（Edge）。边沿指的是电平变化的时刻，有两种类型：上升沿（Rising Edge）和下降沿（Falling Edge）。0 表示处于第一个边沿类型的时刻数据有效（进行采样），1 表示处于第二个边沿类型的时刻数据有效（进行采样）。 图 8：上升沿和下降沿 下图标出了采样的时机： 图 9：工作在（0,1）模式的数据采样 四种模式用下表列出： SPI 通信模式 CPOL CPHA 特点 0 0 0 时钟空闲时保持低电平，工作时在上升沿从输入引脚采样，在下降沿改变输出引脚的数据 1 0 1 时钟空闲时保持低电平，工作时在下降沿从输入引脚采样，在上升沿改变输出引脚的数据 2 1 0 时钟空闲时保持高电平，工作时在下降沿从输入引脚采样，在上升沿改变输出引脚的数据 3 1 1 时钟空闲时保持高电平，工作时在上升沿从输入引脚采样，在下降沿改变输出引脚的数据 注：两个通信的设备要配置为同一个模式才能正常通信。 通信双方采样的时机是一致的，改变输出的时机也是一致的。不然 A 在采样的时候，B 把输出改了，那 A 得到的数据可能是修改前的数据也可能是修改后的数据，就乱了。 如何确保主从处于同一模式？应该以从设备的模式为准，再配置主设备的模式。从设备的模式有两种情况[3]： 从设备的 SPI 模式由硬件决定，已经被固定 从设备有 SPI 控制器，则 SPI 模式由软件决定，可配置 在模式相同且片选信号线拉低时，开始通信。通常芯片内部的 SPI 控制器会自动控制在 8 次取样后停止通信，也就是一次传输 8 bit，即一个字节。但也可以使用 I/O 线模拟片选信号，这样一次可以传送多个字节[4]。 图 10：工作在（0,1）模式的数据采样 图 10 中的 MSB 和 LSB 表示最高权重位（Most Significant Bit）和最低权重位（Least Significant Bit）[5]。权重指的是对这个数值影响程度高，例如十进制的万比千的权重高。 一个字节，即 8 位二进制数据可以从最高位开始发送，也可以从最低为开始发送。 举个例子： 一个字节的数据：1000 0000 MSB - - - - - - LSB 1 0 0 0 0 0 0 0 从最高位开始发送就是先发 1。 如果通信双方没有达成一致，则接收者得到的数据和发送者的数据的含义不一致。不过这个通常不需要我们配置。 ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:2:3","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"总结 如果要用到 SPI 协议，需要接四条线。 主设备的 MISO 接从设备的 MISO 主设备的 MOSI 接从设备的 MOSI 主设备的 SCK 接从设备的 SCK 主设备任选一个 I/O 接从设备的 CS 主设备在编程的时候需要配置与从设备相同的 SPI 模式。 主设备在编程的时候需要配置第一位（First Bit）[6]先从 MSB 开始发还是 LSB 开始发。从设备会规定第一位是 MSB 还是 LSB。 ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:3:0","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"参考： [1]: https://zhuanlan.zhihu.com/p/47925844 (常见硬件通信协议介绍) [2]: https://blog.csdn.net/heda3/article/details/89053635 (IIC、SPI、UART、USART、USB、CAN等通讯协议原理及区别) [3]: https://blog.csdn.net/u013165704/article/details/81076890 (SPI 接口配置) [4]: https://blog.csdn.net/qq_25814297/article/details/86190106 (SPI通讯有单字节模式和多字节连续模式) [5]: https://www.cnblogs.com/shuaifeng/archive/2009/12/23/1630195.html (MSB与LSB) [6]: https://www.stmcu.org.cn/module/forum/thread-627137-1-1.html ([分享] SPI原理超详细讲解---值得一看) ","date":"2021-03-10","objectID":"/2021/03/10/spi-protocol/:4:0","tags":["SPI"],"title":"设备间数据通信 —— 串行外设接口（SPI）协议","uri":"/2021/03/10/spi-protocol/"},{"categories":null,"content":"安装 USB 转 TTL 芯片驱动 我的 ESP32 开发板用的 USB 转 TTL 芯片是 CH340，因此需要安装 CH340 的驱动。 CH340 芯片的官方网站是： http://www.wch.cn/product/CH340.html Windows 驱动在以下页面下载： http://www.wch.cn/downloads/CH341SER_EXE.html 下载完后默认安装即可。如果提示失败，那就关掉程序重新打开。实在不行就重启操作系统再试。 安装后查看 Windows 能否识别。 把 ESP32 开发板连接到 Windows。 打开 Windows 的设备管理器，在【端口（COM 和 LPT）】底下可以看到 CH340，后面的 COM 带了个数字。 这个数字不固定，比如我的是 COM7，实际以设备管理器显示的为准。 打开 Arduino 或者其他串口工具，选择刚刚看到的 COM 口进行连接，选择 115200 波特率。就可以看到输出了[1]。 此时的输出可能是： rst:0x10 (RTCWDT_RTC_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT) flash read err, 1000 ets_main.c 371 现在可以忽略这个错误。 ","date":"2021-03-08","objectID":"/2021/03/08/esp32-install-micro-python/:1:0","tags":["ESP32","MicroPython"],"title":"ESP32 安装 MicroPython","uri":"/2021/03/08/esp32-install-micro-python/"},{"categories":null,"content":"刷入 MicroPython 固件 第一步：下载 MicroPython 固件 进入 MicroPython 的官方网站下载页面： https://micropython.org/download/ 找到 ESP32，例如 “Generic ESP32 module”。点进去。 https://micropython.org/download/esp32/ 找到 Firmware with ESP-IDF。这个表示它包含了 ESP 官方的 ESP-IDF[2]，不用怕下载错。选择 bin 文件的方式： 如果买的开发板是基于 ESP32-WROVER 模组，则选择 GENERIC-SPIRAM，否则选 GENERIC。 v 版本选最新，但不选包含 unstable 的 bin 文件，因为不稳定。 第二步：电脑安装 Python （不是 ESP32）。电脑如已安装 Python 则跳过。 Python 的官方网站： https://www.python.org/downloads/ 随便下载个版本，最新的也行。按默认选项的安装。 可以先将软件源设置为清华大学的镜像（也可以不配置）： pip install pip -U pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 第三步：安装 ESP 工具 通过 Python 的 pip 安装 ESP 工具。 pip install esptool 安装过程会出现以下内容： Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting esptool Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dd/3d/d1d4c004927e6e6807c441ce70330ed969c725d2906053fbd2ff994b4439/esptool-3.0.tar.gz (149 kB) |████████████████████████████████| 149 kB 1.1 MB/s Collecting bitstring\u003e=3.1.6 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/fc/ffac2c199d2efe1ec5111f55efeb78f5f2972456df6939fea849f103f9f5/bitstring-3.1.7.tar.gz (195 kB) |████████████████████████████████| 195 kB 6.4 MB/s Collecting cryptography\u003e=2.1.4 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/64/03/b2a66da95d0a0acac2b5348526f9b92302136563444b33c7049cbdfecf69/cryptography-3.4.6-cp36-abi3-win_amd64.whl (1.6 MB) |████████████████████████████████| 1.6 MB 6.4 MB/s Collecting cffi\u003e=1.12 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/0c/20c3ccdb32fdf86e38901d548f0e11b47d7e037b95373efc1c2379129358/cffi-1.14.5-cp39-cp39-win_amd64.whl (179 kB) |████████████████████████████████| 179 kB 6.4 MB/s Collecting ecdsa\u003e=0.16.0 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/98/16/70be2716e24eaf5d81074bb3c05429d60292c2a96613a78ac3d69526a |████████████████████████████████| 104 kB 6.4 MB/s Collecting pyserial\u003e=3.0 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/07/bc/587a445451b253b285629263eb51c2d8e9bcea4fc97826266d186f96f558/pyserial-3.5-py2.py3-none-any.whl (90 kB) |████████████████████████████████| 90 kB 2.6 MB/s Collecting reedsolo\u003c=1.5.4,\u003e=1.5.3 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c8/cb/bb2ddbd00c9b4215dd57a2abf7042b0ae222b44522c5eb664a8fd9d786da/reedsolo-1.5.4.tar.gz (271 kB) |████████████████████████████████| 271 kB 6.4 MB/s Collecting six\u003e=1.9.0 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl (10 kB) Collecting pycparser Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ae/e7/d9c3a176ca4b02024debf82342dab36efadfc5776f9c8db077e8f6e71821/pycparser-2.20-py2.py3-none-any.whl (112 kB) |████████████████████████████████| 112 kB 6.8 MB/s Using legacy 'setup.py install' for esptool, since package 'wheel' is not installed. Using legacy 'setup.py install' for bitstring, since package 'wheel' is not installed. Using legacy 'setup.py install' for reedsolo, since package 'wheel' is not installed. Installing collected packages: pycparser, six, cffi, reedsolo, pyserial, ecdsa, cryptography, bitstring, esptool Running setup.py install for reedsolo ... done Running setup.py install for bitstring ... done Running setup.py install for esptool ... done Successfully installed bitstring-3.1.7 cffi-1.14.5 cryptography-3.4.6 ecdsa-0.16.1 esptool-3.0 pycparser-2.20 pyserial-3.5 reedsolo-1.5.4 six-1.15.0 看到 Successfully installed 表示安装成功。 使用这个工具查看 ESP32 的串口： esptool.py chip_id 比如我的输出是： esptool.py v3.0 Found 4 serial ports Serial port COM7 Detecting chip type... ESP32 Chip is ESP32-D0WDQ6 (revision 1) Features: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None Crystal is 40MHz MAC: xx:xx:xx:xx:xx:xx Uploading stub... Running stub... Stub running... Warning: ESP32 has no Chip ID. Reading MAC instead. MAC: xx:xx:xx:xx:xx:xx Hard resetting via RTS pin... Serial port COM7 表示 ESP32 在 COM7。 第四步：清除 ESP32 开发板的 flash 芯片已有的内容 这一步是为了确保 MicroPython 刷入 f","date":"2021-03-08","objectID":"/2021/03/08/esp32-install-micro-python/:2:0","tags":["ESP32","MicroPython"],"title":"ESP32 安装 MicroPython","uri":"/2021/03/08/esp32-install-micro-python/"},{"categories":null,"content":"简介 图 1：USB Host Shield Mini USB Host Shield Mini 是一块基于 MAX3421E 芯片 的模组。MAX3421E 芯片是带 SPI 接口既可以用作外设也可以用作主机的的 USB 2.0 控制器[1][2]。 不想每次都输入一大段名字，下面用缩写 UHS 表示 USB Host Shield Mini。 对于要扩展 USB 通信功能的模组来说，需要使用 SPI 协议[3]与 UHSM 通信。 UHSM 是 USB Host Shield 的简化版本，拥有更小的体积，避免成品体积过大。 图 2：USB Host Shield 体积小是优点，缺点是网络上比较难找到各个焊盘对应的引脚，只能自己标。 可以在 Arduino 上面找到这块板的资源[4]。在其他网站上面有 Mini 的资源[5]，不过 PCB 板的文件版本过老，打不开。 于是我决定先把这个缺点干掉。 ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:1:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"引脚与焊盘的对应关系 板上大多数孔都直接连接到了中间的芯片，芯片上写着 MAX3421E 。 找到 MAX3421E 芯片的官方网站： https://www.maximintegrated.com/cn/products/interface/controllers-expanders/MAX3421E.html 里面的【下载数据手册】可以查看芯片引脚信息。以下引脚图来自数据手册： 图 3：MAX3421E 芯片引脚图 现在需要把引脚的关系对应起来。注意到 Mini 板上背面有一个 RST，这是 Reset 的缩写。从图 3 中找到 RES（也是 Reset 的缩写），就是芯片右侧从下往上数第四根引脚。 回到板上找到连接 RST 焊盘的导线连接的引脚，会发现如果旋转到如图 4 的角度，就能和芯片图对上。 图 4：UHSM 模组焊盘与芯片引脚对应 把正背面的过孔[6]对应起来： 图 5：UHSM 正背面过孔对应 注意，图 5 中左边的那些 VBUS、INT、GPX、MAX_RST、SS 都是对焊盘的标记，跟它们所覆盖到的导线没有关系。 根据过孔的对应关系推出正背面导线的连接关系，把各个焊盘对应的引脚标注出来： 图 6：UHSM 焊盘对应引脚名称 取反的 SS、MOSI、MISO、SCLK 用于 SPI 通信[3]。 INT 是用于 SPI 的可选项，用于发送中断（INTerrupt）信号，告知主设备有 USB 事件发生。 VL 是逻辑电平[7]的参考电压，是 SPI 接口和所有其他数字输入及输出的参考电平。以 MAX3421E 用于 SPI 的输出引脚 MISO 为例子。在 VL ≥ 2.5V 且 VL 引脚电流为 +10mA 的时候，MISO 引脚的电压要高于 VL-0.4 才算输出高电平。 从 USB Host Shield 的电路图来看（Mini 的看不了），MAX3421E 芯片的 Vcc 和 VL 用导线连接在一起，共同连接到 3.3V 的电源。 GPIN 0~7 以及 GPOUT 0~7。对于一些不含 SPI 硬件接口的 SPI 主设备，跟这块芯片通信时需要使用 I/O 引脚模拟，占用了宝贵的 I/O 引脚资源。不过这个芯片提供了 8 个通用输入（GPIN）和 8 个通用输出（GPOUT），让 SPI 主设备不仅不会因为接入该芯片而减少总体 I/O 引脚数量，反而增加了。可以说是非常贴心。 取反的 RES 为低时，会把一些寄存器的状态设置为默认状态。 GPX 能表示五种信号，根据一个寄存器的某两个位选择（其中两种信号互斥，由另一个寄存器的某一位决定）。 ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:2:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"从设备还是主设备？ 它作为 USB 端的接收者，是 USB 主设备。同时它作为 SPI 数据发送方，是 SPI 从设备。 ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:3:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"SPI 通信 SPI 协议占用四个引脚，从设备选择（取反的 SS，Slave Select），时钟信号（SCLK），主设备输出（MOSI），从设备输出（MISO）。 关于 SPI 协议的介绍以及四个引脚的作用，可以看上一篇： https://www.cnblogs.com/schaepher/p/14521055.html (设备间数据通信 —— 串行外设接口（SPI）协议) 文章中提到主设备在编程时，需要根据从设备的信息配置两个选项： SPI 模式 First Bit 从数据手册[1][2]的SCLK（串行时钟）部分写着： MAX3421E 在 SCLK 的下降沿改变其输出数据（MISO），在 SCLK 的上升沿采样输入数据（MOSI） 因此主设备选 (0,0) 或者 (1,1) 都行。 数据手册的应用信息一节里的 SPI 接口部分写着： 所有 SPI 传送都是 MSB 在前 所以设置 First Bit 时，应设置为 MSB。 另外 MAX3421E 对 SCLK 最高频率限制在 26MHz。 ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:4:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"供电 MAX3421E 的工作电压范围为 3.0V ~ 3.6V，通常约定使用 3.3V 的电源[8]。 由于 USB 外设（如键盘）的工作电压通常为 5.0V，因此不能使用板上提供的电压。 图 7：USB 四条导线及供电焊盘 USB 的 Vcc 连接着一个焊盘，即图 7 中标号 1 的地方。这个是 PCB 板设计者预留的一个供电口。 如果只需要 3.3V 的电源，则不需要任何改造，使用默认的导线即可。但如果需要更高的电压，则需要使用这个预留的供电口。 如果要使用这个供电口，则应先把原先的供电导线切断。如图 7 中的黄色标记所示，用小刀或者其他工具按照指示将导线切断。这样需要把 5.0V 的电源接到标号 1 的焊盘上。 在这样操作后，总共需要引入两个电压不同的电源到这块板上。一个 5.0V 的电源引到图 7 标号 1 的焊盘，另一个 3.3V 的电源引到 VL 焊盘。 ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:5:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"编程 UHSM 自身不支持写入程序，但接入 UHSM 的模块（例如 Arduino、ESP32）在编程时可以使用 Github 上开源的库通过 SPI 协议操作 MAX3421E 芯片里的寄存器。 https://github.com/felis/USB_Host_Shield_2.0 (USB_Host_Shield_2.0) ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:6:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"参考 [1]: https://datasheets.maximintegrated.com/cn/ds/MAX3421E_cn.pdf (MAX3421E 数据表（第三版）——中文) [2]: https://datasheets.maximintegrated.com/en/ds/MAX3421E.pdf (MAX3421E 数据表（第四版）——英文) [3]: https://www.cnblogs.com/schaepher/p/14521055.html (设备间数据通信 —— 串行外设接口（SPI）协议) [4]: https://www.arduino.cc/en/Main/ArduinoUSBHostShield\u0026lang= (Arduino USB Host Shield) [5]: https://chome.nerpa.tech/downloads/#Arduino_USB_Host_Shield_Documentation ( Circuits@Home) [6]: https://www.cnblogs.com/schaepher/p/14492102.html (快速了解线路板（PCB）基础知识) [7]: https://baike.baidu.com/item/%E9%80%BB%E8%BE%91%E7%94%B5%E5%B9%B3 (逻辑电平) [8]: https://www.zhihu.com/question/22687846/answer/31508409 (为什么很多低功耗的芯片都采用3.3v的电源，这个电压有什么科学依据吗？) 其他： https://www.pjrc.com/teensy/td_libs_USBHostShield.html https://www.itead.cc/wiki/Arduino_USB_Host_Shield https://chome.nerpa.tech/arduino_usb_host_shield_projects/ ","date":"2021-03-07","objectID":"/2021/03/07/usb-host-shield-mini/:7:0","tags":["Board","USB"],"title":"给模块添加 USB 支持的 USB Host Shield Mini","uri":"/2021/03/07/usb-host-shield-mini/"},{"categories":null,"content":"芯片 芯片又称为集成电路（Integrated Circuit，IC），处理器是一种芯片，CPU 一种处理器[1]。 通常我们看到的芯片是经过封装的，目前主流的封装类型有 SOP 、 QFN 、 BGA 三种[2][3]。下面会展示经过不同方式封装的芯片。 ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:1:0","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"中央处理器 中央处理器（Central Processing Unit，CPU）是一种芯片。 下图是经过 DIP 封装的 Intel 8086 CPU。 图 1：Intel 8086 CPU ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:1:1","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"单片机 单片机又称为微控制单元（MicroController Unit，MCU），在处理器的基础上集成了 ROM、RAM、计数器等芯片。单片机也是芯片。 例如下图是经过 QFN 封装后的 ESP32 MCU[4]： 图 2：ESP32 的正面和背面 ESP 32 的内部[5]： 图 3：ESP32 的内部 可以看到 ESP32 使用的是 Xtensa 的 32 位处理器[6]，内部含有 448KB ROM 和 520KB RAM。 虽然 ESP32 支持 WiFi 和蓝牙，但这个芯片本身并不能直接发送 WiFi 和蓝牙信号。 ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:1:2","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"模组 模组由芯片和外围设备组成。例如 ESP32-WROOM-32 模组[7]： 图 4：ESP32-WROOM-32 模组的正面和背面 ESP32-WROOM-32 模组在 ESP32 芯片的基础上添加了 4MB 的闪存（Flash） 、 MIFA 天线[8][9]（发送蓝牙和 WiFi 信号）、 40 MHz 晶振等等[10]。 顺便提一句，模组顶部的那个弯弯曲曲的就是 MIFA 天线（板载天线）。另一种模组 ESP32-WROOM-32U 使用的是 U.FL 天线，如下图右上角是一个 U.FL 天线座，使用时需要接上一条天线。 图 5：ESP32-WROOM-32U 模组的正面 可以直接把程序通过串口写入模组，不过需要自己买 USB 转串口工具。 ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:2:0","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"开发板 开发版主要是用于开发测试，测试完毕后再换成模组，将模组集成到最终的线路板上。 开发板通常添加了 USB 转串口编程接口（也可用于供电）、按钮、LED 灯等组件。 下图是 ESP32 DevKitC V4 开发板[11]： 图 6：ESP32 DevKitC V4 开发板的正面 ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:3:0","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"业余爱好者如何选择 对于只是自己做点东西玩的非专业人士，最好一开始就买开发板，不要因为模组体积小就选择它。 选开发板至少有以下几个好处： 不用自己买 USB 转串口工具，只要有 USB 线就行了 而且就算自己买了 USB 转串口工具，还需要面对选择引脚高低电平表示下载模式（刷入固件）还是工作模式。 开发板上有按键可以完成这种切换，例如 ESP32 DevKitC V4 开发板的 Boot Button。 买模块的时候可能买到只有少数几个 GPIO 引脚的模组。 GPIO 比较麻烦的一点是在使用时需要先选择 IO 引脚，例如选择 5，那就是让 GPIO = IO5。 上面的 ESP32-WROOM-32 模组的背面可以看到有多个 IO 引线，不需要像 GPIO 那样还需要先选择。 网络上大多数教程都基于开发板 总的来说，最主要的还是避免因各种麻烦让信心受到打击。等做出一些东西后，再考虑自制 PCB 板放模组，缩小体积。 ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:4:0","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"参考 [1]: https://blog.csdn.net/weixin_41939983/article/details/105893225 (半导体元件、芯片、处理器、CPU、MCU的区别) [2]: http://www.openpcba.com/web/contents/get?id=278 (BGA、QFN、SOP，不同封装，怎么使用？-燚智能硬件开发周教授) [3]: https://blog.csdn.net/LEON1741/article/details/104783084 (浅谈各种常见的芯片封装技术DIP/SOP/QFP/PGA/BGA) [4]: https://blog.csdn.net/wangyx1234/article/details/107300913 (芯片、模组、开发板的区别与联系-结合ESP32浅谈) [5]: https://www.espressif.com/sites/default/files/documentation/esp32_datasheet_cn.pdf (ESP32 系列芯片 技术规格书) [6]: https://blog.csdn.net/whatday/article/details/87268727 (XTENSA处理器介绍) [7]: https://docs.espressif.com/projects/esp-idf/zh_CN/latest/esp32/hw-reference/modules-and-boards.html#esp32-wroom-32 (ESP32 系列模组和开发板) [8]: https://blog.csdn.net/ASKLW/article/details/99687718 (PCB天线知识) [9]: https://www.cypress.com/file/437221/download (天线设计和射频布局指南) [10]: https://www.espressif.com/sites/default/files/documentation/esp32-wroom-32_datasheet_cn.pdf (ESP32­-WROOM­-32 技术规格书) [11]: https://docs.espressif.com/projects/esp-idf/zh_CN/latest/esp32/hw-reference/esp32/get-started-devkitc.html#id5 (ESP32-DevKitC V4 入门指南) ","date":"2021-03-07","objectID":"/2021/03/07/chip-module-devboard/:5:0","tags":["芯片","模组","开发板"],"title":"芯片、模组、开发板以及业余爱好者如何选择","uri":"/2021/03/07/chip-module-devboard/"},{"categories":null,"content":"线路板 线路板（又称为电路板，PCB板），是印制了导线的一块版。它的主要作用是减少导线占用空间，并将导线按照清晰的布局组织起来。 图 1：导线与线路板 线路板是覆铜板加工的结果，覆铜板是在绝缘基材的表面上覆盖一片铜箔做成的。 通常使用蚀刻[1]加工覆铜板。做法是在覆铜板上要保留的线路上涂上抗腐蚀的材料，然后用化学试剂将其余部分腐蚀掉，剩下的就是作为导线的铜箔。以下视频展示了蚀刻过程： 【自制PCB电路板腐蚀全过程】 https://www.bilibili.com/video/BV1yk4y127B5 在看实际的电路板（如图 1）时，会发现有些地方导线比较宽，甚至是一大块（通常是地线）。这是根据公式 电流=电压/电阻 ，减少导线的电阻对电流大小的影响[2]。通常线宽的关系为：地线 \u003e 电源线 \u003e 信号线[3]。 ","date":"2021-03-06","objectID":"/2021/03/06/pcb/:1:0","tags":["PCB"],"title":"快速了解线路板（PCB）基础知识","uri":"/2021/03/06/pcb/"},{"categories":null,"content":"如何在成本可接受的条件下快速地制作线路板 想要获取一块线路板，有两个步骤： 设计 生产 在电脑上使用软件进行设计并输出 PCB 文件，然后到某宝找“PCB 打样”，下单后把 PCB 文件发给店家就行了。 打样就是打印样品，毕竟线路板通常都是大批量生产，而大批量生产之前需要先打印一些样品用于调试。如果只是自己做一些玩具，到打样这一步就行了。 不要以为打样很贵，PCB 版的长宽如果不超过 10cm ，20 块钱 10 片。 如果别人已经设计好我们想要的版，可以直接下载他的 PCB 文件，更加简便。 ","date":"2021-03-06","objectID":"/2021/03/06/pcb/:2:0","tags":["PCB"],"title":"快速了解线路板（PCB）基础知识","uri":"/2021/03/06/pcb/"},{"categories":null,"content":"如何得到 PCB 图 通常不是一步到位直接画 PCB 图的，而是先把电路图画出来，再转换成 PCB 图。 这里使用的绘制工具是立创EDA[4]，下载安装后不用注册也能免费使用。 以下是一个用 micro usb 点亮 LED 灯的 PCB 的示例。 第一步：电路图 图 2：电路图 第二部：PCB 图 图 3：PCB 图 接下来可以看看线路板 2D 的预览： 图 4：线路板 2D 还可以预览 3D： 图 5：线路板 3D 通常会让元器件紧紧贴住线路板以减小体积，都是在板的另一面焊接，因此背面也会有焊盘。在打孔之后，由于板的中间是绝缘层，所以需要在孔壁涂上金属层连接到背面的焊盘用来导电。 这个示例来自于 B 站 UP 主 BenBlue 的分享： 【E03、从原理图到PCB只需3步】 https://www.bilibili.com/video/BV1SA411b7L5?p=3 ","date":"2021-03-06","objectID":"/2021/03/06/pcb/:3:0","tags":["PCB"],"title":"快速了解线路板（PCB）基础知识","uri":"/2021/03/06/pcb/"},{"categories":null,"content":"双面布线 前面演示的是仅在一面布线。当线路复杂时，由于不相连的线路不能交叉，只能绕道或者打孔绕到另一面。 这里用到的板都是双面板，双面板的两面都有铜箔。在双面板上钻孔并在孔壁涂上金属层，用于连接两面的导线。这个孔称为“过孔”。 单面板是只有一面有铜箔。有铜箔的一面用于布线和放焊盘，元器件放到另一面，并在布线一面焊接。 在画 PCB 图的时候，如果在同一面有导线会出现交叉，则软件会控制不让线穿过去，如下图所示： 图 6：不同线路不能交叉 尽管鼠标点到 R1 的右孔，但线穿不过去。 可以选择从外围绕过去，也可以如下图所示选择打孔： 图 7：过孔 注：使用 45 度折线布线而不是 90 度，以减小高频信号的辐射[3]。 蓝色线表示这条线在线路板的背面，下面的 2D 实物图比较直观： 图 8：双面板过孔 2D 图 以及 3D 图： 图 9：双面板过孔 3D 图 ","date":"2021-03-06","objectID":"/2021/03/06/pcb/:4:0","tags":["PCB"],"title":"快速了解线路板（PCB）基础知识","uri":"/2021/03/06/pcb/"},{"categories":null,"content":"PCB 板的组成部分 图 10：PCB 板的组成部分 器件：元器件。在PCB光板上器件以焊盘+丝印的方式出现[5]。 焊盘：焊盘包含了引线孔及周围的铜箔[6]。 走线：导线。 过孔：跨层连接导线的孔。 印丝：用数字或者字母等在板上标注信息。 阻焊层：避免焊接时误焊临近但不相连的管脚。 ","date":"2021-03-06","objectID":"/2021/03/06/pcb/:5:0","tags":["PCB"],"title":"快速了解线路板（PCB）基础知识","uri":"/2021/03/06/pcb/"},{"categories":null,"content":"参考 [1]: http://m.elecfans.com/article/937356.html (电路板蚀刻是什么意思) [2]: https://www.sohu.com/a/319874935_819258 (PCB 线宽与电流关系，查表与计算！) [3]: https://zhuanlan.zhihu.com/p/50036481 (一般线路板基本设计流程) [4]: https://lceda.cn/ (立创EDA官网) [5]: http://www.elecfans.com/bandaoti/eda/20180814729008.html (印刷电路板（PCB）的主要概念及组成部分) [6]: https://baike.baidu.com/item/PCB%E7%84%8A%E7%9B%98/9847197 (pcb焊盘) ","date":"2021-03-06","objectID":"/2021/03/06/pcb/:6:0","tags":["PCB"],"title":"快速了解线路板（PCB）基础知识","uri":"/2021/03/06/pcb/"},{"categories":["Database","InfluxDB"],"content":" 这篇最早是 2021 年 3 月写的，最近又拿出来复习一遍，也补充了一些新的内容。上一篇博客发表过后已经 3 个月没有发表新的博客，就把这篇拿出来了。内容没有完全梳理完毕，算是笔记。先发出来，后续再逐渐完善。 InfluxDB 是开源的时序数据库，采用列式存储。原先有开源集群版本，但在 0.11 版本之后，集群版本仅在商业版中提供。 在一些场景中，需要用到集群，但又不需要完整的集群功能，只需要简单地实现分片存储和查询或者副本集就行了。可以在开源的基础上添加这些简单的功能。 添加功能的方式可以分为通过顶层的 HTTP API 操作和参考 InfluxDB 源码写相应扩展。 例如通过 HTTP API 写入数据时可以通过 Nginx 的一致性 hash 将数据转发到不同的 InfluxDB 实例，用 HTTP API 把请求发送到所有 InfluxDB 然后手动合并，或者写个中间层合并。但是资源消耗比较大，而且不够灵活。因此很有必要了解 InfluxDB 的源码。 一个系统的发展通常是越来越复杂，并且这个过程中的代码会受到其发展过程中的影响。选择一个合适的版本就行了。这里选择的是 0.11.1 版本的代码。按照惯例，省略的代码用 // ... 代替。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:0:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"基础概念 InfluxDB 的 measurement 类似关系数据库的 table，Point 类似关系数据库的行，存储着某一时刻的数据。Point 的接口定义如下： type Point interface { // Measurement 部分 Name() string SetName(string) // Tag 部分 Tags() Tags AddTag(key, value string) SetTags(tags Tags) // Field 部分 Fields() Fields // Timestamp 部分 Time() time.Time SetTime(t time.Time) UnixNano() int64 // Measurement + Tags Key() []byte // ... } Point 有四大块： Measurement：类似于表。 Tag：起到索引的作用。标签可以有多个，每个标签是一个 key,value 映射。 Field：字段，具体的值。 Timestamp：时间点。 这四块中，Measurement 和 Tag 组成了 Series，类似于一个集合。由于 Series 没有 Timestamp 维度，因此一个 Series 底下有多个时间点的数据，也就意味着有多个 Point。 Series 的 key 由一个 measurement 名称、 多个 tag_key=tag_value 组成，例如 series.Key = \"cpu,host=A\"。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:1:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"influxDB 是怎么读取一行数据的？ 从查询语句中解析出所有 field 找到这些 field 的类型对应的 iterator 根据查询语句对 field 的要求（例如聚合），用装饰模式创建对应的 iterator，包裹底层的 iterator。 由 Emitter 来调用 iterator 读取数据 遍历所有 iterator，读取每个字段的下一个数据，放到 buffer 里面。buffer 是一个数组，按照顺序存放每个 iterator 的结果。 这次读取时的【时间、measurement、tags】会发生变化，作为 row 的 key。 如果读取后在 buffer 里面找不到这个 key，则创建一个 row；如果存在，则加入这次读的数据。 集群在查询时，用了一层 remoteIteratorCreator，来读取远程 shard 的数据。 底层的 iterator 在 tsdb/engine/tsm1/engine.go 里面。 // buildIntegerCursor creates a cursor for an integer field. func (e *Engine) buildIntegerCursor(measurement, seriesKey, field string, opt influxql.IteratorOptions) integerCursor { cacheValues := e.Cache.Values(SeriesFieldKey(seriesKey, field)) keyCursor := e.KeyCursor(SeriesFieldKey(seriesKey, field), opt.SeekTime(), opt.Ascending) return newIntegerCursor(opt.SeekTime(), opt.Ascending, cacheValues, keyCursor) } ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:2:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"Iterator 是如何工作的？ 举一个 sum 的例子。 sum 是函数，在 influxql/call_iterator.go 里面由 NewCallIterator 创建 integerReduceIntegerIterator 或者 floatReduceFloatIterator。 由于 InfluxDB 的数据类型只有四种：浮点数、整数、字符串和布尔值，所以仅需要支持浮点数和整数。 以 integerReduceIntegerIterator 为例。 对于要合并的数据，两者的 field_key 和 tags 是一致的，这样可以生成一个唯一 key。对于每个 key，生成一个对应的 integerReduceIntegerPoint。 integerReduceIntegerPoint 保存了一个 value，放在 prev 结构体里面。每次得到一个相同 key 的时候，获取 integerReduceIntegerPoint，并将当前值与 prev 的值相加，并保存到 prev 里面。在对上一个阶段的结果集做一遍这个合并后，得到一批 IntegerPoint，并从大到小排序。每次 Next() 从中取最后一个。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:2:1","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"嵌套的 Iterator 是如何配合的？ 假设有 IteratorB(IteratorA) 这样的嵌套。 每次 IteratorB 执行 Next() 的时候，会先执行一次 reduce()，目的是合并或过滤掉符合条件的相邻的数据。它会去找 IteratorA 获取一个时间段的所有数据，然后将这些数据转换为只剩下一条。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:2:2","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"一行的数据是如何合并的？ 【时间、measurement、tags】 每次读取的时候都有可能发生变化。这是因为每个字段虽然是按照时间顺序读取，但某一时刻，不一定所有的字段都有值，此时读取的是指定时刻之后的数据。 这样一个 buffer 中，各个字段的时间可能不一致。如果查询语句是按时间增序，则取时间最小的那个字段。以这个字段的 【时间、measurement、tags】 去 buffer 中找相匹配的 field。以此组合成 row。 字段的顺序怎么办？字段的顺序在一开始创建 iterator 集合的时候就固定了。每次读取的时候都是按照 iterator 的顺序读取。在组合 row 的时候，如果一个 iterator 的数据不能满足 【时间、measurement、tags】，则其对应的下标的值为 nil。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:2:3","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"集群应该如何处理？ 集群使用 remoteIteratorCreator，它会创建 ReaderIterator，从 io.Reader 里读取数据。这个 io.Reader 是一个 TCP 连接。 根据 select 指定的时间范围，找到时间范围内的所有 shard 及其所在的集群节点。然后对每个节点都建立一个连接。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:2:4","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"influxDB 是怎么存储数据的？ 存储格式： http://blog.fatedier.com/2016/08/05/detailed-in-influxdb-tsm-storage-engine-one 在启动后，会定期检查是否应该将缓存中的数据刷入到磁盘中。 数据文件是 tsm 类型。整个文件包含四个部分： ┌────────┬────────────────────────────────────┬─────────────┬──────────────┐ │ Header │ Blocks │ Index │ Footer │ │5 bytes │ N bytes │ N bytes │ 4 bytes │ └────────┴────────────────────────────────────┴─────────────┴──────────────┘ Blocks 存储着实际数据。每个 Block 表示一批 series + field 相同且经过压缩后的数据。 ┌────────┬────────────────────────────────────┬─────────────┬──────────────┐ │ Header │ Blocks │ Index │ Footer │ │5 bytes │ N bytes │ N bytes │ 4 bytes │ └────────┴────────────────────────────────────┴─────────────┴──────────────┘ ↑ 取这个部分得到下图 ┌───────────────────────────────────────────────────────────┐ │ Blocks │ └───────────────────────────────────────────────────────────┘ ↓ 切割 ┌───────────────────┬───────────────────┬───────────────────┐ │ Block 1 │ Block 2 │ Block N │ └───────────────────┴───────────────────┴───────────────────┘ ↓ 切割 ┌─────────┬─────────┬───────────────────┬─────────┬─────────┐ │ CRC │ Data │ CRC │ Data │ CRC │ Data │ │ 4 bytes │ N bytes │ 4 bytes │ N bytes │ 4 bytes │ N bytes │ └─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘ 在向 InfluxDB insert 多条数据的时候， InfluxDB 会将每条数据转换成一个 Point，并按照 Point 的 series + field 分组。 func (e *Engine) WritePoints(points []models.Point, measurementFieldsToSave map[string]*tsdb.MeasurementFields, seriesToCreate []*tsdb.SeriesCreate) error { // ... key := string(p.Key()) + keyFieldSeparator + k values[key] = append(values[key], NewValue(p.Time().UnixNano(), v)) // ... } 这里的 p 是 point，p.Key() 是 measurement + series，k 是 field key。values 的类型是 map[string][]Value 每个分组的数据会分别加入到缓存里面。接着写一份到 WAL（Write Ahead Log，写日志） 文件里面，便于程序重启或崩溃后恢复内存数据。 缓存里面包括原先存储在磁盘上的数据，这些数据会和后续新增的数据合并并且排序。 然后对缓存中每个分组的数据执行压缩，压缩的结果是一个 Block。每个分组保存的是一个 Field 的数据，因此一个 Block 对应一个 Field。 block, err := values.Encode(nil) 这里的 values 的类型是 []Value，即上面的那个 values 取其中一个 field 的分组。说明每个 block 存储的是一个 field 的数据。 ┌─────────┬─────────┐ │ CRC │ Data │ │ 4 bytes │ N bytes │ └─────────┴─────────┘ 其中的 Data 部分有三个内容： ┌───────────────────┬───────────────────┬───────────────────┐ │ first timestamp │ all timestamp │ all value │ └───────────────────┴───────────────────┴───────────────────┘ Data 部分第一个数据的时间戳 所有数据的时间戳 所有数据的值 这里的时间戳和它对应的值是分成两部分存储的。 接着计算这个 Block 的 CRC 校验码： func (t *tsmWriter) WriteBlock(key string, minTime, maxTime int64, block []byte) error { // ... var checksum [crc32.Size]byte binary.BigEndian.PutUint32(checksum[:], crc32.ChecksumIEEE(block)) // ... } 先写 CRC 校验码再写 Data。 写入 CRC 校验码之前会判断文件是否已经有内容，如果没有，则写入 Header。 ↓ 这个部分 ┌────────┬────────────────────────────────────┬─────────────┬──────────────┐ │ Header │ Blocks │ Index │ Footer │ │5 bytes │ N bytes │ N bytes │ 4 bytes │ └────────┴────────────────────────────────────┴─────────────┴──────────────┘ 之后把这个 Block 的 【key（即 series + field）、field 的数据类型、时间段、该 Block 在文件的偏移量、该 Block 的长度】 添加到内存中的索引。 t.index.Add(key, blockType, values[0].UnixNano(), values[len(values)-1].UnixNano(), t.n, uint32(n)) 写完所有 Block 之后，再把索引写入到文件。之后把原先的 tsm 文件删掉。 ↓ 索引是这个部分 ┌────────┬────────────────────────────────────┬─────────────┬──────────────┐ │ Header │ Blocks │ Index │ Footer │ │5 bytes │ N bytes │ N bytes │ 4 bytes │ └────────┴────────────────────────────────────┴─────────────┴──────────────┘ ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:3:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"连续查询（Continues Query） 连续查询是由 InfluxDB 按照一定时间间隔执行查询，并将结果插入到指定表中。 连续查询分为基础语法和高级语法。它们的区别在于对查询时间范围和两次查询的间隔的配置。 首先是基础语法： CREATE CONTINUOUS QUERY \u003ccq_name\u003e ON \u003cdatabase_name\u003e BEGIN SELECT \u003cfunction[s]\u003e INTO \u003cdestination_measurement\u003e FROM \u003cmeasurement\u003e [WHERE \u003cstuff\u003e] GROUP BY time(\u003cinterval\u003e)[,\u003ctag_key[s]\u003e] END 基础语法的查询间隔为：GROUP BY time() 的 interval。即如果 interval 为 1m，则一分钟执行一次。 基础语法的查询时间范围为：执行查询的时间点 now() 减去 GROUP BY time() 的 interval。即 WHERE time \u003e= (now() - interval) AND time \u003c now()。 高级语法： CREATE CONTINUOUS QUERY \u003ccq_name\u003e ON \u003cdatabase_name\u003e RESAMPLE EVERY \u003cinterval\u003e FOR \u003cinterval\u003e BEGIN SELECT \u003cfunction[s]\u003e INTO \u003cdestination_measurement\u003e FROM \u003cmeasurement\u003e [WHERE \u003cstuff\u003e] GROUP BY time(\u003cinterval\u003e)[,\u003ctag_key[s]\u003e] END 高级语法比基础语法多了一行：RESAMPLE EVERY \u003cinterval\u003e FOR \u003cinterval\u003e 其中 EVERY 确定查询间隔，FOR 确定查询时间范围。其中一个可以不填，例如： RESAMPLE EVERY \u003cinterval\u003e 或者 RESAMPLE FOR \u003cinterval\u003e。 其中一个不填时，相当于自动把它的 interval 设置为 GROUP BY time() 的 interval。 当数据输入有延迟时，需要使用高级语法的 FOR 扩大查询范围。 例如聚合一分钟的数据需要延迟半分钟，那么将 FOR 设置为 2 分钟。这样就能在第三分钟开始的时候，查询第一和第二分钟的数据。由于第二分钟的数据需要再等半分钟才有，所以相当于是获取第一分钟的数据。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:4:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"写入 https://docs.influxdata.com/influxdb/v1.8/guides/hardware_sizing/ 单机写入的极限是每秒 75 万个字段。估算方法是一秒的 point 数量乘以一条数据的字段数量。point 数量可以从 _internal 库的 write 表的 pointReq 获取。字段数量可以执行 show field keys 获取。 在某些场景下，不是每一秒钟都有 75 万个字段，而是在每分钟内的某一秒会同时收到超过 75 万个字段的数据。这种情况要取这些数据在一分钟每秒的平均值。如果平均值不超过 75 万个字段，那么单机可以承受住。 也就是 InfluxDB 最多一分钟可以写 4500 万个字段，实际值可能会比这个小一些，可以通过业务数据测试实际负载。 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:5:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"写入的优化 https://docs.influxdata.com/influxdb/v2.0/write-data/best-practices/optimize-writes/ 使用 gzip 压缩 批量写，最理想的是一次 5000 行 转换为行的时候，按字典顺序排序 tag 的字段 设置最合适的时间精度。例如同一个 tag 不会有两条同一秒的数据，那么就把时间精度设置为秒级 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:6:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":["Database","InfluxDB"],"content":"Series File https://www.jianshu.com/p/4e6fda6d6b63 ","date":"2021-03-06","objectID":"/2021/03/06/influxdb-read-write/:7:0","tags":["InfluxDB"],"title":"InfluxDB 存储结构、读、写","uri":"/2021/03/06/influxdb-read-write/"},{"categories":null,"content":"并发编程时，经常要用到锁。锁的概念有很多，例如悲观锁、乐观锁、自旋锁等等。在使用锁的过程中，始终有一个疑问：在使用多核处理器的情况下，位于不同 CPU 的线程在修改同一个全局变量的时候，是如何做到加锁的？ 最早的操作系统没有线程这个概念，只有进程的概念，此时进程是操作系统分配资源和调度的基本单位。而我们现在正在使用的操作系统通常都是多线程操作系统，线程是操作系统调度的基本单位，进程仍是操作系统分配资源的基本单位。 在不考虑使用超线程技术的 CPU 的时候，CPU 本身没有线程这种概念。以下均不考虑超线程 CPU。 ","date":"2021-02-23","objectID":"/2021/02/23/concurrency-parallelism-and-lock/:0:0","tags":["Concurrency","Parallelism","Lock"],"title":"并发、并行与锁","uri":"/2021/02/23/concurrency-parallelism-and-lock/"},{"categories":null,"content":"单核处理器 多线程操作系统使用单核处理器，同一时刻只能有一个线程正在执行。 这样在加锁的时候，不会有两个线程同时执行加锁指令，必然是有一个先后顺序。如果两个线程要获取同一个锁，在锁被其中一个线程获取到后，在线程释放该锁前，另一个线程无法获取该锁。 线程在执行过程中会因为中断而切换到其他线程。在程序上观察加锁操作可能只有一条调用的代码，但在执行的时候是多条指令。一旦发生中断，就可能会出现问题。操作系统提供了锁的原子操作。这个操作主要通过开关中断来实现。 关闭中断后，不会被切换到其他线程。此时可以加载内存里的锁状态，然后判断锁是否已被占用。如果被占用则开启中断，让其他线程继续执行。然后再关闭中断，再次判断，直到锁的状态是空闲。此时由于没有开启中断，所以可以将锁在内存的状态更新为已被占用，然后开启中断。这样完成了获取锁的任务。 加锁的方式不只有这一种，还可以用 CPU 提供的 test_and_set 指令。 ","date":"2021-02-23","objectID":"/2021/02/23/concurrency-parallelism-and-lock/:1:0","tags":["Concurrency","Parallelism","Lock"],"title":"并发、并行与锁","uri":"/2021/02/23/concurrency-parallelism-and-lock/"},{"categories":null,"content":"多核处理器 多线程操作系统使用多核处理器，同一时刻可能有多个线程正在执行。 但是要区分两种情况： 程序限制只使用单核 例如 CPython 解释器是单线程的，虽然 threading 封装了操作系统的原生线程，但只有获取到 GIL （Global Interpreter Lock，全局解释器锁）的线程才能执行，因此同一时刻只有一个线程能够执行。 程序开启多核支持 例如 C++ 可以用 std::thread；Golang 通过执行 runtime.GOMAXPROCS(runtime.NumCPU()) 开启多核支持。 在使用多核的情况下，一个进程的多个线程可能分布在不同核心上同时执行。如果这些同时执行的线程想要获取同一个锁，就会产生冲突。 操作系统级别的锁原语无法发挥作用，只能由 CPU 去处理。CPU 必须将多个核心对同一个内存区域的访问串行化。 CPU 提供了总线锁和缓存锁两种方式。 CPU 总线是所有核心与芯片组连接的主干道。当一个线程要获取锁（操作共享内存）的时候，其所在核心会发出一个 LOCK 信号，阻止其他处理器访问内存。 由于锁住总线会阻止其他核心访问其他内存地址的数据，所以提供了缓存一致性机制来减少开销。 ","date":"2021-02-23","objectID":"/2021/02/23/concurrency-parallelism-and-lock/:2:0","tags":["Concurrency","Parallelism","Lock"],"title":"并发、并行与锁","uri":"/2021/02/23/concurrency-parallelism-and-lock/"},{"categories":null,"content":"参考 超线程 https://blog.csdn.net/ybhuangfugui/article/details/107502992 并发、并行、切换 https://blog.csdn.net/tscaxx/article/details/102799508 线程切换耗时 https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html 原语：从0到1，从硬件指令集到OS原语，锁原语的哲学 https://www.cnblogs.com/niuyourou/p/11917919.html ","date":"2021-02-23","objectID":"/2021/02/23/concurrency-parallelism-and-lock/:3:0","tags":["Concurrency","Parallelism","Lock"],"title":"并发、并行与锁","uri":"/2021/02/23/concurrency-parallelism-and-lock/"},{"categories":null,"content":"终端是一种输入输出设备。把终端连接到计算机上，就可以跟计算机进行交互。当今个人电脑最常用的两种终端设备分别是作为输入终端的键盘以及作为输出终端的显示器。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:0:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"各种概念的关系 在 Windows 上的 CMD、 Powershell、 XShell 或者 PuTTY 被称为终端模拟器（Terminal Emulator）。 使用终端模拟器登录到 Linux 时，使用的是伪终端（PTY，Pseudo TTY）。 伪终端分为主和从两个部分，与终端模拟器交互的部分是主（PTM，Pseudo TTY Master），与用户进程交互的部分是从（PTS，Pseudo TTY Master）。 与机器相连接的终端设备，被称为远程打字机（TTY，TeleTYpewriter）。 在 Linux 操作系统里面，物理设备被对应到 TTY 串口（TTYS，TTY Serial）上。将一套物理的终端设备模拟为多套终端设备，在系统里面表示为 TTY。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:1:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"电报机 图 1：电报机 电报是一种最早用电的方式来传送信息的、可靠的即时远距离通信方式。 1837 年发展出了电报机以及电报系统[1]。电报机的发送设备只有一个发报按键（如图 1 所示），通过短按表示短信号（“.” 滴）和长按表示长信号（“-” 嗒）。发送方按下按键后，信号会立马发送出去，接收方很快就能收到消息。 将长短信号按照一定顺序排列得到不同的组合，这些组合可以各自对应一种信息，这些对应关系的对应表称为电码。最出名的电码是摩尔斯电码。 图 2：摩尔斯电码表 有兴趣的话，可以听听看： 三分钟教你学会摩尔斯电码 https://www.bilibili.com/video/BV1Hk4y1y7RH 国际通用求救信号是 SOS，S 由三个“滴”表示，O 由三个“嗒”表示，则 SOS 的摩尔斯电码为 ...---...（滴滴滴 嗒嗒嗒 滴滴滴） 。 电码只有包括滴、嗒、停顿等少数信息，因此需要发送者先 人为地 将字母转换为电码（编码），接收者再 人为地 将电码转换为字母（解码）。接收者需要带上耳机听电报音。 我们希望能有设备代替收发双方做编码和解码的工作，直接与 abcd 这种字母交互。于是找上了打字机。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:2:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"机械打字机（typewriter） 第一台商业化的机械打字机出现于 1874 年[2]。打字机主要由键盘、印字机组成。键盘提供了更多地按键，包括了 26 个字母、数字和其他一些按键。 图 3：机械打字机 机械打字机通过机械的转动将字母打在纸上。如果你有看过《紫罗兰永恒花园》，女主的工作就是使用机械打字机帮别人写信。比如下面这个视频里的： 【开箱】我把京紫女主用的打字机给买回了家 https://www.bilibili.com/video/BV18W411v7x9 你会发现看到的打字机的键盘布局是 QWERT 布局。现在 QWERT 的键盘布局实际上是继承于机械式打字机。机械打字机使用 QWERT 布局是因为如果在机械式打字机上打字太快会发生卡壳，而这种布局能减缓使用者的打字速度。 如果看了打字机的使用过程，你就能知道现代换行用 \\r\\n 表示的含义了。 \\r 表示 carriage return。carriage 的英文释义为 “A moving part of a machine that carries other parts into the required position”[3]。因此 carriage return 是让打字机印字时移动的那块返回到行首。 接着由表示 Newline 的 \\n 让印字机旋转到下一行。\\n 也称为 line feed。 因此文本编辑器里的 CRLF 是 carriage return + line feed 的缩写，表示 \\r\\n。 从机械打字机的使用者的角度看，按下字母 a 的按键后纸条上就显示 a，不需要使用者做编码和解码工作。这正好做电报机的编码解码器，只需要设计一个转换器，将字母按键的机械转换为对电报机的操作就行了。 图 4：打字机、摩尔斯电报机发送器、电传打字机 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:3:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"电传打字机（teletypewriter，teleprinter） 图 5：电传打字机 首次用于发送电报的电传打字机出现于 1887 年[4]。它主要由键盘、印字机和收发报机等组成。看起来是打字机 + 电报机的组合对吧？ 感受一下电传打字机的运行： [转载]在30多年前的电传打字机上聊天是怎样一种体验? https://www.bilibili.com/video/BV1Xx41137Tp 电传打字机将信号通过收发报机发送出去。上面那个视频 04:04 时间点，能听到电报机的声音。虽然电传打字机也是用电报机，但它通常使用五单位电码（博多电码），而不是长短不一的摩尔斯电码。 电传打字机传送方式有两种： 一种可以看作电报的改进版，即连接的双方有一方按下按键时，会把电码立即传送到另一方的印字机上； 另一种用打字机把消息信号以在穿孔纸带[5]上凿孔，再把纸带放到发报器上发报（如图 5）。 下面这个视频就是第二种方式的演示： Teletype Model 35ASR 电传打字机 https://www.bilibili.com/video/BV1Gt411B7Xf 如果只将电传打字机看作电报的改进版，你会发现少了个东西：看不到已经打了什么字。不过打字机自身有带打印功能，可以让打字机在收发报时将字打印在纸上。这个步骤称为“回显”。 由于电传打字机没有缓冲，如果发送方太快，接收方可能来不及打印。于是发送方允许接收方回传两个控制信号，一个告诉发送方停止发送，一个告诉发送方继续发送。现在这两个特性仍然被终端模拟器保留，快捷键分别是 ctrl + s 和 ctrl + q。这就是有时候按错按键发现终端模拟器“卡住了”的原因[6]。 随着技术的发展，计算机出现了。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:4:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"把电传打字机改造为计算机的终端 图 6：把电传打字机改造为计算机的终端 第一台计算机在 1946 年问世，但它是通过操作按钮来控制。不过后来的大型机发展到包含了控制台、终端和主机。控制台用于管理设备的硬件，终端则用于和计算机交互。这个时候现代键盘还没出现，用到的终端是从电传打字机改造而来的，而且和整台机器组合在一起。 UNIX 系统就是用电传打字机写出来的[7][8]。 图 7：Ken \u0026 Den 现在也可以把电传打字机接到电脑上作为输入终端和显示终端，只是需要调制解调器（modem，猫）连接到收发报机： 用电传打字机作为 Linux 的显示终端 https://www.bilibili.com/video/BV1aK4y187yp 前面说过，计算机包含了控制台和终端，这两者在现在似乎没有区别，但以前是有区别的。如下图： 图 8：控制台和终端[9] ","date":"2021-02-23","objectID":"/2021/02/23/tty/:5:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"多终端 计算机数量少，运行的费用高，且一个终端未必能完全利用 CPU 资源。于是出现了分时操作系统，允许有多个终端，且可以是远程终端。这样可以在只有一台终端设备连接到计算机的情况下，模拟出有多套终端设备连接的样子。同时也可以让其他人付费通过远程使用多出来的终端。 在本地接多个终端的图片不好找，不确定下面这篇文章里的图片算不算： 图 9：IBM System/390 [10] 硬件的部分就到这里，接下来是跟 Linux 操作系统相关的内容。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:6:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"串行终端 ttyS (ls /dev/ttyS*) Linux 系统里面有表示接入的硬件设备的文件，用 ls /dev/ttyS* 查看。这些文件与计算机的串口（serial port）对应。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:7:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"终端 tty (ls /dev/tty) 和控制台 console (ls /dev/console) 在大型机时代，终端和控制台是分开的，分别表示一台物理设备，但现在两者的界线没有那么清晰。 tty 在经过模拟后得到多个虚拟设备，统称为虚拟终端，就好像有多套物理的输入输出设备，而 console 只有一个。 通过 ls /dev/tty[0-9]* 可以看到所有虚拟终端，将这个集合表示为 ttyN，其中 N 表示数字。用 ctrl + alt 加上 F1 ~ F6 其中一个快捷键来切换到 tty1 ~ tty6。执行 tty 会显示当前所在 tty。 注：如果通过类似 XShell 的终端模拟器远程连接，得到的结果与上述不同。到伪终端的部分就知道为什么了。 你会发现少了 tty0。/dev/tty0 和 /dev/console 的行为类似，都是表示当前终端，但只允许操作系统和 root 用户写入数据。而对于普通用户，/dev/tty 这个文件就是表示当前终端，且当前用户可以写入数据。 如何使用“当前终端文件”？可以试试执行 echo \"test\" \u003e /dev/tty。你会发现屏幕输出了 test。如果执行 tty 得到的是 /dev/tty3，那么执行 echo \"test\" \u003e /dev/tty3 将得到相同结果。/dev/tty 是 /dev/tty3 的别名。 Linux 是多用户操作系统，可以切换到不同的终端登录不同的用户。执行 ls -l /dev/tty[0-9]* 能查看各终端的拥有者。例如执行 ls -l /dev/tty3 可以看到： crw--w---- 1 schaepher tty 4, 3 Feb 24 12:20 /dev/tty3 这表示 schaepher 这个用户持有 /dev/tty3 这个设备。 到目前为止，仍然是使用文本终端，并且在虚拟出的多套输入输出设备中选择。通俗地说，一个屏幕同一时刻只能展示其中一个终端的输出。在有图形界面后，出现了如何在图形界面里使用文本终端的问题。也就是想让输出到某个设备的内容单独地展示在屏幕的某一块区域，以及聚焦到这块区域后，可以输入内容到设备。 我们希望在图形界面里单独创建一个进程，让它在屏幕的一块区域内作为文本终端运行，于是这个进程就作为一个终端模拟器来使用了。 说到终端模拟器，上述的虚拟终端就是经过模拟多套设备后得到的。不过问题是图形界面运行在用户空间，而终端模拟器运行在内核空间。我们需要将终端模拟器移动到用户空间，但同时又要避免用户空间绕过内核直接操作设备。 于是伪终端 pseudo tty 被发明出来解决这个问题。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:8:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"伪终端 pty (ls /dev/pts/*) 伪终端被分为主（master）从（slave）两个部分。pty slave 用于模拟硬件文本终端设备，pty master 提供了一套终端模拟器进程控制 pty slave 的方法。 终端模拟器运行于用户空间，与 pty master 交互，pty master 再通过 LDISC 与 pty slave 交互，pty slave 再与用户进程交互。 例如登录到 Linux 图形界面后打开终端窗口执行 tty，可以得到 /dev/pts/0。或者使用 ssh 登录到远程服务器执行 tty，也会得到 /dev/pts/某个数字。这里的 pts 指的是 pty slave。 执行 ls -l /dev/pts/ 能看到都有哪些用户使用了 pty。 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:9:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"参考 [1]: https://en.wanweibaike.com/wiki-Electrical_telegraph (Electrical telegraph) [2]: https://en.wanweibaike.com/wiki-Typewriter (Typewriter) [3]: https://www.lexico.com/search?utf8=%E2%9C%93\u0026filter=en_dictionary\u0026dictionary=en\u0026s=t\u0026query=carriage (carriage) [4]: https://site.xavier.edu/polt/typewriters/tw-history.html (A Brief History of Typewriters) [5]: https://www.ccf.org.cn/Computing_history/Updates/2020-09-10/720617.shtml (我的计算机收藏之旅（9）：你见过或没有见过的存储器) [6]: https://unix.stackexchange.com/questions/137842/what-is-the-point-of-ctrl-s/137846#137846 (What is the point of Ctrl-S?) [7]: https://www.bell-labs.com/usr/dmr/www/picture.html (An amusing photo) [8]: https://www.zhihu.com/question/59318669/answer/164408260 (为何 Linux 的系统 API 相比 Win32 到处是缩写？有何优劣? 造成两者差别的原因是什么？) [9]: https://www.cnblogs.com/pluse/p/6897830.html (关于Unix/Linux的终端、伪终端、控制台和shell) [10]: https://servers.pconline.com.cn/gc/1202/2679853_5.html (人类登月不可或缺 大型机半个世纪发展史) 其他： Linux 终端(TTY) https://blog.csdn.net/smilefxx/article/details/102766993 Pseudo TTY https://en.wanweibaike.com/wiki-Pseudo%20TTY The TTY demystified http://www.linusakesson.net/programming/tty Linux TTY/PTS概述 https://segmentfault.com/a/1190000009082089 扫盲 Linux＆UNIX 命令行——从“电传打字机”聊到“shell 脚本编程” https://blog.csdn.net/JunSIrhl/article/details/104374649 ","date":"2021-02-23","objectID":"/2021/02/23/tty/:10:0","tags":["Terminal","TTY","PTY","PTS"],"title":"终端发展过程及 tty、pty、pts 的区别","uri":"/2021/02/23/tty/"},{"categories":null,"content":"我打算先写跟技术相关的内容，然后再详细说换工作的想法。 我在生活篇里面提到了以下几点： 阅读 PHP 函数注册、函数调用、垃圾回收、PHP 对象相关的源码，加深了对脚本语言的理解。 阅读 JAVA、PHP 5、PHP 7、Redis、Go 的 HashMap 的源码，比较及整理到博客，并在分享会做了分享。 阅读一部分 Linux 内核的 Socket 相关源码，以及 Golang 的 HTTP Server 跟 TCP 相关的源码。 阅读 InfluxDB 查询和写入相关的源码，并用文字描述出来。 阅读跟内存分配相关的文章以及源码，主要是 jemalloc 、 TCMalloc 和 ptmalloc。 整理和翻译一小部分 HTTP 1.1 的 RFC 内容。 还有为了面试去深入学习的一些内容。 这些大多数都是只了解了一部分，没有完全深入去理解。凡是我没有输出到博客的技术内容，我都一律认为自己没有完全掌握。就算是输出到博客，也很有可能只是有了片面的理解。 2020 年总共创建了 63 篇博客。大多数是技术博客，但通常只有几句话。另外其中有一些是我自己捣鼓服务器时的记录。仅发布了少数几篇博客。不过我会把一些只有骨架的半成品发表到 Github 博客上，只是国内大多数时候没法访问。其他的都躺在草稿箱。2021 年的任务是把这些博客补充完整然后发出来。 Github 博客： https://schaepher.github.io 技术栈的变更 这次我对自己非常狠，新工作几乎把我技术栈换掉了。 主力编程语言从 PHP 换到 Golang 刚毕业的时候还想着写 JAVA，后来第一份工作用的是 PHP。虽然现在换成 Golang 了，但我并不排斥 PHP。 用 PHP 搭配上 Laravel 后，写业务非常舒服。Laravel 借鉴了 Rails 的理念。它很全面，可以从它那里学到很多设计模式和最佳实践，现在偶尔还会去参考 Laravel 的实现。它也很自由，可以用第三方库替换框架自带的库。 The Rails Doctrine - Rails 信条 https://ruby-china.org/wiki/the-rails-doctrine PHP 和 Laravel 是在 WEB 业务方向特化的语言，这是它们的优点。而再加上 Swoole ，PHP 就可以用来编写高性能高并发的服务了。有人说 Laravel 的学习成本高，但我没有什么感觉，反倒是感觉用起来很舒服。 但是我仍然选择了 Golang。原因有很多。 主要原因： PHP 能做的东西有限 我见识比较少，想不起来有什么重要的组件是用 PHP 写的。我特地到 Github 的 Trending 看了一圈，基本都是跟业务系统或者基础组件管理页面的项目。没有数据库、消息队列或者中间件。 毕竟 PHP 就不是用来做这些的，它的定位是写 WEB 应用，因此它在其他方向上的上限就比较低。 我认为写业务是非常普遍但很重要的事情，毕竟没有那么多的基础组件可以写，大多数人都是在写业务代码。 但我更希望自己能够参与到更加基础的组件中去，这样我才能更多地借现实场景的需要去接触和学习核心的技术，更多地满足自己对底层技术的好奇心。 我曾经想过在工作之余去深入学习另一门语言，并参与到开源的基础组件的发展中。方法不是没有，而是我目前还没突破这方面的局限性。而且这也只是其中一个原因。 大公司的 PHP 岗位有限 很现实的一点。 PHP 和 Laravel 由于其自身的局限，很少被用于大型项目中，因此很难成为大多数中大公司的主力编程语言。 比如腾讯虽然有些项目在用 PHP，但也只是因为是老项目，需要维护。新项目应该不会考虑 PHP。 由于以前 Laravel 在国内的接受度不是很高，所以老项目大多不使用 Laravel。甚至有的项目不使用框架，就算用了框架也可能是多年前的版本。 光是想想就能感受到维护这种项目的痛苦。 PHP 岗位对前端知识有要求，也会被要求去做前端的活 我只想暂时当个纯粹的后端。平常我自己想要做点什么的时候，也不需要有什么界面，用命令行就够了。 前端也有很多理念和知识可以深挖学习，前后端不一定孰优孰劣。问题在于一旦前后端都涉及，对我这种普通人来说就是灾难。 所以我只学习必要的前端知识，把重心放在后端。 不太重要的原因： 部署 PHP 部署起来相对比较复杂，需要安装 PHP 解释器。如果是 WEB 服务，还需要 PHP-FPM。 如果是用在工作上，这其实不是什么问题。但是我还想让自己写的程序在更多平台上跑，有些系统没法装。例如路由器和 Wifi 模块。 而 Golang 直接编译成一个可执行文件丢上去就行了。像路由器这种 ROM 小的设备，还可以压缩二进制文件减小可执行文件的体积。而像 Wifi 模块这种更小存储的设备，还可以使用像 Tiny Go 这种简化版。 并发 有些任务需要并发执行。如果用 PHP，那么得开多进程，或者转成多个 HTTP 请求，或者使用 Swoole，比较麻烦。 其他的原因以后想起来了再补充。 其实没有运行时和垃圾回收的 Rust 也是一个很好的选择，但显然 Go 更适合这个阶段的我。我会在空闲时间去实践 Rust，但目前不会将其作为主力工作语言。 另外如果我现在再去学习 C++ ，感受到的难度肯定和以前不一样。然而我觉得没有多大必要，如果能做到看懂开源项目（例如 MySQL）的源码，那我就很满意了。 数据库加入 MongoDB 和 InfluxDB 虽然我还没完全掌握 MySQL 的底层原理，但对 InnoDB 的部分原理还是有一定的了解。但是对于其他的存储服务的了解就很浅，比如列式存储的 InfluxDB，和文档型的 MongoDB。 大多数场景使用 MySQL 是最合适的。在一些特殊的场景中，才会需要用到其他数据库。 例如我们有个项目主要是存储服务器的统计数据，统计数据以分钟为粒度存储。我们使用 InfluxDB 存储服务器维度的数据，使用 MongoDB 存储业务维度的数据。最开始业务维度的数据存储在 MySQL 上，后来业务量上来了，延迟变高才切换到 MongoDB。 如果要用 MySQL 存储一天中每分钟的数据，有多种方式。 每分钟一行，但数据量大 每一行存储一天所有时间点。 但 InnoDB 对列数有限制，就算在最新的 MySQL 8.0 里面也最多只能是 1017 列，而一天有 1440 分钟。 https://dev.mysql.com/doc/refman/8.0/en/innodb-limits.html MyISAM 可以支持超过 1440 列，但它是表锁，我们碰到了数据延迟高的问题。同时它还有只能存储其中一种数据的问题。 每行存储半天的时间点。这个可以。但仍然没有解决只能存储一种数据的问题。 一种是每行加个字段表示数据类型，另一种同时也能解决上面的问题。 使用 json 存储。这样一行就能存储一整天的所有类型的数据。 会不会引入其他问题？这个我暂时没去考虑。 在我入职的时候，同时存在两个版本。MySQL 的老版本和 MongoDB 的新版本。不过新版本有几个 bug 导致数据没有对上。我最初的主要工作之一就是帮忙修复 bug，把数据对齐。另外一个是实现新的数据上报协议，使上报的数据字段可以根据需要添加，同时兼容旧协议。 不过这个版本还存在一个严重的隐患。它让服务在内存中合并一段时间的数据再每隔一定时间刷到数据库。如果服务发生故障，或者需要升级程序，重启的那段时间的数据会丢失。 为了解决这个问题，我重写了这个服务。让上传的数据先存储到 Kafka ，再从 Kafka 消费。这样就算升级也不会影响到数据，而且可以启动多个消费者，提高可用性和处理效率。 这个版本还没有完全上线。一个是因为在我升级上报协议之后，直到现在都没有必须升级的功能点。另一个原因是有另外一个项目需要做。所以准备到春节后再升级。 一开始我不知道这个服务已经有提供接口给其他一个服务使用。因为这个服务的数据没校准，基本都是使用老系统的数据。结果在修复 bug 的过程中，某次升级导致数据往下掉。触发了那个服务的告警，出现了个问题。影响到了我的绩效。血的教训。 通过这个项目，让我熟悉了 MongoDB 的使用，后续就靠我自己去研究它的实现原理了。使用起来的感受是 MongoDB 的查询非常强大。我用得比较多的是 Aggregation。甚至可以在里面用 javascript 处理数据。不过它占用的内存实在是太多了 由于 InfluxDB 是用 Go 写的，源码看起来就方便多了。它的开源版本已经不再提供集群模式，我就想自己实现个集群功能的子集，只用于查询数据。我下载了包含集群功能的版本的源码，看了功能的实现，但是在最底层的数据通信这块停下了。看起来新版本和老版本的通信方式有所变化，后续找机会再试试看。后来另外看了它的数据存储过程和单机查询过程。目前只有简略地列出关键点，后面找个时间补充完整，然后发出来。 ","date":"2021-02-02","objectID":"/drafts/2020-work/:0:0","tags":null,"title":"2020 工作篇","uri":"/drafts/2020-work/"},{"categories":null,"content":"数据结构 第一次使用小顶堆。这个是因为项目中有个功能需要算出特定业务一个月的 95 峰值带宽。具体可以去搜索引擎搜索 95 峰值（或者 ninety-fifth percentail）这个词，用于跟机房计费用的。每五分钟一个点，从小到大排序，然后从小到大数到第 95% 那个点作为数据。 如果一次性查一个月的数据进行合并，数据量太大，又占内存又慢。所以就分开查，每次查一天的数据。然后把数据丢到小顶堆里面，给小顶堆设置为从大到小 95 点所需的个数。这样每次都保留极少数的数据。 这里有个问题：为什么不先把数据合并起来？这样就只需要对 8640 个点排序了。原因有好几个，一个是要查上千种情况下的数据，预先合并的话，数据量太大。 发表的博客及花 C 语言初步实现面向对象的三个基本特征 发表在博客园： https://www.cnblogs.com/schaepher/p/12498512.html 我刚毕业那会儿，有个面试要求我用 C 语言实现面向对象，我没答好。过了将近三年才想到把它整理出来。 这次是由于看了 PHP 的源码，想到了以前遗留了这么个问题，就尝试写了个简化版的面向对象。 查找资料的过程中发现没什么人发过相关的博客。有的也是只有部分代码，缺少完整的例子，提高了理解难度。 我这个虽然是简单版，但已经提供了应有的思路。如果要实现更多，需要添加 HashMap 的内容。我在犹豫是否花时间把它做成一个系列。 其实继续深入下去就可以直接用 PHP 或者 Python 的实现了。直接看它们的源码也行，只是它们一开始就是完整的实现，内容比较多，容易乱。 我现在又去读了一遍，发现很多地方不通顺，增加了理解难度。后面再找时间改一改，反正我现在已经找到写博客的节奏，不怕鸽。 ","date":"2021-02-02","objectID":"/drafts/2020-work/:1:0","tags":null,"title":"2020 工作篇","uri":"/drafts/2020-work/"},{"categories":null,"content":"配置 /etc/bluetooth/main.conf 将蓝牙配置为键盘模式： Class = 0x000500 见：https://www.bluetooth.com/specifications/assigned-numbers/baseband/ service class major device class minor device class type 00000000001 00101 010000 00 比特位置关系： 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 位置 值 作用 13 1 Limited Discoverable Mode 12 11 10 9 8 00101 Peripheral (mouse, joystick, keyboard, … ) 7 6 01 Keyboard 顺便一提，如果想要鼠标和键盘都可以，则将 7 和 6 都设置为 1。 启动蓝牙 systemctl restart bluetooth bluetoothctl power on bluetoothctl discoverable on 安装开发包 sudo apt-get install libbluetooth-dev python-gobject bluez bluez-tools bluez-firmware python-bluez python-dev python-pip pip install pybluez evdev pydbus future 蓝牙服务测试 我们的流程是： 键盘 -A-\u003e 树莓派(转换) -B-\u003e 树莓派蓝牙 -C-\u003e Windows 蓝牙 上面已经完成了 A 的部分，现在我们先跳过 B，测试一下 C。也就是在不依赖于上面的键盘事件的情况下，让 Windows 端收到一个键盘输入。 对于蓝牙这类系统服务，与其的通信应该通过系统的进程间通信。D-Bus 是消息总线系统。 我们的目标是获取蓝牙在消息总线上的通信对象，然后将键盘按键信息发送出去。 from pydbus import SystemBus # 获取系统级别的消息总线对象 system_bus = SystemBus() # 从消息总线中获取蓝牙对象 bus.get('org.bluez') # 循环监听或发送 from gi.repository import GLib loop = GLib.MainLoop() loop.run() 蓝牙服务端 我们要实现的是树莓派连接 USB 的键盘，键盘的输入最终到 windows。对于 Windows 来说，它是在跟蓝牙键盘通信。它需要主动连接蓝牙键盘。 此时蓝牙键盘作为蓝牙服务端。因此我们要在树莓派上面配置蓝牙服务端。 蓝牙设备有多种类型，由于我们想要将蓝牙设备当作键盘使用，所以在启动服务端的时候，应该将其设置为键盘类型。 在创建服务端的时候，有多种协议可选择：HCI, L2CAP, RFCOMM 和 SCO。其中 HCI 在链路层之上，L2CAP 在 HCI 层之上。RFCOMM 基于 L2CAP，用于串行设备。SCO 类似于 UDP，用于实时性高的场景。acl 类似 TCP。基于应用场景，选则 L2CAP 。 服务端通信的配置和 TCP 类似，用 socket 那套操作。 import bluetooth PORT_PSM_INTR = 0x13 server_sock = bluetooth.BluetoothSocket(bt.L2CAP) server_sock.setblocking(False) try: server_sock.bind(('', PORT_PSM_INTR)) except: print(\"For bluez5 add --noplugin=input to the bluetoothd commandline\") print(\"Else there is another application running that has it open.\") sys.exit(errno.EACCES) server_sock.listen(1) client_sock, address = server_sock.accept() print(\"Accepted connection from\", address) data = client_sock.recv(1024) print(\"Data received:\", str(data)) while data: client_sock.send(\"Echo =\u003e\", str(data)) data = client_sock.recv(1024) print(\"Data received:\", str(data)) client_sock.close() server_sock.close() bluetooth 库要求安装 pybluez。 参考文章 https://projects-raspberry.com/emulate-a-bluetooth-keyboard-with-the-raspberry-pi/ 基础知识 ","date":"2021-02-01","objectID":"/drafts/raspberry-bluetooth-keyboard-temp/:0:0","tags":null,"title":"树莓派蓝牙键盘","uri":"/drafts/raspberry-bluetooth-keyboard-temp/"},{"categories":null,"content":"Linux 输入子系统事件 当我们直接用 USB 键盘插入到使用 Linux 系统的树莓派上的之后，键盘的输入将会通过 Linux 的输入子系统，以事件的形式发送出去。 因此我们需要先监听这个些事件。这里选择 evdev 这个库。 evdev 官方文档见： https://python-evdev.readthedocs.io/en/latest/usage.html import evdev.ecodes help(evdev.ecodes) 事件类型： https://www.kernel.org/doc/html/latest/input/event-codes.html ","date":"2021-02-01","objectID":"/drafts/raspberry-bluetooth-keyboard-temp/:1:0","tags":null,"title":"树莓派蓝牙键盘","uri":"/drafts/raspberry-bluetooth-keyboard-temp/"},{"categories":null,"content":"HID 协议 ","date":"2021-02-01","objectID":"/drafts/raspberry-bluetooth-keyboard-temp/:2:0","tags":null,"title":"树莓派蓝牙键盘","uri":"/drafts/raspberry-bluetooth-keyboard-temp/"},{"categories":null,"content":"BLE 蓝牙 蓝牙键盘是 BLE 设备。 BLE 不使用 socket 连接。 https://www.cnblogs.com/SA226343/p/5995674.html https://www.cnblogs.com/SA226343/p/6019457.html BLE 有两种通信，分别是有连接的通信和广播通信。 广播通信信道为 LE Advertisement Broadcast Channel，用于广播 有连接的通信信道 LE Piconet Channel 用在处于连接状态的蓝牙设备之间的通信 在通信过程中，使用 GATT 来通信。其中 Characteristic 表示一个特定的数据。 这样外围设备向中心设备发送 Characteristic 数据。如果外围设备是键盘，则把键盘的数据放到 Characteristic 的 Value 里面。 sudo busctl monitor org.bluez sudo apt-get install libglib2.0 libboost-all-dev sudo pip install gattlib ","date":"2021-02-01","objectID":"/drafts/raspberry-bluetooth-keyboard-temp/:3:0","tags":null,"title":"树莓派蓝牙键盘","uri":"/drafts/raspberry-bluetooth-keyboard-temp/"},{"categories":null,"content":"完整代码 https://github.com/schaepher/keyboard_mouse_emulate_on_raspberry 大致流程 Bluetooth Keyboard +------------------------+ | | | +-----------+ | | | USB | | | | keyboard | | | +-----+-----+ | | | HID protocol | | v | | +-----+-----+ | | |Linux input| | | +-----+-----+ | | | event | | v | | +-----+-----+ | | | evdev | | | +-----+-----+ | | | | | v | | +-----+-----+ | | |event code | | | | to | | | | HID code | | | +-----+-----+ | | | dbus | | v | | +-----+-----+ | | | Bluetooth | | | | keyboard | | +-----------+ | | service | | | Windows | | +-----+-----+ | +-----+-----+ | | send | ^ HID protocol | v | | | +-----+-----+ | +-----+-----+ | | Bluetooth | | | Bluetooth | | | service | | +-------\u003e | service | | +-----------+ | L2CAP +-----------+ | | +------------------------+ 主要做三件事： 获取 Linux 的输入事件 将输入事件转化为 HID 协议的数据 将转换后的 HID 协议的数据发送到蓝牙服务 下面会先测试获取输入事件和发送数据到蓝牙，即输入输出。然后再处理数据转换。 安装依赖 sudo apt-get install libbluetooth-dev python-gobject bluez bluez-tools bluez-firmware python-bluez python-dev python-pip pip install pybluez evdev pydbus future 获取 Linux 的输入事件 当我们直接用 USB 键盘插入到使用 Linux 系统的树莓派上的之后，键盘的输入将会通过 Linux 的输入子系统，以事件的形式发送出去。 我们可以使用 evdev 这个库来监听这些事件。 evdev 官方文档见： https://python-evdev.readthedocs.io/en/latest/usage.html ","date":"2021-01-31","objectID":"/2021/01/31/raspberry-bluetooth-keyboard/:0:0","tags":["蓝牙","键盘"],"title":"通过树莓派把 USB 键盘变成蓝牙键盘","uri":"/2021/01/31/raspberry-bluetooth-keyboard/"},{"categories":null,"content":"输入事件获取测试 分两步： 查看有哪些输入设备，选择其中一个 获取选择设备的事件，并把事件打印出来 查看设备列表： import evdev devices = [evdev.InputDevice(path) for path in evdev.list_devices()] for device in devices: print(device.path, device.name, device.phys) 会有类似以下的输出： /dev/input/event1 USB Keyboard usb-0000:00:12.1-2/input0 /dev/input/event0 USB Optical Mouse usb-0000:00:12.0-2/input0 选择键盘输入： import evdev device = evdev.InputDevice('/dev/input/event1') print(device) for event in device.read_loop(): print(evdev.categorize(event)) 如果有多个 Keyboard，则需要一个个尝试。 此时按键盘，屏幕将会有输出。例如： event at 1612004615.311680, code 04, type 04, val 458756 key event at 1612004615.311680, 30 (KEY_A), down synchronization event at 1612004615.311680, SYN_REPORT key event at 1612004615.576959, 30 (KEY_A), hold synchronization event at 1612004615.576959, SYN_REPORT key event at 1612004615.616967, 30 (KEY_A), hold synchronization event at 1612004615.616967, SYN_REPORT event at 1612004615.856933, code 04, type 04, val 458756 key event at 1612004615.856933, 30 (KEY_A), up synchronization event at 1612004615.856933, SYN_REPORT 发送数据到蓝牙服务 ","date":"2021-01-31","objectID":"/2021/01/31/raspberry-bluetooth-keyboard/:0:1","tags":["蓝牙","键盘"],"title":"通过树莓派把 USB 键盘变成蓝牙键盘","uri":"/2021/01/31/raspberry-bluetooth-keyboard/"},{"categories":null,"content":"启动蓝牙并设置为键盘服务 hciconfig 命令用于控制蓝牙设备。 /etc/init.d/bluetooth 用于控制蓝牙服务。 首先确保蓝牙设备已被关闭，然后启动蓝牙服务。 sudo hciconfig hci0 down sudo /etc/init.d/bluetooth start 现在再次启动蓝牙设备： sudo hciconfig hci0 up # 将蓝牙类型设置为键鼠设备 sudo hciconfig hci0 class 0x0025C0 # 设置蓝牙名称 sudo hciconfig hci0 name my-keyboard # 让其他蓝牙设备可以发现这个蓝牙设备 sudo hciconfig hci0 piscan ","date":"2021-01-31","objectID":"/2021/01/31/raspberry-bluetooth-keyboard/:0:2","tags":["蓝牙","键盘"],"title":"通过树莓派把 USB 键盘变成蓝牙键盘","uri":"/2021/01/31/raspberry-bluetooth-keyboard/"},{"categories":null,"content":"配置蓝牙服务 Linux 的官方蓝牙协议栈是 BlueZ，我们需要对 BlueZ 做一些配置。一共有两个必要的配置：UUID 和 ServiceRecord。 如果不配置 ServiceRecord，那么连接后键盘的输入会混乱。比如在 Windows 上按 d 这个按钮，会变成 Win + d。 ServiceRecord 是一个 xml 格式的数据。具体内容可以先不理解，直接用就行。内容如下： sdp_record.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003crecord\u003e \u003cattribute id=\"0x0001\"\u003e \u003csequence\u003e \u003cuuid value=\"0x1124\" /\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0004\"\u003e \u003csequence\u003e \u003csequence\u003e \u003cuuid value=\"0x0100\" /\u003e \u003cuint16 value=\"0x0011\" /\u003e \u003c/sequence\u003e \u003csequence\u003e \u003cuuid value=\"0x0011\" /\u003e \u003c/sequence\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0005\"\u003e \u003csequence\u003e \u003cuuid value=\"0x1002\" /\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0006\"\u003e \u003csequence\u003e \u003cuint16 value=\"0x656e\" /\u003e \u003cuint16 value=\"0x006a\" /\u003e \u003cuint16 value=\"0x0100\" /\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0009\"\u003e \u003csequence\u003e \u003csequence\u003e \u003cuuid value=\"0x1124\" /\u003e \u003cuint16 value=\"0x0100\" /\u003e \u003c/sequence\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x000d\"\u003e \u003csequence\u003e \u003csequence\u003e \u003csequence\u003e \u003cuuid value=\"0x0100\" /\u003e \u003cuint16 value=\"0x0013\" /\u003e \u003c/sequence\u003e \u003csequence\u003e \u003cuuid value=\"0x0011\" /\u003e \u003c/sequence\u003e \u003c/sequence\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0100\"\u003e \u003ctext value=\"Raspberry Pi Virtual Keyboard\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0101\"\u003e \u003ctext value=\"USB \u003e BT Keyboard\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0102\"\u003e \u003ctext value=\"Raspberry Pi\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0200\"\u003e \u003cuint16 value=\"0x0100\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0201\"\u003e \u003cuint16 value=\"0x0111\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0202\"\u003e \u003cuint8 value=\"0xC0\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0203\"\u003e \u003cuint8 value=\"0x00\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0204\"\u003e \u003cboolean value=\"false\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0205\"\u003e \u003cboolean value=\"false\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0206\"\u003e \u003csequence\u003e \u003csequence\u003e \u003cuint8 value=\"0x22\" /\u003e \u003ctext encoding=\"hex\" value=\"05010906a101850175019508050719e029e715002501810295017508810395057501050819012905910295017503910395067508150026ff000507190029ff8100c005010902a10185020901a100950575010509190129051500250181029501750381017508950305010930093109381581257f8106c0c0\" /\u003e \u003c/sequence\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0207\"\u003e \u003csequence\u003e \u003csequence\u003e \u003cuint16 value=\"0x0409\" /\u003e \u003cuint16 value=\"0x0100\" /\u003e \u003c/sequence\u003e \u003c/sequence\u003e \u003c/attribute\u003e \u003cattribute id=\"0x020b\"\u003e \u003cuint16 value=\"0x0100\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x020c\"\u003e \u003cuint16 value=\"0x0c80\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x020d\"\u003e \u003cboolean value=\"false\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x020e\"\u003e \u003cboolean value=\"false\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x020f\"\u003e \u003cuint16 value=\"0x0640\" /\u003e \u003c/attribute\u003e \u003cattribute id=\"0x0210\"\u003e \u003cuint16 value=\"0x0320\" /\u003e \u003c/attribute\u003e \u003c/record\u003e 配置： import dbus bus = dbus.SystemBus() bluez = bus.get_object(\"org.bluez\", \"/org/bluez\") manager = dbus.Interface(bluez, \"org.bluez.ProfileManager1\") f_sdp = open(\"./sdp_record.xml\", \"r\") opts = { \"AutoConnect\": True, \"ServiceRecord\": f_sdp.read() } uuid = \"05262649-d40f-483d-b445-73b000d19028\" manager.RegisterProfile(\"/org/bluez/hci0\", uuid, opts) ","date":"2021-01-31","objectID":"/2021/01/31/raspberry-bluetooth-keyboard/:0:3","tags":["蓝牙","键盘"],"title":"通过树莓派把 USB 键盘变成蓝牙键盘","uri":"/2021/01/31/raspberry-bluetooth-keyboard/"},{"categories":null,"content":"创建服务器接收请求 创建蓝牙 Socket 等待连接 蓝牙需要创建两个 Socket，分别是控制管道 (control) 和中断管道 (interrupt)。其中 interrupt 用于发送数据。 https://blog.csdn.net/zhoutaopower/article/details/82469665 控制管道主要用于下面3个方面： 接收/响应USB主机的控制请求以及相关的类数据 在USB主机查询时传输数据（如响应Get_Report请求等） 接收USB主机的数据 中断管道主要用于下面两个方面： USB主机接收USB设备的异步传输数据 USB主机发送有实时性要求的数据给USB设备 以下端口要与 sdp_record 里面配置的一致。 import socket PORT_CTRL = 17 s_control = socket.socket(socket.AF_BLUETOOTH, socket.SOCK_SEQPACKET, socket.BTPROTO_L2CAP) s_control.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) s_control.bind((socket.BDADDR_ANY, PORT_CTRL)) s_control.listen(5) conn_control, addr_info = s_control.accept() PORT_INTR = 19 s_interrupt = socket.socket(socket.AF_BLUETOOTH, socket.SOCK_SEQPACKET, socket.BTPROTO_L2CAP) s_interrupt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) s_interrupt.bind((socket.BDADDR_ANY, PORT_INTR)) s_interrupt.listen(5) conn_interrupt, addr_info = s_interrupt.accept() # 13 是按键 j message = [ 0xA1, 1, 0, 0, 13, 0, 0, 0, 0, 0 ] conn_interrupt.send(bytes(message)) message = [ 0xA1, 1, 0, 0, 0, 0, 0, 0, 0, 0 ] conn_interrupt.send(bytes(message)) 发送给蓝牙的数据是 HID 协议格式的数据。 第一个数字表示这是一个输入报告。二进制表示为 10100001，前面的 101000 表示 Collection ，后面的 01 表示 Application (mouse, keyboard)。 第二个数字当 1 时表示键盘，2 时表示鼠标。 第三个数字表示 Modifier Keys。二进制的 8 个 bit 分别代表：左 ctrl, 左 shift, 左 alt， 左 Win， 右 ctrl, 右 shift, 右 alt， 右 Win 第四个数字是保留字符。 最后六个数字分别代表一个普通按键。例如数字键和字母键。这就表示一次按下最多有效的普通按键是 6 个。 https://www.usb.org/document-library/device-class-definition-hid-111 可下载 pdf 文档。 第一个数字的解释来自于： 6.2.2.4 Main Items 第二个数字的解释来自于： 4.3 Protocols 第三个开始的数字的解释来自于：Appendix B: Boot Interface Descriptors ","date":"2021-01-31","objectID":"/2021/01/31/raspberry-bluetooth-keyboard/:0:4","tags":["蓝牙","键盘"],"title":"通过树莓派把 USB 键盘变成蓝牙键盘","uri":"/2021/01/31/raspberry-bluetooth-keyboard/"},{"categories":null,"content":"Go 是怎么处理的 在 Go 的 time 库里面，使用 time.ParseInLocation 或者 time.Date 时，都需要传入 time.Location 参数，然后生成 time.Time 对象。 time.Time 底层存储的是秒数、纳秒数以及时区 time.Location，这个秒数和纳秒数的时区必须是 UTC，与 time.Location 的值无关， time.Location 仅在格式化为可读形式的时候有用。如果在上述传的 time.Location 不是 UTC，则需要对生成后的秒数进行校正。 如果在存储到数据库的时候使用这个秒数格式化为可读形式，则会发现原先 Asia/Shanghai 时区的 2021-01-05 00:00:00 变成 2021-01-04 16:00:00，这是从 +8 时区还原到 0 时区时减去了 8 小时的秒数。 Go 的 MySQL 驱动在处理结构体里的 time.Time 对象时，会将其转换为一个已事先配置好的时区，这个默认时区是 UTC。所以如果不另外配置，就会出现上述的转换。 可以通过 DSN （Data Source Name）配置时区，格式为 user:password@tcp(192.168.1.1)/dbname?charset=utf8mb4\u0026parseTime=true\u0026loc=Asia%2fShanghai 后面的 loc=Asia%2fShanghai。这里按照要求将 / 转换为 %2f。 具体的转换是通过 time.Time 的 In() 方法实现的，它会把时区设置为 DSN 指定的时区。在转换为 SQL 时，会使用 time.Time 的 Date 和 Clock 方法。这两个方法返回的数据受到时区的影响，因此与预期的时区的展示时间一致。 使用这个方法的好处是，不需要修改数据库的配置就能在数据库里面存入展示形式为当前时区的数据，方便查看。缺点是涉及到不同时区的设备在把数据存储到数据库的时候，如果没有全部指定为同一个时区，则会出现混乱。如果指定为同一个时区，则会使得处于其他时区的人在查看数据库里的原始数据时，必须知道这是哪个时区的时间。对于与设置不同的时区的人来说，就变得不直观了。 因此如果不涉及不同时区的人员想要用他们的时区直接查看数据库原始数据，则所有的连接统一配置为当前时区，这样就不用对其他查询工具提出要求。 而如果涉及，那么则应该统一存储 UTC 时区的形式，然后在客户端配置时区，由客户端转换。 ","date":"2021-01-05","objectID":"/2021/01/05/datetime/:1:0","tags":["datetime"],"title":"处理多时区的时间","uri":"/2021/01/05/datetime/"},{"categories":null,"content":"我本来想用“改变”这个词来描述我的 2020。 因为这一年中我有很大的变化：离开了让我感觉很温馨的公司、第一次在省外工作、开始学习并使用新的编程语言和技术栈、开始大量阅读历史和政治相关的内容、开始跑步并保持了一个很好的频率、开始尝试解决单身问题…… 但是我后来觉得“改变”这个词不够准确。上面这些事情，最重要的其实是我主动创造巨大了不确定性，并且积极应对。比如坚决地换到另一个省份工作。 在 2020 年的末尾，“不确定性”这个词经常出现在我的脑海中。如果要我选择在 2020 年对我影响最大的一句话，那一定是： 你需要在生活中加入不确定性，运气才能发挥作用。 这句话来源于微信公众号作者“九边”的文章，我做了简化和修改，原文是： 你需要在生活中加入不确定性，运气才能开始出现。运气和奋斗是个人进步的两个轮子，但是绝大部分人都是独轮车。你天天按部就班，运气很难发挥作用。确保不赔钱的前提下，折腾点事，让运气帮你赚钱。 —— 《向上生长》 by 九边 我本来就对努力和运气的关系有一些思考。我认为努力的众多作用中，很重要的一点是能够在运气出现时抓住它。不过在理论指导实践时，却变成了独轮车。现在多了一种可能性，那就是通过主动创造不确定性来让运气更多地发挥作用。例如让运气的作用从 1% 提高到 10%。 2020 年，有些事情我做得很好，有些没什么改变，有些则做的不好。这篇主要将它们记录下来。 做得好的部分 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:0","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"换工作 我不喜欢被困在工作上舒适圈里面。工作如果充满着确定性，很容易被锁死在当前的阶段，难以得到显著的提高。 原先的工作比较出色，得到了领导的肯定，拿了个公司的年度优秀员工。工作内容比较轻松，965 工作制，每周五还有学习分享会，经常能上去跟大家分享我最新的学习成果，和领导的关系很好，同事之间相处得很愉快，公司里面跟我同年龄段的女同事也很多…… 不过这种舒适的生活还能保持多久呢？哪天公司为了更加精简人员把我裁了怎么办？我当前的技术水平能让我找到一份薪资和当前差不多甚至更高的工作么？ 我能明显地感受到自己的成长速度减缓了很多。还好 2019 年的重构以及同年年底去参加在北京举办的系统架构师大会帮我续了一命，不然我会感到难受。 我的危机感比较强，过一段时间就会问自己：如果仅仅是保持当前的状态，那么 5 年后或者 10 年后，我还能过上像现在这样舒适的生活么？ 上面这些是根本原因。而直接原因则是公司在 19 年年底开始裁员。领导说不可能裁我，但按照通常的”三年换一次工作“的思路，我最晚也会在 2020 年 7 月离开。如果因为占着这个位置，导致其他人被裁了，那我也很过意不去，所以就提前跟领导沟通这件事。 我大概在 2020 年的四月下旬正式开始找新的工作，六月初就去深圳工作了。 关于这次换工作，可以写很多，这里不展开，我打算把这些单独放到工作篇。 我从这次换工作的巨大的不确定性中，得到了很多的收益。无论是工资，还是心态和习惯都有显著提升。所以我认为做这个选择是成功的。 关于具体的技术，本来是想说技术能力没有质的突破，新学的技术原理过少。不过列出来后，感觉也不算做得太差。 阅读 PHP 函数注册、函数调用、垃圾回收、PHP 对象相关的源码，加深了对脚本语言的理解。 阅读 JAVA、PHP 5、PHP 7、Redis、Go 的 HashMap 的源码，比较及整理到博客，并在分享会做了分享。 阅读一部分 Linux 内核的 Socket 相关源码，以及 Golang 的 HTTP Server 跟 TCP 相关的源码。 阅读 InfluxDB 查询和写入相关的源码，并用文字描述出来。 阅读跟内存分配相关的文章以及源码，主要是 jemalloc 、 TCMalloc 和 ptmalloc。 整理和翻译一小部分 HTTP 1.1 的 RFC 内容。 还有为了面试去深入学习的一些内容。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:1","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"锻炼 上一次持续实践较长的锻炼是从 18 年春节后到 19 年三月底，这段时间的锻炼以跟着 Keep 做室内运动为主。后来 19 年为了重构赶进度而停下了，这一停就是九个月。期间只有从七月开始游的十几次泳这种运动了。 在项目进入维护阶段之后的两三个月后，进入了 2020 年。我决定开始新的一轮锻炼。每次锻炼完都会在健身群里打卡。由于这次采用了一个合适的模板，现在很容易将这些数据整理出来。 2020 年第一次锻炼是在 1 月 25 日。从此基本保持了两天锻炼一次的频率。这个时期仍然是跟着 Keep 做室内运动。在 5 月准备面试的那段时间降低了频率。1 月 25 日到 5 月 31 日之间，总共打卡 35 次。 从 6 月开始，也就是我搬到深圳后，由室内运动（力量锻炼）改到室外运动（心肺锻炼）。从 6 月 10 日开始跑步，每次 3.5 公里。 我一直很喜欢跑步，但是我右脚膝盖很容易在跑步的时候受伤，所以毕业后就没怎么跑了。以前觉得护膝估计也没啥用，但这次抱着试一试又不亏的心态买了一套，发现意外的有效果。以前跑个 4 公里膝盖就受不了了，而买了护膝后我专门测了极限，得跑 10 公里膝盖才会疼。 虽然我能跑 10 公里，但我一直把距离控制在 3.5 公里左右。这是为了在保护膝盖的前提下提高跑步频率。现在这个阶段，更重要的是频率而不是量。 我为自己设定的频率是每周跑三次，分别在周一、周三、周五。这样容易坚持。周六日的不确定性比较高，如果经常因为各种原因没法去跑步，那么必定会影响到周一、周三、周五的计划。 2020 年 6 月到 12 月底，共跑步 68 次。 在 8 月初的时候决定对跑步做一些改进， 10 公里的测试就是这个时候做的。我在网上查询跑步相关的资料后，发现有个平均 180 步频又快又省力的说法。平均 180 步频就是一分钟跑 180 步。 180 平均步频大概是什么样的节奏？从下面这首我边跑边听的跑步专用音乐，能够得到比较直观的感受。 http://music.163.com/song?id=33414766 为了对进步有一个直观的感受，我先列出原先的数据。我原先的平均步频是 155 步/分钟，平均配速是 6 分 27 秒。 8 月 10 日第一次有意识地提高平均步频，这次跑了 5 公里。平均步频为 172 步/分钟，平均配速为 5 分 28 秒。一开始提高平均步频会非常累，还不习惯这么快。跑了三次 5 公里后，就调回 3 公里。 从 8 月 13 日开始有了新的手环，看了静息心率是 58。从那时到现在基本都在这个数左右波动。 8 月 21 日第一次达到了 180 的平均步频，然后一直保持。八月底的平均配速已经降到 4 分 30 秒左右。 9 月 30 日由于国庆回家，借着老家空气新鲜，尝试跑 4 公里 180 平均步频，结果成功了。本来想一直保持 4 公里，不过实在太困难，就只有状态好的时候才跑。2020 年共跑了 10 次，偶尔会在最后一公里降低步频，不然受不了。 12 月气温开始下降，好几次改为在公司的跑步机上跑。2020 年最后一次在跑步机上跑步时，我选择了 14 公里/小时的速度。在保持 180 平均步频的情况下，平均配速为 4 分 14 秒。 接下去会尝试保持在跑步机上以 14 公里/小时的速度以及 180 平均步频的情况下跑 4 公里。 我对于跑步最关注的一点是可持续性，上面的描述也有所体现。在一个阶段（3 公里）保持一定时间后，尝试下一个阶段（4 公里）。如果可以，就保持在下一个阶段。如果会影响到可持续性，则说明进入下一个阶段的准备还不充足，那么我会选择回到当前阶段继续保持，然后偶尔尝试下一个阶段。 我认为普通人在稳定的运动频率的前提下，保持得越久收益越高。运动会提高身体素质，进而对人的精神和心态产生积极的影响。持续的运动会带来持续的积极的影响。对于精神和心态的影响的形成需要一个过程，频率小但运动量大的方式会使得这种影响难以形成。 另外我在 2020 年后面两三个月打了好几次羽毛球。最初是抱着混个安慰奖去参加公司的羽毛球比赛，没想到队友太强，让我混了个大奖。销售同事在十一月找上我，说和客户组了个羽毛球局，让我去压场子。 一开始我当然是拒绝的，也不是谦虚，我一个小小的菜鸟怎么压得住场子，请那几个高手出场吧。没想到他们要么临时有事，要么小孩刚出生要照顾，要么受伤了。我只好念两句诗，苟…… 总之今年的运动让我很满意。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:2","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"阅读 2020 年阅读的主要的非技术书籍有四本： 《文明、现代化、价值投资与中国》，读完 《20 岁，光阴不再来》，读了一半觉得没有什么收获，先收起来 《中国人史纲》，读完 《向上生长》，读完 在读书群的打卡是从 4 月 23 日开始的，2020 年总共 39 次打卡。4 到 7 月打卡 29 次，8 到 12 月只打卡了 10 次。 我的原定计划是多看几本书，这样打卡会保持连续。但 2020 年太特殊了，国内和国际上发生了太多事情，这些事情对于未来会产生巨大的影响，以至于我认为不能错过这个历史的转折点所发生的事情。如果不发生这些事情，我会再找一两本跟历史相关的书来读。 选历史相关的书主要是用来和《中国人史纲》做对照和补充，或者重读一遍《中国人史纲》。另外也可能选世界史。 在没有打卡的这段时间内我在阅读哪些内容呢？ 微信公众号 沈逸的微博 2020 年主要关注了与时事相关的几个公众号，花了不少时间阅读这些公众号的文章： 天涯时事 天涯时事是 2019 年我在看完好友在微信“看一看”分享的《前脚抓了暴徒，后脚就被放，香港高等法院的局如何破？》这篇文章后关注的。 九边 九边是我在看完好友在微信“看一看”分享的《为什么全世界都在“印度化”？》这篇文章后关注。 维舟 维舟应该是因为有个群经常分享他的文章，我才去关注。 另外还有卢克文工作室和天下传播，不过这两个我看得比较少。 从文章的理解难度来看，除了维舟之外，其他公众号都在尽力地把一件事情用通俗的语言描述清楚。关于这点，维舟其实有专门解释过，但是理由比较牵强。 从文章的主题来看，天涯和九边的文章能够让人从宏观上加深对整个世界的了解；维舟的文章能够让人从微观上加深对国内人民困难的那一面的了解；卢克文有宏观方面的文章，也有微观方面的文章，他会去实地考察，而不是只通过网络获取信息；天下传播的文章能够矫正人们对美国的认知偏差。 微博上也有矫正对某个国家或地区认知偏差的博主，例如日本的 @日本傻事，中国台湾的 @台湾傻事。他们通常转发来自那些地方的新闻。尽管这些地方可以通过在中国购买微博热搜或者花钱请“知识分子”向中国人宣传这些地方的美好，但是他们没法完全阻止本地新闻的报道。 为了利益最大化，有些资本会尝试通过公共社交平台或者一些所谓“高级知识分子”来控制舆论，这会给整个社会带来风险。微博是一个覆盖范围很广的公共社交平台，很容易成为资本控制舆论的实验场所。微博上有一个群体因为过于嚣张而引起沈逸老师关注。 起初沈逸老师是担心这个群体被资本利用，没想到这个群体后来却不断地用他们在沈逸老师微博评论区的回复证明他们已经被资本洗脑了。这让我觉得很魔幻。具体经过可见： https://weibo.com/ttarticle/p/show?id=2309404561340584034630 正如他所说的，由这个群体以及反对这个群体的群体所组成的样本表现出了典型性，全面性和复杂性，堪称完美的样本。关注整个过程后，可以了解到一些混淆是非、挑起矛盾的话术的逻辑，以及看沈逸老师如何破解它们。 我没有参与到他们的讨论中，而是作为一个观察者。我对这个圈子没兴趣，其实只要他们圈地自萌，并不会引起普通人的反感。 我最初关注沈逸老师是在 Tiktok 事件的那段时间。他在 bilibili 上发了 【TikTok的选择，特朗普赌的就是你“不抵抗”】 这个视频，我看完后开始关注他的 b 站账号，后来才关注他的微博。他提供了更高更实际且多方面的看问题的角度，我买了他在 bilibili 上的课程，多多向他学习知识以及思考问题的框架。 视频严格来说应该不算阅读，要不下次把 【阅读】 改为 【知识负熵流】 ？ bilibili 的 up 主“智能路障”的鲁迅系列视频做得很棒，很值得推荐给所有人，特别是推荐给中学生。up 主的声音好听，让人更容易看完。这里抽几个他的鲁迅系列中的视频： 【围炉夜话】鲁迅是被吹捧出来的吗？（谈鲁迅1） https://www.bilibili.com/video/BV1Sf4y1D7H1 【围炉夜话】学医救不了中国人？鲁迅弃医从文的真相。（谈鲁迅3） https://www.bilibili.com/video/BV1vf4y1B7nH 【围炉夜话】震撼人心！没想到我们的初中课本上，有这么牛逼的课文。（谈鲁迅8） https://www.bilibili.com/video/BV1H54y1t7Fm 这个是解读《孔乙己》的视频 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:3","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"日记 正经人谁写日记啊 —— 《邪不压正》 吕思勉先生在《中国简史》的绪论里面提出并回答了一个问题：历史究竟有什么用处？ 历史者，所以说明社会进化的过程者也。 历史可以让我们知道曾经发生了哪些事情，使社会成为现在的社会。而对于个人来说，曾经发生在他身上的事情，使他成为现在的他。 历史记录了所有人的共性，日记则记录了一个人的个性，共性和个性铸成了文化相同但又有个性的一群人。 这里不做过多讨论，接下来说说我在 2020 年写日记这件事吧。 2020 年的第一篇日记是在 5 月 10 日。这个时候的我正在准备面试，压力很大。很可惜的是，换工作期间我只写了这一篇，没能把这段宝贵的经历全部记录下来。 我本来打算在刚开始做准备面试的时候就开始写日记，但踏出第一步很难，我只好先在脑中埋下一颗写日记的种子（非物理种子），然后等待机会。之后它顺利在 5 月 10 日发芽，不过只是昙花一现。 还好后来出现了转机。在已经找到工作并开始搬东西的 5 月 29 日，我开始了第二次尝试。从这一次开始保持一天一记，一直写到 8 月 5 日，由于去上海总部培训而停下。 表面上看，第二次的尝试又以失败告终，但我认为这个一个可以接受的结果。 因为以我的实践经验来看，想要最终完成一件事情，最重要的是走出第一步，然后等待时机成熟。 现在重要的是：我从这两个多月的日记坚持中获得了什么？ 不需要事无巨细全部记录，也没必要每天都记，只需记录那些对我能够产生影响的事件。 尽管在学生时代就被告知写日记不应该是记流水账，但究竟为什么不应该呢？现在我已经有了答案。 日记内容未必是这一天发生的事情，也可以是在这一天的思考，以及引起这一思考的人、事、文章。 这样将来在回顾的时候，能够看到自己不同时期对同一件事情的看法。以后可以将这些零碎思考整理出来，形成系统的思考框架。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:4","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"心态 我的心态变得更好了。有很多原先会让我非常难过的事情，现在对我几乎没啥影响。 心态更好的原因有很多，目前没法确定主要原因是哪一个。有可能是因为我对的认识更加接近于客观世界；可能是我的工资变高了；可能是我经常跑步让身体变得更好了；也可能是我对于自己的定位更加清晰了；或者是我知道我会一直保持上进心…… 但可以肯定的是，我不是用所谓的心灵鸡汤来安慰自己，而是实实在在的通过提升自己和改变客观环境来让自己心态变好。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:5","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"成长的果实 前一阵子我发出了个感慨。要是学生时代——特别是中学——的我能够知道工作三年后的我成长到什么地步，一定会非常开心。因为现在的我已经非常接近我学生时代想象中的我应该成为的样子了。每次想到这里，就有种复杂的情绪，这就是感动了自己么？ 这么多年间做的选择和努力，终于在 2020 年结出果实。接下来就是对一些不够完善的地方进行修正，然后再根据新的目标调整当前面临的选择。 2020 年是各种意义上的转折点。 没变化的部分 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:6","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"体重 我这体重大概还是大学毕业时的样子。虽然春节会让我体重增长比较多，但是节后会逐渐降回到原来的样子。 有的大学同学会说我长胖了，大概是我拍照的技术问题吧哈哈。我同事就吐槽我本人起码比照片上的瘦十斤。 总体来说，我挺满意当前的体重。小肚子要是减下去就更完美了。虽然平常别人看不出来，但是我认为这小肚子代表我处于不健康的状态。 有的小伙伴会说我跑步跑了大半年，体重也没减下去。我觉得无所谓，因为我跑步并不是为了减肥，减肥只能算是顺带。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:7","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"单身 维持单身到现在挺需要定力的。我有时候也会幻想两个人幸福的生活，只是很快就停下了。 2020 年完成了目标，让我认为条件已经成熟，可以开始重视单身问题了。 另外我在对未来进行规划的时候，不能只考虑我一个人，但是现在又没办法知道她所希望的未来是什么样的。所以需要有个人和我一起规划未来。 当前只做了一些必要的准备。由于我自身还存在一些已经发现的和还未发现的小问题，需要在尝试解决单身问题的过程中去发现和解决，因此还需要不少时间。 这是一件非常重要的事情，需要谨慎和耐心。 如果两年内能找到合适的对象，那我就觉得自己非常幸运。要是超过两年，那我就得深刻反思自己了。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:8","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"危机感 这刻在 DNA 上的危机感，恐怕只有我未来的对象能帮我治了。 不过我并不排斥这种危机感，毕竟危机感所产生的压力没办法压垮我，而这压力能够转化为动力促使我不断进步。 不足的部分 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:9","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"面试的那段时间没有做好准备 由于没有准备充分，以及我自己一些表达上的问题，导致我把自己做过的工作说得没多少价值，甚至让面试官怀疑我是不是真的参与这些工作。这个问题一定要解决。 由于我强行把编程语言切换成 Golang，但没有提前打好 Golang 的基础，给面试管留下了不好的印象。如果当时有像现在这样的熟悉程度，那会好很多。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:10","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"花在娱乐的时间没有减少 这其实跟睡觉的时间点往后延有关。如果睡觉的时间点和以前一样，那么娱乐的时间会减少。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:11","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"在扩展人际关系方面还有所不足 其实这方面已经做得挺好的了，包括主动交流并建立起良好关系。唯一不足的是没有跟那几个打羽毛球的客户建立起交流。这个接下来我会持续反思，并尝试从当时一起去的同事那学习做法。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:12","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"输出到博客的内容过少 大部分都是草稿，并且发表到我的 Github 博客上。很少整理出来发表到博客园。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:13","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"王者荣耀还差 9 颗星才到荣耀王者 很难回忆起其他比较大的不足之处，就先拿这个凑数吧。 2021 的大致计划 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:14","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"运动 保持每周三次运动，仍然以跑步为主。 这个习惯存在无法保持的风险。 春节回家 我有不少好习惯都在回家后没法继续保持。 好在 2020 年国庆回家后，我发现我还能继续保持跑步的习惯。 加班 上次没有继续保持锻炼，就是因为加班太晚。但好在现在不需要加班。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:15","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"阅读 2021 年应该不会再像 2020 年这样需要关注太多时事了，所以会看更多的书。 前面说过，原定计划是多看几本历史相关的书。但 2021 年我做了些调整，历史相关的我先选择两本： 吕思勉的《极简中国史》 九边的《西方博弈往事》 这两本我已经都有看一点，2021 年会把他们看完。 那么主要看什么呢？看马克思《资本论》。我现在快看完第一卷第一篇 【商品和货币】 了。 看马克思的著作主要是为了学习系统的科学的分析方法，让我的认识能够更符合客观世界的现象和规律，为以后分析问题和解决问题提供更多思路。 除此之外，还有个顺带的好处。无论任何时候，都会有人尝试污名化马克思以及跟马克思相关的一切。虽然不太可能对我产生影响，但还是得提前建立更巩固的客观认识。 《极简中国史》和《资本论》我会在外出的时候用手机的 Kindle 软件看。其中一块比较大的时间是在等公交车和坐公交车，通常都得半小时以上。 《西方博弈往事》我买的是纸质版，回宿舍后有机会就看。《资本论》虽然也买了纸质版本，但直接寄回家了。 以上是非技术书籍。 技术相关的书籍我打算先把领域驱动设计的两本书看完，分别是《领域驱动设计》和《实现领域驱动设计》。2020 年我两本都看了一半。 其他的技术书籍可能得在需要某一方面的知识的时候再去有针对性的查看。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:16","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"记录生活 记事的粒度从时间跨度主要分为：按日记，按周记，按月记，按季度记和按年记。 四个阶段中，最后一个年度总结我已经能够保持。日记能阶段性保持。其他的没试过。 从 2020 年的尝试中，我知道我目前还无法长时间保持记日记的习惯。所以我打算先从周记这个粒度开始，然后再不断尝试向日记发展。可能周记也无法保持，那就按月记。反正实在不行还能保持按年记。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:17","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"作息 我会尝试逐步把工作日的睡觉时间提早到晚上十一点，并且起床时间同步地提早到早上六点 作息时间的变更会涉及到多个习惯的调整，以及下班后的各项安排。这对我来说是一个挑战，因为需要把一些在晚上做的事情调整到白天。目前我还没有非常具体的规划。 总体的规划是先减少娱乐时间，把晚上的娱乐时间长度逐渐控制在一个小时以内。这个娱乐时间的调整会考虑到跑步计划的影响。 我并不担心压缩娱乐时间会减少我的快乐。很多事情都能让人感到开心，只是大多数的启动成本远远大于娱乐活动。但是这些启动成本高的事情所带来的快乐更多，持续时间也更久。 我以前花在娱乐的时间太多，以至于短暂的快乐结束后，会比原先感受到更加的无聊。不只是我这样，我在网络上也有看到一些和我有相同感受的人。 我不打算对周末加以限制，周末要小伙伴痛痛快快地五排。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:18","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"开始学习投资，并做适当的实践 我觉得价值投资特别适合我。难度高、需要耐心，而且目的是为了陪伴公司成长，从公司的成长中获取收益。我不喜欢短线，并且非常排斥做空的行为。 我打算先从掌握基本概念开始，然后再读一遍《文明、现代化、价值投资和中国》，接着学习分析财务数据。 2021 年用于实践的金额应该是我一个月剩下的量。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:19","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"考高中数学或物理的教师资格证 我在学生时代就对当老师很感兴趣，不过因为喜欢写代码远远超过当老师，所以最终选择当程序员。 之所以对当老师感兴趣，最开始是因为我有时候会在书本中找到有趣的点，也能对不同科目的知识进行关联。这些能对理解和记忆有所帮助，但老师通常不会讲这些。我就想如果能分享出来，帮助到学生——特别是那些和我一样的普通人，一定会带来很大的成就感。 后来就是随着对世界了解得越来越多，认识到让所有人提高知识水平以及接触那些更能接近客观真实的知识的重要性。说教师是一个神圣的职业，一点都不过分。 当然，也有一个现实的问题。那就是我能在 IT 界待多久？我个人是非常想一直在 IT 界待下去的，但是现在很难说我的成长速度能否跟得上要求，所以需要给自己创造更多的选择。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:20","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"技术上的提升 读通 Linux 内核的 TCP 源码，把 TCP 的相关流程梳理到博客 面试的那段时间，让我对 TCP 的实现充满了兴趣。我搜索了一些博客，但没能让我完全明白是怎么回事。所以就下载了 Linux 的源码并尝试读了一段时间，但还没有悟了的感觉。 为了将网络的理解再提升一个档次，必须将整个过程梳理清楚。 为啥知道能提升一个档次？因为在还没有完全梳理清楚的现在，我已经从梳理的过程中将大学的一些知识点串起来，且有了新的思考的角度。 LeetCode 算法题每周至少解一道简单和一道中等难度的题目或者一道困难的题目 保持思维活跃，避免大脑退化，为将来可能的教师职业提供更好的基础条件。 同时也为了在接下去短期的几年内由于各种原因不得不离开公司时，能够更加顺利地找到下一份工作。 ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:21","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"博客 每个月至少发布一篇博客，可能是技术内容，也可能是我对人生的一些思考。 以前一直没怎么输出博客，是因为回到宿舍就不想打字，太麻烦。有的时候是想要娱乐，有的时候是搞自己的那些服务器或者动手改造些东西。 比如最近几天把我的 USB 键盘通过树莓派变成了蓝牙键盘，我打算过年回家后，基于单独的蓝牙模块做改造，毕竟树莓派太贵也太大。相关内容至少能输出两篇文章，其中一篇我写一半多了。 我在外面走路或者坐公交车的时候，经常会思考很多事情，然后想着回宿舍一定要把它们写下来，结果一回宿舍就开始娱乐了。这个问题如果能解决，那我可以输出非常多的博客。目前来看，可能需要其他方面的积累。比如我现在能输出比较多的文字，跟我 2020 年坚持写一段时间日记有一些关系，但还有待提高。 不过我最近的一个发现可能对此有改善：我可以利用在公司时的专注性和高效率来写博客。 在午休期间和晚上下班后，都有一大块的时间，这段时间可以用于写作。比如我这篇总结的一部分就是在下班后写的。 之后可能更多地整理我在技术方面的积累，也有可能是我之前在构思的让非程序员能更快更好地理解程序员的工作和计算机基础知识的内容。 现在有了一种再不梳理成长可能就会停滞的压力，应该会促使我多总结多输出。 多说无益，明年再回过头来看看输出了多少篇。 最后 2020 年做的几件大事都让我获得了不小的成就感，但我还是会保持反思自己的习惯，努力让自己变得更加优秀。 最后的最后，很感谢我的那些小伙伴，还有曾经的和现在的同事，认识你们让我感到非常幸运。以后还请继续多多关照！ ","date":"2021-01-01","objectID":"/2021/01/01/2020-life/:0:22","tags":null,"title":"让不确定性成为朋友 —— 我的 2020","uri":"/2021/01/01/2020-life/"},{"categories":null,"content":"分布式锁用于解决分布式系统的资源竞争问题。例如避免多台服务器同时修改一个数据导致数据出错。 一个比较简单的场景是基于 crontab 的定时任务。当系统采用单机时，可以直接把定时任务放到 crontab 触发。但是如果要为系统部署一台热备，就得在主机故障后让备机自动代替主机触发定时任务的功能。 有两种可选项： 备机 crontab 不加入定时任务，当主机故障后，通过某种方式为备机加入 crontab 主备机的 crontab 保持一致，但阻止他们同时执行 第一种方式需要获取服务器列表及其健康信息。在发现主机故障后，选择一台服务器登录并修改 crontab。比较麻烦。 如果选择第二种方式，则需要考虑多台机器同时启动一个任务时，如果仅让其中一台服务器去执行。这就需要分布式锁。 每台服务器在启动任务的时候，都去尝试获取这个任务的锁。如果获取到，就执行；获取不到就放弃执行，退出任务。 ","date":"2020-11-16","objectID":"/2020/11/16/distributed-lock/:0:0","tags":["分布式","锁"],"title":"分布式锁","uri":"/2020/11/16/distributed-lock/"},{"categories":null,"content":"分布式锁的实现方式 分布式锁的支撑组件有两个要求： 本身支持锁住数据，或者所有请求按照顺序逐个执行 支持集群。即本身是高可用的。 这些组件可以是 MySQL 等关系型数据库、Redis、Zookeeper。以下主要介绍这三者的实现方式。 ","date":"2020-11-16","objectID":"/2020/11/16/distributed-lock/:1:0","tags":["分布式","锁"],"title":"分布式锁","uri":"/2020/11/16/distributed-lock/"},{"categories":null,"content":"MySQL 实现分布式锁 字段 作用 lock_id 用于唯一标识某个任务的 ID，设置 unique 索引 owner 锁的持有者，可以主动释放锁 expire_at 锁的过期时间 分为两步： 查看锁是否存在，如果不存在就创建。创建成功则表示获取到锁。 锁存在的情况下，尝试更新。更新成功则表示获取到锁。 第一步用 SELECT key FROM lock WHERE key = ? FOR UPDATE，将 key 锁住。 第二步在更新的时候，需要满足以下两个条件中的一个： owner 为空。即持有者主动释放锁。 expire_at 小于当前时间。表示锁过期。 ","date":"2020-11-16","objectID":"/2020/11/16/distributed-lock/:1:1","tags":["分布式","锁"],"title":"分布式锁","uri":"/2020/11/16/distributed-lock/"},{"categories":null,"content":"Redis https://www.jianshu.com/p/7e47a4503b87 ","date":"2020-11-16","objectID":"/2020/11/16/distributed-lock/:1:2","tags":["分布式","锁"],"title":"分布式锁","uri":"/2020/11/16/distributed-lock/"},{"categories":null,"content":"Zookeeper ","date":"2020-11-16","objectID":"/2020/11/16/distributed-lock/:1:3","tags":["分布式","锁"],"title":"分布式锁","uri":"/2020/11/16/distributed-lock/"},{"categories":null,"content":"桌面使用 1. 任务栏 Windows 样式： 右键任务栏 | 模式|高效 任务栏放在屏幕下方： 右键任务栏 | 位置|下 2. 输入法 右键任务栏输入法图标 输入法 | 注：仅保留 “拼音” 和 “键盘” 两项 全局配置 | 快捷键 | 切换激活/非激活输入法 | 左 shift 全局配置 | 快捷键 | 额外的激活输入法快捷键 | 禁用 全局配置 | 快捷键 | 上一页 | , 全局配置 | 快捷键 | 下一页 | . 全局配置 | 快捷键 | 显示高级选项 | 注：尽量清除其他所有快捷键，避免和其他快捷键冲突。方式是选中后按 Esc。 3. 文件管理器 隐藏系统盘： 文件管理器右上角汉堡图标 | 设置 | 高级设置 | 其他 | 隐藏系统盘 打勾 3. 应用商店下载应用 微信 网易云音乐 VS Code 迅雷 / Free Download Manager Chrome 4. Chrome 关闭顶部标题栏： 配置 | 外观 | 使用系统标题栏和边框 5. 其他软件 看图：XnView Multi Platform 6. Surface Go ","date":"2020-09-12","objectID":"/2020/09/12/deepin-20/:1:0","tags":["deepin","linux"],"title":"deepin 20(深度系统社区版)","uri":"/2020/09/12/deepin-20/"},{"categories":null,"content":"类似 yum whatprovides https://askubuntu.com/questions/481/how-do-i-find-the-package-that-provides-a-file sudo apt-get install apt-file sudo apt-file update apt-file find kwallet.h ","date":"2020-09-12","objectID":"/2020/09/12/deepin-20/:2:0","tags":["deepin","linux"],"title":"deepin 20(深度系统社区版)","uri":"/2020/09/12/deepin-20/"},{"categories":null,"content":"桌面快捷方式 [Desktop Entry] Exec=xfreerdp /u:username -wallpaper -themes +clipboard /sec:nla /w:1920 /h:1080 /v:192.168.1.1 Icon=dde-file-manager Terminal=true Type=Application ","date":"2020-09-12","objectID":"/2020/09/12/deepin-20/:3:0","tags":["deepin","linux"],"title":"deepin 20(深度系统社区版)","uri":"/2020/09/12/deepin-20/"},{"categories":null,"content":"raspberry: groupadd nfs useradd -m -g nfs nfs apt-get install rpcbind nfs-kernel-server vim /etc/exports /etc/exports : /home/nfs 192.168.15.0/24(rw,sync,no_subtree_check) systemctl restart rpcbind systemctl start nfs-kernel-server openwrt: opkg install kmod-fs-nfs-common kmod-fs-nfs nfs-utils kmod-fs-nfs-v4 portmap service portmap start mount -t nfs4 192.168.15.222:/home/nfs /mnt/nfs /etc/fstab : 192.168.15.222:/home/nfs /mnt/upan nfs auto,noatime,bg,nfsvers=4,intr,tcp,actimeo=1800 0 0 ","date":"2020-07-29","objectID":"/2020/07/29/nfs/:0:0","tags":["NFS","OpenWRT"],"title":"NFS 搭建给 openwrt 扩展存储","uri":"/2020/07/29/nfs/"},{"categories":null,"content":"2019 年 7 月本来想写 201807~201907 的部分，但由于当时项目比较赶，就一拖再拖。另外由于想写得详细一些，需要搜集很多信息，导致一直没进展。以下是 201807~202005 这段时间的内容，主要使用 PHP + MySQL。包含了接口设计、日志收集、分布式存储、高可用等内容。 ","date":"2020-07-12","objectID":"/2020/07/12/working-2-and-3/:0:0","tags":["工作","阶段回顾"],"title":"工作的第二和第三年（201807~202007）","uri":"/2020/07/12/working-2-and-3/"},{"categories":null,"content":"18 年后半年 这段时间仍然是以维护和开发旧系统为主。 比较可以一提的有： 写了套简易的异步任务管理器（10月） 写这个是因为我们系统会发送一个异步任务给外部系统，他们在执行完后，并不会回调我们系统的 API 通知我们任务状态。所以我们每次都是在流程的节点里面轮询调用接口去获取结果。我想要把轮询检测的部分独立出来，转成任务完成后再变更流程实例的状态。 刚好也有个新需求是外部系统会用回调通知任务状态，所以就写了个支持。 我是用 crontab 完成定时查询状态，毕竟业务上可容忍一分钟的延迟，所以不需要太复杂的组件。 引入 IoC 容器（11月） 原先创建一个类是直接 new 或者使用 get/set 注入，而在 11 月的时候我把 Laravel 的 Container 库引入进来。使用服务容器获取服务对象。主要是便于单元测试。 在了解了 IoC 容器后，我在部门做了一次分享。以发展的角度，从最基础的代码过渡到使用 IoC 容器的九个阶段，更深刻地理解 IoC 容器存在的原因以及使用场景。 优化了个冗长且经常改动的 if-else（11月） 这是一个根据多个条件判断选取哪些数据的 if-else，包含了很多个分支，并且直接嵌在某个业务代码里面，其他地方无法使用。 我把它抽取成一个函数。此外，利用类似于表驱动法的优化方式，让它到 Json 文件中读取所需要的数据。然后用 for 循环去依次匹配，匹配到的时候返回其对应的数据。 原先需求方提需求的时候，会发一个 Excel 过来，然后把变更点用特殊颜色标记。开发人员根据这些变化点修改代码，在发布到线上后才能生效。经过我的修改，需求方只需要直接到系统界面上上传 Excel 就能应用修改。 其实如果有前端的小伙伴，我觉得做成界面直接配置更好。 18 年总体表现还不错，年会上领了个公司级别的优秀员工奖。现在看来，这玩意儿最大的用处就是奖金和放老家让我爸开心一阵了。就算面试的时候想通过这个来表示比其他人努力，也没什么用。 公司这会儿被阿里和腾讯折腾得很难受，所以就算拿了优秀员工奖，年终奖金也没多少。 ","date":"2020-07-12","objectID":"/2020/07/12/working-2-and-3/:1:0","tags":["工作","阶段回顾"],"title":"工作的第二和第三年（201807~202007）","uri":"/2020/07/12/working-2-and-3/"},{"categories":null,"content":"19 年新系统 这一年挺关键的。以前就有离职的想法了，但因为有计划重构系统，所以我就留下来争取主导重构的机会。年初终于开始推动了。 从现在往回看，确实学到了很多东西。不过也因为参与人数少，没能多学一些。最初的时候只有一个同事每周花两天左右的时间做前端，其余的都由我负责。 说是重构，其实是重新做。新的系统大致是下面这个样子： 接口设计 为了设计接口，专门去了解了一下 RESTful API。主要从以下几个方面： 指南。比如： REST API Design Guide https://github.com/NationalBankBelgium/REST-API-Design-Guide/wiki 博客。比如： steps toward the glory of REST https://martinfowler.com/articles/richardsonMaturityModel.html 跟着 Github 学习 Restful HTTP API 设计 https://cizixs.com/2016/12/12/restful-api-design-guide/ RESTful API 设计最佳实践 https://www.oschina.net/translate/best-practices-for-a-pragmatic-restful-api API 文档。比如： GitHub API v3 https://developer.github.com/v3/ API： GitHub RESTful https://api.github.com/ RFC 标准： https://tools.ietf.org/html/rfc7230 https://tools.ietf.org/html/rfc7231 了解完之后，给我的感觉是 RESTful 的内容大多数都是按照 RFC 标准来的，在此基础上强调两点： URL 尽量都用名词 超媒体（Hypermedia） 有一次我在公司内部的 Wiki 上试图搜索一些 RESTful 内容，结果发现有一篇文章标题写着 RESTful，内容确是让人不要使用 DELETE/PATCH/PUT 这些方法。并且自己设计了一套 code 和 data 的格式，所有返回的 http code 都是 200。这不就是以前那种古老的设计方法嘛。如果说 Wiki 上那篇文章和 RESTful 有一丁点关系的地方，大概就只有 URL 尽量都用名词这一条了吧。这样看连 RESTful like 的层次都没达到。 超媒体这一条，我本来打算实现，但后面想想也没多大必要。内部系统给自己用的，做到 RESTful like 就差不多了。 而对于 RESTful like 来说，URL 尽量都用名词这一条我感觉是最考验接口设计者的。不仅要对业务非常熟悉，而且要能够把概念抽象出来。毕竟有时候一个操作涉及的东西特别多，普通的命名会导致 URL 地址太长。 还有一些要统一的，比如： 时间使用 ISO 8601 或者更准确的 RFC 3339 标准。 http://www.rfcreader.com/#rfc3339 翻页的链接放在 Header 的 Link 中。 http://www.rfcreader.com/#rfc8288 RFC 标准是把数据放 Body，把其他的往请求头放。毕竟这些信息也占不了多少空间。 花了挺多精力在 Restful 上面。不过我发现研究这些的用处并不大，毕竟在国内也不会有多少项目会去参考 RFC 标准来设计。大多数项目都是参考国内的通用设计，在此之上自己搞出一套标准出来。就算如此，也没有一套靠谱的标准，所以还是得根据项目所处环境调整。除非换到一个一开始就参考 Restful 的要求来实现接口的公司或团队。 以前刷微博时，看到一条吐槽无论成功与否，都返回 200 状态码的 API： 上图那个链接点进去是： 不过难道就真的只因为这个原因吗？我觉得更多的人是不知道有 RFC HTTP 标准的存在。如果不是 RESTful 流行起来，估计会更少有人知道这些标准的存在。而知道标准存在的那部分有决定权的人，愿意去了解，愿意去应用到新项目的，就更少了。 接口文档 有了接口，总也得写接口文档吧？怎么写接口文档也是个问题。 最开始是写到内部的 WiKi 上，但是写起来不太方便。每个 API 都得写一个页面。 尝试了 Swagger，语法比较多，一开始吃力。感觉如果真用 Swagger，可能会被打，就放弃了。 最后还是写到 WiKi 上，不方便就不方便吧，也没啥。 我们系统是前后端分离，前后端并行开发的时候，前端需要获取接口的 Mock 数据。 我本来想用公司内部自己搞的一套 Mock 系统，但是发现太难用了。 后来自己搭了一套 Easy Mock，还可以从 Swagger 导入。但是 Easy Mock 有个毛病，就是它假设你是使用国内自己搞的那套接口标准，无论正确错误都返回 200 那种，上面吐槽过了。你要按照 RESTful 来做接口，它 Mock 起来返回的状态码或者数据就不会按你定义的来。无奈之下我去找到相关逻辑的代码，把它们改成我想要的样子。但这样也不是个办法。 最后干脆不要了。前端自己 Mock 去吧。 GraphQL 在研究 RESTful API 怎么设计而去 GitHub 参考它的文档时，发现了 GitHub 的新版 API 使用的是 GraphQL。 这让我很感兴趣，为啥 GitHub 要从 RESTful 转向 GraphQL？于是就各种找资料了解 GraphQL 是啥？解决了啥问题？ 这一了解下去，突然兴奋。这不就正是能解决我们当前查询上的问题吗？ 这个问题出在公司的 CMDB 和集群架构平台的接口太弱，以至于要查一些数据的时候，得调用一大堆 API。例如多对多关联的情况下，需要调用三个接口，就像是执行三条 SQL 语句，特别恶心。之前我们这边的应对办法是，定期获取所有接口的全量数据，放到 MySQL 数据库，然后从数据库里面用 join 查。 但是这对开发人员来说很不友好。当然有一部分原因是旧项目没有 ORM。如果有的话，就不会这么难了。由于没有 ORM，导致每个查询都需要写纯 SQL。而且之前的同事又一直没有复用的概念，基本上每次要查询都重新写一个 SQL，最多是把以前写的 SQL 语句复制过来，然后稍微改改。于是一大堆的表，要经过很长一段时间才知道各表之间的关联关系。 GraphQL 可以先定义好各表之间的关系，然后使用 HTTP 把想要的数据及其关联的数据一起拿到。由于它自带 API 文档，可以在一个名为 playground 界面中查看所有 API 及从某个字段找到关联的其他表的字段。 我把 GraphQL 引入重构项目，后来也在分享会上给部门的小伙伴介绍。 使用过后的感受可以说是好坏都有。好的一点是查询比较灵活，而且不会获取多余的字段。坏处是有时候获取的字段的数据会有重复，GraphQL 的基础库没有提供这种支持。另外 GraphQL 由于每个数据的每个字段都要执行一次 resolve() 函数，量一大就会消耗很多资源。也有一些其他的优缺点。但我发现好多介绍 GraphQL 的文章，都不喜欢谈它的缺点。 不过虽然引入了 GraphQL，我还是用它去查数据库里面的数据，而不是用来封装对 CMDB 接口的调用。但是使用 GraphQL 就为以后将查询切换为接口提供了方便。这是按字段 resolve() 所带来的灵活性。 流程引擎 之前写过《流程引擎为什么选 Camunda》和《Camunda 流程引擎的一种 Adapter 层实现》。最开始设计系统的时候，大概花了一个月的时间把 Camunda 了解了一遍，然后根据 Camunda 已有接口组合出我们业务需要的样子。查了很多文档（特别是官方文档）和做了 N 多实验，有时候为了解决一个问题，干到凌晨两三点才下班。也算是体会了一把传说中的加班到十二点后。 Laravel 当时选它的原因估计是因为旁边有个大佬（目前在微信支付）用了很长时间的 Laravel，有一次在我们的每周分享上专门吹了一把 Laravel。另外也因为稍微尝试过 Yii2，感觉不太喜欢这种很固定 MVC 的方式。 很早就听说 Laravel 这框架很重，学起来很难。不过我当时在用的时候，没感觉到难在哪。虽然有那么一次出现问题，调试的时候在框架代码跳来跳去，不过这也没什么，而且只有一两次。之后越用越觉得好用，越觉得 Laravel 牛逼。 比较直接的是两点： IoC 容器 服务注册 作为一个菜鸡，当时也没啥经验。此处羡慕一下学 JAVA 的同学，天生就接触了这俩概念。 在接触 Laravel 之前，由于不知道这两者，让我走了不少弯路。 首先是 IoC 容器。用容器来获取对象非常灵活，而且对单元测试有很大的帮助。 以前我写过一篇单元测试的博客，主要是参考《单元测试的艺术》。我当时的说法是“如果要单元测试，首先要保证代码是可测试的”。而所谓的“可测试的”就表示需要使用依赖注入。当时其实也没理解清楚，依赖注入也只了解了常见的几种。后面找个时间重新写一篇关于单元测试的博客。 我上一篇《入职一年啦》提到过重构一个基于 Zend Framework 框架写的项目。当时我还不知道有容器，所以大量使用 getter\u0026setter 注入。 后来接触了 Laravel，然后专门有一次讲了 IoC 容器的由来。不过这由来是我自己推理来的。当时用的代码示例在： https://github.com/scha","date":"2020-07-12","objectID":"/2020/07/12/working-2-and-3/:2:0","tags":["工作","阶段回顾"],"title":"工作的第二和第三年（201807~202007）","uri":"/2020/07/12/working-2-and-3/"},{"categories":null,"content":"TCP 位于计算机网络七层模型中的传输层。 TCP 协议在操作系统层面实现，实现方式为 TCP Socket。 除了 TCP Socket 之外，还有同样是主要用于主机间进程通信的 UDP Socket 和用于本机进程间通信的 Unix Domain Socket。它们提供的接口是一样的，但底层实现细节不一致。本文主要内容是 TCP Socket ，因此以下默认 Socket 都是表示 TCP Socket。 操作系统向应用开发程序员隐藏 Socket 的具体实现细节，仅提供一套统一的操作接口。WEB 应用或者基础应用开发人员（操作系统开发人员除外）通常能操作的最底层的通信接口就是 Socket 接口。如果了解 Socket 的工作原理，对理解其他上层协议（如 HTTP 协议）有很大的帮助。 在正式介绍前，如果读者对操作系统实现 Socket 的细节感到好奇，想先看看代码长什么样，可以进入以下链接查看 Linux 的相关源码。本文的后面部分会介绍一些源码相关的内容。以下都以 Linux 为例。 https://github.com/torvalds/linux/blob/master/net/ipv4/af_inet.c ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:0:0","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"两台不同主机上的进程通信 此处主要关注传输层，跳过物理层、数据链路层和网络层的内容。 前文我多次使用\"主机间进程通信\"这样的表述，而不是\"主机间通信\"，是因为主机间的通信问题已在网络层中解决。因此我们脑海中应该始终有一张图：只有两个进程，以及一条连接这两个进程的管道。 于是我们就有了第一个问题： ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:1:0","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"客户端进程怎么把管道接到服务端进程上的？ 在网络层，客户端所在主机已经可以通过 IP 找到了服务端主机。接下来需要传输层找到服务端进程了。 假设我们不知道已有的实现方式，需要从头设计。该怎么做？ 进程不都有个进程 ID 嘛，让客户端带上这个进程 ID 就能找到了 但问题是进程 ID 不是固定的，程序重启后就变了。而让程序和进程 ID 始终绑定则会导致更多问题。 既然 ID 会变，那选择相对稳定的进程名呢？客户端传一个服务端的进程的进程名，让服务端主机根据进程名找到服务端进程 但问题是这限制了服务端主机只能开启一个拥有该进程名的进程。同时每个进程只能提供一个服务。 这样我们很自然地想到添加一个相对固定的机制。就像数据库里面除了自增 ID 外，有时会添加一列 UUID。在进程间通信的问题上，由于一台机器可以同时提供的服务有限，用 16 位二进制就够用了（0 - 65535）。我们称这个 ID 为“端口”。 服务端进程启动时，绑定由服务提供者指定的端口。客户端或用户从服务提供者那里获取服务端进程绑定的端口，请求时带上该端口 就能找到目标进程了。 只要服务端的配置不变，进程重启后仍然会绑定同样的端口。如果想开启多个相同服务，只需给这些不同的进程绑定不同的端口就行了。 以上解决了第一个问题。客户端进程通过端口找到位于服务端的进程。 在实践中，不可能每个服务端进程仅与一个客户端进程通信，因为这会浪费资源。我们希望服务端进程能同时与同一台客户端服务器或者多台不同服务器上的多个客户端进程建立通信。这样当某个通信进入等待状态时，可以让服务端进程切换到与另一个客户端进程的通信，之后再切回来继续处理。 于是我们就有了第二个问题： ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:1:1","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"服务端进程如何知道自己是在和哪个进程通信？ 同样地，我们得给客户端进程在建立通信的时候绑定一个 ID。为了方便，也使用与服务端进程相同的端口机制。不过由于客户端可能同时需要与多个服务端进程通信，如果每次都要手动分配端口，那就太麻烦了，于是改为由进程向操作系统申请一个随机的端口。通常情况下用户并不需要知道这个端口是啥。 这样就有了【\u003c客户端 IP ：客户端进程端口\u003e，\u003c服务端 IP ：服务端进程端口\u003e】这样的关联。客户端和服务端操作系统都会在内存的内核空间中保存该信息。服务端进程通过客户端 IP 和客户端进程端口知道自己当前在和哪个进程通信。为了描述方便，我们把这个用于描述通信双方进程信息的关联称之为“套接字”，对应英文单词 Socket。 “套接” 是指它像连接两条水管的套接管一样，提供一个通道让它们可以连通。而 “字” 在计算机里面通常表示 “一对” 或者 “两个” 的意思，这就是为了说明必须有一对标识，也就是双方的 IP + 端口。这里是参考以下的内容： https://www.zhihu.com/question/21383903/answer/1024419470 以上解决了第二个问题。 第三个问题： ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:1:2","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"两个进程如何通过套接字进行通信？ 两者的通信就像是双方进程往套接字读写数据。 一旦涉及到读写，对于 Linux 这样把一切都抽象成文件的系统来说，套接字也会被关联到文件上。我们将套接字关联的文件称之为“套接字文件”。 套接字文件与我们通常使用的文件不一样。操作系统不会将套接字文件的数据刷入到磁盘，而是将数据放在内存的内核空间中的缓冲区（队列），然后程序通过内核接口读写缓冲区。缓冲区是一个字节数组，在 C 语言中用 char[n] 表示，这里的 n 用于限制缓冲区最多能存放多少字节。 其他 Linux 文件类型可到搜索引擎搜索 “Linux 文件类型” 如果每个套接字只有一个缓冲区用于读写，则同一时刻只能有一方在写。为了提高通信效率，双方的操作系统为每个套接字分配了两个缓冲区以便通信的双方进程同时写，这两个缓冲区分别为读缓冲区和写缓冲区。 通信的过程是这样的：客户端进程通过系统调用将数据写入套接字文件，操作系统将对套接字文件的写操作转化为对套接字的写缓冲区的写操作，然后产生一个写事件，加入到操作系统内核的写队列。操作系统会将写缓冲区的数据打包到 TCP 报文中，通过网络接口控制器（NIC，我们通常叫网卡）发送出去。服务端操作系统会从 NIC 收到 TCP 报文，从报文中获取到套接字信息，并用它找到操作系统内核中保存的与之对应的套接字，然后把数据复制到套接字的读缓冲区。然后通知套接字绑定的进程，服务端进程再通过系统调用对套接字文件执行读操作，操作系统会将其转换为对套接字读缓冲区的读操作，位于内核空间的套接字读缓冲区的数据复制到进程内的缓冲区。 ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:1:3","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"进程如何读写套接字文件？ 操作系统创建的套接字对应的文件的文件描述符会存放在 /proc/进程ID/fd/ 底下。 [root@localhost /]# ls -l /proc/7376/fd total 0 lr-x------. 1 root root 64 Jun 18 15:51 0 -\u003e /dev/null lrwx------. 1 root root 64 Jun 18 15:51 1 -\u003e socket:[13119730] lrwx------. 1 root root 64 Jun 18 15:51 2 -\u003e socket:[13119730] lrwx------. 1 root root 64 Jun 18 15:51 3 -\u003e socket:[14312193] lrwx------. 1 root root 64 Jun 18 15:51 4 -\u003e anon_inode:[eventpoll] lrwx------. 1 root root 64 Jun 18 15:51 5 -\u003e socket:[14312194] lrwx------. 1 root root 64 Jun 18 15:51 6 -\u003e socket:[13119755] lrwx------. 1 root root 64 Jun 18 15:51 7 -\u003e socket:[14311807] lrwx------. 1 root root 64 Jun 18 15:51 8 -\u003e socket:[14311808] 0 ~ 8 是 fd （file descriptor，文件描述符，在 Windows 中叫做句柄），箭头后面指这个文件描述符指向的文件。 你可以执行命令 ss -t -l -p 来查看到关于 Socket 更详细的信息。这里以某台机器上进程号（pid）为 7376 的进程为例子，直观地感受一下两者的关联： [root@localhost /]# ss -t -l -p State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 100 127.0.0.1:smtp *:* users:((\"master\",pid=29939,fd=13)) LISTEN 0 128 *:ssh *:* users:((\"sshd\",pid=2419,fd=3)) LISTEN 0 100 [::1]:smtp [::]:* users:((\"master\",pid=29939,fd=14)) LISTEN 0 128 [::]:55554 [::]:* users:((\"test\",pid=7376,fd=6)) LISTEN 0 128 [::]:31689 [::]:* users:((\"test\",pid=7376,fd=5)) LISTEN 0 128 [::]:35849 [::]:* users:((\"test\",pid=7376,fd=8)) LISTEN 0 128 [::]:31085 [::]:* users:((\"test\",pid=7376,fd=7)) LISTEN 0 128 [::]:35728 [::]:* users:((\"test\",pid=7376,fd=3)) LISTEN 0 128 [::]:ssh [::]:* users:((\"sshd\",pid=2419,fd=4)) 最后一列中也有 fd。 对于进程来说，它关注的是对套接字文件的读写。 双方在执行系统调用的时候，都是传入文件描述符，由操作系统去找对应的 Socket。 这里面说得比较详细： https://colobu.com/2019/07/27/How-TCP-Sockets-Work/ ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:2:0","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"操作系统提供了哪些套接字文件的接口？ 既然是文件，那就得有创建文件、打开文件、关闭文件、删除文件的接口吧？于是就有了： int socket(int domain, int type, int protocol) 相当于创建和打开。此时 socket() 会为套接字随机分配一个端口号。 int close(int fd) 相当于关闭和删除 这样客户端和服务端就可以各自打开一个 Socket 文件了。 由于服务端进程需要绑定由服务提供者指定的端口，所以得加上一个操作： int bind(int sockfd, const struct sockaddr* myaddr, socklen_t addrlen) myaddr 包含了本地 IP 地址和端口号，将其绑定到 Socket 上面。 由于服务端套接字不是主动发起连接，它需要让系统 的处理逻辑是不一样的，所以两者得区分开。 当使用 socket() 接口创建和打开 Socket 时，它默认将其设置为主动类型。主动和被动在之后的处理逻辑是不一样的，所以两者得区分开。那么服务端要怎么将其改成被动的呢？于是就有了： int listen(int sockfd, int backlog) 那么主动和被动有啥区别？其中一个区别是因为客户端和服务端是 n:1 的关系，服务端需要维护一些队列，用于存放客户端的连接请求，backlog 指定的就是这个队列的长度。listen() 会初始化这些队列，同时也把套接字转成 监听套接字。 终于可以开始联系了。总得给客户端提供一个主动联系的接口吧？于是就有了： int connect(int sockfd, struct sockaddr *serv_addr, int addrlen) 这时候趁机指定了对方的 IP 地址和端口。 服务端咋知道有人想要联系它？我们先简要说一下 tcp 的连接过程，即三次握手。 客户端发送 SYN 包。 服务端收到 SYN 包，创建 请求套接字，放入到半连接队列。回复 ACK + SYN。 客户端发送 ACK。服务端收到 ACK，将请求套接字放到全连接队列。 此时的服务进程需要有一个接口，将套接字取出来。于是就有了： int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen) 如果全连接队列不为空，则从中取出一个请求套接字。然后重新获取一个文件描述符，以及一个新的 Socket，并将两者绑定起来。新的 Socket 会从请求套接字中取出必要的信息，如客户端的 IP 和端口。以后和客户端的交流都通过这个新的 Socket。 为每个连接分配一个专用的 Socket，清晰又方便。 叹了口气，双方可算是联系上了。 为了能互相说话。得再提供两个接口： int send(int sockfd, const void *msg, int len, int flags) 用于发送数据 int recv(int sockfd, void *buf, int len, unsigned int flags) 用于接收数据 总结一下， TCP 套接字共有以下这些接口： int socket(int socket_family, int socket_type, int protocol) int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen) int listen(int sockfd, int backlog) int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen) int aaccept(int sockfd, struct sockaddr *addr, socklen_t *addrlen) ssize_t send(int sockfd, const void *buf, size_t len, int flags) ssize_t recv(int sockfd, void *buf, size_t len, int flags) int close(int fd) 可以从 net/ipv4/af_inet.c 这个文件作为入口。如果没有读过开源项目的 C 源码，需要注意源码中对 Socket 的操作，所以一旦需要看更底层的细节，必须到 static int __init inet_init(void) 这个函数底下看各种 Socket 的操作函数的注册。 af 是 address family 的缩写。 对套接字借口的调用都会进入到 net/socket.c 的 kernel_xxx 系列函数。例如 connect 进入到 kernel_connect 函数，并且在里面调用 sock-\u003eops-\u003econnect()，这会调用在 inet_init 里注册的对应的 connect 函数。例如在 net/ipv4/tcp_ipv4.c 里的 tcp_prot ： struct proto tcp_prot = { .name = \"TCP\", .owner = THIS_MODULE, .close = tcp_close, .pre_connect = tcp_v4_pre_connect, .connect = tcp_v4_connect, .disconnect = tcp_disconnect, .accept = inet_csk_accept, .ioctl = tcp_ioctl, .init = tcp_v4_init_sock, .destroy = tcp_v4_destroy_sock, .shutdown = tcp_shutdown, .setsockopt = tcp_setsockopt, .getsockopt = tcp_getsockopt, .keepalive = tcp_set_keepalive, .recvmsg = tcp_recvmsg, .sendmsg = tcp_sendmsg, // 以下省略 }; EXPORT_SYMBOL(tcp_prot); 其中 tcp_v4_connect 函数注册到 connect 上面。在调用 sock-\u003eops-\u003econnect() 的时候，实际调用的是 tcp_v4_connect() 函数。 为什么选择协议的时候，通常都没有让人选择 HTTP 或者 HTTPS 之类的，而是只给出 TCP 和 UDP。正是因为 HTTP 这种应用层协议是基于 TCP 或者 UDP 的。 __sys_socket -\u003e socket() Linux 系统调用 系统调用表：sys_call_table x64 架构的系统调用表位于 arch/arm64/include/asm/unistd32.h。修改的时候不直接修改这个文件，而是下面这个文件。 系统调用实际定义 include\\uapi\\asm-generic\\unistd.h： /* net/socket.c */ #define __NR_socket 198 __SYSCALL(__NR_socket, sys_socket) #define __NR_socketpair 199 __SYSCALL(__NR_socketpair, sys_socketpair) #define __NR_bind 200 __SYSCALL(__NR_bind, sys_bind) #define __NR_listen 201 __SYSCALL(__NR_listen, sys_listen) #define __NR_accept 202 __SYSCALL(__NR_accept, sys_accept) #define __NR_connect 203 __SYSCALL(__NR_connect, sys_connect) 系统调用定义 net/socket.c： SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol) { return __sys_socket(family, type, protocol); } 实际定义 net/socket.c： int __sys_socket(int family, int type, int protocol) { int retval; struct socket *sock; int flags; /* Check the SOCK_* constants for consistency. */ BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC); BUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK); BUILD_BUG_ON(SOCK_CLOEXEC \u0026 SOCK_TYPE_MASK); BUILD_BUG_ON(SOCK_NONBLOCK \u0026 SOCK_TYPE_MASK); flags = type \u0026 ~SOCK_TYPE_MASK; if (flags \u0026 ~(SOCK_CLOEXEC | SOCK_NONBLOCK)) return -EINVAL; type \u0026= SOCK_TYPE_MASK; if (SOCK_NONBLOCK != O_NONBLOCK ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:3:0","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"安全调用 security_socket_create() --\u003e selinux_socket_create() LSM_HOOK_INIT(socket_create, selinux_socket_create) ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:3:1","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"Socket 文件（sock_map_fd） 在为 socket 分配文件时，主要有三个步骤： 找一个未被使用的文件描述符 fd（file descriptor） 申请一个新文件，这个文件的 iNode 为 socket 之前预先分配的 iNode。socket 和文件会互相把对方的指针保存到自己的结构体中。 把一开始获取到的 fd 和申请的新文件建立映射关系，并返回 fd 这样在用户程序传入 fd 的时候，可以找到该文件，并且从该文件的结构体里面找到对应的 socket。 ","date":"2020-07-02","objectID":"/2020/07/02/tcp-socket/:4:0","tags":["TCP","Socket"],"title":"TCP 协议和 TCP Socket","uri":"/2020/07/02/tcp-socket/"},{"categories":null,"content":"当我们开始执行一个程序时，会创建一个进程。这个进程可能需要与其他进程通信。例如向操作系统发送一条消息，让它展示给用户。 如果只是在自己的机器上通信，那就不有趣了。我们还想跟其他机器的进程通信，比如聊天应用就涉及到了多台机器。这种跨多台机器（主机）的通信通过套接字（Socket）实现。 我还无聊地去查了为什么 Socket 范围为套接字： https://www.zhihu.com/question/21383903/answer/1024419470 ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:0:0","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":"进程间通信是如何实现的？ 通常我们会认为进程间的通信就像是一个进程直接把消息塞给另一个进程。但抽象起来说，其真实过程通常是一个进程将信息发送给某个中介（如通过系统调用复制到内核内存空间），另一个进程从中介获取这些信息。 ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:1:0","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":"进程间通信有哪几种方式？ 按照主要功能，分为主机内进程间通信和主机间进程间通信。主机内的通信会简要介绍，主要的部分在于套接字通信。 主机内： 信号 内核利用信号通知用户空间的进程发生了哪些系统事件。例如 SIGKLL 表示关闭该进程。 管道（半双工） 匿名管道（限于父子进程、子进程间） 内核空间开辟的一块内存。写方使用系统调用将数据从用户空间复制到内核空间，读方使用系统调用将数据从内核空间复制到用户空间。 https://www.cnblogs.com/sparkdev/p/10997135.html 匿名管道分为两部分，一个是写专用的文件描述符，另一个是读专用的文件描述符。其中一个进程关闭写描述符，另一个关闭读描述符。因此通信方向是单向的（半双工）。 命名管道 存储在文件系统上的一个文件。进程通过指定文件名来选择命名管道。只是把存储方式改为文件，仍是半双工（一个进程以只读方式打开，另一个以只写方式打开）。 https://www.cnblogs.com/sparkdev/p/11008978.html System V IPC 信号量 对于互斥的共享资源，使用信号量进行互斥的控制。 信号量可大于 1。当等于 0 时，消费者需要等待。如果是写锁，信号量最大设置为 1。 服务端申请信号量，并获取信号量 ID。客户端要从服务端获取该 ID ，才能对信号量操作。 https://www.cnblogs.com/sparkdev/p/8692567.html 共享内存 多个进程将同一段物理内存连接到进程自己的地址空间中。可读写。内存处于用户空间。 服务端申请共享内存，并获取共享内存 ID。客户端要从服务端获取该 ID，才能对共享内存操作。读或写之前，用信号量实现互斥。 消息队列 在内核中消息以链表的形式存储。不同队列以队列 ID 标识。 Unix 域套接字（Unix domain socket，全双工，应用最广泛） 在 TCP/IP 套接字的框架上发展出来。由于主机内部无需做可靠性的保证，因此比 TCP/IP 套接字用于主机内通信时更快。 https://serverfault.com/questions/124517/whats-the-difference-between-unix-socket-and-tcp-ip-socket 主机间： TCP/IP 套接字 ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:2:0","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":"套接字（Socket）通信 为啥要了解 Socket？它是传输层（TCP/UDP）的一种实现方式。对于 web 开发或者一些底层服务开发，想要深刻理解一些协议，总是要了解 Socket 的工作原理。例如 HTTP 协议为啥长这样？HTTP 2 协议为啥是那样？为啥要有 Content-Length 这个请求头？为啥一次连接可以有多个请求？这些在同一个连接上的请求是串行还是并发处理的？ 套接字通信基于套接字文件。如果使用 Linux 系统，会经常看到以 .sock 结尾的文件。例如 docker.sock，这个就是用于套接字通信的文件。 Socket 分为两种类型： Unix domain socket 用于主机内部进程间通信 TCP/IP 套接字主要用于主机之间的进程间通信，也可用于主机内部进程间通信 ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:3:0","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":"Socket 与并发的关系 那么如果是一个 WEB 服务，Socket 如何支持并发的请求呢？它并没有支持。一个 Socket 只能对应一个客户端。当处于并发的状态，服务器会为每个客户端创建一个对应的 Socket。 https://zhuanlan.zhihu.com/p/130247159 处理这些客户端的请求的方式分为五种： 一次处理一个客户端的连接，其他阻塞其他客户端的连接请求。（并发 1） 主线程负责分发，每个客户端绑定一个工作线程来处理消息业务，并读写套接字。客户端:线程 = 1:1。客户端数量一旦变多，线程数会达到硬件所能支持的上线。（读写并发 n，业务处理并发 n） 仅用一个主线程处理，基于事件的机制（I/O 多路复用）。客户端:线程 = n:1。同一时间只能处理一个客户端的读写。当有一个客户端业务阻塞时，切换到其他客户端。（Redis）（读写并发 1，业务处理并发 ?） 主线程负责分发和读写套接字，工作线程负责处理消息业务。主线程仍然使用 I/O 多路复用，意味着工作线程和客户端的比例是 m:n。（读写并发 1，业务处理并发 m） 如果还想再提高性能，那就从读写并发入手。主线程开启 m 个读写线程，这些线程内部使用多路复用，不额外开工作线程。（读写并发 m，业务处理并发 m） 从安全性考虑，由于线程共享同一内存空间，可能会出现一个线程干扰其他线程。因此将上一条的线程改成进程。（读写并发 m，业务处理并发 m，更安全）（Nginx） 如果要再提高业务处理并发数，可以将 4 和 5 结合起来。但是线程切换会带来大量成本。（读写并发 m，业务处理并发 m*x） 虽然一个 Socket 只能对应一个客户端，但一台主机可以启动多个客户端，所以客户端主机可以同时发起多个请求。 ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:3:1","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":"为什么 Socket 是全双工的？ 这就得看 Socket 的底层实现了。 我们可以从前面的主机内通信的命名管道得到一些灵感。命名管道是只能一个进程读，另一个进程写。如果开启两个命名管道，两边各持有一个作为写，另一个作为读，岂不是逻辑上就是全双工了？ 事实上 Socket 在内存中会有两个 FIFO 队列。分别是用于发送的 SendQ，和用于接受的 RecvQ。 https://blog.csdn.net/jiaomingliang/article/details/45950591 ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:3:2","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":"Socket 文件的内容都有些啥？ linux 中的文件有七种类型，除了最常接触到的普通文件类型以及目录，还有管道文件、链接文件、套接字文件、块设备文件、字符设备文件。 由于最经常接触普通文件，以为读写文件最终都会写入磁盘或者从磁盘读取，但套接字文件并不是，它只是一个便于寻找的标记。 网络的一个很重要的知识是 TCP 的建连过程。 对于 TCP/IP Socket 文件并没有存放什么东西。它是用于和 socket 绑定在一起，表示内核中的网络文件。 https://blog.51cto.com/weiguozhihui/1585297 底层会调用 tcp_sendmsg() 发送消息，其最终调用 sk.sk_write_space()。sk_write_space 是一个函数指针。根据不同的 driver，会注册不同的函数。 例如： https://github.com/torvalds/linux/blob/master/drivers/net/tap.c https://github.com/torvalds/linux/blob/master/net/dccp/output.c https://github.com/torvalds/linux/blob/master/drivers/net/tun.c https://github.com/torvalds/linux/blob/master/net/ceph/messenger.c https://github.com/torvalds/linux/blob/master/net/unix/af_unix.c 以最后这个 Unix Domain Socket 为例。 static struct sock *unix_create1(struct net *net, struct socket *sock, int kern) { // ... sk = sk_alloc(net, PF_UNIX, GFP_KERNEL, \u0026unix_proto, kern); // ... sk-\u003esk_write_space = unix_write_space; // ... } // ... static void unix_write_space(struct sock *sk) { struct socket_wq *wq; rcu_read_lock(); if (unix_writable(sk)) { wq = rcu_dereference(sk-\u003esk_wq); if (skwq_has_sleeper(wq)) wake_up_interruptible_sync_poll(\u0026wq-\u003ewait, EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND); sk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT); } rcu_read_unlock(); } ","date":"2020-06-27","objectID":"/2020/06/27/ipc/:3:3","tags":["Socket"],"title":"进程间通信（IPC，Inter-process Communication）","uri":"/2020/06/27/ipc/"},{"categories":null,"content":" 进程间通信方式有哪些？ Socket 通信如何处理并发通信的情况？它的底层结构是怎么样的？ 内存分配中的栈内存、堆内存分别存储什么？TCmalloc 和 Jemalloc 中的内存是申请一大块内存，然后切割成小内存，然后有一块内存用于记录其他内存空间。跟这个是否有关系？ 栈内存分配给定长的数据，例如指针。 堆内存分配给不定长的对象。 由于进程隔离，进程会得到一个 4GB 的虚拟地址空间。这是根据 32 位处理器的寻址能力确定的。那么如果是 64 位处理器，这个虚拟地址空间还是 4GB 么？ 内核空间和用户空间分别指什么？ 针对 Linux 操作系统，将最高的1G字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF ）供内核使用，称为内核空间，而较低的 3G 字节（从虚拟地址 0x00000000 到0xBFFFFFFF），供各个进程使用，称为用户空间。每个进程都可以通过系统调用进入到内核。其中在 Linux 系统中，进程的用户空间是独立的，而内核空间是共有的，进程切换时，用户空间切换，内核空间不变。—— https://www.cnblogs.com/huansky/p/13170125.html fork之后，子进程从父进程那继承了什么？ 子进程会获得其父进程所有文件描述符的副本，这些文件描述符在执行 fork 时打开。 文件描述符是啥？有什么相关的数据结构？ 文件描述符，在 Windows 中被称为文件句柄，是一个整数。它是系统内核打开文件后给出的一个数字索引。内核会在每个进程空间中维护一个文件描述符表。这张表记录着文件描述符和文件的结构体的对应关系。 https://www.cnblogs.com/xiangtingshen/p/11961434.html C 语言的 fopen 为什么返回的是 FILE 指针？为什么操作的都是指针？ opaque 类型，隐藏结构体内部实现。 https://blog.csdn.net/xgbing/article/details/2775426 ","date":"2020-06-27","objectID":"/2020/06/27/questions/:0:0","tags":["问题集"],"title":"问题集","uri":"/2020/06/27/questions/"},{"categories":null,"content":"vim 到某一行 #!/bin/bash onlyRead='' while getopts ':R' optchar; do case \"${optchar}\" in R) onlyRead='-R' ;; *) break ;; esac done shift $((OPTIND-1)) act=$1 result=$(grep -n \"act=='$act'\" /path/to/dir) fileName=$(echo \"$result\" | awk -F ':' '{print $1}') line=$(echo \"$result\" | awk -F ':' '{print $2}') vim $onlyRead +$line $fileName bc - 计算器 echo $(cat a.txt | tr \"\\n\" \"+\") | bc cat a.txt | tr \"\\n\" \"+\" | sed 's/$/\\n/' | bc 不加换行会报错 sort - 排序 echo -e \"$counts\" | sort -n -k 1 -r | column -t -s ' ' | cat -n Crontab crontab -l \u003e ~/crontab echo \"* * * * * echo test\" \u003e\u003e ~/crontab crontab ~/crontab 直接加载指定文件的内容到 crontab 里面 crontab -l | sed 's///g' | crontab - 跳过输出到文件 dmesg(时间转换为可读形式) 新版系统： `dmesg -T` 老版本系统： dmesg | sed -r 's#^\\[([0-9]+\\.[0-9]+)\\](.*)#echo -n \"[\";echo -n $(date --date=\"@$(echo \"$(grep btime /proc/stat|cut -d \" \" -f 2)+\\1\" | bc)\" +\"%c\");echo -n \"]\";echo -n \"\\2\"#e' | less 批量重命名 去掉所有文件的 .new 后缀 ls | grep new | xargs -I {} bash -c 'a={}; mv $a $(echo $a | sed \"s/.new//\")' less 命令自动换行（word wrap） https://superuser.com/questions/272818/how-to-turn-off-word-wrap-in-less -S, --chop-long-lines 可以直接在查看界面 -S 然后 Enter lsof 与某台主机的连接 lsof -i@192.168.1.1 某个进程 ID 的连接 lsof -p 进程ID 某个端口的进程 lsof -i:端口号 umount: /mnt: target is busy. 用 lsof /mnt 查看占用的进程，kill 掉就行。 nohup 命令输出文件太大 cat /dev/null \u003e stdout.nohup 如果不小心删除了文件，则需要找到打开的文件描述符： pid=$(ps aux | grep your_process | grep -v grep | awk '{print $2}') lsof -p ${pid} 可以看到对应的文件描述符 ID，例如是 fd 为 1 的文件被删除。则执行： cat /dev/null \u003e /proc/${pid}/fd/1 应用当前状态 function status() { pid=$(getpid) if [[ ! -z \"${pid}\" ]]; then ps_source=$(ps -p ${pid} -o user,pid,ppid,pcpu,pmem,vsize,rssize,etimes,command) to_ISO3339=$(echo \"${ps_source}\" | sed 's/ELAPSED/START/' | awk 'BEGIN{now=systime()} NR\u003e1 {$8=strftime(\"%Y-%m-%dT%H:%M:%S\", now-$8);}{print;}') to_MB=$(echo \"${to_ISO3339}\" | awk 'NR\u003e1 {$6=int($6/1024)\"M\";$7=int($7/1024)\"M\";}{print;}') echo \"${to_MB}\" | column -t return fi } Systemctl 文件存放位置： /etc/systemd/system/ /usr/lib/systemd/system /lib/systemd/system tcpdump #!/bin/bash tcpdump -i eth1 -w target.cap -c 1000 -s 0 -vv -A '(src host 192.168.1.1 or src host 192.168.1.2)' 文本行从下到上输出（tac） tac 是 cat 的反向。 这是 cat： $ cat thegeekstuff.txt 1. Linux Sysadmin, Scripting etc., 2. Databases Oracle, mySQL etc., 3. Hardware 这是 tac： $ tac thegeekstuff.txt 3. Hardware 2. Databases Oracle, mySQL etc., 1. Linux Sysadmin, Scripting etc., tmux 强行关闭窗口 ctrl+b \u0026 替换维基百科地址 // ==UserScript== // @name 替换维基百科地址 // @match *://www.google.com/* // @match *://cn.bing.com/* // @grant none // ==/UserScript== var links = document.getElementsByTagName(\"a\"); for (var i=0,imax=links.length; i\u003cimax; i++) { if (links[i].href.match(/\\.wikipedia\\.org\\/wiki\\//) || links[i].href.match(/\\.wikipedia\\.org\\/zh-hk\\//) || links[i].href.match(/\\.wikipedia\\.org\\/zh-hans\\//)) { links[i].href = links[i].href.replace(/en\\.wikipedia\\.org\\/wiki\\//i,\"en.wanweibaike.com/wiki-\"); links[i].href = links[i].href.replace(/zh\\.wikipedia\\.org\\/wiki\\//i,\"www.wanweibaike.com/wiki-\"); links[i].href = links[i].href.replace(/zh\\.wikipedia\\.org\\/zh-hk\\//i,\"www.wanweibaike.com/wiki-\"); links[i].href = links[i].href.replace(/zh\\.wikipedia\\.org\\/zh-hans\\//i,\"www.wanweibaike.com/wiki-\"); links[i].removeAttribute(\"onmousedown\") } } git log 单行显示 git log --oneline --format=\"%an %cs %s\" --author=xxxx ffmpeg 剪切 ffmpeg -i test.mp4 -ss 00:10:00 -to 00:20:00 -codec copy cut.mp4 ffmpeg 合并 ffmpeg -f concat -i vfilelist -c copy concat.mp4 ffmpeg 下载视频 ffmpeg -i “https://example.com/test.m3u8\" -c copy test.mp4 ffmpeg -i “https://example.com/test.m3u8\" -c copy -bsf:a a ac_adtstoasc test.mp4 ","date":"2020-06-05","objectID":"/2020/06/05/scripts/:0:0","tags":null,"title":"脚本集合","uri":"/2020/06/05/scripts/"},{"categories":null,"content":"熟悉 Redis 要包含以下知识点： Redis 数据类型及使用场景 单线程模型 两种持久化方式 与 Memcached 的区别 I/O 多路复用 Redis 主从同步 Redis 集群（Sentinel） 性能 QPS 高级一点的是： Redis 各种数据类型的底层实现 Redis 实现分布式锁的两种方式（RedLock/多独立集群） ","date":"2020-05-09","objectID":"/2020/05/09/redis/:0:0","tags":["Redis"],"title":"Redis","uri":"/2020/05/09/redis/"},{"categories":null,"content":"数据类型 字符串 列表 哈希表 集合（无序） 有序集合 Bitmaps（不常用） HyperLogLogs（不常用） 场景 字符串 与 Memcached 用法相似，缓存的基本功能 列表 消息队列 异步操作 哈希表 存储对象属性 集合 获取唯一值的场景，例如访问页面的所有 IP 有序集合 排序的场景。获取前 10 个访问量最高的页面；延迟队列 ","date":"2020-05-09","objectID":"/2020/05/09/redis/:1:0","tags":["Redis"],"title":"Redis","uri":"/2020/05/09/redis/"},{"categories":null,"content":"Redis 和 Memcached 的区别是什么？ Redis： https://github.com/antirez/redis Memcached： https://github.com/memcached/memcached 功能 Redis Memcached 数据结构 字符串(string)、列表(list)、哈希(hash)、集合(set)、有序集合(zset) 只有字符串 持久化 支持 不支持 字符串键最大上限 512MB 250B 字符串值默认上限 512MB 1MB 内存管理器 Jemalloc（默认）、TCMalloc SLAB Allocator 主从同步 支持 不支持 集群 key 所在服务器 服务端计算 客户端计算 线程 单线程（CPU非瓶颈） 多线程（多核/加锁） 单机 QPS 5~6万 几十万 ","date":"2020-05-09","objectID":"/2020/05/09/redis/:2:0","tags":["Redis"],"title":"Redis","uri":"/2020/05/09/redis/"},{"categories":null,"content":"Redis 的底层数据结构 我们可以很容易了解到以下信息： 类型 底层数据结构 字符串 SDS 列表 双向链表 哈希表 哈希表 集合 哈希表 有序集合 跳跃表（Skip Table） 那么问题来了，这只是数据类型的底层数据结构。往上一层看，Redis 如何根据我们给出的 key 找到这些数据类型对应的对象？ 例如： \u003e redis-cli put key value \u003e redis-cli lpush list item Redis 是如何存储这些 key 和 value 的对应关系，以及 list 和 item 的对应关系？ 如果由我们自己设计，肯定会选择哈希表。而 Redis 也确实是这么实现的。 server.h typedef struct redisDb { dict *dict; /* The keyspace for this DB */ dict *expires; /* Timeout of keys with a timeout set */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP)*/ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */ unsigned long expires_cursor; /* Cursor of the active expire cycle. */ list *defrag_later; /* List of key names to attempt to defrag one by one, gradually. */ } redisDb; 这也是为什么对同一个 key 设置不同类型的数据的时候， Redis 会报错： (error) WRONGTYPE Operation against a key holding the wrong kind of value 集合 如果要列举一个集合实现方式的列表，可以直接到 JAVA 去找。 HashSet： 使用 Hash Table 的 key 存储元素 LinkedHashSet： HashSet + LinkedList TreeSet： 红黑树 Redis 采用 HashSet 的方式实现集合， Hash Table 的 value 固定为 null。 有序集合 使用跳跃表（skip tables）实现。比红黑树实现简单。Redis 的跳跃表的时间复杂度为 O(log(N)) 。 说到跳跃表，要先说指数单链表。其每 2 的次方位置都添加一个 Level 节点。 跳跃表与指数单链表的不同之处在于 Level 节点数量与指数单链表一致，但位置随机。 ","date":"2020-05-09","objectID":"/2020/05/09/redis/:3:0","tags":["Redis"],"title":"Redis","uri":"/2020/05/09/redis/"},{"categories":null,"content":"Remote Procedure Call Protocol （RPC，远程过程调用），与进程内本地函数调用区分，只跨进程的函数调用。 调用方将函数和入参序列化为字节流，并发送给服务方。服务方将字节流反序列化为函数和参数，并调用函数。将结果序列化为字节流，返回给调用方。调用方将结果的字节流反序列化，得到结果。 RPC 框架封装了这个过程。并且提供更多的功能。 RPC 客户端的职责有： 序列化/反序列化 连接池管理 负载均衡 故障转移 队列管理 超时管理 异步管理 ","date":"2020-05-09","objectID":"/2020/05/09/rpc/:0:0","tags":["RPC"],"title":"RPC","uri":"/2020/05/09/rpc/"},{"categories":null,"content":"部署方式主要有三种： 滚动部署 蓝绿部署 金丝雀部署 ","date":"2020-05-09","objectID":"/2020/05/09/deployment/:0:0","tags":["项目部署","CI/CD"],"title":"项目部署方式","uri":"/2020/05/09/deployment/"},{"categories":null,"content":"滚动部署 逐台停止服务并部署，新旧并存。 出现问题时，回滚已部署的机器。 需要注意： 回滚耗时 新旧 API 会同时服务，新 API 需要兼容旧 API ","date":"2020-05-09","objectID":"/2020/05/09/deployment/:1:0","tags":["项目部署","CI/CD"],"title":"项目部署方式","uri":"/2020/05/09/deployment/"},{"categories":null,"content":"蓝绿部署 又称为 A/B 部署、红黑部署。 两个集群，单个集群提供服务。 部署备份集群，然后流量切到备份集群。如果没问题，再升级主。 出现问题时，把流量切回主集群。 需要注意： 等待未完成的事务结束才升级主 数据库迁移（表结构修改）的回滚 ","date":"2020-05-09","objectID":"/2020/05/09/deployment/:2:0","tags":["项目部署","CI/CD"],"title":"项目部署方式","uri":"/2020/05/09/deployment/"},{"categories":null,"content":"金丝雀部署 又叫灰度发布。 A/B Test 是灰度发布的一种方式。 让部分用户使用新版本，如果这部分用户没有什么反对意见，则逐步扩大范围。 自动化要求高。 ","date":"2020-05-09","objectID":"/2020/05/09/deployment/:3:0","tags":["项目部署","CI/CD"],"title":"项目部署方式","uri":"/2020/05/09/deployment/"},{"categories":null,"content":"整体结构： +---------+ | +----------+ | Source | | | +\u003c------+ | +--+--+---+ | | Back ^ | | | To | | | | The | | | | Source | v | | +--+--+---+ | | | | | | | Static | | | | Relay | | | | Cluster | | | | | | | +--+--+---+ | | ... ^ | | | One | | | | Or | | | | More | v | v +--+--+---+ +--+--+---+ | | | | | Static | | Dynamic | | Relay | | Relay | | Cluster | | Cluster | | | | | +--+--+---+ +--+--+---+ ^ | ^ | | | | | | v | v +--+--+-----------+--+---+ | | | Edge Cluster | | | +--+--+-----------+--+---+ ^ | ^ | Request | | Request | | Static | | Dynamic | | Source | v Source | v +--+--+-----------+--+---+ | | | User Agent | | | +------------------------+ 类似于 CPU 的多级缓存。把 Edge Cluster 当成一级缓存，把 Source 当成内存。寄存器呢？ User Agent 自身的缓存（如浏览器缓存）在这个架构上相当于寄存器。 Static Relay Cluster 的磁盘容量比 Edge Cluster 大。就像 CPU 的 L2 缓存比 L1 缓存大。 Dynamic Relay Cluster 用于 User Agent 和 Source 运营商不一致时，在机器内做转换，提高速度和减少运营商转换的费用。该 Cluster 里的每台机器都有两个或者多个运营商的 IP。 源服务器是 CDN 客户自己搭建的，也可能是客户使用 CDN 服务商的云服务器（主要用于静态资源存储）。 预热（主动缓存）：在资源提供给用户使用之前，先自己请求一次资源，让这些数据预先加载到 Edge Cluster 。 集群架构： xxxx.com(VIP) + +---------------------+ v +------+---------------------------------------+ | Layer 4 Load Balance | | VIP | | +--------+ +------------------\u003e +--------+ | | | | | | | | | LVS | Heartbeat/Keepalived | LVS | | | | Master | | Backup | | | | | \u003c------------------+ | | | | +--------+ +--------+ | | | +----+------------+-------------+-----------+--+ | | | | | | | | | | | | v v v v +------+------------+-------------+-----------+-----+ | Layer 7 Load Balance | | | | | | +---------+ +---------+ +---------+ | | | | | | | | | | | HAProxy | | HAProxy | ...... | HAProxy | | | | | | | | | | | +---------+ +---------+ +---------+ | | | +------+------------+-------------+-----------+-----+ | | | | | | | | | | | | v v v v +------+------------+-------------+-----------+-----+ | | | | | +---------+ +---------+ +---------+ | | | | | | | | | | | Real | | Real | | Real | | | | Server | | Server | ...... | Server | | | | | | | | | | | +---------+ +---------+ +---------+ | | | +---------------------------------------------------+ LVS 可以使用 ospfd 让每一台 LVS 服务器都提供服务，充分利用资源。此时 LVS 的机器数量上限仅受三层交换机设备的影响。 ","date":"2020-05-07","objectID":"/2020/05/07/cdn/:0:0","tags":["CDN"],"title":"CDN","uri":"/2020/05/07/cdn/"},{"categories":null,"content":"用户访问过程 可以通过 dig 查看相关信息。 [root@hostname]# dig example.com ; \u003c\u003c\u003e\u003e DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.2 \u003c\u003c\u003e\u003e example.com ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 41101 ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 512 ;; QUESTION SECTION: ;example.com. IN A ;; ANSWER SECTION: example.com. 599 IN CNAME example.com.xxcdn.com. example.com.xxcdn.com. 59 IN A xxx.xx.xxx.xxx example.com.xxcdn.com. 59 IN A xxx.xx.xxx.xx ;; Query time: 203 msec ;; SERVER: xxx.xxx.xxx.xxx#53(xxx.xxx.xxx.xxx) ;; WHEN: Fri May 08 14:04:47 UTC 2020 ;; MSG SIZE rcvd: 101 使用 CDN 的域名会将其主域名在 dns 上配置 CNAME，指向 CDN 厂商的域名，该域名称为一级域名。 一级域名会指向 CDN 厂商的 DNS 服务器。DNS 服务器会根据用户购买的服务（套餐），将该域名 CNAME 到另一个域名（二级域名，Map）。即根据套餐配置一个对应的域名。 这样在客户变更套餐时，可以创建另一个域名，分配好资源，然后 DNS 服务器对一级域名的解析切换到到这个新域名。 获取二级域名后，会根据情况找到发起请求的用户所在的资源区域（因为有些 DNS 服务器不支持获取请求地的信息），称为 view。每个 view 都会配置多个用于提供服务的机器集群（覆盖）。 资源区域所包括的集群未必是同一个地理区域的集群。 例如福建的资源区域可能包含浙江地区的集群。一般会选择地理区域近的集群。 每个集群都有一个虚 IP，用于高可用以及屏蔽集群的架构信息。 例如上面解析到的 xxx.xx.xxx.xxx 和 xxx.xx.xxx.xx 都是虚 IP，代表着两个集群。 由于二级域名会根据情况选取不同资源区域的集群提供服务，因此在不同地区 dig 出来的结果不一致。 每个 view 拥有多个 DNS Host Cluster（DHC），每个二级域名会唯一对应一个资源区域里的 DHC。二级域名会使用一个 DHC 里的部分集群。 不同客户的二级域名可能指向同一个集群。机器资源的利用率。 ","date":"2020-05-07","objectID":"/2020/05/07/cdn/:1:0","tags":["CDN"],"title":"CDN","uri":"/2020/05/07/cdn/"},{"categories":null,"content":"服务角度 应用服务是相同服务特性的跨节点的多个集群的集合。这些集群的机器拥有相近或相同的硬件配置。 应用服务组是由边缘集群、静态中转集群、动态中转集群的应用服务组成的集合。 应用服务和应用服务组是多对多的关系。 应用服务的粒度太小，配置麻烦；应用服务组的粒度太大，配置失误的影响很大。所以在两者之间加了一个 DHC 的逻辑概念。 DHC 一般按区域划分，如华南地区某个应用服务组底下的集群划分为一个 DHC。虽然该应用服务在华北地区也可能有集群。 一个 DHC 有多个列表，每个列表代表不同区块。例如南非、中国大陆… 同一个列表里面通常表示一个区块的运营商在各个更小粒度地区的集群。例如 DHC 某个列表包含了中国大陆的某些省份的集群。通常一个省份对应一个集群。 ","date":"2020-05-07","objectID":"/2020/05/07/cdn/:2:0","tags":["CDN"],"title":"CDN","uri":"/2020/05/07/cdn/"},{"categories":null,"content":"参考 L1，L2，L3 Cache究竟在哪里？ https://zhuanlan.zhihu.com/p/31422201 ","date":"2020-05-07","objectID":"/2020/05/07/cdn/:3:0","tags":["CDN"],"title":"CDN","uri":"/2020/05/07/cdn/"},{"categories":null,"content":"epoll 是其中一种方式。 Redis、Nginx 内置。 仅用于 linux2.6。 ","date":"2020-05-07","objectID":"/2020/05/07/epoll/:0:0","tags":["optimizing"],"title":"I/O Multiplexing（I/O 多路复用）","uri":"/2020/05/07/epoll/"},{"categories":null,"content":"后端优化分为三个方向 组件配置调优，偏运维 架构调优，偏架构 代码层面的调优，偏开发 ","date":"2020-05-07","objectID":"/2020/05/07/backend-optimizing/:0:0","tags":["optimizing"],"title":"后端的优化","uri":"/2020/05/07/backend-optimizing/"},{"categories":null,"content":"配置调优 以 Nginx、PHP、MySQL 为例。 LNMP中web高并发优化配置以及配置详解 https://phpartisan.cn/news/55.html Nginx 从简单粗暴的角度，就是提高连接数。 增加进程数，每个 CPU 配置一个进程。 进程数配置项： worker_processes CPU 配置项： worker_cpu_affinity ，该选项使得 Nginx 每个进程都执行在不同 CPU 提高单进程允许的最多连接数。 配置项： worker_connections 理论上一台机器的最大连接数 = worker_processes * worker_connections PHP-FPM 总体思想是控制进程数。 选项：pm static 固定进程数。如果是 PHP 专用服务器，则可以将其设置为固定，并给定一个比较大的值。 dynamic （默认） 根据以下几个因素变化： 启动时进程数 最大进程数 至少有多少个空闲进程，少了就创建新空闲进程 至多有多少个空闲进程，多了就销毁空闲进程 每个 PHP-FPM 进程大致占用 20 MB 的内存，用内存除以 20 MB 就是极限数量。但是要注意，如果设置极限数量，在有其他应用占用较大内存时，会导致服务异常。 PHP 去掉没有用到的扩展。 启用 OPCache 扩展。 在某一台机器上启用 tideways 扩展，分析具体函数的耗时。 MySQL（InnoDB） MySQL 的内存缓存大小对于性能的影响较大。 MySQL 的缓存分为两部分： 索引 行记录 配置项是： innodb_buffer_pool_size 这也是索引不能加太多的原因。索引加太多会导致索引占用更多的缓存，进而使得行记录的缓存减少。 索引的更多优化： 索引不要加到重复数据多的列上。 索引有一个参数 Cardinality，用于评估索引中唯一值的数目的估值。如果该值和表行数的比值小于一定程度，则不会使用索引。 字段太长应使用部分索引。 使用短 ID 作为主键。因为辅助索引的叶子节点存储的是主键，如果主键太大，会使得辅助索引也变大。因此通常使用自增 ID 而不是 UUID 作为主键。 必要情况下创建联合索引。多条件情况下，单表只会命中其中一个单列索引。 ","date":"2020-05-07","objectID":"/2020/05/07/backend-optimizing/:1:0","tags":["optimizing"],"title":"后端的优化","uri":"/2020/05/07/backend-optimizing/"},{"categories":null,"content":"架构调优 瓶颈主要在数据库。 Nginx 使用双 Nginx 服务器（或者更多），用上 Keepalived + VIPA 组合确保高可用。 可以设置多个 VIPA ，分布到不同机器上，这些机器互为主备。接着让域名同时解析到这些 VIPA。这样可以充分利用多台 Nginx 服务器，并且保证高可用。 MySQL（InnoDB） 从读性能和写性能两方面入手。 提高读性能： 添加从机（冗余数据），读写分离。读取数据时，从不同的从机读取。 一般一主三从，两从用于提供服务，一从用于后台访问。 后台访问的服务如果是大数据服务，则可为这台机器设置更多索引来提升读性能。但会给运维带来维护的麻烦，所以慎用。通常来说保持与其他服务器相同的配置。 水平切分。将表中的旧数据转存到同库其他表或者其他库。 可以优先考虑分库。因为磁盘满的时候，还是要把表迁移到其他库。 垂直切分。将表中不常用的和长度较大的字段拆到另一张表。 冷热分离。如果只有近三个月的数据访问量大，则将近三个月的数据尽量放到固态硬盘。将三个月之前的数据放到机械硬盘。 索引外置。把数据冗余一份到 Elastic Search 里面。 外部缓存。业务数据缓存到 Redis 里面。Cache Aside Pattern。 注：所有数据冗余都会带来数据一致性的问题。 两种一致性问题： 主从不一致 业务允许时无视不一致 强制读主。从库读不到时再去主库读一次。 选择性读主（Redis）。数据更新通知 Redis，毫秒级缓存，查询前先看更新的数据是否在 Redis 里面，有则读主。 缓存不一致（Redis） 发生在写后立即读。缓存了旧数据。 通过 binlog 了解主从同步进度，同步完删除缓存。 提高写性能： 多主多写 要解决 ID 冲突的问题。两种方式： 设置不同起始 ID ，提高自增 ID 步长（会导致数据库配置不一致） 客户端生成 ID。生成 ID 的方式可以参考分布式 ID 的几种生成方式。 分库： 单 key 场景：用户表查询比登录多 其他字段如果要加速，则专门做一个单字段到 UID 的映射表（可放入缓存加速） 1 对多 场景：用户查订单比订单查用户多 用户订单。订单 ID 携带用户 ID 的信息。让同一个用户的订单落在同一个库。 多对多 场景：关注与粉丝。 创建两个库，分别用其中一个字段作为分库依据。 多 key 场景：买家比卖家查订单多，查订单比查用户多 忽略最少的部分，退化为 1 对多。 架构不能为 1% 的性能而带来 20% 甚至更高的复杂性。 服务 无状态化，可根据需要横向扩容。 用 JWT（Json Web Token）验证身份。 文件存储放分布式文件存储上面，如 MinIO。 ","date":"2020-05-07","objectID":"/2020/05/07/backend-optimizing/:2:0","tags":["optimizing"],"title":"后端的优化","uri":"/2020/05/07/backend-optimizing/"},{"categories":null,"content":"代码层面 分为： 减少连接次数 多线程/多进程 缓存 异步处理 数据库 减少连接次数 例如项目中有一个模块，要传输脚本到目标机器上执行。分为两步： 传输脚本 执行 要建立两次连接。 优化方式：将脚本 base64_encode，然后把执行命令拼接在后面。 echo \"base64_encoded string\" | base64 -d -i \u003e /usr/local/src/xxx.sh; bash xxx.sh \"param0\"; 多线程/多进程 碰到有多个耗时任务，为每个任务创建一个新的线程或者进程执行。 缓存 分为应用内缓存和外置缓存。 应用内缓存有些场景需要自己维护多台机器之间的缓存信息，根据情况使用。 外置缓存（如 Redis/Memcached）。 将请求外部接口的数据缓存到 Redis，减少接口调用的耗时。 异步任务 将业务中最重要的部分执行完后，尽快返回给前端。剩余的任务丢入队列，后台处理。 MySQL（InnoDB） 总体思想是尽可能减少数据量，尽可能早结束查询，尽可能命中索引，尽可能减小锁的粒度。 在执行语句前，先用 Explain 查看执行计划，尽量命中索引，避免全表扫描。 尽量避免使用 select *，需要多少字段拿多少字段 非唯一索引尽量使用 limit 使用索引来代理 limit 处理分页 limit 会扫描前面不要的数据，然后逐一抛弃。在 Where 里面指定 ID 范围会更快。 业务层提供上一页和下一页的操作，避免用户一次跳多页。URL 要使用 after_xxx ，避免用户直接修改 page。例如 GitHub 的 release 列表界面。 用 Union 替代 OR 注：MySQL 的优化器会尝试使用索引合并来自动优化 OR。 当数据集不会重复时，用 Union All 替代 Union 联合索引最左匹配原则 联合索引在范围查询的字段后就不会再走索引了 删除由最左匹配原则覆盖的索引 使用 like 时，避免把 % 放前面 放在前面不走索引。 使用 Where 加更精确的条件限制来减少传输的数据量 以前见过判断用户登录用户名密码的时候，把整个用户表查出来再逐一判断的代码。 避免对索引列使用 MySQL 内置函数。 优先使用 Inner Join 而不是其他 Join。 如果使用 Left Join 或者 Right Join，驱动表数据量尽可能小。 避免在索引列上使用不等号。如果索引能用范围扫描，则使用范围操作符。 例如 a != 1，转化为 a \u003c 1 AND a \u003e 1。 大量数据使用批量分块插入数据 其中一个影响因素是锁。一个事务插入已知数量的多条数据，只需获取一次锁。 使用覆盖索引 使用索引就能获取想要的值，不需要从数据表中读。 用于辅助索引。 因为索引的执行顺序是： 用辅助索引找到主键 通过主键索引找到数据 如果 select 的值只包括辅助索引和主键，则使用覆盖索引。 尽量不要在 select 字段多的时候使用 Distinct 批量删除数据要谨慎 分批操作。 如果全部数据删除，且不需要恢复，则使用 truncate 。 如果不是全部删除，则把保留的数据插入到新表，再整个删除旧表。 批量删除会加锁 批量删除过程要写 undo 日志，一旦回滚，需要更多时间 避免数据类型隐式转换 隐式转换会使索引失效 ","date":"2020-05-07","objectID":"/2020/05/07/backend-optimizing/:3:0","tags":["optimizing"],"title":"后端的优化","uri":"/2020/05/07/backend-optimizing/"},{"categories":null,"content":"为了避免服务单点，也为了负载均衡，我们会加一层 Nginx 层。这个 Nginx 层要有多于一台机器，不然它自身也成为一个单点。 最初加 Nginx 层会变成这样： schaepher.com + | +-------+ | v +---+---+ +-------+ | | | | | Nginx | | Nginx | | | | | +--+----+ +---+---+ | | +---------+---+-------------+-+-----------+ | | | | v v v v +---+----+ +---+----+ +---+----+ +---+----+ | | | | | | | | | Real | | Real | | Real | | Real | | Server | | Server | | Server | | Server | | | | | | | | | +--------+ +--------+ +--------+ +--------+ 如果有一台 Real Server 发生故障，则 Nginx 就不会转发到故障的机器，保证服务正常进行。 但是如果主 Nginx 故障了呢？ ","date":"2020-05-07","objectID":"/2020/05/07/keepalived-vip/:0:0","tags":["Keepalived","VIPA","高可用"],"title":"高可用的实现（Keepalived + 虚 IP）","uri":"/2020/05/07/keepalived-vip/"},{"categories":null,"content":"故障切换的五种方式 客户端自己配置多个 Nginx IP，故障时自己切换。 优点： 后端不需要做调整 缺点： 客户端要自己维护 IP 列表。 客户端要实现一套应对故障的逻辑。 不能用于用户和服务之间，只能用于服务与服务之间。 单一服务的情况下，把另一台 Nginx 服务器的 IP 发给用户，让用户访问这个 IP。 优点： 简单 缺点： 只有内部系统用户才可能接受这种做法。面向外部用户的时候，外部用户不会接受。 运维人员手动修改 DNS 解析，将域名解析到另一台 Nginx 服务器。或者程序定期检测，发现有问题就自动发起修改 DNS 解析的请求。 解决了什么问题？ 用户需要手动修改 hosts 或者需要切换 URL 的问题。 没有解决什么问题？ 在 DNS 解析生效之前，服务完全不可用。 客户端会缓存 DNS 解析，也要等客户端缓存过期。 将域名解析到所有 Nginx 服务器。程序定期做检测，当发现有问题的时候发起请求，让 DNS 将故障的 IP 移除掉。 DNSPod 自带这个功能 解决了什么问题？ 在 DNS 解析生效之前，服务完全不可用的问题。 没有解决什么问题？ 仍然有部分用户无法访问服务。 使用虚 IP（Virtual IP Address，以下称为 VIPA）。域名固定解析到这个 IP，当 VIPA 所在服务器故障时，让 VIPA 自动漂移到另一台服务器。 解决了什么问题？ 不需要修改 DNS 解析，秒级别的生效延迟。 用户无感知。 带来了什么问题？ 多了一个故障点，即使得 VIPA 自动漂移的那个程序，如 Keepalived。 需要额外申请一个 IP 作为 VIPA 。 涉及存储的时候，由于切换速度很快，可能会导致数据不一致。 ","date":"2020-05-07","objectID":"/2020/05/07/keepalived-vip/:1:0","tags":["Keepalived","VIPA","高可用"],"title":"高可用的实现（Keepalived + 虚 IP）","uri":"/2020/05/07/keepalived-vip/"},{"categories":null,"content":"VIP 是什么 全称为 VIPA（Virtual IP Address），虚拟 IP 地址。 通常一个 IP 只能绑定在某台机器的一个网卡上。一旦这台机器故障，根据 IP 找到这台机器的请求就会得不到响应。 正常 schaepher.com 192.168.1.101 + | | | v +---------------+ +---------------+ | | | | | 192.168.1.101 | | 192.168.1.102 | | | | | +---------------+ +---------------+ 故障 schaepher.com 192.168.1.101 + + +-+ 不通 | v 故障 +---------------+ +---------------+ | | | | | 192.168.1.101 | | 192.168.1.102 | | | | | +---------------+ +---------------+ 而 VIPA 虽然最终也会配置在一台机器上，但一旦这台机器故障，备用机器就会把这个 IP 抢过去配置到网卡上（VIPA 漂移到备机）。这样可以在 IP 不变的情况下，让请求转移到正常的机器，减少服务故障时间。 正常 schaepher.com 192.168.1.100(VIPA) + | | | v +---------------+ +---------------+ | 192.168.1.100 | | | | 192.168.1.101 | | 192.168.1.102 | | | | | +---------------+ +---------------+ 故障 schaepher.com 192.168.1.100(VIPA) + | +-------------------------+ | v 故障 +---------------+ +---------------+ | | | 192.168.1.100 | | 192.168.1.101 | | 192.168.1.102 | | | | | +---------------+ +---------------+ 而 Keepalive 则是实现 VIPA 漂移的一种工具。 另一种是比较复杂的 Heartbeat。 ","date":"2020-05-07","objectID":"/2020/05/07/keepalived-vip/:2:0","tags":["Keepalived","VIPA","高可用"],"title":"高可用的实现（Keepalived + 虚 IP）","uri":"/2020/05/07/keepalived-vip/"},{"categories":null,"content":"Keepalived Keepalived 实现 VIPA 漂移的基础是 VRRP 协议。 VRRP 最早用于路由器，将多台路由器设备虚拟成一台路由器。每台设备都有一个角色。角色有两种 Master 和 Backup。Master 会持有虚 IP。每台设备都有一个权重，权重最高的正常设备会被选举为 Master。 现在 Keepalived 实现了 VRRP 协议，使得可以将其用在其他设备上。 将 Keepalived 安装在一组设备上，它们之间会互相通信来检测状态。如果发现 Master 超过一定时间没有反应，则重新选举 Master。 Master 可能直接故障没有反应，也可能只是服务出现问题。如果是后一种情况，需要主动关闭 Keepalived 进程。 以下展示配置文件。 IP 所属 角色 192.168.1.100 Master 192.168.1.101 主机 A Master 192.168.1.102 主机 B Backup 192.168.1.101 - 主机 A - Master /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { script_user root router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict # 严格模式使得无法使用 unicast_peer 配置，好处是无需指定其他机器的 IP vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_http_port { script \"/etc/keepalived/chk_nginx.sh\" # 检测当前机器的服务是否故障，如果故障则关闭 keepalived interval 2 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state MASTER # 主备配置不一致 interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass aaaaaaa # 主备该配置必须一样 } virtual_ipaddress { 192.168.1.100 } track_script { chk_http_port # 在 vrrp_script 定义的名字 } notify_master \"/etc/keepalived/notify.sh 192.168.1.100 master\" # 当这台机器成为 Master 时发送通知 notify_backup \"/etc/keepalived/notify.sh 192.168.1.100 backup\" notify_fault \"/etc/keepalived/notify.sh 192.168.1.100 fault\" } 192.168.1.102 - 主机 B - Backup 只需改两个配置的值 /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { script_user root router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_http_port { script \"/etc/keepalived/chk_nginx.sh\" interval 2 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP # 主备配置不一致 interface eth0 virtual_router_id 51 priority 90 # 备机权重比主机低 advert_int 1 nopreempt # 防止争抢 VIPA，只有 state 设置为 BACKUP 才能使用该选项 authentication { auth_type PASS auth_pass aaaaaaa } virtual_ipaddress { 192.168.1.100 } track_script { chk_http_port } notify_master \"/etc/keepalived/notify.sh 192.168.1.100 master\" # 当这台机器成为 Master 时发送通知 notify_backup \"/etc/keepalived/notify.sh 192.168.1.100 backup\" notify_fault \"/etc/keepalived/notify.sh 192.168.1.100 fault\" } /etc/keepalived/chk_nginx.sh #!/bin/bash if [[ -z \"$(ps aux | grep \"nginx: master process\" | grep -v grep)\" ]]; then systemctl restart nginx # 尝试重启 nginx sleep 5 if [[ -z \"$(ps aux | grep \"nginx: master process\" | grep -v grep)\" ]]; then # 启动失败则关闭 keepalived，触发 VIPA 漂移 systemctl stop keepalived fi fi /etc/keepalived/notify.sh #!/bin/bash # contact=xxx.schaepher.com notify() { vip=$1 role=$2 mailSubject=\"$(hostname) to be ${role}: ${vip} floating\" mailBody=\"$(date '+%F %H:%M:%S'): vrrp transition, $(hostname changed to be ${role})\" # echo ${mailBody} | mail -s \"${mailSubject}\" \"${contact}\" echo ${mailSubject} \u003e\u003e /var/log/keepalived.mail echo ${mailBody} \u003e\u003e /var/log/keepalived.mail echo \"\" \u003e\u003e /var/log/keepalived.mail } notify $1 $2 ","date":"2020-05-07","objectID":"/2020/05/07/keepalived-vip/:3:0","tags":["Keepalived","VIPA","高可用"],"title":"高可用的实现（Keepalived + 虚 IP）","uri":"/2020/05/07/keepalived-vip/"},{"categories":null,"content":"扩展 提高利用率 由于一个 VIPA 只能配置在一台机器上，如果共有两台机器，则浪费了 50% 的资源。 如果要提高资源的利用率，可以再申请一个 VIPA。把备机配置为 Master，把主机配置为 Backup。 VIPA 争抢 由于 nopreempt 只能配置在 BACKUP 上，如果 state 为 MASTER 的机器故障并恢复，则会把 VIPA 抢过去。 整个过程是： 主机 A 故障 VIPA 漂移到主机 B 主机 A 恢复 VIPA 漂移到主机 A 这样就会导致第四步多漂移了一次。而漂移可能会对服务有很短暂的影响。如果希望主机 A 恢复后，仍然让主机 B 持有 VIPA，则要在主机的 Keepalived 启动之前修改配置中的 state，改为 BACKUP。 避免丢包 在 VIPA 漂移到备机之间，短暂的时间内数据包仍然会发送到主机。如果主机能够连上，则可以使用防火墙将数据包转发到备机。然后停止 Keepalived 。 iptables -F iptables -t nat -I PREROUTING -i eth0 -j DNAT --to-destination 192.168.1.102 iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE 数据库的问题 数据库如果使用双主，在 VIPA 切换的时候，数据可能未同步完成，可能会造成自增 ID 冲突。 可以配置 Keepalived 等一段时间后再发送 ARP 请求，以此等待同步完成。 配置项是：vrrp_garp_master_delay 10 表示延迟 10 秒返送。 ","date":"2020-05-07","objectID":"/2020/05/07/keepalived-vip/:4:0","tags":["Keepalived","VIPA","高可用"],"title":"高可用的实现（Keepalived + 虚 IP）","uri":"/2020/05/07/keepalived-vip/"},{"categories":null,"content":"参考 https://www.linuxprobe.com/keepalived-nginx.html ","date":"2020-05-07","objectID":"/2020/05/07/keepalived-vip/:5:0","tags":["Keepalived","VIPA","高可用"],"title":"高可用的实现（Keepalived + 虚 IP）","uri":"/2020/05/07/keepalived-vip/"},{"categories":null,"content":"在实践 RESTful API 设计的时候，会碰到很多需要选择的地方。而这些在 RFC 7230 - 7235 里面没有说明。例如这次要说的 DELETE 方法。 根据 RFC 7231 的说法， DELETE 是幂等的。 幂等（idempotent）是什么？在 RFC 7231 定义幂等方法的时候，是这样说的： A request method is considered “idempotent” if the intended effect on the server of multiple identical requests with that method is the same as the effect for a single such request. —— RFC 7231 如果使用同一种请求方法的多个等效请求对服务器的预期影响，与一个等效请求对服务器的预期影响一样，那么这种请求方法被认为是幂等的。 ","date":"2020-05-04","objectID":"/2020/05/04/http-delete-method/:0:0","tags":["HTTP","HTTP Methods"],"title":"HTTP 两次 DELETE 同一个资源应该返回什么","uri":"/2020/05/04/http-delete-method/"},{"categories":null,"content":"DELETE 幂等带来了什么问题？ 问题是：多次使用 DELETE 方法作用于同一个资源，除了第一个成功对服务端造成影响（删除了资源）的请求，其他请求应该返回什么状态码？ 针对这个问题，有两种观点。一种观点认为应该返回 404 Not Found；另一种观点认为应该返回 2xx 状态码。 第一种观点的理由是： 不能删除一个不存在的资源，因为它已经不存在了。而表示资源不存在的状态码是 404，应该返回 404 Not Found。 第二种观点的理由是： 客户端删除一个资源的意图是使得该资源不存在或者不可用。那么删除一次还是多次，都会让服务端对于该资源处于同一个状态。无论是客户端还是服务端，都认为这个结果是合理的，所以应该返回成功的状态码。 从经验上来看，我赞成第二种观点。 ","date":"2020-05-04","objectID":"/2020/05/04/http-delete-method/:1:0","tags":["HTTP","HTTP Methods"],"title":"HTTP 两次 DELETE 同一个资源应该返回什么","uri":"/2020/05/04/http-delete-method/"},{"categories":null,"content":"返回 4xx 会有什么问题？ 在项目中，我们需要调用其他系统的 API 来操作资源。在某些场景下当多次调用删除同一个资源的 API 时，第一个响应是删除成功，但后续响应都是资源不存在的错误。 这个时候应该认为执行成功还是执行失败？ 如果认为成功，那么应该捕获这个异常并且不做任何处理。既然我不需要做任何处理，为何服务端不返回成功的状态码呢？ 如果认为失败，那么应该捕获异常并且做处理。但是要处理什么呢？“使服务端的该资源不存在” 明明就是预期的结果，却还要做错误处理。这让人很矛盾。 服务端返回 404 暴露了对该请求的处理方式，而客户端并不关心这个信息。客户端只想要该资源不再存在，即使该资源从未存在过。如果执行过后，该资源确实不存在了，就达到了客户端的目的。应该告诉客户端成功的消息。 ","date":"2020-05-04","objectID":"/2020/05/04/http-delete-method/:2:0","tags":["HTTP","HTTP Methods"],"title":"HTTP 两次 DELETE 同一个资源应该返回什么","uri":"/2020/05/04/http-delete-method/"},{"categories":null,"content":"应该返回什么状态码？ 如果操作立即成功，且无需给客户端更多的信息，则返回 204 No Content 如果操作立即成功，且需要给客户端更多的信息，则返回 200 OK 并在响应体里附带信息 如果操作被接受了，但可能需要比较长的时间才能执行结束，则返回 202 Accepted ","date":"2020-05-04","objectID":"/2020/05/04/http-delete-method/:3:0","tags":["HTTP","HTTP Methods"],"title":"HTTP 两次 DELETE 同一个资源应该返回什么","uri":"/2020/05/04/http-delete-method/"},{"categories":null,"content":"多次操作成功是否应该返回不同的成功状态码？ 有这个考虑是因为要识别这个删除成功的状态究竟是否由这一次请求所造成的。 但绝大多数情况下，并不关心这一点。所以返回同一个状态码就行了。 ","date":"2020-05-04","objectID":"/2020/05/04/http-delete-method/:4:0","tags":["HTTP","HTTP Methods"],"title":"HTTP 两次 DELETE 同一个资源应该返回什么","uri":"/2020/05/04/http-delete-method/"},{"categories":null,"content":"参考 PayPal 的 API 风格指南： https://github.com/paypal/api-standards/blob/master/api-style-guide.md#http-method-to-status-code-mapping Is a HTTP DELETE request idempotent? http://leedavis81.github.io/is-a-http-delete-requests-idempotent/ ","date":"2020-05-04","objectID":"/2020/05/04/http-delete-method/:5:0","tags":["HTTP","HTTP Methods"],"title":"HTTP 两次 DELETE 同一个资源应该返回什么","uri":"/2020/05/04/http-delete-method/"},{"categories":null,"content":"英汉翻译 ","date":"2020-05-04","objectID":"/2020/05/04/translation/:0:0","tags":["翻译"],"title":"翻译","uri":"/2020/05/04/translation/"},{"categories":null,"content":"RFC 7230 - 7235 系列 目标 翻译 备注 enclosed representation 封装的表现形式 HTTP 请求体有效载荷中目标资源的表现形式；Body 的内容。 汉英翻译 ","date":"2020-05-04","objectID":"/2020/05/04/translation/:1:0","tags":["翻译"],"title":"翻译","uri":"/2020/05/04/translation/"},{"categories":null,"content":"执行 ALTER 创建索引的时候， MySQL 的处理过程是怎么样的呢？ ","date":"2020-04-30","objectID":"/2020/04/30/mysql-index-creation/:0:0","tags":["MySQL","索引"],"title":"MySQL 创建辅助索引的底层过程","uri":"/2020/04/30/mysql-index-creation/"},{"categories":null,"content":"MySQL 5.5 之前（默认使用 MyISAM 作为存储引擎） 过程： 取旧表 ALTER TABLE 后的结构，以该结构创建临时表 原表加写锁，原表数据导入到临时表 删除原表 临时表重命名为原表的表名 缺点： 原表是大表时，速度很慢 过程中服务不可用 ","date":"2020-04-30","objectID":"/2020/04/30/mysql-index-creation/:1:0","tags":["MySQL","索引"],"title":"MySQL 创建辅助索引的底层过程","uri":"/2020/04/30/mysql-index-creation/"},{"categories":null,"content":"MySQL 5.5 开始（默认使用 InnoDB 作为存储引擎） 过程： 对表加共享锁 扫描表，生成辅助索引 B+Tree 解锁 解决了哪些问题？ 速度慢的问题 索引创建过程中无法读的问题 没有解决哪些问题？ 索引创建过程中无法写的问题 带来了哪些问题？ 无 ","date":"2020-04-30","objectID":"/2020/04/30/mysql-index-creation/:2:0","tags":["MySQL","索引"],"title":"MySQL 创建辅助索引的底层过程","uri":"/2020/04/30/mysql-index-creation/"},{"categories":null,"content":"Online Schema Change（Facebook） 过程： 创建和原表结构一样的新表 确保原表有主键，并且不存在外键和触发器。记录所有索引信息。 创建 deltas 表 在原表中创建触发器，将 INSERT、UPDATE、DELETE 操作记录到 deltas 表 不是把数据迁移过取。类似于 Redis 的 AOF。 开始 OSC 操作的事务 导出原表数据 做法：对数据分片，每次只到处部分数据，减少原表锁定时间 删除新表所有辅助索引 数据导入新表 将记录在 deltas 表的操作重放一次，目标表为新表 对新表重建辅助索引 重放 deltas 表在重建辅助索引过程中新记录的操作 交换新旧表的名字 删除旧表和 deltas 表 解决了哪些问题？ 索引创建过程中无法写的问题 数据导出对资源的占用问题 没有解决哪些问题？ 无 带来了哪些问题？ 原表必须存在主键 原表不能存在外键和触发器 ","date":"2020-04-30","objectID":"/2020/04/30/mysql-index-creation/:3:0","tags":["MySQL","索引"],"title":"MySQL 创建辅助索引的底层过程","uri":"/2020/04/30/mysql-index-creation/"},{"categories":null,"content":"Online DDL （MySQL 5.6） 四种关于锁的选择： NONE 不对原表加锁 SHARE 对原表加共享锁 EXCLUSIVE 对原表加排他锁 DEFAULT 按顺序判断能否使用 NONE，SHARE，EXCLUSIVE NONE 原理（无法过程描述）： INSERT、UPDATE、DELETE 操作不加锁，而是把操作记录写入缓存 创建索引 重放缓存的操作记录 解决了哪些问题？ 索引创建过程中无法写的问题 数据导出对资源的占用问题 对原表的要求问题 没有解决哪些问题？ 无 带来了哪些问题？ 需要控制 innodb_online_alter_log_max_size ，其默认为 128MB。超过限制导致写事务报错。 ","date":"2020-04-30","objectID":"/2020/04/30/mysql-index-creation/:4:0","tags":["MySQL","索引"],"title":"MySQL 创建辅助索引的底层过程","uri":"/2020/04/30/mysql-index-creation/"},{"categories":null,"content":" array_merge() 、array_combine() 和 + 处理 key 冲突的区别 操作 数字 key 字符串 key array_merge 自增 覆盖已存在值 + 使用已存在值 使用已存在值 array_combine 覆盖已存在值 覆盖已存在值 通常使用 array_merge 比 + 多。+ 的使用场景例如以数字 ID 为 key 做深度遍历的时候，由于数组会加上子节点。如果用 array_merge，则会使 key 重制，并从 0 递增。 函数式编程相关函数： array_map()、 array_filter()、 array_reduce() 只有 array_map 把匿名函数放在第一个参数位置 如果使用 Laravel 的 Collection，其 map 实现会传入 key，导致不能直接使用单参数的 trim 函数。 list() 如果使用数组，则 PHP 5 和 PHP 7 的顺序不同 $a = 1; $b = 2; $arr = []; list($arr[0], $arr[1]) = [$a, $b]; // $arr == [ 2, 1 ]; // PHP 5 // $arr == [ 1, 2 ]; // PHP 7 关联数组元素为空的情况下， json_encode() 会将其解释为 []，而不是 {} 三种解决方法： 使用 JSON_FORCE_OBJECT 选项 不使用关联数组，而是对象 [ 'a' =\u003e new \\stdClass() ] 对有可能为空的关联数组都加上强制转换 (object)$arr Json 包含过大的整数时，json_decode() 会将其转换为浮点数 解决方法： 使用 JSON_BIGINT_AS_STRING 善用 filter_var() 函数 可用于做以下判断： 是否为整数 是否为浮点数 是否为有效的 IP 是否为有效的 IPv4 是否为有效的 IPv6 是否为内网 IP 是否为有效的 Email 地址 魔术方法 __call() 和 __callStatic() 仅在目标方法不可访问时调用 例如方法不为 public 或者方法不存在 魔术方法 __get() 、 __set() 、 __isset() 、 __unset() 仅在目标属性不可访问时调用 例如属性不为 public 或者属性不存在 数组在使用等于号比较的时候，会用 (key, value) 一起验证。 这意味着索引数组如果顺序不一致，则不相等。 也意味着关联数组即使顺序不同，只要 (key, value) 对的上就相等。 strpos 在第二个参数 needle 非字符串的时候，会将其转换为十六进制字符串 例如 strpos(\"1\", 1) 结果为 false，而 strpos(\"\\x1\", 1) 结果为 0 关联数组中的整数字符串在会被当成数字处理 例如： foreach (['0' =\u003e 'a'] as $key =\u003e $value) { var_dump(is_int($key)); // true var_dump(is_string($key)); // false } 结合 strpos 的特点，要注意。 isset($arr['a']) 比 array_key_exists('a', $arr) 快 isset() 判断变量是否存在，或者是否为 NULL， empty() 判断更多 变量和数组以值传递，对象以引用传递 如果要让方法返回的数组是用以用传递，则需要在方法名前面加上 \u0026 array_filter() 会保留索引数组的索引，导致其变成关联数组 在 json_encode 的时候，会变成对象。因此要先用 array_values() 处理 array_reduce() 匿名函数第一个参数是结果值，第二个是遍历到的值 __sleep() 和 __wakeup() 用于 serialize() 和 unserialize() __sleep() 返回 public 字段名数组。__wakeup() 是在反序列化后做初始化。 __wakeup() 之前会先设置字段的值，并且不调用构造函数。 Serializable 接口需要实现 serialize() 方法和 unserialize($serialized) 方法。 serialize() 方法获取值（把私有变量放到数组），然后返回调用 serialize() 的结果。 unserialize($serialized) 方法需要先将 $serialized 反序列化，然后手动恢复属性值。 __clone() 用于 clone 后修改对象。 由于属性值是对象时， clone 会复制引用（浅拷贝），所以要在 __clone() 里修改属性，再次执行 clone 。以此达到深拷贝的效果。 ","date":"2020-04-29","objectID":"/2020/04/29/php-detail/:0:0","tags":["PHP"],"title":"PHP 语言使用细节","uri":"/2020/04/29/php-detail/"},{"categories":null,"content":"其他 十个 PHP 开发者最容易犯的错误 ","date":"2020-04-29","objectID":"/2020/04/29/php-detail/:1:0","tags":["PHP"],"title":"PHP 语言使用细节","uri":"/2020/04/29/php-detail/"},{"categories":null,"content":"孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 解决僵尸进程的两种方法： 子进程向父进程发送 SIGCHILD 信号。 fork 两次，将子进程转为孤儿进程。A -\u003e B -\u003e C 的情况下，让 B 先于 C 退出。此时由 init 进程接管孤儿进程。 https://www.cnblogs.com/Anker/p/3271773.html ","date":"2020-04-29","objectID":"/2020/04/29/process/:0:0","tags":["进程"],"title":"孤儿进程和僵尸（僵死）进程","uri":"/2020/04/29/process/"},{"categories":null,"content":"原文： MySQL 自增主键 https://schaepher.github.io/2020/04/28/mysql-auto-inc/ 以下仅考虑 InnoDB 存储引擎。 自增主键有两个性质需要考虑： 单调性 每次插入一条数据，其 ID 都是比上一条插入的数据的 ID 大，就算上一条数据被删除。 连续性 插入成功时，其数据的 ID 和前一次插入成功时数据的 ID 相邻。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:0:0","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"自增主键的单调性 为何会有单调性的问题？ 这主要跟自增主键最大值的获取方式，以及存放位置有关系。 如果最大值是通过计算获取的，并且在某些情况下需要重新获取时，会因为最新的数据被删除而减小。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:1:0","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"自增主键最大值怎么取的？存放到哪里？ MySQL 5.7 及之前的版本，自增主键最大值会在启动（重启）后从数据库中取出放到内存： SELECT MAX(ai_col) FROM table_name FOR UPDATE; 这样获取是通过计算的，并且由于存放在内存而容易丢失。 如果删除最新一条数据（假设 ID 为 10），因故障或者其他必要重启后再插入一条数据时会使用之前的 ID （即 ID 为 10）。 问题在于如果有其他表依赖了该 ID，则其他表的数据关联到的数据就符合要求了。除非设置了外键。 比如我要向最大一个 ID 的账号充了 100 万。但是在充值之前，该账号被删除，然后服务器故障重启，重启后有人新注册了一个账号。结果我的 100 万充到了他的新账号上。注册新账号的人以为是新手福利，笑嘻嘻。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:1:1","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"如何解决单调性的问题？ 从 MySQL 8.0 开始，自增主键最大值会在每次修改后写入到 redo log，并且在每个检查点写入引擎私有的系统表。 如果是正常重启，则读取系统表里的值。 如果是故障重启，则先读取系统表里的值放到内存。接着扫描 redo log 里存储的值。如果扫描到的值大于内存的值，则将该值覆盖到内存。 但由于数据库可能在 redo log 刷入磁盘前就故障了，所以可能会用到之前申请的 ID。 注：如果 redo log 都没刷入，就更不用说将数据插入数据表了。 InnoDB AUTO_INCREMENT Counter Initialization https://dev.mysql.com/doc/refman/8.0/en/innodb-auto-increment-handling.html#innodb-auto-increment-initialization ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:1:2","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"自增主键插入时的连续性 这里不考虑由于删除导致的连续性问题 为何会有连续性问题？ 这主要是跟插入事务回滚有关系。 对于两个插入事务，事务 A 先执行插入语句，之后事务 B 执行插入语句。在这之后，事务 A 回滚，导致 A 执行插入语句时占用的 ID 被抛弃。 之所以事务 A 没提交的情况下，事务 B 就能执行插入语句，跟 InnoDB 的自增长锁（AUTO-INC Locking）相关。该锁是一种特殊的表锁（table-level lock），但会在插入语句执行后立即释放，不会等到事务结束。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:2:0","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"如何解决连续性问题？ 使用最高隔离级别 SERIALIZABLE （串行）。 由于性能上的考虑，通常不这样做。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:2:1","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"多事务批量插入的连续性 事务 A 和事务 B 都在执行 不确定数量 的批量插入（INSERT … SELECT）： 保证事务 A 的数据的 ID 连续： innodb_autoinc_lock_mode = 0 （AUTO-INC Locking） 必须等待语句执行结束才释放锁。 保证事务 A 的数据的 ID 连续： innodb_autoinc_lock_mode = 1 （AUTO-INC Locking） 和上面的区别在于，当执行 确定数量 的批量插入时，使用轻量级互斥量（mutex）而不是特殊表锁（AUTO-INC Locking），从而提前向内存的计数器申请相应数量的 ID。之后立即释放，不用等语句执行结束。 会因为回滚而使得全局 ID 不连续。 不保证事务 A 的数据的 ID 连续： innodb_autoinc_lock_mode = 2 （mutex） 三种插入定义： 简单插入 能够提前知道插入的行数 批量插入 不能提前知道插入的行数 混合插入 批量插入中的一部分的 ID 是指定的（非 0 且非 NULL），另一部分未指定，使用数据库生成的自增 ID。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:3:0","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"其他 如果主动指定 ID 为 0 或者 NULL 插入，则会使用数据库生成的自增 ID。 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:4:0","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"参考文档 为什么 MySQL 的自增主键不单调也不连续 https://database.51cto.com/art/202004/614923.htm 《MySQL技术内幕——InnoDB存储引擎》 第 6 章：锁 ","date":"2020-04-28","objectID":"/2020/04/28/mysql-auto-inc/:5:0","tags":["MySQL","索引","主键"],"title":"MySQL 自增主键","uri":"/2020/04/28/mysql-auto-inc/"},{"categories":null,"content":"面向对象设计需要尽可能地符合高内聚与低耦合。它们的视角分为模块内部（内聚）和模块之间（耦合）。 模块内部各元素应紧密结合，一个模块只做一件事。 模块之间应该尽量独立，不直接产生依赖。 面向对象的七大原则可以按照两个视角划分为： 高内聚 单一职责原则 开闭原则 低耦合 里氏替换 依赖倒置 接口隔离 迪米特法则 组合聚合原则 ","date":"2020-04-26","objectID":"/2020/04/26/high-cohesion-low-coupling/:0:0","tags":["系统设计"],"title":"高内聚与低耦合","uri":"/2020/04/26/high-cohesion-low-coupling/"},{"categories":null,"content":"可以先按照较为理想情况学习。学完后在看待握手和挥手的问题时，要假设每一个消息都可能丢失。 另外握手要特别注意服务端为连接分配资源的问题。 ","date":"2020-04-25","objectID":"/2020/04/25/http-connect-and-disconnect/:0:0","tags":["HTTP"],"title":"HTTP 三次握手与四次挥手","uri":"/2020/04/25/http-connect-and-disconnect/"},{"categories":null,"content":"三次握手 客户端：发起连接请求 服务端：分配资源，将其加入半连接队列，响应 ACK 客户端：分配资源，响应 ACK。可附带数据。 SYN 攻击（DDoS 攻击的一种）所用的客户端只执行第一步，不执行第三步。使得流程只执行到第二步。 如果半连接队列满了，新来的 SYN 请求会被丢弃。 第三步的另一个作用： 由于服务端很晚才收到客户端的连接请求，所以客户端发现第一步的请求超时，会再次发送。如果没有第三步，先前发送的连接请求有可能在连接关闭后才到达服务端，此时服务端分配资源。但由于服务端发送 ACK 后客户端没有执行第三步（因为客户端确实不想要再连接了），服务端在等待超时后释放资源。 ","date":"2020-04-25","objectID":"/2020/04/25/http-connect-and-disconnect/:1:0","tags":["HTTP"],"title":"HTTP 三次握手与四次挥手","uri":"/2020/04/25/http-connect-and-disconnect/"},{"categories":null,"content":"四次挥手 客户端：发送关闭请求，表示无更多请求。 服务端：收到客户端关闭请求。服务端继续返回数据。 服务端：发送关闭请求，表示无更多数据可以返回。 客户端：收到服务端关闭请求，发送确认。等待 2MSL 后关闭。 这个表示无更多请求的行为，可以联想到 Golang 中关闭 Channel。关闭 Channel 只表示不会继续往 Channel 里面存放数据，但仍然可以被消费，直到 Channel 为空。 ","date":"2020-04-25","objectID":"/2020/04/25/http-connect-and-disconnect/:2:0","tags":["HTTP"],"title":"HTTP 三次握手与四次挥手","uri":"/2020/04/25/http-connect-and-disconnect/"},{"categories":null,"content":"为什么等 2MSL MSL（Maximum Segment Lifetime） 因为如果服务端没有收到客户端的确认，会重传关闭请求。客户端会在 2MSL 之内收到服务端的重传。 如果超过了 2MSL ，则客户端默认服务端已收到了 LAST-ACK。 客户端每次收到服务端重传并发送 LAST-ACK 后，会重制计时。 +---------+ ---------\\ active OPEN | CLOSED | \\ ----------- +---------+\u003c---------\\ \\ create TCB | ^ \\ \\ snd SYN passive OPEN | | CLOSE \\ \\ ------------ | | ---------- \\ \\ create TCB | | delete TCB \\ \\ V | \\ \\ +---------+ CLOSE | \\ | LISTEN | ---------- | | +---------+ delete TCB | | rcv SYN | | SEND | | ----------- | | ------- | V +---------+ snd SYN,ACK / \\ snd SYN +---------+ | |\u003c----------------- ------------------\u003e| | | SYN | rcv SYN | SYN | | RCVD |\u003c-----------------------------------------------| SENT | | | snd ACK | | | |------------------ -------------------| | +---------+ rcv ACK of SYN \\ / rcv SYN,ACK +---------+ | -------------- | | ----------- | x | | snd ACK | V V | CLOSE +---------+ | ------- | ESTAB | | snd FIN +---------+ | CLOSE | | rcv FIN V ------- | | ------- +---------+ snd FIN / \\ snd ACK +---------+ | FIN |\u003c----------------- ------------------\u003e| CLOSE | | WAIT-1 |------------------ | WAIT | +---------+ rcv FIN \\ +---------+ | rcv ACK of FIN ------- | CLOSE | | -------------- snd ACK | ------- | V x V snd FIN V +---------+ +---------+ +---------+ |FINWAIT-2| | CLOSING | | LAST-ACK| +---------+ +---------+ +---------+ | rcv ACK of FIN | rcv ACK of FIN | | rcv FIN -------------- | Timeout=2MSL -------------- | | ------- x V ------------ x V \\ snd ACK +---------+delete TCB +---------+ ------------------------\u003e|TIME WAIT|------------------\u003e| CLOSED | +---------+ +---------+ https://tools.ietf.org/html/rfc793 ","date":"2020-04-25","objectID":"/2020/04/25/http-connect-and-disconnect/:2:1","tags":["HTTP"],"title":"HTTP 三次握手与四次挥手","uri":"/2020/04/25/http-connect-and-disconnect/"},{"categories":null,"content":"如果服务端一直重传，在客户端 2MSL 后也没有收到 LAST-ACK ，会怎么样？ 服务端会在发送 FIN 后，碰到以下三种情况的任意一种时关闭连接： 收到客户端的 LAST-ACK 正常情况。客户端收到 FIN 后发送 收到客户端的 RST 客户端发送 LAST-ACK 没有被服务端收到，并且 2MSL 内没有收到服务端的 FIN，关闭了连接。 2MSL 后收到服务端的 FIN 时，发送 RST。 重传超时 重传的上限 ","date":"2020-04-25","objectID":"/2020/04/25/http-connect-and-disconnect/:2:2","tags":["HTTP"],"title":"HTTP 三次握手与四次挥手","uri":"/2020/04/25/http-connect-and-disconnect/"},{"categories":null,"content":"参考 https://zhuanlan.zhihu.com/p/86426969 https://www.zhihu.com/question/27564314/answer/162476313 https://www.jianshu.com/p/ff26312e67a9 ","date":"2020-04-25","objectID":"/2020/04/25/http-connect-and-disconnect/:3:0","tags":["HTTP"],"title":"HTTP 三次握手与四次挥手","uri":"/2020/04/25/http-connect-and-disconnect/"},{"categories":null,"content":"在可靠传输的传输层协议 TCP 基础上，为 TCP 传输的有效载荷定义人类可读的文本的规则，得到了 HTTP。这些 HTTP 的内容没有被加密，因而可以被任何中间设备看到。为了保护数据的安全，在将 HTTP 数据放入 TCP 之前，使用 SSL/TLS 加密 HTTP 数据，就得到了 HTTPS。 建立 HTTPS 的过程： TCP 三次握手建立 TCP 连接 TLS 四次握手建立 TLS 连接 四次握手主要完成两件事： 客户端对服务端身份认证（证书验证） 对称密钥协商 ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:0:0","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"密钥与服务端身份认证 通常两台通信的计算机就算直接使用网线连接，也可能在物理层面被窃听，更不用说互联网上通信的两端之间有很多设备。这些设备可以拷贝甚至修改通过该设备的数据。 对于总是可以确定的通信两端，我们可以生成一个密钥，双方都使用这个密钥加解密。双方使用相同密钥的方式，叫做对称加密。 对称加密的问题是，这个密钥在一方生成后，需要通过可信的方式同步给另一方。如果是可以确定的、数量不多的通信，可以通过交通运输的方式同步。如果和不同的组织通信，还需要生成不同的密钥，避免各方伪造其他组织的身份。 但互联网上的通信设备巨多，无法这样一一地生成对称密钥，再通过交通运输方式同步，而且这种方式的延迟也特别高。 如果有一种加密算法能够实现同一段密文的加密和解密的密钥不同，那么就可以将其中一个密钥公布出去（称为公钥），另一个密钥放在提供服务的那一方（称为私钥）。聪明的人们从数学上验证了其可行性。 现在问题转变为：公钥以什么形式公布？如果所有设备都内置这些公钥，那么设备里将会有茫茫多的公钥，不管有用的还是没用的都放进去。如果不内置，那么如何确保网上看到的公钥是官方的？就算内置了公钥，或者确认公钥是官方的，也没法保证服务方的私钥不会泄露，一旦泄露就会被中间人解密。 现在需要一个机制，确保客户端可以即时地获取服务端公钥，并且验证公钥确实是服务端生成的。 在现实生活中，如果我们要让一个第三方机构来确保一个信息的可信，会要求他们在文件上签名，或者盖上公章。在公钥的验证中，服务端可以找第三方权威机构给它的公钥签名。如果有第三方权威机构用它自己的私钥给服务端公钥签名，那么我们只需验证第三方权威机构的可信，就能够信任服务端给的公钥。 现在问题转变为：如何验证第三方机构的可信？由于服务端找第三方机构签名，那么客户端所需验证的数量从茫茫多的服务端减少到数量不多的第三方机构的数量，这时就可以将这些权威的第三方机构的公钥直接保存在设备中。 当然，这还是有风险的。正如前面说过，第三方机构也需要确保它自己的私钥不泄露。由于第三方机构的主要职责是保证安全性，因此它会投入大量的资源来确保自己的私钥不泄露。这样其他组织想要获取这些机构的私钥就会非常困难。但这也不代表不可行，例如美国政府可以在某些情况下根据法律要求权威机构提供私钥。 我们在完全信任权威机构的基础上继续讨论这个问题。既然第三方权威机构的私钥不会泄露，那么只要我们使用存储于设备上的权威机构公钥能够解开用权威机构私钥加密过的内容，就证明这些内容经过可信权威机构的认证。 总结一下，在验证服务端可信的过程中，对于服务端公钥的验证是最主要的内容，服务端公钥的可信度由权威机构保证，权威机构的可信由预先存储于设备上的权威机构公钥保证。 实际使用时，不只有公钥，还会有其他辅助信息。包含了所有相关信息的数据被称为“证书”。当然，证书里必须包含最重要的公钥。 ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:1:0","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"证书与签名 由于证书内容多，如果用私钥给整个证书加密，则密文占用数据量大，解密时间长。我们需要减少加密的内容。 最先想到的是压缩。这的确可行，但内容仍然很多。能否进一步减少？ 我们要验证的是整个证书的可信，而不是避免被别人看到证书的内容。证书本身的内容不需要保密，给任何人看都可以。所以我们可以找到一个信息，它尽可能地小，又能唯一代表这个证书，只要让权威机构用私钥加密这个信息即可。 hash 是一种不可逆的加密方式，它可以把任意内容转换成信息量很少的 hash 值，同时又在一定程度上确保了很难通过另一个不同的内容计算出相同的 hash 值。 使用 hash 算法计算证书的 hash 值，这个值就代表了证书本身。由权威机构计算出并加密这个 hash 值得到证书签名，将签名交给服务端，服务端会把签名跟证书一起传给客户端。 客户端收到服务端证书后，如果用权威机构的公钥能够解密签名得到 hash 值，就表示这个 hash 值是权威机构通过 hash 算法计算某个证书得到的。如果客户端自己计算证书的 hash 值，结果与解密签名后得到的 hash 值一致，就表示这个证书跟权威机构计算 hash 值时使用的证书是同一个。因此证明了这个证书是可信的。 ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:2:0","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"证书的内容 服务端传给客户端的证书包含三大部分： 签名前的证书，包含了公钥在内的相关信息 签名算法 签名，由权威机构私钥加密签名前的证书的 hash 值生成 签名前的证书包含以下内容（仅列出部分）： 签发对象（部分内容） 使用者 域名 公钥 服务端生成的非对称密钥中的公钥，客户端用该公钥加密的内容只有服务端能解开 签名 Hash 算法（如 sha256） 对签名前的证书内容做 hash 指纹 验证证书是否被篡改。对签名后的证书做 hash。 授权信息访问（Authority Information Access） 证书颁发者信息，包括证书颁发者自身的证书下载地址。 签发信息 颁发者 地址 有效期从 有效期至 CRL（证书吊销列表） 在证书到期之前吊销证书，并于证书到期后删除该信息 现用 OCSP（在线证书检查） 替代。 CRL 用于兼容。 签名算法 对【签名 Hash 算法】得到的 Hash 值做签名 如 sha256RSA ，表示用 RSA 加密的 sha256 签名 签名算法对签名 Hash 算法生成的 Hash 值签名后的内容，用于验证证书信任链。 ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:3:0","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"证书验证 客户端：发送支持的 SSL 或 TLS 版本，以及客户端随机数（Client Random，未加密）。 服务端：选择加密方式，并返回证书和服务端随机数（Server Random，未加密）。 证书内容： 客户端：验证证书 使用签名 Hash 算法取得证书的 Hash 值 使用颁发者公钥解密签名得到 Hash 值 对比两个 Hash 值，相等则验证成功 验证颁发者的证书 …，直到证书是 Root 证书 客户端：生成预主密钥（Premaster Secret），用服务端证书公钥加密后发给服务端。 对称密钥 = gen(客户端随机数 + 服务端随机数 + 预主密钥) 服务端：生成对称密钥 对称密钥 = gen(客户端随机数 + 服务端随机数 + 预主密钥) ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:4:0","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"为什么不能只使用 Premaster Secret 生成对称密钥？ 防止重放攻击。 什么是重放攻击？ 中间人获取客户端发送的请求，然后多次发送该请求。 比如客户端的请求是转账 10 万元，多次请求后会导致多次转账。 为什么能够防止重放攻击？ 中间人发送请求的时候，会建立新连接，此时的客户端随机数与原先的随机数不一样。服务端生成的对称密钥与原先的对称密钥不一致，无法解密。 为什么中间人不避免重新建立连接，直接使用原先的连接不就行了？ 每个请求里面都有一个序列号，从 0 开始递增。服务端会维护一个滑动窗口，在这个窗口以外的序列号或者窗口内已有的序列号的请求会被丢弃。 ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:4:1","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"参考 https://en.wikipedia.org/wiki/Public_key_certificate https://www.jianshu.com/p/46e48bc517d0 https://www.jianshu.com/p/6bf2f9a37feb https://security.stackexchange.com/questions/89383/why-does-the-ssl-tls-handshake-have-a-client-and-server-random http://www.moserware.com/2009/06/first-few-milliseconds-of-https.html https://security.stackexchange.com/questions/218491/why-using-the-premaster-secret-directly-would-be-vulnerable-to-replay-attack https://en.wikipedia.org/wiki/Anti-replay https://www.jianshu.com/p/29e0ba31fb8d ","date":"2020-04-25","objectID":"/2020/04/25/https-certification/:5:0","tags":["HTTPS","TLS"],"title":"HTTPS 证书","uri":"/2020/04/25/https-certification/"},{"categories":null,"content":"并发资源争抢就会涉及到锁。锁的种类有很多，如果不分类容易搞混。 以下涉及公平锁/非公平锁、乐观锁/悲观锁、独享锁/共享锁、互斥锁、读写锁、可重入锁（递归锁）/不可重入锁、自旋锁、自适应锁、偏向锁（意向锁）/轻量级锁/重量级锁、分段锁。 根据锁的性质可以分为： 公平锁/非公平锁 根据是否按照申请的先后顺序获得锁区分 乐观锁/悲观锁 根据认为获取的数据被修改的可能性区分 独享锁/共享锁 根据获取锁后，其他实例是否还能获取同样的锁来区分 实现方式： 互斥锁：独享锁的实现 读写锁：有读状态和写状态。读状态时，仍可申请读锁，为共享锁；写状态时，不可申请其他锁，为独享锁。 读状态时，碰到写锁申请的请求会阻塞后续读锁的申请。 可重入锁（递归锁）/不可重入锁 根据获取锁的实例能否再次获取该锁区分 根据锁的涉及方案可以分为： 自旋锁 获取不到锁的时候循环获取锁，直到获取到锁。可以减少线程切换。 自适应锁 最近刚获得过锁意味着更容易成功获得锁，则增加自旋次数。 如果很少成功获取，则减少自旋次数。因为增加自旋次数能获取成功的机率也很低，并且自旋次数越大，浪费的 CPU 越多。 意向锁（偏向锁）/轻量级锁/重量级锁 会根据争抢激烈程度，逐渐升级。意向锁 -\u003e 轻量级锁 -\u003e 重量级锁。 意向锁：对更细粒度加锁之前，先对路径加意向锁，避免其他实例获取锁的判断需要遍历所有数据。 轻量级锁：当另一个事务要进入争抢的时候，意向锁升级为轻量级锁，通过自旋的方式等待，线程不阻塞。 重量级锁：当自旋一定次数后，线程会被阻塞，进行线程切换（消耗资源大），轻量级锁升级为重量级锁。 分段锁 Hash 结构每条冲突链设置一个锁，减小锁影响的粒度。 重量级的加锁操作伴随着用户态到内核态切换、进程上下文切换等高消耗过程。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:0:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"悲观锁/乐观锁 悲观锁假设获取的数据会被其他事务修改，所以读取时加锁以防止其他事务修改。如果其他事务需要修改数据，则需要等待悲观锁的释放。 乐观锁假设获取的数据不会被其他事务修改，所以读取时不加锁。更新时判断数据和读取时的数据是否一致，如果一致则将当前数据写入，否则等待该条件得到满足（自旋锁）或者驳回（版本号）。 从应用场景来看，悲观锁用于由于写多导致容易产生数据冲突的地方，以及不接受数据发生变化的情况。乐观锁用于读多写少不容易产生数据冲突的地方，提高吞吐量。 悲观锁举例：InnoDB 的共享锁和排他锁。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:1:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"排他锁（写锁） 客户端如果获得不到锁，就进入睡眠状态，等待锁释放时的唤醒。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:2:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"共享锁（读锁、独占锁） 一个事务获取共享锁之后，其他事务也可以获取共享锁。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:3:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"互斥锁 互斥锁会导致获取不到锁的线程被挂起。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:4:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"自旋锁 线程如果获得不到锁，就 自己循环 直到获得到锁，被挂起的几率低。 优势： 减少线程被挂起的几率 效率高 劣势： CPU 消耗高 要求： 锁竞争不激烈 锁占用时间短 其他种类： 阻塞锁 可重入锁 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:5:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"轻量级锁 CAS （compare and swap）实现 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:6:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"乐观锁的实现 仅在写入时可能需要等待。 版本号 如果不一致，则驳回或者合并（类似于 Git 解决冲突） CAS （compare and swap）原子操作 假设有三种数据：待更新数据 A，事务开始时读取的数据 B，事务修改数据 B 得到 C。如果 A = B，则将 A 改为 C。 如果不相等，则进入循环等待，直到相等。 CAS 会出现 ABA 问题，即数据虽然与之前一致，但已发生过变化。并非所有场景都对该问题敏感，根据情况可以忽略该问题。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:7:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"四种锁状态 无锁 意向锁：只有一个线程执行同步块 轻量级锁：线程交替执行同步块 重量级锁：依赖操作系统的 Mutex Lock 锁升级：由于锁竞争，升级锁。 锁升级的单向过程：意向锁 -\u003e 轻量级锁 -\u003e 重量级锁 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:8:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"公平锁/非公平锁 有优先级的锁为公平锁，反之为非公平锁 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:9:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"参考 java锁的种类 https://www.jianshu.com/p/7e3cf7469c83 mysql锁机制 乐观锁\u0026悲观锁，共享锁\u0026排他锁\u0026意向锁\u0026间隙锁 https://blog.csdn.net/xushiyu1996818/article/details/105558662 互斥锁（排它锁、独占锁、写锁、X锁）和共享锁（读锁、S锁） 自旋锁 https://my.oschina.net/u/2307114/blog/908009 java中synchronized的底层实现 https://www.jianshu.com/p/c97227e592e1 深入理解各种锁 https://www.jianshu.com/p/5725db8f07dc 漫画|Linux 并发、竞态、互斥锁、自旋锁、信号量都是什么鬼？ https://zhuanlan.zhihu.com/p/57354304 Java并发编程：Synchronized底层优化（偏向锁、轻量级锁） https://www.cnblogs.com/paddix/p/5405678.html Let’s Talk Locks! https://www.infoq.com/presentations/go-locks/ Java并发问题–乐观锁与悲观锁以及乐观锁的一种实现方式-CAS https://www.cnblogs.com/qjjazry/p/6581568.html 独占锁（写锁） / 共享锁（读锁） / 互斥锁 https://www.cnblogs.com/bbgs-xc/p/12791913.html 如何理解互斥锁、条件锁、读写锁以及自旋锁？ https://www.zhihu.com/question/66733477/answer/246535792 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:10:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"InnoDB 三种锁算法 Record Lock Gap Lock Next-Key Lock = Gap Lock + Record Lock ","date":"2020-04-24","objectID":"/2020/04/24/locks/:11:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"Record Lock 唯一索引，粒度最细 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:12:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"Gap Lock 间隙锁用于非唯一索引。在可重复读的隔离级别中，用于防止其他事务插入数据导致的幻读。 使用 SELECT ... FOR UPDATE 或者 SELECT ... LOCK IN SHARE MODE 。 对于有序数字 1, 5, 10 ，它们之间都有一个范围可以存放其他数字。在这个空白的范围内称之为间隙。 在加排他锁时： 如果目标只有一个值： 如果目标是第一个索引值，则锁住无穷小到第一个索引值的范围 如果目标是最后一个索引值，则锁住最后一个索引值到无穷大的范围 否则锁住目标和前一个与目标值不相同的索引之间的范围 如果目标是范围，则锁住范围起止两个索引值之间的范围。 例1： WHERE num BETWEEN 1 AND 5，会锁住 (1, 5) 这个范围。 例2： WHERE num = 5，会锁住 (1, 5) 这个范围。 ","date":"2020-04-24","objectID":"/2020/04/24/locks/:13:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"Next-Key Lock 对于间隙锁，如果范围中的右侧不是无限大，则同时锁住右侧的记录。 (1, 5] 与之相对的是 Previous-Key Lock，会锁住左侧的记录。 [1, 5) 对于范围查询，会直接使用范围。例如 \u003e 2 ，则是 (2, +∞) https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html ","date":"2020-04-24","objectID":"/2020/04/24/locks/:14:0","tags":["锁"],"title":"各种锁","uri":"/2020/04/24/locks/"},{"categories":null,"content":"https://www.cnblogs.com/clphp/p/6398667.html https://zhuanlan.zhihu.com/p/134012175 ","date":"2020-04-24","objectID":"/2020/04/24/ms/:0:0","tags":["锁"],"title":"秒杀","uri":"/2020/04/24/ms/"},{"categories":null,"content":"深度优先/广度优先 前序、中序、后序 ","date":"2020-04-24","objectID":"/2020/04/24/scan-al/:0:0","tags":["Tree","遍历"],"title":"遍历算法","uri":"/2020/04/24/scan-al/"},{"categories":null,"content":"事务隔离级别有四种。它们的区别在于一个修改数据的事务在提交前和提交后，另一个进行中的事务读取到的数据是修改前还是修改后的数据。 READ-UNCOMMITED = 读-未提交 READ-COMMITED = 读-已提交 REPEATABLE-READ = 可重复-读 （InnoDB） SERIALIZABLE = 串行 以下假设事务B都处于进行中。 事务A进行中： 隔离级别 事务B 问题 READ-UNCOMMITED 修改后 脏读（事务回滚） READ-COMMITED 修改前 REPEATABLE-READ 修改前 SERIALIZABLE 阻塞 并发能力低 事务A提交后： 隔离级别 事务B 问题 READ-UNCOMMITED 修改后 READ-COMMITED 修改后 不可重复读（update/delete） REPEATABLE-READ 修改前（快照数据 undo段） 幻读（insert）可通过 Next-Key Lock 解决 SERIALIZABLE 修改后 ","date":"2020-04-24","objectID":"/2020/04/24/acid-isolation/:0:0","tags":["InnoDB","事务"],"title":"事务隔离级别","uri":"/2020/04/24/acid-isolation/"},{"categories":null,"content":"锁 修改数据会加 X 锁。 查询数据需要使用以下方式： SELECT ... FOR UPDATE 加 X 锁 SELECT ... LOCK IN SHARE MODE 加 S 锁 锁会在事务结束后释放。 由于每条语句都是单独一个事务，且数据库使用 Auto Commit，结果是在执行语句后会自动提交事务。 所以如果不主动开启事务或者关闭 Auto Commit，就算执行时加了锁，在 SELECT 事务结束后会立即释放。无法达到目的。 关闭自动提交： SET AUTOCOMMIT=0; 开启事务（两者等效）： BEGIN; START TRANSACTION; 两种开启事务的方式都使用 COMMIT; 来提交事务。 ","date":"2020-04-24","objectID":"/2020/04/24/acid-isolation/:1:0","tags":["InnoDB","事务"],"title":"事务隔离级别","uri":"/2020/04/24/acid-isolation/"},{"categories":null,"content":"强一致性/最终一致性 ","date":"2020-04-24","objectID":"/drafts/acid-con/:0:0","tags":["一致性"],"title":"一致性","uri":"/drafts/acid-con/"},{"categories":null,"content":"参考： https://www.zhihu.com/question/34873227/answer/518086565 ","date":"2020-04-22","objectID":"/2020/04/22/browser-enter-url/:0:0","tags":null,"title":"在浏览器地址栏输入一个URL后回车，背后会进行哪些技术步骤？","uri":"/2020/04/22/browser-enter-url/"},{"categories":null,"content":"原文地址： 第一个 Go 语言程序：漫画下载器： https://schaepher.github.io/2020/04/11/golang-first-comic-downloader 之前学了点 Go 语言，但没有写出一个比较有用的工具，基本上算白学。得选一个又简单又比较有有价值的功能来实现。 之前用 PHP + Laravel 写的漫画下载器不好用，这刚好是一个简单又实用的功能，干脆用 Go 语言重新写一个。 所有代码在 GitHub 上： https://github.com/schaepher/comic-downloader-example 实现的功能和获得对应的实践如下： hello world 程序的结构 包的引用 编译和运行代码 函数/方法的可见性 fmt 库输出字符串 请求网页和写入文件 变量定义和赋值 字符串 if 语句 无返回值的函数 net/http 库发起请求和接收响应 io/ioutil 库将网页内容写入文件 漫画标题和下载 ID 的解析 结构体的定义和初始化 结构体的方法 单返回值的函数 fmt 库格式化输出字符串 regexp 库正则表达式 除了用正则，还可以用 goquery 来解析 html，但这里不使用。 代码整理，抽取函数 多返回值的函数 自定义错误信息 strconv 库将字符串转为整数 代码整理，放到类里面 方法内部修改结构体的值（引用） 空白标识符 获取漫画的所有文件名 数组和切片的声明 字符串转 byte 切片 strings 库替换字符串 encoding/json 库解析 Json fmt 库打印结构体 下载漫画 字符串类型元素的切片的初始化 字符串拼接 for range 循环 普通的 for 循环 os 库获取当前所在工作目录的路径、判断文件或文件夹是否存在、创建文件夹 strconv 库将整数转为字符串 并发下载漫画 Go协程（goroutines）和通道（channel） 引用类型与 make() 匿名函数（闭包） defer sync 库等待 goroutines 执行结束 接口类型 类型转换 再次执行时避免下载已有页面 判断一个字符串是否存在于字符串切片中 往切片中添加元素 io/ioutil 库读取文件夹里的文件列表 将配置抽取到配置文件 获取程序所在的目录 io/ioutil 库读取文件内容 没有全部下载成功时重试 自定义错误类型 注，编译和执行环境都是 Windows 10 一开始尝试对每份代码做分析，写了一些后发现很费时间，所以还未写解析的部分主要列出相关资料，并作必要的补充。主要来源是 《The Way To Go》 的中文版： https://github.com/Unknwon/the-way-to-go_ZH_CN/blob/master/eBook/directory.md 注意，在运行代码前需要确保已安装 Go 环境。 ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:0:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v1: hello world 先从最简单的开始。 创建项目 comic-downloader ，在目录里面创建 main.go 文件。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v01-hello-world/main.go main.go package main import \"fmt\" func main() { fmt.Println(\"hello world\") } 需要说明的内容有两点： Go 的代码不需要在代码行结束后加分号 ; 。 Go 语言通过函数/方法名的首字母大小写控制访问权限。大写首字母代表 public，小写首字母代表 private。 执行命令： go run main.go 输出： hello world go run main.go 会将代码编译为可执行文件，然后执行。 如果要分开，可以这样执行： go build -o main.exe main.go ./main.exe ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:1:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v2: 请求网页和写入文件 对于下载功能，我最关心的是如何发送 http 请求和如何读取结果。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v02-http-get-write-file/main.go main.go package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) func check(e error) { if e != nil { panic(e) } } func main() { var err error var url = \"https://cn.bing.com\" res, err := http.Get(url) check(err) data, err := ioutil.ReadAll(res.Body) check(err) ioErr := ioutil.WriteFile(\"cn.bing.com.html\", data, 644) check(ioErr) fmt.Printf(\"Got:\\n%q\", string(data)) } 这里展示了变量声明和赋值的不同形式。 首先看 var err error ，这里用到的语法是 var 变量名 变量类型，因此这一句定义了一个类型为 error 的变量 err 。 4.4 变量： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.4.md 注意： 当一个变量被声明之后，系统自动赋予它该类型的零值：int 为 0，float 为 0.0，bool 为 false，string 为空字符串，指针为 nil。记住，所有的内存在 Go 中都是经过初始化的。 Go 语言和 C 语言或者 JAVA 把变量类型放在前面的形式不同，Go 语言总是把类型放在后面。这点在下面的例子中都可以看到，无论是变量名、函数参数（例如上面的 check 函数）还是函数返回值，类型都放在后面。 对于有弱类型语言（例如 PHP）编程经验的人来说，这种顺序会舒服很多。因为写代码的时候不需要先想/查清楚返回值的类型再开始写，或者写完后面的函数调用再到前面补类型。 res, err := http.Get(url) 这里涉及四个知识点： 变量省略类型声明并赋值 4.4 变量： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.4.md 发起一个 HTTP GET 请求 15.3 访问并读取页面： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/15.3.md 函数多个返回值，用逗号隔开 6.2 函数参数与返回值（第一部分）： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/06.2.md Go 语言没有“异常”这一设计，通常给函数多加一个返回值表示错误 Why does Go not have exceptions? https://golang.org/doc/faq#exceptions 13 错误处理与测试： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/13.0.md 下一行的 check(err) 用于检查是否有错误： func check(e error) { if e != nil { panic(e) } } 涉及三个知识点： Go 的 if 判断不需要加括号。 5.1 if-else 结构： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/05.1.md 当一个指针被定义后没有分配到任何变量时，它的值为 nil。 注意，只有指针才能为 nil。假设有代码： var str string = nil 编译时会报错：cannot use nil as type string in assignment 4.9 指针： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.9.md panic() 用于终止程序 13.2 运行时异常和 panic： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/13.2.md 回到主函数，再往下是读取结果： data, err := ioutil.ReadAll(res.Body) 然后是将结果写到文件里面： ioErr := ioutil.WriteFile(\"cn.bing.com.html\", data, 644) 12.2 文件读写 https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/12.2.md fmt.Printf(\"Got:\\n%q\", string(data)) 这里 string(data) 将 data 转换为字符串。 7.6.4 修改字符串中的某个字符： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/07.6.md ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:2:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v3: 解析标题和下载 ID 这个漫画下载网站漫画的 ID 和下载时 URL 的 ID 不一致，所以要将这个 ID 提取出来。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v03-regex-struct-method/main.go main.go package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" ) func check(e error) { if e != nil { panic(e) } } type ComicSite struct { MainPageUrl string } func (cs ComicSite) GetComicMainPageUrl(comicId int) string { return fmt.Sprintf(\"%s/cn/s/%d/\", cs.MainPageUrl, comicId) } func main() { comicSite := ComicSite{ MainPageUrl: \"https://*****\", } // 获取漫画页 comicMainPageUrl := comicSite.GetComicMainPageUrl(282526) res, err := http.Get(comicMainPageUrl) check(err) data, err := ioutil.ReadAll(res.Body) check(err) html := string(data) // 匹配标题 titleR, err := regexp.Compile(`\u003ctitle\u003e(.+?)\u003c/title\u003e`) check(err) titleMatches := titleR.FindStringSubmatch(html) if titleMatches == nil { panic(\"comic title not found\") } title := titleMatches[1] fmt.Println(title) // 匹配下载 ID downloadR, err := regexp.Compile(`cn/(\\d+)/1.(jpg|png)`) check(err) downloadMatches := downloadR.FindStringSubmatch(html) if downloadMatches == nil { panic(\"download id not found\") } downloadIdStr := downloadMatches[1] fmt.Println(downloadIdStr) } 这里引入了结构体。 type ComicSite struct { MainPageUrl string } 初始化和赋值： comicSite := ComicSite{ MainPageUrl: \"https://*****\", } 10 结构（struct）与方法（method）： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/10.0.md 10.1 结构体定义： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/10.1.md 10.6 方法： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/10.6.md 结构体的方法： func (cs ComicSite) GetComicMainPageUrl(comicId int) string { return fmt.Sprintf(\"%s/cn/s/%d/\", cs.MainPageUrl, comicId) } 注意与函数做比较： func GetComicMainPageUrl(comicId int, mainPageUrl string) string { return fmt.Sprintf(\"%s/cn/s/%d/\", mainPageUrl, comicId) } func 后面多了个 (cs ComicSite) 。在 Go 语言中，将其称为接收者（receiver）。由于 Go 里面没有 this 关键字，所以这里也可以写成： func (this ComicSite) GetComicMainPageUrl(comicId int) string { return fmt.Sprintf(\"%s/cn/s/%d/\", this.MainPageUrl, comicId) } 熟悉的味道。 再看看客户端的调用： comicSite := ComicSite{ MainPageUrl: \"https://*****\", } comicSite.GetComicMainPageUrl(282526) 正则库的使用： titleR, err := regexp.Compile(`\u003ctitle\u003e(.+?)\u003c/title\u003e`) check(err) titleMatches := titleR.FindStringSubmatch(html) if titleMatches == nil { panic(\"comic title not found\") } title := titleMatches[1] 这里在编译正则表达式的时候，用到了反引号，表示这是一个非解释字符串。 4.6 字符串： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.6.md 9.2 regexp 包： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/09.2.md 正则表达式30分钟入门教程： https://deerchao.cn/tutorials/regex/regex.htm 由于只需要获取 () 里的内容，因此用 FindStringSubmatch。 假设 html 的值是 aaa\u003ctitle\u003e标题\u003c/title\u003ebbb ，则 titleMatches 的值为： [ \"\u003ctitle\u003e标题\u003c/title\u003e\", \"标题\" ] ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:3:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v4-5: 代码整理 分为两部分。 代码整理的第一部分是把匹配的代码放到函数里面。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v04-function-error/main.go main.go 部分代码： func getDownloadId(html string) (int, error) { downloadR, err := regexp.Compile(`cn/(\\d+)/1.(jpg|png)`) if err != nil { return 0, err } downloadMatches := downloadR.FindStringSubmatch(html) if downloadMatches == nil { err := errors.New(\"download id not found\") return 0, err } downloadId, err := strconv.Atoi(downloadMatches[1]) if err != nil { return 0, err } return downloadId, nil } 说明三个点： 自定义错误信息内容 err := errors.New(\"download id not found\") 13.1 错误处理： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/13.1.md 函数多返回值 6.2 函数参数与返回值： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/06.2.md 字符串转整数 4.7.12 字符串与其它类型的转换 https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.7.md 代码整理的第二部分是把函数转为结构体的方法。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v05-reference-param/main.go main.go 部分代码： type Comic struct { Id int Title string DownloadId int ComicSite ComicSite } func (comic *Comic) LoadMeta() error { var err error var mainPageHtml string comicMainPageUrl := comic.ComicSite.GetComicMainPageUrl(comic.Id) mainPageHtml, err = comic.getComicMainPageHtml(comicMainPageUrl) if err != nil { return err } comic.Title, err = comic.findTitle(mainPageHtml) if err != nil { return err } comic.DownloadId, err = comic.findDownloadId(mainPageHtml) if err != nil { return err } return nil } func (_ Comic) findTitle(html string) (string, error) { titleR, err := regexp.Compile(`\u003ctitle\u003e(.+?)\u003c/title\u003e`) if err != nil { return \"\", err } titleMatches := titleR.FindStringSubmatch(html) if titleMatches == nil { err := errors.New(\"comic title not found\") return \"\", err } title := titleMatches[1] return title, nil } 对比以下两段代码： func (_ Comic) findTitle(html string) (string, error) { // ... } func (cs ComicSite) GetComicMainPageUrl(comicId int) string { // ... } 有个不同的地方是这里结构体变量设置为空白标识 _。因为 findTitle 这个函数不需要用到 Comic 这个结构体的内容。 再对比： func (comic *Comic) LoadMeta() error { // ... comic.Title, err = comic.findTitle(mainPageHtml) // ... } 多了个 * ，表示 comic 是一个 Comic 类型的指针，对其内容的修改会影响到外部的变量。 另外无论是值类型还是指针，其调用方式都是 obj.method(...) ，Go 会自动识别。 10.6.3 指针或值作为接收者： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/10.6.md ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:4:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v6: 获取漫画的所有文件名 接下来要准备下载了。不过在此之前要先获取下载链接。 漫画主页提供的预览图是缩小版的图片，因此不能直接使用。 漫画主页还提供了页面总数。虽然文件名是按照数字顺序的，但是文件扩展名可能是 jpg 或者 png 或者其他的。 通过观察，我发现在点击下载的时候，会去请求一个 js 文件。内容格式如下： var galleryinfo = [{\"lan\": \"cn\",\"name\": \"1.jpg\"},] 把后面的数组匹配出来然后做 json 解码就行了。正好还能学习 encoding/json 库和 strings 库。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v06-decode-json-replace-string/main.go main.go 部分代码： type ComicFile struct { Name string `json:\"name\"` } type Comic struct { Id int Title string DownloadId int ComicSite ComicSite ComicFiles []ComicFile } func (comic *Comic) LoadMeta() error { // ... comicIndexUrl := comic.ComicSite.GetComicIndexUrl(comic.Id) comic.ComicFiles, err = comic.readComicIndexes(comicIndexUrl) // ... } func (_ Comic) readComicIndexes(comicIndexUrl string) ([]ComicFile, error) { res, err := http.Get(comicIndexUrl) if err != nil { return nil, err } htmlByte, err := ioutil.ReadAll(res.Body) if err != nil { return nil, err } html := string(htmlByte) r, err := regexp.Compile(\"\\\\[.+]\") if err != nil { return nil, err } jsonStr := r.FindString(html) validJson := strings.Replace(jsonStr, \",]\", \"]\", 1) var pages []ComicFile err = json.Unmarshal([]byte(validJson), \u0026pages) if err != nil { return nil, err } return pages, nil } 说明三个点： 第一：切片的声明 var pages []ComicFile 7.2 切片： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/07.2.md 数组的声明呢？ var pages [100]ComicFile 二维数组呢？ var pages [X][Y]ComicFile // pages[x][y] 把 [Y]ComicFile 当成 COMICFILE 的话，上述声明就变成了： var pages [X]COMICFILE 第二：字符串的替换 validJson := strings.Replace(jsonStr, \",]\", \"]\", 1) 1 表示替换一次。 4.7.4 字符串替换： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.7.md 第三：Json 解码： var pages []ComicFile err = json.Unmarshal([]byte(validJson), \u0026pages) 12.9 JSON 数据格式： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/12.9.md 由于 Unmarshal 第一个参数指定为 byte 类型的切片，所以要先做一次转换。 第二个参数是传指针， Unmarshal 直接在函数里面修改这个变量。 还可以： pages := new([]ComicFile) err = json.Unmarshal([]byte(validJson), pages) 因为 new() 得到的是结构体的指针。 10.2.2 map 和 struct vs new() 和 make()： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/10.2.md 看一下 json 串和结构体： [{\"lan\": \"cn\",\"name\": \"1.jpg\"}] type ComicFile struct { Name string `json:\"name\"` } ComicFile 这个结构体只定义了一个字段，而且由于字段名称与 json 串里面的大小写不一样，所以后面加一个补充说明 json:\"name\" 。 解码的时候只会把 name 的值放到 Name 里面，并且忽略掉 lan 。 如果 json 字段本身就是大写，则不需要加后面的补充。 要确保结构体的字段以大写字母开头，否则 Json 解析后该字段为空。 ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:5:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v7: 下载漫画 终于要下载漫画了。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v07-encode-json-log-create-dir-for-range/main.go main.go 部分代码： func (comic Comic) GetDirPath() string { pwd, _ := os.Getwd() return pwd + \"/comics/\" + strconv.Itoa(comic.Id) } func createDirIfNotExist(dir string) error { if _, err := os.Stat(dir); os.IsNotExist(err) { err = os.MkdirAll(dir, 0755) if err != nil { return err } } return nil } func download(comic Comic) error { log.Printf(\"Downloading: %s\\n\", comic.Title) err := createDirIfNotExist(comic.GetDirPath()) if err != nil { return err } data, err := json.Marshal(comic) if err != nil { return err } err = ioutil.WriteFile(comic.GetMetaFilePath(), data, 0644) if err != nil { return err } log.Printf(\"Meta file saved: %s\\n\", comic.GetMetaFilePath()) for _, comicFile := range comic.ComicFiles { log.Printf(\"Start downloading: %s\\n\", comicFile.Name) for i := 0; i \u003c len(comic.ComicSite.DownloadSourceUrls); i++ { downloadUrl, err := comic.ComicSite.GetComicDownloadUrl(comic.DownloadId, comicFile.Name, i) if err != nil { break } log.Printf(\"Trying: %s\\n\", downloadUrl) resp, err := http.Get(downloadUrl) if err != nil || resp.StatusCode != 200 { log.Printf(\"Failed: %s\\n\", downloadUrl) continue } data, err := ioutil.ReadAll(resp.Body) err = ioutil.WriteFile(comic.GetFilePath(comicFile.Name), data, 0644) if err != nil { return err } log.Printf(\"Saved : %s\\n\", comic.GetFilePath(comicFile.Name)) } } return nil } func main() { comicSite := ComicSite{ MainPageUrl: \"https://******\", DownloadSourceUrls: []string{ \"https://******/img/cn\", }, } comic := \u0026Comic{ComicSite: comicSite, Id: 282526} err := comic.LoadMeta() check(err) err = download(*comic) check(err) } 该漫画网站有两种域名用于获取漫画图片： 在线阅读时请求的域名 下载时请求的域名 有时候在线阅读请求不到图片，但用于下载的域名可以获取到。有时候反之。所以当下载出错时，要换另一个域名试试。 先看 download 函数。 在下载前，要先创建文件夹。首先获取文件夹路径： func (comic Comic) GetDirPath() string { pwd, _ := os.Getwd() return pwd + \"/comics/\" + strconv.Itoa(comic.Id) } 这里获取的是当前的工作目录，不是程序文件所在的目录。获取程序文件所在的目录会在后面给出例子。 整数转字符串： 4.7.12 字符串与其它类型的转换 https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/04.7.md 字符串拼接： pwd + \"/comics/\" 如果要在循环中拼接字符串（例如将数组每个元素用逗号拼接起来），用 + 号拼接不是高效的做法。 var strB strings.Builder strB.WriteString(\"abc\") strB.WriteString(\"def\") fmt.Println(strB.String()) // abcdef 在 《The Way To Go》 里面还会看到用 bytes.Buffer 。区别在于 Go 1.10 才引入的 strings.Builder 效率更高。 接下来创建文件夹的 createDirIfNotExist 就不做说明了。 接着是把漫画的基本信息保存到文件里面。 前面介绍过 Json 字符串解码，现在要把结构体编码成字符串： data, err := json.Marshal(comic) 写完基本信息，接下来就是下载漫画图片了。 下面用了嵌套循环，展示了 for 的两种不同写法。 首先是遍历漫画所有文件用的 for range： for _, comicFile := range comic.ComicFiles { // ... } 和 PHP 的 foreach 很相似。 这里会返回 index 和 value。 index 被我用 _ 忽略掉了。 接着是遍历不同的下载域名链接： for i := 0; i \u003c len(comic.ComicSite.DownloadSourceUrls); i++ { // ... } 注：该网站在某个在线阅读方式中用到了 CDN， 图片下载速度快很多。这个 CDN 用的是 HTTP/2 。由于 Go 的 http 库默认开启 HTTP/2 ，所以无需修改代码。 Starting with Go 1.6, the http package has transparent support for the HTTP/2 protocol when using HTTPS. https://golang.org/pkg/net/http/ ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:6:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v8: 并发下载漫画 上面下载的时候，用 for 循环一张张下载，必须得等一张下载结束才能继续。这样效率太低，要下载半天。 那么就要想办法并发下载。 但是要注意控制并发的数量。如果不做控制，有的漫画两百多页一下子两百多个并发请求，对源站不友好。 并发的代码一开始是参考下面链接中的方案二： 来，控制一下 Goroutine 的并发数量： https://segmentfault.com/a/1190000017956396 这篇写得很好，感谢！ 但是我当时没理解过来，写下了有问题的代码。下面我先解析我改正后的代码， 解析完再说说之前我哪里理解错了，以及基于错误理解写的代码。 并发示例 用一个简单的例子来理解这部分内容，然后再将其改造成一个并发库。 上正确版的代码： https://github.com/schaepher/comic-downloader-example/blob/master/v08-channel-wait-group-go-func-defer/thread-v1-fix/thread.go thread.go package main import ( \"log\" \"math/rand\" \"sync\" \"time\" ) var wg sync.WaitGroup func main() { maxTask := 10 maxThread := 3 ch := make(chan int, maxThread) for i := 0; i \u003c maxThread; i++ { threadId := i go func() { wg.Add(1) defer wg.Done() log.Printf(\"Worker [%d] started at %d\\n\", threadId, time.Now().Unix()) for taskId := range ch { seconds := 1 + rand.Intn(9) log.Printf(\"Task [%d] will sleep %d seconds\\n\", taskId, seconds) time.Sleep(time.Second * time.Duration(seconds)) log.Printf(\"Task [%d] finished\", taskId) } log.Printf(\"Worker [%d] finished at %d\\n\", threadId, time.Now().Unix()) }() } for i := 0; i \u003c maxTask; i++ { ch \u003c- i } close(ch) wg.Wait() } 14.1 并发、并行和协程（前两部分） https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/14.1.md 14.2 协程间的信道 https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/14.2.md 大致知道五点： go 关键字执行函数或方法时，会创建协程。 通道（Channel）是一个先进先出的队列。多个协程可使用同一个通道。通道里的一个数据只会被其中一个协程访问到。 当通道满时，发送者无法再发送数据，只能阻塞并等待接收者消费通道的数据。如果通道已经空了，则接收者无法消费，只能阻塞并等待发送者发送数据。 通道默认无缓冲，即只能一发一收。可以创建带缓冲的通道，这样可以同时发送多个和接收多个。 close() 使得通道无法再接收数据，但剩下的数据可以被消费。用 for-range 消费时会自动检测通道是否关闭且无剩余数据。 回到代码中。 maxThread := 3 ch := make(chan int, maxThread) 这里创建了带缓冲的通道，允许通道里存放三个数据。接着启动与通道数量对应的协程： for i := 0; i \u003c maxThread; i++ { threadId := i go func() { // ... }() } 6.8 闭包： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/06.8.md 先忽略匿名函数里面的 WaitGroup 。 for taskId := range ch { seconds := 1 + rand.Intn(9) log.Printf(\"Task [%d] will sleep %d seconds\\n\", taskId, seconds) time.Sleep(time.Second * time.Duration(seconds)) log.Printf(\"Task [%d] finished\", taskId) } 协程里面不断读取通道的数据。但是由于刚启动的时候通道里面没有数据，所以这里会阻塞。三个协程都阻塞了。 继续往下走： for i := 0; i \u003c maxTask; i++ { ch \u003c- i } 这里开始往通道发送数据。由于通道在上面被设置为只能存三个数据，所以这里一开始最多只能放三个。一旦放满又没被消费， for 循环就会被阻塞。 一旦开始放数据，协程就可以从通道里拿数据了。 示例见： https://github.com/schaepher/comic-downloader-example/blob/master/v08-channel-wait-group-go-func-defer/thread-v1-fix/thread.log 2020/04/16 01:59:52 Worker [0] started at 1586973592 2020/04/16 01:59:52 Task [0] will sleep 6 seconds 2020/04/16 01:59:52 Worker [1] started at 1586973592 2020/04/16 01:59:52 Task [1] will sleep 7 seconds 2020/04/16 01:59:52 Worker [2] started at 1586973592 2020/04/16 01:59:52 Task [2] will sleep 3 seconds 2020/04/16 01:59:55 Task [2] finished 当 maxTask 个任务发送完毕后，for 循环就结束了。 但注意，此时协程里的任务未必结束，但 for 循环后面的代码会继续跑。 close(ch) 关闭通道入口，避免协程无限等待。 如果此时直接退出，会导致协程也被中断。 那么我们就要想办法等待协程任务执行结束。这时就要用到 WaitGroup 了。 for i := 0; i \u003c maxThread; i++ { // ... go func() { wg.Add(1) defer wg.Done() // ... }() } // ... wg.Wait() 在匿名函数开始执行时，往里面加了个 1。 紧接着用 defer 指定了一个方法调用（wg.Done 就是 wg.Add(-1)），这个方法会在匿名函数 return 后执行。 6.4 defer 和追踪： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/06.4.md 然后在主函数的最后，用了 wg.Wait() 等到归零时才继续。 什么时候归零呢？在所有协程 return 后都执行了 wg.Done()。而协程要退出，就代表着任务已经执行结束了。 这样就做到了等待所有任务执行完再退出程序。 并发库 为了将上面这个思路应用到漫画下载里面，可以选择将其直接分块写到 main.go 里面，或者抽取到一个专门的库。这里选择另外写一个库，可以借此演示引用项目中其他文件的方法。 下面先展示这个库的使用示例，再解释库自身的内容。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v08-channel-wait-group-go-func-defer/thread-v2-fix/test/main.go test/main.go package main import ( \"../../thread-v2-fix\" \"log\" \"math/rand\" \"time\" ) func main() { tp := Thread.Pool{MaxThread: 3} tp.Prepare(func(param interface{}) { taskId := param.(int) seconds := rand.Intn(9) + 1 log.Printf(\"Task [%d] will sleep %d seconds\", taskId, seconds) time.Sleep(time.Second * time.Duration(seconds)) log.Printf(\"Task [%d] finished\", taskId) }) tasksCount := 10 for i := 0; i \u003c tasksCount; i++ { tp.RunWith(i) } tp.Wait() } 总体上与前面的例子一致。我将 ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:7:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v9: 再次执行时避免下载已有页面 这个下载站有时候下载一张漫画图的时候会失败，然后再请求几次就能成功了。 （每次都能给我整出新花样）.jpg 但我不想总因为中间的某几张下载不了花太多时间重试，于是就放到整个漫画其他文件下载完之后再执行一次程序进行补充下载（后续改为自动重试）。 这样就带来一个问题，直接重试会导致一些已经下载的漫画页面再次被下载。所以我得列出已经下载的漫画页面，然后只下载那些缺失的页面。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v09-list-dir-files/main.go main.go type DownloadParam struct { Comic Comic ComicFile ComicFile } func downloadComic(comic Comic, maxThread int) error { // ... existFiles, err := ListDirFiles(comic.GetDirPath()) if err != nil { return err } log.Println(\"Downloading comic files\") tp := Thread.Pool{MaxThread: maxThread} tp.Prepare(func(param interface{}) { downloadParam := param.(DownloadParam) downloadImg(downloadParam.Comic, downloadParam.ComicFile) }) for _, comicFile := range comic.ComicFiles { if InArray(comicFile.Name, existFiles) { continue } tp.RunWith(DownloadParam{Comic: comic, ComicFile: comicFile}) } // ... } func ListDirFiles(root string) ([]string, error) { var files []string fileInfo, err := ioutil.ReadDir(root) if err != nil { return files, err } for _, file := range fileInfo { files = append(files, file.Name()) } return files, nil } func InArray(item string, items []string) bool { for _, tmpItem := range items { if tmpItem == item { return true } } return false } ListDirFiles 来自于： List directory in Go： https://stackoverflow.com/a/49196644 这个函数创建了一个类型为 string 的切片，然后用 append 不断往切片里添加文件名。 7.5 切片的复制与追加： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/07.5.md InArray 函数是自己实现的判断当前文件名是否在文件列表中。 Go 没有判断元素是否在一个切片中的方法（比如 PHP 中的 in_array），因此每次都需要自己写。 ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:8:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v10: 将配置抽取到配置文件 该版本的代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v10-config/main.go 到目前为止，网站和要下载的漫画 ID 都是放在代码里面的。这样导致要下载新漫画的时候，都得重新编译。因此要把配置抽取出来。 分析 v9 的代码，可以找到以下配置项： 漫画主页 URL 下载地址的域名 漫画 ID 存储漫画的文件夹位置 并发数量 这里打算使用 Json 文件作为配置文件。 因此定义以下结构体： type Config struct { MainPageUrl string `json:\"mainPageUrl\"` DownloadSourceUrls []string `json:\"downloadSourceUrls\"` MaxThread int `json:\"maxThread\"` ComicIds []int `json:\"comicIds\"` ComicsRootDir string `json:\"comicsRootDir\"` } 那么从哪里读取这个 Json 配置文件呢？默认跟可执行文件同一个目录吧。 前面使用 os.Getwd() 获取的是执行时所在的目录，而这里则是可执行文件所在的目录。 首先用 os.Args[0] 获取执行文件时使用的路径。 如果在 Windows 10 上执行，会得到绝对路径。就算使用 ./main.exe，也会得到完整路径。 如果在 Linux 上执行，会得到执行时的路径。例如使用 ./main 执行时，会得到 ./main。 然后用 filepath.Dir() 获取到该文件的文件夹。 最后用 filepath.Abs() 得到绝对路径。上文有提到系统之间的差异，所以用这个函数来确保获取到正确的路径。 接着是读取这个文件，这里用了 ioutil.ReadFile(filePath) 。 ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:9:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"v11: 没有全部下载成功时重试 在 v9: 再次执行时避免下载已有页面 这一节碰到下载不了的漫画页面时，会先下载其他的，然后手动再次执行程序进行补充下载。 重试这种能交给程序做的事情为什么要手动执行？ 如何做？ 在一个漫画下载完后，再次获取文件夹内部的文件列表。如果文件数量和漫画数量对不上，则抛出错误。外部获得这个错误后执行重试。 此时有个问题：外部如何判断抛出的错误是漫画没有全部下载的错误？因为还可能出现其他类型的错误。 用错误的文本信息做比较是一种方法，不过容易出问题，而且不优雅。我们更希望能像 try-catch 那样自定义异常类型，然后根据类型做处理。 其实之前在转换变量类型的时候，会返回两个值： 转换后的变量 转换是否成功（bool 类型的变量） 那么就可以通过自定义错误类型 NotAllComicDownloadedError ，实现 error 接口。然后在外面尝试将错误转换为 NotAllComicDownloadedError 。如果成功，就表示出现这个错误，进入重试。 以下代码在： https://github.com/schaepher/comic-downloader-example/blob/master/v11-custom-error-retry/main.go main.go 部分代码： for retries := 0; ; { err = downloadComic(*comic, config.MaxThread) if err == nil { break } if _, ok := err.(NotAllComicDownloadedError); !ok { panic(err) } if retries++; retries \u003e config.MaxRetry { break } log.Printf(\"Retrying, comic [%d]: %d\", comic.Id, retries) } 当 if 有两个表达式的时候，第一个是初始化，第二个才是判断。 5.1 if-else 结构： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/05.1.md 接下来就是定义错误类型 NotAllComicDownloadedError ，它需要实现 error 接口。 11.1 接口是什么： https://github.com/unknwon/the-way-to-go_ZH_CN/blob/master/eBook/11.1.md 先看看 error 接口的定义： $GOROOT/src/builtin/builtin.go // The error built-in interface type is the conventional interface for // representing an error condition, with the nil value representing no error. type error interface { Error() string } 实现： type NotAllComicDownloadedError struct { Comic Comic } func (err NotAllComicDownloadedError) Error() string { return fmt.Sprintf(\"download error: not all Comic images of [%d] are downloaded\", err.Comic.Id) } 至此已经实现了基本够用的功能，等到需要实现更多功能的时候再继续添加。 ","date":"2020-04-11","objectID":"/2020/04/11/golang-first-comic-downloader/:10:0","tags":["Go"],"title":"第一个 Go 语言程序：漫画下载器","uri":"/2020/04/11/golang-first-comic-downloader/"},{"categories":null,"content":"编译 基本上按照官方的来就行了： https://github.com/Microsoft/php-sdk-binary-tools git clone https://github.com/Microsoft/php-sdk-binary-tools.git c:\\php-sdk cd c:\\php-sdk git checkout php-sdk-2.2.0 ./phpsdk-vc15-x64.bat phpsdk_buildtree phpmaster git clone https://github.com/php/php-src.git \u0026\u0026 cd php-src phpsdk_deps --update --branch 7.4 buildconf \u0026\u0026 configure --enable-cli \u0026\u0026 nmake 注意 phpsdk_deps --update --branch 7.4 这一个命令。官方例子是 master，但我执行的时候会报： Fatal error: Uncaught SDK\\Exception: Unsupported branch 'master' 在工具里面 var_dump 查看支持的分支后发现没有 master，于是选 7.4 。 同样还是这一个命令。由于网络原因，可能会有些 zip 包下载下来后无法解压。 Fatal error: Uncaught SDK\\Exception: Failed to open 'C:\\php-sdk\\.tmp\\c2bc78629d0b6f3212ff42957cc6fb2a\\packs\\freetype-2.9.1-1-vc15-x64.zip'. in C:\\php-sdk\\lib\\php\\libsdk\\SDK\\FileOps.php:172 我尝试用下载软件下载，下载下来的文件可以正常解压。让工具直接读这个手动下载的文件就行了。 由于工具每次运行的时候，都会生成临时目录，因此把它改成固定的目录。 php-sdk\\lib\\php\\libsdk\\SDK\\FileOps.php protected function md(string $name = \"\", bool $tmp = false) : string { // ... $ret = $pre . DIRECTORY_SEPARATOR . md5(uniqid()); // ... } 把 md5(uniqid()) 改成字符串 \"tmp\"。 php-sdk\\lib\\php\\libsdk\\SDK\\Build\\Dependency\\Package.php public function retrieve(string $path) : void {/*{{{*/ $this-\u003efilepath = $path . DIRECTORY_SEPARATOR . $this-\u003ename; $cont = $this-\u003efetcher-\u003egetByUri($this-\u003egetUri()); $fd = fopen($this-\u003efilepath, \"wb\"); fwrite($fd, $cont); fclose($fd); }/*}}}*/ 在 getByUri 上一行加入： if (file_exists($this-\u003efilepath)) { return; } 然后把删除文件的代码注释掉： php-sdk\\lib\\php\\libsdk\\SDK\\Build\\Dependency\\Package.php public function cleanup() : void {/*{{{*/ // unlink($this-\u003efilepath); }/*}}}*/ 然后手动下载压缩包。先获取 URL 。 php-sdk\\lib\\php\\libsdk\\SDK\\Build\\Dependency\\Manager.php public function performUpdate(string \u0026$msg = NULL, bool $force = false, bool $backup = true) : void { // ... foreach ($series_data as $item) { echo \"Processing package $item\", PHP_EOL; $pkg = new Package($item, $this-\u003eseries, $this-\u003efetcher); $pkg-\u003eretrieve($tmp_dir_packs); $pkg-\u003eunpack($tmp_dir_deps); $pkg-\u003ecleanup(); unset($pkg); } // ... } 把循环体的内容改为： echo 'https://windows.php.net/downloads/php-sdk/deps/vc15/x64/' . $item . PHP_EOL; 这里的 vc15 和 x64 根据情况替换。后面文件名会在执行 phpsdk_deps --update --branch 7.4 的时候展示出来。哪个失败就手动下载哪个。 在 foreach 下面加个 die(); 。 执行一次命令获取所有 URL。然后批量复制到下载软件里面下载。 把压缩包放到 php-sdk\\.tmp\\tmp\\packs 底下，再次执行 phpsdk_deps --update --branch 7.4。 成功的提示： Updates performed successfully. 接下来执行： buildconf.bat 成功的提示： Rebuilding configure.js Now run 'configure --help' 接着执行： configure --disable-all --enable-cli --enable-debug 成功的提示： --------------------------------------- | | | --------------------------------------- | Build type | Debug | | Thread Safety | Yes | | Compiler | Visual C++ 2019 | | Architecture | x64 | | Optimization | disabled | | Native intrinsics | SSE2 | | Static analyzer | disabled | --------------------------------------- Type 'nmake' to build PHP 接着执行： nmake 成功的提示： SAPI sapi\\cli build complete 编译后的文件在 php-sdk\\phpmaster\\vc15\\x64\\php-src\\x64\\Debug_TS 。 进入目录执行： php -v ","date":"2020-04-01","objectID":"/2020/04/01/php-compile/:1:0","tags":["PHP"],"title":"编译和调试 PHP","uri":"/2020/04/01/php-compile/"},{"categories":null,"content":"调试 ","date":"2020-04-01","objectID":"/2020/04/01/php-compile/:2:0","tags":["PHP"],"title":"编译和调试 PHP","uri":"/2020/04/01/php-compile/"},{"categories":null,"content":"想法 我以前对于 C 语言的印象是有很强的确定性，而 PHP 在执行的时候会被翻译为 C 语言执行，所以一直很好奇 PHP 怎么调用底层函数。 换句话说就是已知函数名字的情况下如何调用 C 语言中对应名字的函数？ 解决这个问题前，首先根据过往的经验做出假设，然后再去验证。 之前在写《用 C 语言实现面向对象》的时候，就意识到使用 void 指针实现很多功能，包括指向任意的函数。接着在写《PHP 数组底层实现》的时候，了解了 HashTable 的实现，即在 C 语言层面通过字符串 key 找到任意类型值。 现在把两者结合起来，是否就能解决以上问题了？比如说把函数名作为 HashTable 的 key，函数指针作为 HashTable 的 value，这样就可以通过函数名获取函数指针来调用函数了。 接下来通过查看 PHP 的源码来看这个假设与真实情况有多少差距。 总体分为三个步骤： 从 PHP 层进入 C 语言层 找到字符串函数名与函数的关系 函数的调用 注：这篇博客的源码对应的版本是 PHP 7.4.4 。 https://github.com/php/php-src/tree/php-7.4.4 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:1:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"从 PHP 层进入 C 语言层 首先要找到 C 语言层调用函数的地方。怎么找？ 经常使用 PHP 的同学看到前面的问题描述很容易联想到 PHP 中的一个传入函数名及其参数就可以调用函数的函数 call_user_func() 。可以从这里入手。 怎么找到 call_user_func() 在 PHP 源码中的位置？这就要根据 PHP 源码的规律来找了。 当然也可以直接全代码搜索，只是比较慢。 PHP 源码里面在定义一个 PHP 函数的时候会用 PHP_FUNCTION(函数名) ，所以只要找到 PHP_FUNCTION(call_user_func) 就可以了。 另外 call_user_func() 不像 array_column() 这种函数有特定前缀 array_ ，所以属于比较基础的函数，而 PHP 的基础函数会放在两个地方： 内置函数，放在 Zend/zend_buildin_functions.c； 标准库函数，放在 ext/standard/ 。 举个例子： ext/standard/array.c 里有 array_column() 之类的函数。 在这两个地方搜索就能找到 PHP_FUNCTION(call_user_func) ，如下： ext/standard/basic_functions.c PHP_FUNCTION(call_user_func) { // ... if (zend_call_function(\u0026fci, \u0026fci_cache) == SUCCESS \u0026\u0026 Z_TYPE(retval) != IS_UNDEF) { // ... } } 现在我们已经从 PHP 层面进入到 C 语言层面，接下去就是在 C 语言代码里面探索了。 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:2:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"找到字符串函数名与函数的关系 从上文展示位于 ext/standard/basic_functions.c 的 call_user_func() 函数定义可以找到关键点 zend_call_function() ，现在要找到这个函数。 这种以 zend_ 开头的函数都在 Zend/ 文件夹底下，所以我们要换个目录了。 在 Zend/ 文件夹里面随便搜索 zend_call_function ，从搜索结果里面随便挑一个跳转，然后通过 IDE 的功能（ctrl + 鼠标左键）跳转到它定义的地方就可以了。 如果 IDE 能直接跳转就不用在 Zend/ 文件夹搜索了，这里是因为 VS Code 没法直接跳转。 注：以下代码中的 // ... 都表示我省略了一部分代码，但我会尽量保持代码结构。 第一遍看代码的时候不需要掌握所有细节，只需要了解整体概念或者前后关系，否则会陷入细节无法自拔。 Zend/zend_execute_API.c int zend_call_function(zend_fcall_info *fci, zend_fcall_info_cache *fci_cache) /* {{{ */ { // ... if (!fci_cache || !fci_cache-\u003efunction_handler) { // ... if (!zend_is_callable_ex(\u0026fci-\u003efunction_name, fci-\u003eobject, IS_CALLABLE_CHECK_SILENT, NULL, fci_cache, \u0026error)) { // ... } // ... } func = fci_cache-\u003efunction_handler; // ... call = zend_vm_stack_push_call_frame(call_info, func, fci-\u003eparam_count, object_or_called_scope); // ... if (func-\u003etype == ZEND_USER_FUNCTION) { // ... } else if (func-\u003etype == ZEND_INTERNAL_FUNCTION) { // ... func-\u003einternal_function.handler(call, fci-\u003eretval); // ... } else { // ... } // ... return SUCCESS; } /* }}} */ 这里的关键点在于和函数名以及函数调用相关的词。关键词有： function name call return value 上面的代码片段中，我把几个有可能的点抽出来了。从这几个点出发，往前追溯参数来源或者查看后面使用它的地方就行了。 如果被这个函数里面大量的 EG(...) 吸引而想知道其内部结构的话，就离结果非常近了。如果没有被其吸引，那也没关系，继续看。 优先深入看哪个呢？根据以前看数组源码的经验， “查找” 这个行为更容易获得信息，于是先看 zend_is_callable_check_func() 。 Zend/zend_API.c static zend_always_inline int zend_is_callable_check_func(int check_flags, zval *callable, zend_fcall_info_cache *fcc, int strict_class, char **error) /* {{{ */ { // ... if (!ce_org) { // ... /* Check if function with given name exists. * This may be a compound name that includes namespace name */ if (UNEXPECTED(Z_STRVAL_P(callable)[0] == '\\\\')) { // ... func = zend_fetch_function(lmname); // ... } // ... } // ... } zend_fetch_function() 与我们想要的答案有很强的相关性，看它怎么实现的。 Zend/zend_execute.c ZEND_API zend_function * ZEND_FASTCALL zend_fetch_function(zend_string *name) { zval *zv = zend_hash_find(EG(function_table), name); // ... } 来了来了！在这里就可以看到函数的确存在于 HashTable 里面。而这个 HashTable 通过 EG 获取。 Zend/zend_globals_macros.h # define EG(v) (executor_globals.v) 再跳转一次。 Zend/zend_compile.c ZEND_API zend_executor_globals executor_globals; zend_executor_globals 是一个结构体。 PHP 的源码中，结构体的真实定义会以下划线开头。 于是找 _zend_executor_globals 。 Zend/zend_globals.h struct _zend_executor_globals { // ... HashTable *function_table; /* function symbol table */ HashTable *class_table; /* class table */ HashTable *zend_constants; /* constants table */ // ... } 到这里就找到存储函数的地方了。验证了函数名作为 key，函数指针作为 value 的可行性。 不过 PHP 并没有把函数指针直接作为 value，而是包装到了 zval 里面，以实现更多功能。从下面这一句就可以看出。 zval *zv = zend_hash_find(EG(function_table), name); 看看 zval 里面有什么。 Zend/zend_types.h typedef struct _zval_struct zval; struct _zval_struct { zend_value value; /* value */ // ... }; 继续： Zend/zend_types.h typedef union _zend_value { zend_long lval; /* long value */ double dval; /* double value */ zend_refcounted *counted; zend_string *str; zend_array *arr; zend_object *obj; zend_resource *res; zend_reference *ref; zend_ast_ref *ast; zval *zv; void *ptr; zend_class_entry *ce; zend_function *func; struct { uint32_t w1; uint32_t w2; } ww; } zend_value; 注：这个结构体很重要，我保留了全貌。 看到 zend_function 这个结构体，搜索 _zend_function 。 union _zend_function { // ... zend_internal_function internal_function; }; 在 zend_value 联合体中可以看到 zend_internal_function 这个内部函数专用结构体，调用内部函数时用到它。搜索 _zend_internal_function。 Zend/zend_compile.h /* zend_internal_function_handler */ typedef void (ZEND_FASTCALL *zif_handler)(INTERNAL_FUNCTION_PARAMETERS); typedef struct _zend_internal_function { /* Common elements */ zend_uchar type; zend_uchar arg_flags[3]; /* bitset of arg_info.pass_by_reference */ uint32_t fn_flags; zend_string* function_name; zend_class_entry *scope; zend_function *prototype; uint32_t num_args; uint32_t required_num_args; zend_internal_arg_info *arg_info; /* END of common elements */ zif_handler handler; struct _zend_module_entry *module; void *reserved[ZEND_MAX_RESERVED_RESOURCES]; } zend_internal_function; 结构体 _zend_internal_fun","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:3:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"函数的调用 现在知道函数指针是存放在 handler 里面了，接着就是找到使用它的地方。 此时再回过头看 zend_call_function 这个函数。 Zend/zend_execute_API.c int zend_call_function(zend_fcall_info *fci, zend_fcall_info_cache *fci_cache) /* {{{ */ { // ... if (func-\u003etype == ZEND_USER_FUNCTION) { // ... } else if (func-\u003etype == ZEND_INTERNAL_FUNCTION) { // ... func-\u003einternal_function.handler(call, fci-\u003eretval); // ... } // ... } /* }}} */ 可以看到调用函数的地方： func-\u003einternal_function.handler(call, fci-\u003eretval); handler 的参数固定是两个。这里要结合之前的 PHP_FUNCTION(call_user_func) 来看。 为了将 PHP_FUNCTION(call_user_func) 展开，以下连续列出三个定义： main/php.h #define PHP_FUNCTION ZEND_FUNCTION Zend/zend_API.h #define ZEND_FN(name) zif_##name #define ZEND_MN(name) zim_##name #define ZEND_NAMED_FUNCTION(name) void ZEND_FASTCALL name(INTERNAL_FUNCTION_PARAMETERS) #define ZEND_FUNCTION(name) ZEND_NAMED_FUNCTION(ZEND_FN(name)) Zend/zend.h #define INTERNAL_FUNCTION_PARAMETERS zend_execute_data *execute_data, zval *return_value 根据这三个地方的代码展开 PHP_FUNCTION(call_user_func) 可以得到： void ZEND_FASTCALL call_user_func(zend_execute_data *execute_data, zval *return_value) 再看一次 func-\u003einternal_function.handler(call, fci-\u003eretval); 。联系起来了！ ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:4:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"函数调用真正的入口 上文以 PHP_FUNCTION(call_user_func) 作为入口只是其中一种思路。实际上 PHP 在调用函数的时候不是通过 call_user_func ，不然 call_user_func 本身又是如何被调用的呢？ PHP 执行的时候，会在 PHP 虚拟机里面去调用函数。PHP 虚拟机首先会读取 PHP 文件，然后解析为 OPCode （操作码）执行。这里就要借助调试器的力量了。 这里跳过 OPCode 的生成，因为与本次要探索的内容关系不是很大。 开启调试。然后不断往下走，可以找到一个比较接近答案的地方。 Zend/zend_vm_execute.h ZEND_API void zend_execute(zend_op_array *op_array, zval *return_value) { zend_execute_data *execute_data; // ... i_init_code_execute_data(execute_data, op_array, return_value); zend_execute_ex(execute_data); zend_vm_stack_free_call_frame(execute_data); } 先看 zend_execute_ex ： Zend/zend_vm_execute.h // ... # define OPLINE EX(opline) // ... # define ZEND_OPCODE_HANDLER_ARGS_PASSTHRU execute_data // ... ZEND_API void execute_ex(zend_execute_data *ex) { DCL_OPLINE // ... zend_execute_data *execute_data = ex; // ... LOAD_OPLINE(); ZEND_VM_LOOP_INTERRUPT_CHECK(); // ... while (1) { // ... int ret; // ... if (UNEXPECTED((ret = ((opcode_handler_t)OPLINE-\u003ehandler)(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU)) != 0)) { // ... if (EXPECTED(ret \u003e 0)) { execute_data = EG(current_execute_data); ZEND_VM_LOOP_INTERRUPT_CHECK(); } else { // ... return; } // ... } } zend_error_noreturn(E_CORE_ERROR, \"Arrived at end of main loop which shouldn't happen\"); } 又看到了 handler，这里难道就是真正执行函数的地方？ 先找到 OPLINE 的真身，根据： Zend/zend_compile.h #define EX(element) ((execute_data)-\u003eelement) 对 OPLINE 展开后，得到 execute_data-\u003eopline 。 再根据 execute_ex() 前面的定义对整行展开得到： if (UNEXPECTED((ret = ((opcode_handler_t)(execute_data-\u003eopline)-\u003ehandler)(execute_data)) != 0)) 现在出现四个新问题： opline 的 handler 存在哪个结构体？ opline 的 handler 指向哪些函数？ opline 的 handler 在哪里被赋值？ 调用 opline 的 handler 就真的开始执行函数了吗？ ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:5:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"opline 的 handler 存在哪个结构体？ 要解决这个问题，得先找到 opline 是哪来的。 回到 Zend/zend_vm_execute.h 的 zend_execute() ： Zend/zend_vm_execute.h ZEND_API void zend_execute(zend_op_array *op_array, zval *return_value) { zend_execute_data *execute_data; // ... i_init_code_execute_data(execute_data, op_array, return_value); zend_execute_ex(execute_data); zend_vm_stack_free_call_frame(execute_data); } 在 zend_execute_ex() 前面有个 i_init_code_execute_data() ： Zend/zend_execute.c static zend_always_inline void i_init_code_execute_data(zend_execute_data *execute_data, zend_op_array *op_array, zval *return_value) /* {{{ */ { // ... EX(opline) = op_array-\u003eopcodes; // ... } opline 来自于 zend_op_array 的 opcodes ，搜索 _zend_op_array 。 Zend/zend_compile.h struct _zend_op_array { // ... zend_op *opcodes; // ... }; opcodes 是 zend_op 这种结构体，搜索 _zend_op 。 Zend/zend_compile.h struct _zend_op { const void *handler; znode_op op1; znode_op op2; znode_op result; uint32_t extended_value; uint32_t lineno; zend_uchar opcode; zend_uchar op1_type; zend_uchar op2_type; zend_uchar result_type; }; 到这里就找到了 handler 存储的位置。 注：在 Zend/zend_vm_opcodes.h 可以找到 OPCode 对应的整数，在 Zend/zend_vm_opcodes.c 可以找到这些整数和字符串的对应。 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:5:1","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"opline 的 handler 指向哪些函数？ 由于 handler 是函数指针，可以指向任意函数，所以无法直接定位。于是通过调试执行下面这一句来找一些线索： Zend/zend_vm_execute.h ZEND_API void execute_ex(zend_execute_data *ex) { // ... while (1) { // ... if (UNEXPECTED((ret = ((opcode_handler_t)OPLINE-\u003ehandler)(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU)) != 0)) { // ... } } // ... } 在这一句的位置使用 “jump into”，会跳转到一个函数。这个函数就是 handler 指向的函数了。 由于每次跳到的函数都可能不一样，所以选其中一个来查。 Zend/zend_vm_execute.h static ZEND_VM_HOT ZEND_OPCODE_HANDLER_RET ZEND_FASTCALL ZEND_INIT_FCALL_SPEC_CONST_HANDLER(ZEND_OPCODE_HANDLER_ARGS) { // ... } 搜索函数名 ZEND_INIT_FCALL_SPEC_CONST_HANDLER。 Zend/zend_vm_execute.h void zend_vm_init(void) { static const void * const labels[] = { // ... ZEND_INIT_FCALL_SPEC_CONST_HANDLER, // ... }; static const uint32_t specs[] = { // ... }; // ... zend_opcode_handlers = labels; zend_handlers_count = sizeof(labels) / sizeof(void*); zend_spec_handlers = specs; // ... } handler 可以指向 labels 里面包含的所有函数。 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:5:2","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"opline 的 handler 在哪里被赋值？ 上一节列出的 zend_vm_init() 把所有函数都放到了 labels 数组里面，并赋值给了 zend_opcode_handlers ，找找用到它的地方。 Zend/zend_vm_execute.h static const void* ZEND_FASTCALL zend_vm_get_opcode_handler_ex(uint32_t spec, const zend_op* op) { // ... return zend_opcode_handlers[(spec \u0026 SPEC_START_MASK) + offset]; } 如果搜索调用 zend_vm_get_opcode_handler_ex 的代码，那么就很容易找到给 handler 赋值的地方了。 Zend/zend_vm_execute.h ZEND_API void ZEND_FASTCALL zend_vm_set_opcode_handler(zend_op* op) { // ... op-\u003ehandler = zend_vm_get_opcode_handler_ex(zend_spec_handlers[opcode], op); } ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:5:3","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"调用 opline 的 handler 就真的开始执行函数了吗？ 把上面举的例子 handler 指向的函数 ZEND_INIT_FCALL_SPEC_CONST_HANDLER 再拿出来。 为了更加明显，此处不省略代码。 Zend/zend_vm_execute.h static ZEND_VM_HOT ZEND_OPCODE_HANDLER_RET ZEND_FASTCALL ZEND_INIT_FCALL_SPEC_CONST_HANDLER(ZEND_OPCODE_HANDLER_ARGS) { USE_OPLINE zval *fname; zval *func; zend_function *fbc; zend_execute_data *call; fbc = CACHED_PTR(opline-\u003eresult.num); if (UNEXPECTED(fbc == NULL)) { fname = (zval*)RT_CONSTANT(opline, opline-\u003eop2); func = zend_hash_find_ex(EG(function_table), Z_STR_P(fname), 1); if (UNEXPECTED(func == NULL)) { ZEND_VM_TAIL_CALL(zend_undefined_function_helper_SPEC(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU)); } fbc = Z_FUNC_P(func); if (EXPECTED(fbc-\u003etype == ZEND_USER_FUNCTION) \u0026\u0026 UNEXPECTED(!RUN_TIME_CACHE(\u0026fbc-\u003eop_array))) { init_func_run_time_cache(\u0026fbc-\u003eop_array); } CACHE_PTR(opline-\u003eresult.num, fbc); } call = _zend_vm_stack_push_call_frame_ex( opline-\u003eop1.num, ZEND_CALL_NESTED_FUNCTION, fbc, opline-\u003eextended_value, NULL); call-\u003eprev_execute_data = EX(call); EX(call) = call; ZEND_VM_NEXT_OPCODE(); } 从中看不到执行的地方。找到的 func 也只是被放入 fcb，然后 push 到虚拟机调用栈里面。 注：这里另一个值得注意的地方是 ZEND_VM_NEXT_OPCODE(); 。因为最开始的 execute_ex 函数（下一节列出了代码）里面只是一个死循环，且没有修改 OPLINE 的指向，而是在这些 handler 函数里面修改。 那真正调用函数的地方在哪呢？ ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:5:4","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"真正调用函数的地方 回到最开始的 execute_ex() 。 Zend/zend_vm_execute.h ZEND_API void execute_ex(zend_execute_data *ex) { DCL_OPLINE // ... zend_execute_data *execute_data = ex; // ... LOAD_OPLINE(); ZEND_VM_LOOP_INTERRUPT_CHECK(); // ... while (1) { // ... int ret; // ... if (UNEXPECTED((ret = ((opcode_handler_t)OPLINE-\u003ehandler)(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU)) != 0)) { // ... if (EXPECTED(ret \u003e 0)) { execute_data = EG(current_execute_data); ZEND_VM_LOOP_INTERRUPT_CHECK(); } else { // ... return; } // ... } } zend_error_noreturn(E_CORE_ERROR, \"Arrived at end of main loop which shouldn't happen\"); } 通过调试可以知道，如果是一些简单的操作， handler 就会直接处理。比如加减法。但是像函数调用这种，就不会在 handler 这里处理。 那么只能看下面的代码。 只有当 ret 大于 0 的时候会有额外的操作。通过调试可以看到有以下几个大于 0 的情况。 Zend/zend_vm_execute.h # define ZEND_VM_ENTER_EX() return 1 # define ZEND_VM_ENTER() return 1 # define ZEND_VM_LEAVE() return 2 这个信息没有多大影响。 那么接下来就得看 ZEND_VM_LOOP_INTERRUPT_CHECK(); 了。 Zend/zend_execute.c #define ZEND_VM_LOOP_INTERRUPT_CHECK() do { \\ if (UNEXPECTED(EG(vm_interrupt))) { \\ ZEND_VM_LOOP_INTERRUPT(); \\ } \\ } while (0) 继续： Zend/zend_vm_execute.h #define ZEND_VM_LOOP_INTERRUPT() zend_interrupt_helper_SPEC(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU); 继续： Zend/zend_vm_execute.h static zend_never_inline ZEND_OPCODE_HANDLER_RET ZEND_FASTCALL zend_interrupt_helper_SPEC(ZEND_OPCODE_HANDLER_ARGS) { EG(vm_interrupt) = 0; if (EG(timed_out)) { zend_timeout(0); } else if (zend_interrupt_function) { SAVE_OPLINE(); zend_interrupt_function(execute_data); ZEND_VM_ENTER(); } ZEND_VM_CONTINUE(); } 搜索 zend_interrupt_function 发现它是一个函数指针。那么转成搜索 zend_interrupt_function = ，看看哪个函数的指针传给了它。 这时搜索到了两条线。一条是 ext/pcntl/pcntl.c，另一条是 win32/signal.c。 这里选 win32/signal.c： win32/signal.c PHP_WINUTIL_API void php_win32_signal_ctrl_handler_init(void) {/*{{{*/ // ... zend_interrupt_function = php_win32_signal_ctrl_interrupt_function; // ... }/*}}}*/ 接着找函数 php_win32_signal_ctrl_interrupt_function 。 win32/signal.c static void php_win32_signal_ctrl_interrupt_function(zend_execute_data *execute_data) {/*{{{*/ if (IS_UNDEF != Z_TYPE(ctrl_handler)) { zval retval, params[1]; ZVAL_LONG(\u0026params[0], ctrl_evt); /* If the function returns, */ call_user_function(NULL, NULL, \u0026ctrl_handler, \u0026retval, 1, params); zval_ptr_dtor(\u0026retval); } if (orig_interrupt_function) { orig_interrupt_function(execute_data); } }/*}}}*/ 感觉很接近了。 call_user_function 传了两个 NULL，为了避免理解上有偏差，把它的定义列出来。 Zend/zend_API.h #define call_user_function(function_table, object, function_name, retval_ptr, param_count, params) \\ _call_user_function_ex(object, function_name, retval_ptr, param_count, params, 1) 继续： Zend/zend_execute_API.c int _call_user_function_ex(zval *object, zval *function_name, zval *retval_ptr, uint32_t param_count, zval params[], int no_separation) /* {{{ */ { zend_fcall_info fci; fci.size = sizeof(fci); fci.object = object ? Z_OBJ_P(object) : NULL; ZVAL_COPY_VALUE(\u0026fci.function_name, function_name); fci.retval = retval_ptr; fci.param_count = param_count; fci.params = params; fci.no_separation = (zend_bool) no_separation; return zend_call_function(\u0026fci, NULL); } 绕了一圈还是绕回来了。又一次见到 zend_call_function 。上文已经分析过这个函数了，不再重复。 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:5:5","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"小结 本文通过假设 PHP 函数调用方式和查询源码验证，得到了 PHP 底层将 C 语言函数存储到 HashTable 然后通过函数名字找到函数指针来调用这一结论。同时也了解了 PHP 函数执行的大致流程。 虽然了解了也没什么用的样子，但好奇心得到了满足 233 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-call/:6:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数调用","uri":"/2020/03/30/php-function-call/"},{"categories":null,"content":"函数的注册 调用顺序为： - main -- php_cli_startup --- php_module_startup ---- zend_startup ----- zend_startup_builtin_function ------ zend_register_module_ex(*module) ------- zend_register_functions(module-\u003efunctions) sapi/cli/php_cli.c int main(int argc, char *argv[]) { // ... sapi_module_struct *sapi_module = \u0026cli_sapi_module; // ... /* startup after we get the above ini override se we get things right */ if (sapi_module-\u003estartup(sapi_module) == FAILURE) { // ... } // ... exit_status = do_cli(argc, argv); // ... } main/SAPI.h typedef struct _sapi_module_struct sapi_module_struct; struct _sapi_module_struct { char *name; char *pretty_name; int (*startup)(struct _sapi_module_struct *sapi_module); // ... } sapi/cli/php_cli.c static sapi_module_struct cli_sapi_module = { \"cli\", /* name */ \"Command Line Interface\", /* pretty name */ php_cli_startup, /* startup */ // ... const zend_function_entry *additional_functions; // ... } sapi/cli/php_cli.c static int php_cli_startup(sapi_module_struct *sapi_module) { if (php_module_startup(sapi_module, NULL, 0)==FAILURE) { return FAILURE; } return SUCCESS; } main/main.c int php_module_startup(sapi_module_struct *sf, zend_module_entry *additional_modules, uint32_t num_additional_modules) { // ... zend_startup(\u0026zuf); // ... } Zend/zend.c int zend_startup(zend_utility_functions *utility_functions) /* {{{ */ { // ... zend_startup_builtin_functions(); // ... } Zend/zend_builtin_functions.c int zend_startup_builtin_functions(void) /* {{{ */ { zend_builtin_module.module_number = 0; zend_builtin_module.type = MODULE_PERSISTENT; return (EG(current_module) = zend_register_module_ex(\u0026zend_builtin_module)) == NULL ? FAILURE : SUCCESS; } Zend/zend_builtin_functions.c zend_module_entry zend_builtin_module = { /* {{{ */ STANDARD_MODULE_HEADER, \"Core\", builtin_functions, ZEND_MINIT(core), NULL, NULL, NULL, NULL, ZEND_VERSION, STANDARD_MODULE_PROPERTIES }; Zend/zend_builtin_functions.c static const zend_function_entry builtin_functions[] = { /* {{{ */ ZEND_FE(zend_version, arginfo_zend__void) ZEND_FE(func_num_args, arginfo_zend__void) ZEND_FE(func_get_arg, arginfo_func_get_arg) ZEND_FE(func_get_args, arginfo_zend__void) ZEND_FE(strlen, arginfo_strlen) // ... ZEND_FE(function_exists, arginfo_function_exists) // ... } Zend/zend_API.c ZEND_API zend_module_entry* zend_register_module_ex(zend_module_entry *module) { // ... if (module-\u003efunctions \u0026\u0026 zend_register_functions(NULL, module-\u003efunctions, NULL, module-\u003etype)==FAILURE) { // ... } // ... } Zend/zend_API.c /* registers all functions in *library_functions in the function hash */ ZEND_API int zend_register_functions(zend_class_entry *scope, const zend_function_entry *functions, HashTable *function_table, int type) { // ... if (!target_function_table) { target_function_table = CG(function_table); } // ... while (ptr-\u003efname) { // ... memcpy(reg_function, \u0026function, sizeof(zend_internal_function)); if (zend_hash_add_ptr(target_function_table, lowercase_name, reg_function) == NULL) { // ... } // ... } } 基本函数的注册到这里就基本结束了。 但是像 array_column() 这种函数不属于内置函数，它属于 standard 扩展的函数。那么它在哪里被注册呢？ main/main.c int php_module_startup(sapi_module_struct *sf, zend_module_entry *additional_modules, uint32_t num_additional_modules) { // ... /* startup extensions statically compiled in */ if (php_register_internal_extensions_func() == FAILURE) { php_printf(\"Unable to start builtin modules\\n\"); return FAILURE; } // ... } 看到这里就基本结束了。其他细节有兴趣可以继续研究。 结构体： Zend/zend_modules.h typedef struct _zend_module_entry zend_module_entry; struct _zend_module_entry { // ... const struct _zend_function_entry *functions; // ... } ext/standard/basic_functions.c zend_module_entry basic_functions_module = { // ... \"standard\", /* extension name */ basic_functions, /* function list */ // ... }; ext/standard/basic_functions.h extern zend_module_entry basic_functions_module; ","date":"2020-03-30","objectID":"/2020/03/30/php-function-regist/:0:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数注册","uri":"/2020/03/30/php-function-regist/"},{"categories":null,"content":"后记 最开始写这篇文章的时候，是靠分析和 VS Code 的查找功能来理清楚调用顺序的。结果在 php_module_startup 这一层里面找的时候，没注意 zend_startup 就跳过这一行找下面了。导致绕了个弯。最后决定编译 PHP 源码，用调试的方式看源码，就像当初学习 Laravel 那样。这样看起源码来容易多了，而且以后看源码也能减少很多负担。 在开启调试后，回头又补充了前面关于函数调用的解释。最开始使用 array_key_exist() 来查看调用栈，没想到 PHP 并没有调用 ext/standard/array.c 里面 array_key_exist 的代码，而是直接使用 Zend/zend_execute.c 里的 zend_array_key_exists_fast，让我多花了一些时间。 ","date":"2020-03-30","objectID":"/2020/03/30/php-function-regist/:1:0","tags":["PHP","源码"],"title":"【PHP 源码】PHP 函数注册","uri":"/2020/03/30/php-function-regist/"},{"categories":null,"content":"ext/standard/array.c /* {{{ proto bool sort(array \u0026array_arg [, int sort_flags]) Sort an array */ PHP_FUNCTION(sort) { zval *array; zend_long sort_type = PHP_SORT_REGULAR; compare_func_t cmp; ZEND_PARSE_PARAMETERS_START(1, 2) Z_PARAM_ARRAY_EX(array, 0, 1) Z_PARAM_OPTIONAL Z_PARAM_LONG(sort_type) ZEND_PARSE_PARAMETERS_END_EX(RETURN_FALSE); cmp = php_get_data_compare_func(sort_type, 0); // 根据 sort_flags 选择比较函数 if (zend_hash_sort(Z_ARRVAL_P(array), cmp, 1) == FAILURE) { RETURN_FALSE; } RETURN_TRUE; } 接下来找 zend_hash_sort 。 Zend/zend_hash.h #define zend_hash_sort(ht, compare_func, renumber) \\ zend_hash_sort_ex(ht, zend_sort, compare_func, renumber) 用于排序的算法是 zend_sort 。 Zend/zend_sort.c ZEND_API void zend_sort(void *base, size_t nmemb, size_t siz, compare_func_t cmp, swap_func_t swp) 该排序算法来自于 LLVM 项目的 libc++ 实现。 https://github.com/llvm/llvm-project/blob/master/libcxx/include/algorithm template \u003cclass _Compare, class _RandomAccessIterator\u003e void __sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) ","date":"2020-03-30","objectID":"/2020/03/30/php-sort/:0:0","tags":["PHP","源码"],"title":"PHP sort 源码","uri":"/2020/03/30/php-sort/"},{"categories":null,"content":"快排 快排是实践中最快的已知排序算法。平均时间复杂度是 O(N log N)，平均空间复杂度是 O(log N)。 空间复杂度主要是递归使用的栈空间。最优递归深度是 log N 。 思路： 在数组中选一个元素作为中心元素，把数组元素划分为小于中心元素的集合以及大于中心元素的集合，两个集合放到中心元素的两边。然后再对两个集合分别划分，一直划分到元素个数为 1 或 0 的时候才停下。 优化： 小数组 元素较少（N \u003c= 20）时，快排不如插入排序。使用插入排序可以节约大约 15% 的运行时间。 就算有很多元素，经过不断的划分，总会有元素个数小于等于 20 的时候。 中心元素的选取 选最左边或者最右边是最简单的，但是很容易导致花费很多时间。例如数组已经有序的情况下。 随机选取会更好一些，但随机数的生成也比较耗时。 三数中值。取左、中、右位置的三个值，取中数作为中心元素。这是最好的选择，节约快排大约 5% 的运行时间。 zend_sort 是怎么做的？ 元素小于等于 16 个的时候，使用插入排序 if (nmemb \u003c= 16) { zend_insert_sort(base, nmemb, siz, cmp, swp); return; } 元素大于 16 个的时候，使用快排的指针交换法 左右两个指针分别往中间移动，直到左边指针指向大于中心元素，右边指针小于中心元素，然后左右指针值互换，再继续移动。 还有一种是挖坑法。 中心元素的选取根据元素个数分为两种情况： 个数小于 1024 时，将数组做一次平分，取中间数和两边缘数，如 |...|...| 的竖线部分，然后取三数中值。 个数大于等于 1024 时，将数组做两次平分，取三个中间数和两边缘数，如 |...|...|...|...| 的竖线部分，然后取五数中值。 个数非常大时，可能会因为划分后元素个数仍大于 1024 而再取五数中值。 ","date":"2020-03-30","objectID":"/2020/03/30/php-sort/:1:0","tags":["PHP","源码"],"title":"PHP sort 源码","uri":"/2020/03/30/php-sort/"},{"categories":null,"content":"从数组入手。 数组相关函数是 PHP 扩展的一部分，扩展名为 standard，执行 php -m 可以看到该扩展。 该扩展的源码在 PHP 源码包的 ext/standard ，其中数组相关函数的定义在 ext/standard/array.c 。 ","date":"2020-03-30","objectID":"/2020/03/30/read-php-src/:0:0","tags":["PHP","源码"],"title":"阅读 PHP 源码","uri":"/2020/03/30/read-php-src/"},{"categories":null,"content":"创建对象： zend_objects.c ZEND_API zend_object* ZEND_FASTCALL zend_objects_new(zend_class_entry *ce) { zend_object *object = emalloc(sizeof(zend_object) + zend_object_properties_size(ce)); _zend_object_std_init(object, ce); object-\u003ehandlers = \u0026std_object_handlers; return object; } zend_object_handlers.c ZEND_API const zend_object_handlers std_object_handlers = { 0, /* offset */ zend_object_std_dtor, /* free_obj */ zend_objects_destroy_object, /* dtor_obj */ zend_objects_clone_obj, /* clone_obj */ zend_std_read_property, /* read_property */ zend_std_write_property, /* write_property */ zend_std_read_dimension, /* read_dimension */ zend_std_write_dimension, /* write_dimension */ zend_std_get_property_ptr_ptr, /* get_property_ptr_ptr */ NULL, /* get */ NULL, /* set */ zend_std_has_property, /* has_property */ zend_std_unset_property, /* unset_property */ zend_std_has_dimension, /* has_dimension */ zend_std_unset_dimension, /* unset_dimension */ zend_std_get_properties, /* get_properties */ zend_std_get_method, /* get_method */ NULL, /* call_method */ zend_std_get_constructor, /* get_constructor */ zend_std_get_class_name, /* get_class_name */ zend_std_compare_objects, /* compare_objects */ zend_std_cast_object_tostring, /* cast_object */ NULL, /* count_elements */ zend_std_get_debug_info, /* get_debug_info */ zend_std_get_closure, /* get_closure */ zend_std_get_gc, /* get_gc */ NULL, /* do_operation */ NULL, /* compare */ NULL, /* get_properties_for */ }; ","date":"2020-03-30","objectID":"/2020/03/30/php-object/:0:0","tags":["PHP","对象"],"title":"PHP 对象","uri":"/2020/03/30/php-object/"},{"categories":null,"content":" 每个值容器（zend_value）里面都会有引用计数结构体 zend_rrefcounted 引用计数减少到 0 时进行清除 unset() 将值的引用减少 1 值容器引用在减少 1 后如果不为 0 ，则加入回收池 回收池满后，开始执行回收 第一次遍历回收池所有值容器。对所有值容器的引用减 1 。如果是嵌套对象（如数组和对象），则遍历其元素，元素值容器的引用减 1 。 这是一个深度遍历的过程。 这里是模拟减 1，减到 0 时不触发回收 第二次遍历回收池所有值容器。对引用计数不为 0 的值容器引用加 1 （恢复到之前的样子），并将其从回收池中移出。 回收池中引用计数为 0 的值容器被作为垃圾清除。 如： \u003c?php $arr = []; $arr[0] = 'gc'; xdebug_debug_zval('arr'); // 输出： // arr: (refcount=1, is_ref=0)=array (0 =\u003e (interned, is_ref=0)='gc') $arr[1] = \u0026$arr; xdebug_debug_zval('arr'); // 输出： // arr: (refcount=2, is_ref=1)=array (0 =\u003e (interned, is_ref=0)='gc', 1 =\u003e (refcount=2, is_ref=1)=...) $arr[2] = \u0026$arr; xdebug_debug_zval('arr'); // 输出： // arr: (refcount=3, is_ref=1)=array (0 =\u003e (interned, is_ref=0)='gc', 1 =\u003e (refcount=3, is_ref=1)=..., 2 =\u003e (refcount=3, is_ref=1)=...) $arr[3] = $arr; xdebug_debug_zval('arr'); // 输出： // arr: (refcount=5, is_ref=1)=array (0 =\u003e (interned, is_ref=0)='gc', 1 =\u003e (refcount=5, is_ref=1)=..., 2 =\u003e (refcount=5, is_ref=1)=..., 3 =\u003e (refcount=1, is_ref=0)=array (0 =\u003e (interned, is_ref=0)='gc', 1 =\u003e (refcount=5, is_ref=1)=..., 2 =\u003e (refcount=5, is_ref=1)=...)) unset($arr); xdebug_debug_zval('arr'); // 输出： // arr: no such symbol unset 前，$arr 引用计数为 5 。 unset 将 $arr 引用计数减 1 ，得到 $arr 引用计数 4。 遍历 $arr 。 将 $arr[0] 值容器引用计数减 1，不影响 $arr 。 将 $arr[1] 值容器引用计数减 1，得到 $arr 引用计数 3 。 将 $arr[2] 值容器引用计数减 1，得到 $arr 引用计数 2 。 将 $arr[3] 值容器引用计数减 1。 遍历 $arr[3] 。 将 $arr[3][1] 引用计数减 1，得到 $arr 引用计数 1 。 将 $arr[3][2] 引用计数减 1，得到 $arr 引用计数 0 。 回收 $arr 。 ","date":"2020-03-30","objectID":"/2020/03/30/php-gc/:0:0","tags":["PHP","GC"],"title":"PHP 垃圾回收","uri":"/2020/03/30/php-gc/"},{"categories":null,"content":"上一篇说明了选择 Camunda 的理由。这一篇说明如何实现适配层。 当前还没有专门写一篇对 Camunda 各个功能的详细介绍。如果要获得比较直观的感受，可以下载 Modeler 或者使用在线版的 Modeler 。 https://demo.bpmn.io/ 目录： 为什么要做适配层？ 要对 Camunda 做扩充的部分 数据表的扩充 流程实例的操作 前端动态表单的渲染 结语 为什么要做适配层？ 现有引擎无法满足业务 如果能满足，加个代理层就够了。 避免改源码导致升级困难 我们部门有个基于 k8s 源码修改的项目，当年拿过公司的大奖。现在由于没人维护的来，已经凉了。 可以兼容其他流程引擎 当前选的引擎即使能满足当前业务需要，但未必满足其他业务的需要。更何况要形成一个基础设施给其他组件使用。 +---------+ | | | System | | | +--+--+---+ | ^ v | +--+--+---+ | | | Adapter | | | +--+--+---+ | ^ v | +--+--+---+ | | | Camunda | | | +---------+ 要对 Camunda 做扩充的部分 各业务系统有自己的系统界面，也有各自需要展示的内容，不能直接使用 Camunda 自带的管理界面。 Camunda 内置的表单支持使得业务系统对其有一定的依赖。要改成在业务层处理。 Camunda 在一次请求跳转时只能对原任务和目标任务同时应用或者不应用参数检测。但其实应该要在取消原任务的执行时不检测参数，在创建目标任务的执行时检测参数，因此要分两次。 流程和流程实例 ID 是 UUID，但要展示整数 ID 给用户。 流程实例自动化任务出错时，要转交给运维处理。 制定 External Task Client 的规则 数据表的扩充 为了适应业务需要，需要另外创建几张数据表： 流程定义信息表 流程实例信息表 流程节点日志表 服务定义表 服务请求客户端管理表 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:0","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"流程定义信息表 无论是团队内部沟通还是和需求方沟通，最经常用到的用于区别流程的信息是流程的 ID。大家都不太喜欢用流程的名称来沟通，除非是比较少见的流程。而且在各种文档中也是使用流程的 ID。因此这个 ID 信息需要展示给用户。 由于是从旧系统迁移过来的，所以要保证原先流程的 ID 不变。所以不适用自增 ID 作为流程 ID，而是新增一列专门存储这些 ID。 以下 pd 表示 process definition，e 表示 engine 。 字段 作用 id 自增 ID pd_id 流程 ID，兼容旧系统 e_pd_key 底层流程引擎的流程名称，也用于翻译后作为名称展示 e_pd_version 当前使用的流程版本号 e_pd_version_max 底层流程引擎的流程最高版本号 category 流程分类 maintainer 维护人 doc_link 文档地址 note 备注 Camunda 可以通过流程名称和流程版本确定一个特定的流程定义 UUID，所以不需要在这里加上 UUID。 如果要加入其他引擎，且这些引擎不是用 key 和 version 确定，则可以加入 UUID 字段。并且还需要加入 engine 字段用于标识使用哪个引擎。 当新版本发布时，e_pd_version_max 加一， e_pd_version 保持不变，只能人为修改。这样便于做测试。 这一部分没有什么特别的点，实现起来不难。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:1","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"流程实例信息表 pi 表示 process instance 字段 作用 id 自增 ID，作为流程实例 ID pd_id 流程 ID e_pi_id 流程引擎的流程实例 ID，用于与流程引擎的实例保持关联 e_pd_key 底层流程引擎的流程名称，也用于翻译后作为名称展示 e_pd_version 当前使用的流程版本号 e_pi_activity_name 当前所在的节点名称 name 流程实例名称，由创建人填写 creator 创建人 source 创建来源，其他系统可调用 Restful API 创建流程实例 priority 优先级 handlers 处理人，可以有多个 status 当前流程实例状态 tags 标签，作为补充信息 流程实例状态： 类型 作用 创建 流程实例刚创建 待提交表单 需要等待用户提交表单 等待执行器 自动化执行步骤，需要等待 External Task Client 获取任务 执行中 任务执行中 出错 执行时报错 暂停 暂停流程实例执行 结束 正常流程结束 终止 人工关闭流程或者其他非正常结束关闭 自增 ID： 一开始打算直接用 UUID 作为流程实例的 ID，但产品经理说使用跟原系统一样的数字 ID 比较好，用户用起来也比较习惯。 举个例子，这个有点像 B 站以前用 AV 号而现在用 BV 号给用户带来的区别。 由于流程实例本身有权限控制，用自增 ID 也爬不了多少。就算爬了也没有什么影响，所以还是保留了自增 ID，并且初始 ID 设置为比当前流程实例多一个数量级，以保证迁移过程中不会出现 ID 冲突。 当前所在的节点名称： 用户在查看列表的时候想直到流程实例的进度，可以用这个名称展示给用户。 另外还可以作为重启流程实例时跳转的节点。 创建来源： 流程实例有时候碰到问题会回退给创建人，这就要求创建人必须是一个具体的用户。而如果不标注来源，则该具体用户可能无法获取相关信息来解决问题。 优先级： 这个字段用于给 External Task 标注优先级，优先级越高越先执行。 处理人： 分为两种情况： 填写表单的人 自动化步骤出现一些意外的问题，将会转交给相关运维人员处理 流程实例状态： 对底层引擎流程实例状态的缓存，用于列表查看时减少对底层引擎的查询。 但是什么时候更新这个状态就成了一个问题。如果没有及时更新，会给用户带来疑惑。比如说流程实例在底层引擎已经结束了，但是这个状态却不是结束状态。后续会对此做详细说明。 标签： 目前作为一些业务信息的补充，仅用于展示。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:2","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"流程节点日志表 这个信息用于展示哪些节点执行过，以及相关时间点和输出信息。 由于各种数据或者业务问题，或者确实是正常的执行但用户误认为流程实例的流向有问题，这个时候可以参考这些信息。 字段 作用 id 自增 ID，没有额外作用 name 节点标识，用于确定唯一节点 i18n 翻译标识，用于翻译并展示节点名称 pi_id 流程实例 ID，便于关联和应对底层引擎 ID 的变化 status 执行状态。等待执行、执行中、成功、失败、超时 operator 操作人。可以是用户，也可以是 External Task Client 的 ID message 结果信息 started_at 开始执行的时间 ended_at 结束执行的时间 timeout_at 超时的时间 节点标识： 用于确定流程中的一个唯一节点，在绘制流程图的时候配置。 由于同一个功能的节点在流程中可能被使用多次，因此该字段会加上一些无关紧要的信息来相互区分。 翻译标识： 由于同一个功能的节点在流程中可能被使用多次，在不同位置的同一个功能的节点所代表的业务含义不一定是相同的，所以翻译标识要另外指定。 流程实例 ID： 由于特殊业务需求，有些流程实例在被强行关闭或者正常结束后，需要重新激活。 Camunda 提供了这种支持，但它创建了新的流程实例，其流程实例 ID 自然也就和之前的不同。 这时有两种选择： 学 Camunda 将流程实例相关信息复制一份，创建新实例 保持当前实例不变，仅变更实例绑定的底层引擎实例 我选择了第二种。因为第一种成本比较高，且在当前业务场景下没有带来价值，用户也不愿意接受。 超时时间： 设置超时时间是为了避免由于各种不确定性原因导致没有正常结束时，该节点状态没有得到更新，而用户会觉得困惑。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:3","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"服务定义表 该表用于管理节点和 API 的映射关系。 Camunda 有一种叫做 External Task 的节点类型，表示该任务是外部任务。需要外部系统主动拉取任务并提交结果。外部系统可以将这个拉取并提交结果的功能抽取出来，单独创建一个 External Task Client （以下将其简称为 ETC） 。 ETC 从 Camunda 拉取特定 Topic 的 External Task，然后执行业务逻辑。 External Task 的 Topic 在配置流程图的时候指定。ETC 在启动时需要指定 Topic。 最开始写 ETC 的时候，是参照官方的 JAVA 版本写了一个 PHP 版本。 整个流程大致如下： +-----------------------------------------+ | | | Adapter (5) | | Submit | | | | +--------------------------+ | | | | | | v (1) + | (3) | Fetch And Lock | Request | +----------+ +----------+ | +--------+ | | | \u003c-----------+ | External | | +-----\u003e | API | | | Camunda | | Task | | | System | | | | +-----------\u003e | Client | | \u003c-----+ +--------+ | +----------+ +----------+ | | External Task | Response | (2) | (4) | | +-----------------------------------------+ 使用这种方式时要解决的一个问题是：如何通过流程图配置的节点信息来判断该节点执行时要请求哪个接口？ 有一个简单的做法是在节点的 Task ID 上应用一些规则。 这里使用 “Task ID” 这个表述，是为了和 Camunda 保持一致。这个 Task ID 是一串由英文单词组成的有意义的字符串。 例如将 Task ID 设置为：Users_Decisions_ShouldDoSomething 在 ETC 获取该节点任务执行时，配置 ETC 将 ID 转化为 POST /users/decisions/should-do-something 。 它的优点在于实现起来简单，缺点是： 难以控制请求方式。 只能用 POST。不过如果要用其他的方法，可以将 Task ID 的第一个部分设置为方法，但 Task ID 会变得很长。如 Post_Users_Decisions_ShouldDoSomething 。 难以做到近似的 Restful API 。 比如实现 /users/{uid} 这种在 URL 上放用户 ID 的功能，需要再对 Task ID 做定制。 难以控制超时时间。 超时时间是为了在各种问题导致 ETC 无法 complete 一个任务时，只要等待过了超时时间， ETC 就可以重新拉取到该任务。 超时时间是在第一步拉取任务的时候设置的，也就是在 ETC 上做配置。但是每个 ETC 只能配置一种超时时间。 最初的做法是每一种超时时间都设置一个特定 Topic，比如 XXX_3MIN 表示超时时间为 3 分钟。然后启动一些 ETC ，通过启动参数配置对应的超时时间。但是从实践的结果看，这样更新起来不灵活，有时候还需要重启 ETC。 上面这些问题虽然在 Task ID 上或者 Topic 上多做一些定制化就能完成，但是会使得它们自身变得越来越复杂。并且因为它们都是配置在流程图上的，随着 Task 越来越多，越难以更改。一旦要新加一个规则，会导致所有流程都得改一遍。 怎么优化呢？ 在 Adapter 层加一个 External Task 定义表。在 ETC 获取到任务后查询这个表，根据查询结果做相应调整。 字段 作用 id 自增 ID，没有额外用途 task_id External Task ID method HTTP 方法 url_path URL 的 Path 部分 url_query URL 的 Query 部分 timeout 超时时间 优化后的流程如下： +-------------------------------------------+ | | | Adapter (7) | | Complete | | | | +--------------------------+ | | | | | | v (1) + | (5) | Fetch And Lock | Request | +----------+ +----------+ | +--------+ | | | \u003c-----------+ | External | | +-----\u003e | API | | | Camunda | | Task | | | System | | | | +-----------\u003e | Client | | \u003c-----+ +--------+ | +----------+ +----------+ | | External Task | Response | (2) + ^ | (6) | | | | | (3) | | | | Fetch API Info | | | | +------------+ | | | | | External | \u003c----------------+ | | | | Task | | | | | Definition | +---------------------+ | | +------------+ | | API Info | | (4) | | | +-------------------------------------------+ ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:4","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"服务请求客户端管理表 这个表用于管理 ETC 客户端。 主要解决两个问题： 关闭前等待执行中的任务结束 ETC 数量动态调整 关闭前等待执行中的任务结束： 有时候要重启 ETC，但是因为 ETC 总是会获取任务执行，所以只能等深夜没有流程在执行的时候重启。 如果直接关闭的话，会导致任务虽然执行成功了，但由于没有调用 Camunda 的 complete 而超时。超时就会重新执行。 如果是幂等的接口倒是不会出问题，但有些幂等难度大或者消耗的资源大，二次执行会出问题。 那么让 Camunda 在 ETC 请求任务的时候不给任务是否可行？因为获取不到新任务后，总是能等到所有 ETC 执行中的任务都结束。 Camunda 提供了挂起（Suspend）流程实例的功能，虽然能避免流程实例的任务被 Fetch，但同时也使得正在执行的任务无法执行 complete。 那怎么办？ 有两种方式： ETC 每次执行完一个任务后，就自动重启 ETC 在向 Camunda 获取任务前，都先查询一下自己能否获取任务 这里选择第二种，需要额外的表格维护 ETC 的信息。 字段 作用 id 自增 ID，没有其他作用 etc_id 客户端 ID topic 客户端获取的任务的 topic switch on/off 控制是否继续获取任务 ETC 数量动态调整： 调整的依据来自于两方面： 流程实例的数量 业务系统 API 的负载情况 如果流程实例的量大，且业务系统 API 负载比较低，可以添加更多 ETC ，加快整体的速度。 字段 作用 id 自增 ID，没有其他作用 topic 客户端获取的任务的 topic count 启动客户端的数量 手动控制的话，这样就够了。如果要通过采集信息自动控制，那么可以再加两个参数： 字段 作用 count_max 启动客户端的最大数量 count_min 启动客户端的最小数量 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:5","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"动态表单定义表 Camunda 自身支持以下几种类型的表单： 嵌入式 HTML 表单 对 Camunda 依赖性强。 基于 XML 生成表单 在流程图绘制工具里面定义表单，只能做简单的功能。 JSF 表单 和嵌入式 HTML 表单类似。 通用表单 功能太少。 由于业务上的表单比较复杂，又不能太过于依赖 Camunda，因此表单的定义和渲染需要另外做。 表单的功能至少需要包括： 选择项动态加载 丰富的支持 如上传文件和图片展示。 前端页面数据格式校验 表单有两种形式： 静态表单 把表单定义放在前端，前端直接渲染。 动态表单 把表单定义放在后端，前端提供基本组件。前端获取后端对表单的配置，根据这个配置做渲染。 我们选的是动态表单。一是因为原先的系统就是这么做的，同时也比较灵活；二是因为我们团队没有专门的前端。 动态表单可以放业务层，也可以放流程引擎 Adapter 层。 如果想要多个接入流程引擎的系统都可以使用，可以放 Adapter 层。就算有的系统不想用这个动态表单，也完全不影响。流程引擎中台的同事倒是对我们动态表单的实现比较感兴趣。 分为两张表： 表单项组件表 表单定义表 表单项组件表： 字段 作用 id 自增 ID name 组件名称 config 组件的配置（Json），主要是数据校验 default 默认值 前端用的是 Vue，每个表单项直接对应一个 Component 。 表单定义表： 字段 作用 id 自增 ID p_id 流程定义 ID form_key 表单名 version 表单版本 group 表单内部分组，支持翻译 cpn_id Component ID，表单项组件表中的 ID order 在表单中所处的位置 field 变量名 label 渲染表单时，该组件的展示名称，支持翻译 config 组件的配置（Json），主要是数据校验 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:6","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"表单暂存表 用户在编辑完表单时，可能因为各种原因无法全部填写完，又想保存当前已填写的数据。 可以创建一个数据表用于存储这些暂存数据，当表单提交后删除这些数据。 字段 作用 id 自增 ID，没有其他作用 pi_id 流程实例 ID form_key 表单 ID name 变量名称 value 变量值，以 json 形式存储 提交后的表单数据去哪了？ Camunda 里的每个流程实例都可以有对应的流程实例变量集合，可以从下面的接口中获取： Get Process Variables https://docs.camunda.org/manual/latest/reference/rest/process-instance/variables/get-variables/ 为了便于理解，我画了一张图： +-------------------------+ | | | Process Instance | | | | +----------+ | | | | | | | Global +\u003c-+ | | | Variable | | | | | Box | | Publish | | | | | | | +--------+-+ | | | fetch | | | | | | | | +--------------------+ | | | | | | | | | Nodes | | | | | | | | | | | | +---------------+ | | | | | | | | | | | | | Node | | | | | | | | v | | | | | | | +----+----++ | | | | | | | | | | | | | | | Local | | | | | | | | Variable | | | | | | | | Box | | | | | | | | | | | | | | | +----------+ | | | | | | | | | | | +---------------+ | | | | | | | +--------------------+ | | | +-------------------------+ 即流程实例里面会包含一个全局的流程实例变量盒子，所有流程实例级别的变量都会放进去。 然后每个节点都有自己的本地变量盒子。它可以从全局盒子获取变量映射到本地盒子，也可以把本地变量发布到全局盒子。 流程实例的操作 创建 暂停和恢复 节点跳转 表单处理 关闭流程实例 重启流程实例 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:7","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"创建 先在自建流程实例表添加一条记录，然后把流程实例的 ID 、创建人等一些基本信息作为变量，调用流程引擎创建实例的接口时一起传进去。 Start Process Instance https://docs.camunda.org/manual/latest/reference/rest/process-definition/post-start-process-instance/ 这些流程实例基本信息的变量在存储到 Camunda 里面时，会给变量名加一个 meta 前缀。 例如 id 加上前缀后变成 meta__id 。 注意，如果变量名使用下划线，在搜索变量的时候不能用 GET 接口，要用 POST 接口。 Get Variable Instances https://docs.camunda.org/manual/latest/reference/rest/variable-instance/get-query/ Get Variable Instances (POST) https://docs.camunda.org/manual/latest/reference/rest/variable-instance/post-query/ ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:8","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"表单处理 先创建流程实例再填表单还是反之？ 两种都可以。 我选择的是先创建流程实例再填写表单。 接下来分析两者的优缺点。 先填写表单再创建流程实例： 优点： 如果表单填写一半时发现没有必要走流程或者由于数据不足不能填完整，就不会创建流程实例 缺点： 如果流程里有其他表单，则初始表单与其他表单的处理逻辑不统一 先创建流程实例再填表单： 优点： 所有表单处理逻辑统一 缺点： 必须创建流程实例才能填写表单，如果最终不需要该流程实例，则流程实例列表会多出一个无用的实例 这个缺点可以缓解。 我见过一种实现：先创建实例，填完第一个表单后才在自己扩展的实例表中添加该实例。这样用户看流程实例列表就不会有无用的实例。 但是这个实现有个问题。用户很有可能经常创建流程实例后不提交第一个表单，可能直接返回或者刷新页面丢失该信息。经过一段时间会发现底层引擎保留大量流程实例，以至于流程引擎处理速度变慢。 获取当前表单 由于 Camunda 做的是标准的流程引擎，因此界面中每个用户都会有自己要处理的 UserTask（表单） 列表。 我们的场景是流程实例只会有一个节点执行，并且表单是和流程实例放在一起的。并且用户要求要一次性查看所有已填表单，包括其他人的表单。所以要从流程实例的角度处理。 我们需要一个 “获取当前表单” 的接口，但由于上面的原因， Camunda 没有现成的接口。只能自己根据 Camunda 已有接口封装了。 获取流程实例当前节点 Get Activity Instance https://docs.camunda.org/manual/latest/reference/rest/process-instance/get-activity-instances/ 根据节点 ID 获取 Form Key Get Form Key https://docs.camunda.org/manual/latest/reference/rest/task/get-form-key/ 获取 Form Key 后就可以到动态表单定义表里面获取表单的定义，传给前端渲染。 表单的暂存 前面提到表单提交后要删除暂存的数据，因为如果没有删除这些数据，会碰到一个问题： 当用户将流程实例驳回到前面的表单节点时，用户修改表单但是不选择提交而是暂存，下次用户进入这个表单界面时数据是以 Camunda 里面为准还是暂存的数据为准？ 如果选择以暂存的数据为准，那么要考虑一个场景： 用户初次提交表单后，流程实例后面的步骤修改了表单里的某些数据。接着有用户将流程实例驳回到这个表单，此时如果选择以暂存数据为准，会导致表单展示的是未修改的数据，在业务上会出现问题。 如果选择以 Camunda 的数据为准，那么就会导致用户发现其修改并暂存的数据不见了 所以删除暂存数据是一种解决方案，如果暂存表里面有数据，就以暂存表为准，否则以 Camunda 为准。 表单数据校验 数据校验分为两部分： 数据类型校验 业务关系校验 Camunda 自身支持数据类型校验，但如果有复杂的类型就得在引擎层面自定义校验类。 并且由于业务关系校验不能放在引擎层面，所以两者一起放在业务系统层面处理。 数据格式 数据格式 业务关系 校验 校验 +-------+ +--------+ +---------+ | | submit | | submit | | | front | | system | | engine | | end | +----\u003e | | +----\u003e | adapter | | | | | | | +-------+ +--------+ +---------+ 暂存的接口一般只会执行数据格式的校验，并且不对是否必填做校验。 提交表单 提交表单到 Adapter 层的时候要做以下校验： 当前节点是否是表单节点 当前表单节点的 Form Key 是否与提交的 Form Key 一致 流程实例转交 转交分为两种类型： 表单节点转交 自动化节点转交 指的是将操作权交给其他人，一般自动化节点出现错误的时候，会转交给运维人员处理，运维人员可以转给其他运维同事帮忙处理。 两者都需要修改流程实例信息表中的处理人字段。 表单节点除此之外还要调用 Camunda 设置操作人的接口： Set Assignee https://docs.camunda.org/manual/latest/reference/rest/task/post-assignee/ ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:9","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"暂停和恢复 Camunda 提供了一个 suspended 接口，用于挂起整个流程实例。使流程实例处于暂停状态。 https://docs.camunda.org/manual/latest/reference/rest/process-instance/put-activate-suspend-by-id/ 官方文档有关于挂起流程实例的完整说明。 https://docs.camunda.org/manual/latest/user-guide/process-engine/process-engine-concepts/#suspend-process-instances 一旦挂起流程实例，会产生以下影响： 用户无法提交表单 External Task Client （以下简称 ETC） 的 complete 无效 用户无法执行跳转节点 虽然无法变更流程实例的执行节点，但是可以修改流程实例的全局变量。 最初由于节点跳转的需要，没有把挂起直接作为流程实例的暂停功能，下面会对此做出解释。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:10","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"节点跳转 节点跳转最常用的就是驳回功能。之所以不直接说驳回，是因为除了驳回外，有时还需要跳转到后面的节点。 这是因为自动化流程中，有一些节点会出现难以预测的问题。有的可以通过优化流程图来解决，有的难以通过优化流程图解决。所以需要人工干涉，跳过当前节点的执行或者返回前面的节点执行。 跳转的接口 Camunda 对节点跳转的支持是在流程实例修改接口。 https://docs.camunda.org/manual/latest/reference/rest/process-instance/post-modification/ 它可以取消一个节点的执行（cancel），也可以开启一个节点的执行（startBeforeActivity）。 执行修改接口时，有一个参数需要注意： skipIoMappings 。这个参数表示是否跳过节点输入输出映射的校验。为了解释这个参数，得先做个补充说明。 External Task 有个输入输出变量（Input/Output Variable）配置。用于将全局变量映射到本地变量（Input），或者将本地变量发布到全局变量（Output）。 当开始执行一个 Task 之前，会对 Input Variable 执行映射。此时如果映射配置中的全局变量不存在，就会报错。因为变量不存在是一个错误的状态，不能强行执行。结束一个 Task 则会对 Output Variable 执行映射。 如果一个 External Task 没有执行完，就不会生成 Output Variable 所需的本地变量。这个时候如果取消该执行，会默认进入映射变量的逻辑，导致出错。所以用 cancel 的时候需要开启 skipIoMappings 。 而跳转到目标节点时，又需要校验 Input Variable 映射所需的全局变量是否存在，否则强行执行会有问题。此时应该关闭 skipIoMappings 。 但 Camunda 这个 modification 接口的 skipIoMappings 放在最外层，表示一次只能设置一种 skipIoMappings 。 另外 skipCustomListeners 总是开启。 因此想要实现跳转，就得分为两步： 在目标节点 startBeforeActivity ， 请求时关闭 skipIoMappings ，开启 skipCustomListeners 如果上一步成功，则在当前节点 cancel ， 请求时开启 skipIoMappings ，开启 skipCustomListeners 需要注意一个问题： 执行跳转的接口前要保证流程已处于暂停状态。否则如果 cancel 节点时，节点已经完毕并转入下一个节点，就会出现 cancel 失败并且此时流程有两个执行的节点。 但是如果用流程实例挂起接口使流程实例处于暂停状态，也会受到挂起状态的限制而没办法执行跳转。 支持跳转的暂停状态 暂停的实现经历过两个版本。 最初版本中，节点跳转前要求用户必须先手动暂停流程实例。 前面提到挂起流程实例后无法跳转节点，所以专门为当时的流程实例设置一个暂停的状态。 如何实现可跳转节点的暂停？ 这里要处理流程实例节点的两种状态： 还未被 ETC 获取。此时可以想办法让 ETC 没办法获取到处于暂停状态的流程实例的任务。 已经被 ETC 获取。此时可以让 ETC 不执行 complete 接下来详细说明。 首先是 还未被 ETC 获取 的场景。 如何让 ETC 不获取暂停状态的流程实例？ 通过查询文档得知流程实例碰到 failedExternalTask 这种 Incident 的时候， ETC 不会获取该流程实例的任务。 https://docs.camunda.org/manual/latest/user-guide/process-engine/incidents/#incident-types 那么如何生成这种 Incident ？ Incident 没有一个 create 的接口，所以无法直接创建。 从刚才的文档上可以看到当 retries \u003c= 0 的时候会生成 failedExternalTask 类型的 Incident 。 每个 External Task 的 retries 值默认为 1 。当 ETC 报告一个错误的时候，将 retries 减一。 但是用户如果想要跳转节点，不会想要等到当前节点出错，万一它不出错怎么办？ 通过找文档发现 External Task 有一个设置 retries 的接口。 https://docs.camunda.org/manual/latest/reference/rest/external-task/put-retries/ 尝试直接将 retries 设置为 0 ，发现可以生成 failedExternalTask 类型的 Incident 。 这样节点还未被 ETC 获取的场景就得到了处理。 接下来是 已经被 ETC 获取 的场景。 在 ETC 获取任务执行后设置 Incident 就没法阻止，并且 Incident 的情况下 ETC 仍然可以 complete ，使得流程继续往下走。所以如果只有上面的处理，点击暂停可能会出现失败的情况。 这就得在 Adapter 层加一个处理。当 ETC 执行 complete 的时候请求给 Adapter，Adapter 查询流程实例是否有 Incident，如果有就不提交给 Camunda。 但如果在查询到没 Incident 和提交给 Camunda 之间设置了 Incident 呢？ 这个问题在于没办法通过接口请求对 Camunda 的表直接加锁。 不过我们可以在自定义的流程实例信息表里的 status 上想办法。 在执行暂停的时候，将该流程实例所在行加排他锁，然后更新为暂停状态，更新完释放锁。 ETC 执行 complete 之前，对流程实例加排他锁，查询到的状态如果是暂停状态，则放弃 complete，否则执行 complete 。之后释放锁。 已经被 ETC 获取的场景也处理了。 上述的暂停只能针对 External Task，其余的就无法暂停了。 于是我后来重构了这部分代码，把 Incident 和 Suspend 结合在一起，让跳转的时候不需要先手动暂停。 步骤为： 设置 Suspend ，防止 ETC 获取任务 流程实例信息表设置状态为 incident 设置流程引擎实例 Incident 取消 Suspend 设置新节点位置 取消当前节点，这个动作会连同 Incident 一起删除 如何获取跳转获取目标节点的 Task ID 有三种做法。 以下 “用户” 表示运维人员或者某个用户部门的公共账号，不是所有人都有跳转的权限。 通常的做法，即把执行过的节点以下拉菜单的形式列出来，用户选择一个，然后执行。 要实现这个功能，可以使用节点日志的信息。将节点执行日志中的 Task ID 取出来去重。它的缺点是只能跳转到已经执行过的节点。 将流程中所有节点列出来，让用户选择。 解决了第一种无法跳转到未执行过的节点的问题。但带来新的问题：流程所有节点的信息如何获取？ Camunda 的接口没有提供这个信息，最多只有流程图的 xml 。解析 xml 是一种方法，不过也比较麻烦。 如果在发布流程定义的时候将所有节点信息放入一张记录节点信息的表。这不仅需要解析 xml ，还需要添加一张数据表，更麻烦。 直接将流程图展示给用户，用户在流程图上选择一个节点，然后点击跳转。 不仅直观，而且不用自己写解析，直接用 Camunda 的 bpmn-js 。 https://github.com/bpmn-io/bpmn-js bpmn-js 提供了很多示例。 https://bpmn.io/toolkit/bpmn-js/examples/ https://github.com/bpmn-io/bpmn-js-examples 例如： interaction： 与流程图的交互，点击节点 overlays： 添加覆盖层。可以在流程节点上加悬浮图标来表示当前所在节点 colors： 给节点加颜色。比如将所有未执行过的节点设置为灰色，将执行过的节点设置为黑色。 我选择第三种方式。 驳回功能 节点跳转的另一个应用是驳回。驳回是流程实例当前具有控制权的用户可以做的动作。 驳回通常有两种场景： 用户选择驳回到已经执行过的某个节点 前端限制流程图中只能选择已执行过的节点，后端在跳转前查询执行日志判断该节点是否已执行过。 所有流程实例只会驳回到第一个节点 绘制流程图的时候，为所有流程图的开始节点设置相同的 ID 跳过当前节点 有的节点只执行一个操作，不生成任何对流程实例流转有影响的数据。这种节点会因为各种奇怪的原因执行出错，运维人员需要介入处理这些问题。处理完后跳过这些节点。 Camunda 有一个接口可以直接做到这件事： https://docs.camunda.org/manual/latest/reference/rest/signal/post-signal/ 相关说明文档： https://docs.camunda.org/manual/latest/reference/bpmn20/events/signal-events/ 最开始用过这个接口，不过后来取消了。还是让运维人员选择目标节点比较安全。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:11","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"流程实例迁移（升级） 当流程发布新版本之后，不会对已有的流程实例造成影响。如果想应用最新版本流程，则需要升级旧流程实例。 流程实例迁移分为三个步骤： 生成迁移计划 Generate Migration Plan https://docs.camunda.org/manual/latest/reference/rest/migration/generate-migration/ 验证迁移计划 Validate Migration Plan https://docs.camunda.org/manual/latest/reference/rest/migration/validate-migration-plan/ 执行迁移计划 Execute Migration Plan https://docs.camunda.org/manual/latest/reference/rest/migration/execute-migration/ 生成迁移计划和验证迁移计划的时候，不会涉及具体的流程实例 ID。 执行迁移计划的时候，可以选择要迁移的具体流程实例 ID，也可以用查询的方式指定要迁移的流程实例。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:12","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"关闭流程实例 使用 Camunda 的 Delete 接口就行了。 Delete Process Instance https://docs.camunda.org/manual/latest/reference/rest/process-instance/delete/ ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:13","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":"重启流程实例 Camunda 会用旧流程实例的信息来启动一个新的流程实例。 Restart Process Instance https://docs.camunda.org/manual/latest/reference/rest/process-definition/post-restart-process-instance-sync/ 由于会创建一个新的流程实例，其 ID 与旧实例的 ID 不一致，因此得将流程实例信息表中的引擎流程实例 ID 替换掉。 这里会碰到一个问题： Camunda 重启实例的接口不会返回新实例的 ID 。 还好我们之前在创建流程实例的时候，会往底层 Camunda 的全局变量盒保存自增 ID ： meta__id。 可以通过流程实例搜索接口找到有保存 meta__id 为指定 ID 的流程实例。 Get Instances (POST) https://docs.camunda.org/manual/latest/reference/rest/process-instance/post-query/ 然后将获取到的流程实例 ID 更新到流程实例信息表里面。 前端动态表单的渲染 写一个主 Component，然后在里面写具体的各个组件。 遍历后端传的各组件名称，创建多个主 Component。然后用组件名称依次匹配里面的各个组件，如果匹配到则展示。这里用到了 Vue 的 v-if 。 结语 目前能想到的基本上都写了。有一些细节的地方没有深入讨论，待后续继续完善。 这篇没有完全地按照当前项目写的适配层的实践来写，而是在此基础上做了一些优化。 ","date":"2020-03-27","objectID":"/2020/03/27/camunda-adapter/:0:14","tags":["Camunda","BPMN"],"title":"一种 Camunda 流程引擎 Adapter 层的实现","uri":"/2020/03/27/camunda-adapter/"},{"categories":null,"content":" FROM centos:7 COPY ./CentOS-Base.repo /etc/yum.repo.d/CentOS-Base.repo RUN groupadd -r redis \u0026\u0026 useradd -r -g redis redis ENV REDIS_VERSION 5.0.3 ENV BUILD_DEPS=' \\ gcc \\ make \\ ' ADD ./redis-${REDIS_VERSION}.tar.gz /usr/src/ RUN yum install -y ${BUILD_DEPS} \\ \u0026\u0026 mv /usr/src/redis-${REDIS_VERSION} /usr/src/redis \\ \u0026\u0026 grep -q '^#define CONFIG_DEFAULT_PROTECTED_MODE 1$' /usr/src/redis/src/server.h \\ \u0026\u0026 sed -ri 's!^(#define CONFIG_DEFAULT_PROTECTED_MODE) 1$!\\1 0!' /usr/src/redis/src/server.h \\ \u0026\u0026 grep -q '^#define CONFIG_DEFAULT_PROTECTED_MODE 0$' /usr/src/redis/src/server.h \\ \u0026\u0026 make -C /usr/src/redis -j \"$(nproc)\" \\ \u0026\u0026 make -C /usr/src/redis install \\ \u0026\u0026 rm -r /usr/src/redis RUN mkdir /data \u0026\u0026 chown redis:redis /data VOLUME /data WORKDIR /data COPY docker-entrypoint.sh / RUN chmod a+x /docker-entrypoint.sh ENTRYPOINT [\"/docker-entrypoint.sh\"] EXPOSE 6379 USER redis CMD [\"redis-server\"] ","date":"2020-03-27","objectID":"/2020/03/27/compile-redis-docker/:0:0","tags":[""],"title":"编译 Redis 源码","uri":"/2020/03/27/compile-redis-docker/"},{"categories":null,"content":"概念说明 在了解各种模式之前，要把自己代入一个基础库的开发者（库作者）的角度。 客户端： 使用库的人 主体： 实际实现功能的库类 ","date":"2020-03-27","objectID":"/2020/03/27/design-patterns/:1:0","tags":["设计模式"],"title":"设计模式","uri":"/2020/03/27/design-patterns/"},{"categories":null,"content":"代理模式（Proxy） \u0026 适配器模式（Adapter） 两者都是在客户端和主体之间构建一个中间层，让客户端间接访问主体。 客户端 ---\u003e Proxy/Adapter ---\u003e 主体 它们在客户端视角的区别是： 是否与主体实现了同一个接口（interface）。 Proxy implements AInterface 主体 implements AInterface Adapter implements BInterface ","date":"2020-03-27","objectID":"/2020/03/27/design-patterns/:2:0","tags":["设计模式"],"title":"设计模式","uri":"/2020/03/27/design-patterns/"},{"categories":null,"content":"代理模式 客户端不直接访问主体，而是通过代理访问主体。即客户端创建代理对象和操作代理对象，不直接接触主体对象。 要从客户端和库作者的角度和不同场景去理解。 代理分为正向代理和反向代理。 正向代理： 客户端创建代理类，弥补基础库的缺陷 反向代理： 库作者创建代理类，隐藏主体的一些方法，避免客户端调用到这些方法 正向代理 对于客户端来说，基础库有可能写得不够完善。这里举两个例子： 创建主体对象的成本很高 例如创建主体对象需要花 10 秒钟，或者需要消耗 10 MB 内存。 由于业务原因，客户端必须先创建主体对象。但如果太早创建主体对象，会影响到用户体验或者影响系统整体性能。 主体没有对操作的结果做缓存 例如主体负责下载一个视频，但客户端如果多次发起下载同一个视频的请求，主体仍然会触发多次下载。 如果创建成本高，那么可以先创建代理对象。当要做真正的操作的时候，代理对象创建主体对象，然后调用主体对象的操作。 interface Graphic { public function draw(); } class ImageProxy implements Graphic { private image; public function draw() { if ($this-\u003eimage === NULL) { $this-\u003eimage = new Image(); } $this-\u003eimage-\u003edraw(); } } class Image implements Graphic { public function draw() { // ... 绘制 } } 如果是主体没有对操作的结果做缓存，则用 Proxy 维护缓存。 interface UserQueryService { public function allUserInfo(); } class UserQueryServiceProxy implements UserQueryService { private $userQueryServiceImpl = new UserQueryServiceImpl(); private $cache = []; public function allUserInfo() { if (! isset($this-\u003ecache['all-user'])) { $this-\u003ecache['all-user'] = $this-\u003euserQueryServiceImpl-\u003eallUserInfo(); } return $this-\u003ecache['all-user']; } } class UserQueryServiceImpl implements UserQueryService { public function allUserInfo() { // ... 调用 HTTP 接口获取所有用户信息 } } 反向代理 对于库作者来说，他不想让客户端调用类的某些 Public 方法，但想在库内部调用这些方法。 构建器模式 类生成的构建器，做成类似于多级菜单，减少每次所需要看到的对象。 缺点是编写库的人要写很多类。优点是使用库的人很清晰。 访问者模式 对同一个对象的不同目的的操作。例如访问人的姓名、年龄等。避免太多操作放到对象里面会污染到对象。 两种方式： 传被访问数据自身 被访问数据定义访问者必须实现的接口，由被访者决定要传哪部分数据给接口，不需要被访者的接口与访问者的接口一致。被访问者只需调用 me.Visit(Visitor) 抽象的被访问者可以通过 visitor.Visit(this) 把自己这个具体的类型传入进 Visitor，这样 Visitor 就可以知道访问者的具体类型。 https://stackoverflow.com/a/63652850 返回计算结果： 把计算结果存入 Visitor 把计算结果存入被访问者 策略模式 为了完成同一个动作，采取的不同方式。例如压缩文件，可以压缩为 tar，也可以压缩为 zip，或者 rar。 由被访问者定义策略应该实现的接口，其接口与被访问者的接口一致，类似于代理。被访问者调用实际的方法，并传入参数。 如果各种策略所需参数不一致，则需要使用访问者模式。 ","date":"2020-03-27","objectID":"/2020/03/27/design-patterns/:2:1","tags":["设计模式"],"title":"设计模式","uri":"/2020/03/27/design-patterns/"},{"categories":null,"content":"2019 年初我在重新设计我们组负责的流程系统时，选择了 Camunda 流程引擎，并基于该流程引擎实现了一套适配方案。以前就想做一次总结，但总拖着。 最近公司中台在做流程引擎选型，相关同事找我了解 Camunda 以及基于 Camunda 的应用方案。经过我一番说明，对方表示收获很大。我想着也是时候把这些经验写下来了。 我把内容分为两部分： Camunda 自身的介绍（为什么选 Camunda ？） 基于 Camunda 的一种适配层实现方案 这一篇只包括第一部分的内容。 为什么选 Camunda ？ 做选型的时候，需要说明我们需要什么样的功能和不需要什么样的功能。 我们项目的特点是什么？ 流程以自动化为主 极少数节点需要人工操作（审批、补充信息） 使用 PHP 作为业务层语言 流程引擎必须作为一个服务存在，不能为了使用流程引擎而更改语言。 必须有一个机制，使得流程实例执行自动化操作时，请求业务层 API。 流程同一时间最多只有一个节点在执行 不需要支持并行加签、多分支同时执行、单节点多并发执行。 可以强行跳转到其他节点执行 由于业务的不确定性因素，难以或无法通过优化流程来预先规划好各种情况下的分支。 因此需要在自动化步骤出现某些无法预知的情况时，由运维修改流程实例的状态，跳过当前节点的执行或者回到前面的节点重新执行。 可以重启一个处于结束或终止状态的流程实例 同样是业务的要求。 支持多分支 有些流程引擎只能选择 “是” 或者 “否” 这两个分支，无法支持多种情况。 支持失败重试 当自动化任务节点失败后，流程引擎需要支持重试当前节点。 接下来了解比较常见的流程引擎。例如 JBPM 、 Activiti 、 Flowable 、 Camunda 、 Zeebe 。 更多流程引擎请见： https://github.com/meirwah/awesome-workflow-engines 它们之间的关系是： 2004 2006 2009 2010 2013 2015 2017 2018 2020 +----------------------------------------------------------------------------------------\u003e JBPM 2 3 4 5 6 7 7.15 7.34 + 推 翻 架 构 | 继 承 | 架 构 | 2010 2013 2015 2017 2018 2020 +-----------------------------------------------------------------------\u003e Activiti 5 6 7 7.1 + + | | | | fork | fork | 2018 2020 | +------------------------------\u003e Flowable | | 6 6.4 6.5 | | | | | 2015 2017 2018 2020 +---------------------------------------------------\u003e Camunda 7.0 7.3 7.8 7.10 7.12 ^ | | 同 团 队 | v 2017 2018 2020 +-------------------------------\u003e Zeebe 0.1 0.14 0。22 如果从 GitHub 的 start 数看，Camunda 是远远没有优势的。以下数据是 2020-03-25 的数据。 Activiti： 6.4k Flowable： 2.8k Camunda ： 1.3k 在其他数据上的比较： https://www.openhub.net/p/_compare?project_0=Activiti\u0026project_1=camunda+BPM+platform\u0026project_2=Flowable 从功能上看呢？ CSDN 有一篇对比写得很详细： https://blog.csdn.net/qq_30739519/article/details/86682931 我比较关注的点有以下几个： 支持外部任务（External Task） External Task 应该和 HTTP Task 做对比。 对于 HTTP Task ，在执行的时候会请求一个 HTTP API。等待这个请求结束后，流程继续往下走。这里的问题是： API 超时如何处理？如果业务修改导致 API 处理时间变长时，要修改所有流程里配置的超时时间吗？ 如何区分测试环境和正式环境？ 当 Camunda 执行到外部任务节点时，会发布一个任务单元。外部系统定时向 Camunda 获取外部任务单元，然后做一些业务逻辑或者请求 HTTP API。做完之后，再提交给 Camunda，流程继续往下走。 外部任务具有超时时间。这个时间后，其他客户端请求接口可能获取到该任务。但可以请求 API 延长超时时间。 一个外部任务只能被一个客户端获取，获取后会加上一个锁。除非超时，否则只有获取到该任务的客户端可以继续操作。 外部任务可以配置优先级，并且这个优先级可以动态修改。 外部任务支持重试。当以任务处理失败的方式提交给 Camunda 后，Camunda 会检查配置的重试次数有多少，当前剩下多少。如果还有次数，则再次将该任务发布出去。 可以专门实现一个 External Task Client ，实现一套根据情况请求业务 API 的方式。虽然同样是请求 HTTP API ，但是可以拥有更灵活的配置。 支持任意节点的跳转 实际上 Camunda 不是直接支持跳转。它支持取消某个节点的执行，也支持在任意节点创建一个执行。 如果要实现节点的跳转，需要封装两个操作： 在目标节点创建一个执行 取消当前节点的执行 支持重启（Restart）已经关闭的流程实例 虽然是叫重启，但实际上是创建一个新实例，然后将已关闭的流程实例的信息复制一份到这个新实例。 支持流程实例的迁移（Migration） 随着流程的更新，流程会有多个版本。每个流程实例会固定绑定一个流程版本，按照该版本的方式走。 Camunda 可以让旧版本的流程实例迁移到其他版本的流程，目标流程版本可以是更新的，也可以是更旧的。 迁移分为两步： 生成迁移计划 执行迁移 执行迁移的时候，可以从迁移计划中选择一部分流程实例做迁移。并且可以指定迁移后从哪个节点开始走（继续）。 支持批量（Batch）操作的 API 例如批量挂起流程实例、批量激活流程实例、批量重启流程实例 流程图绘制工具有桌面版本 你可以把流程图绘制工具下载到 Windows 系统上（其他系统也支持），绘制完流程图后，通过这个工具把流程图发布到 Camunda 。 你可以直接把这个软件丢给需求方，让他们把理想中的流程图绘制出来。 Camunda 的流程绘制工具对业务方和开发者都是友好的。 甚至如果你不是为了将流程图发布到 Camunda ，仅仅想绘制一个流程图，这个工具也很好用。 Activiti 需要搭建后端服务才能通过 web 的方式绘制流程图，而且它是偏向于开发者的。 除此之外， Camunda 和其他流程引擎一样，支持以下功能： 定时节点（Timer Intermediate Catch Event） 可以选择相对时间或者绝对时间。例如 10 分钟之后继续往下走或者 2021-11-11 的 11:11:11 的时候往下走。 时间的语法采用 ISO 8601 标准。这个标准在编程语言库中基本都会支持。 可以设置时间常量，也可以引用流程引擎变量。在执行到定时节点之前设置或者修改时间。 网关节点（Gateway） 汇聚多个分支。不同网关有不同的作用。常用的互斥网关（Exclusive Gateway）表示与其相连的下游分支的条件中，一旦有一个分支的条件符合要求，就走那个分支，并且不再继续判断其他分支条件。 消息接收节点（Receive Task） 流程引擎在执行到该节点的时候，会等待一条消息。客户端向该流程实例发送这条消息，流程继续往下走。 执行监听器（Execution Listener） 当以下事件发生时，会触发一次通知： 流程实例的开始和结束 流程实例内一个节点的开始和结束 可以为 Camunda 写一个扩展，用于通知流程实例的状态。 参考： https://github.com/camunda/camunda-bpm-reactor/tree/master/examples/bpmn-execution-listener 从业务的角度上讲，其他流程引擎满足的部分 Camunda 也满足，其他流程引擎不满足的部分 Camunda 也满足，因此选择 Camunda 。 下一篇会详细介绍一种基于 Camunda 做适配层的方案。 如果想快速了解 Camunda 的功能，可以下载 Camunda Modeler 了解其流程图支持的各种组件，以及查看官方文档。 直接访问官方文档会很慢，于是我把官方文档做成 Docker 镜像，可以下载到本地访问。 docker pull schaepher/camunda-docs:latest docker run -d -p \"8080:80\" schaepher/camunda-docs:latest 扩展阅读 Camunda 官方文档： https://docs.camunda.org/manual/latest Camunda Rest API： https://docs.camunda.org/manual/latest/refere","date":"2020-03-25","objectID":"/2020/03/25/camunda/:0:0","tags":["Camunda","BPMN"],"title":"BPMN 流程引擎 —— Camunda","uri":"/2020/03/25/camunda/"},{"categories":null,"content":"在 Memcached 和 Redis 的比较中，总会提到它们存储字符串的区别。 Memcached 默认上限是 1MB （最大上限是 1GB），而 Redis 是 512MB 。 但是这样就够了吗？我们很自然的会对此提出一些问题。 这个“字符串存储容量上限”的限制配置在哪？ 看看源码吧。 memcached/memcached.c： static void settings_init(void) { // ... 省略代码 settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. 著名的 1MB 上限 */ settings.slab_page_size = 1024 * 1024; /* chunks are split from 1MB pages. 数据块由 1MB 的页分割得到 */ // ... 省略代码 } 当然，这是默认的 1MB。启动时可以通过参数修改这个最大上限。这个最大上限能调整到什么程度呢？ memcached/memcached.h： /** Maximum length of a key. */ #define KEY_MAX_LENGTH 250 // ... 省略代码 /* * Valid range of the maximum size of an item, in bytes. */ #define ITEM_SIZE_MAX_LOWER_LIMIT 1024 #define ITEM_SIZE_MAX_UPPER_LIMIT 1024 * 1024 * 1024 顺便可以看到，上限还能往下调，调到 1KB。 Key 的最大长度为 250 Byte。 redis/src/t_string.c： static int checkStringLength(client *c, long long size) { if (size \u003e 512*1024*1024) { addReplyError(c,\"string exceeds maximum allowed size (512MB)\"); return C_ERR; } return C_OK; } 这个配置是由什么引起的？ ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:0:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"Memcached Memcached 在紧挨 item_size_max 后面还有个重要的配置 —— slab_page_size ，它决定了每次向系统申请多少内存，将这一块内存称之为页（page）。 在申请一页的内存后， Memcached 会将其切割成大小相等的数据块 （chunk）。 但是数据块并不是最小的单位， chunk 还会做等量切割，形成一系列的 item 。这也是上限的 item_size_max 前缀 item 的指向。 all = x * page page = n * chunk chunk = m * item page 的大小限制了数据的上限，如果想要存储超过该上限的值，则必须在客户端切割字符串。 ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:1:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"Redis Redis 是直接把大小限制放在存入字符串之前，直接用大小判断。与内存无直接联系。 ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:2:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"内存是如何申请的？ ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:3:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"Memcached do_slabs_newslab ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:4:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"Redis Redis 由于是直接使用 TCMalloc 和 Jemalloc ，所以其实是要了解这两者。 ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:5:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"TCMalloc ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:6:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"Jemalloc divided into region_size = 2 pages +--\u003e page_0 + | +--\u003e region_0 divided into +--\u003e page_1 + | +--\u003e run_0 +---------\u003e +--\u003e ... ... | | +--\u003e run_1 \u003c--+ +--\u003e page_n-1 + | | | +--\u003e region_y +--\u003e chunk_0 +--\u003e +--\u003e run_2 | +--\u003e page_n + | | | | +--\u003e run_3 \u003c---------------------------------+ | | | | | +--\u003e run_4 | | | | | | | +--\u003e run_5 \u003c---------------------------------------+ +--\u003e chunks +--\u003e +--\u003e ... | | | | | | +--\u003e ... | | | | | | | | | | | +--\u003e run_n | | | | | | | | | | | | | | | | | | | +--\u003e chunk_x | | | | point to | | | | | | | | +---\u003e run_1 +-+ + | | | | | | | | +--\u003e bin_0 +--\u003e +---\u003e ... +--\u003e region_size_3_runs | | | | | | | | | | +---\u003e run_p + | | arena +--\u003e +--\u003e bins +--\u003e +--\u003e ... | | | | | | | | | | | +--\u003e bin_z | | | | | | point to | | | | | | +--\u003e run_3 +-------------------------------------------------+ | | | | +--\u003e avail_clean +--\u003e +--\u003e ... | | | | | +--\u003e run_t | | | | point to | | | | +--\u003e run_5 +-------------------------------------------------------+ | | +--\u003e avail_dirty +--\u003e +--\u003e ... | +--\u003e run_j arena ：最高层次的内存管理单元，具有多个。每个 arena 管理多个内存单元 chunk ， arena 的其他信息是对 chunk 内部信息的汇总。 chunk ：内存单元。chunk 的空间被切割成多个大小相等的 run 。 run ：每个 run 由一个或多个连续的页（page）组成。每个 run 的空间被切割成多个大小相等的 region 。这些 region 以页为单位。 region ：用户申请到的内存。划分为三类： 小（Small）：单个 region 小于 page。 大（Large）：单个 region 大于 page 但小于 chunk 。 巨大（Huge）：单个 region 大于 chunk ，不归 arena 管。 bin ：每个 bin 管理 arena 内所有 region 相同的 run 。 未完待续… 参考文档 Structures in jemalloc： https://medium.com/iskakaushik/eli5-jemalloc-e9bd412abd70 jemalloc 源码分析： https://youjiali1995.github.io/allocator/jemalloc/ ptmalloc,tcmalloc和jemalloc内存分配策略研究： https://cloud.tencent.com/developer/article/1173720 图解 TCMalloc： https://zhuanlan.zhihu.com/p/29216091 Memcache-内存模型-源码分析： https://www.jianshu.com/p/a824ae00d9bb https://medium.com/iskakaushik/eli5-jemalloc-e9bd412abd70 ","date":"2020-03-24","objectID":"/2020/03/24/memcached-redis-memory/:7:0","tags":["Memcache","Redis"],"title":"Memcached Redis 字符串存储容量上限","uri":"/2020/03/24/memcached-redis-memory/"},{"categories":null,"content":"https://blog.csdn.net/guoziqing506/article/details/64122287 ","date":"2020-03-23","objectID":"/2020/03/23/trees/:0:0","tags":["B树","B+树"],"title":"各种树","uri":"/2020/03/23/trees/"},{"categories":null,"content":"传统的内存分配和现代的内存分配 传统的内存分配是在需要内存的时候使用 malloc() 函数直接向操作系统申请内存，在释放内存的时候用 free() 把内存还给操作系统。 malloc = memory allocate 直接使用这两个函数来管理内存的问题在于，每次申请内存都是一个很耗时的操作，而且频繁申请和释放内存会导致内存有很多碎片（外部碎片）。 外部碎片：分散在内存中的小块内存，其总和可以满足某个进程的申请要求，但由于内存碎片不连续，进程无法一次性申请这些内存。 例如： +-----+-----+-----+-----+-----+-----+-----+-----+-----+ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | +-----------------------------------------------------+ | |-------| | |-------| | |-------------| | | | |-------| | |-------| | |-------------| | | +-----------+-----------+-----------------+-----+-----+ +-----+-----+-----+-----+-----+-----+-----+-----+-----+ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | +-----------------+-----------------------------------+ | |-------| | | |-------------| | | | |-------| | | |-------------| | | +-----------+-----------+-----------------+-----+-----+ 申请 2 块单位为 2 的内存和 1 块单位为 3 的内存； 释放掉中间一块单位为 2 的内存； 此时如果要申请一块长度为 3 的内存，无法申请到。 为了解决这些问题，现代的内存分配器会一次性向操作系统申请一块（或者一堆）大内存。大内存可按照特定单位切割成一块块大小相等的内存块，不同类型的单位切割成的内存块大小不同。当进程内部需要内存的时候，找到一块与所需内存大小非常接近但又比所需内存大的内存块，并直接使用。 例如： 内存以 3 为单位做切割。 +-----+-----+-----+-----+-----+-----+-----+-----+-----+ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | +-----------+-----------------+-----------------------+ | |--------| | |-------| | |-------------| | | |--------| | |-------| | |-------------| | +-----------------+-----------------+-----------------+ +-----+-----+-----+-----+-----+-----+-----+-----+-----+ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | +-----------+-----------+-----+-----------------------+ | |--------| | | |-------------| | | |--------| | | |-------------| | +-----------------+-----------------+-----------------+ 申请 2 块单位为 2 的内存和 1 块单位为 3 的内存； 释放掉中间一块单位为 2 的内存； 此时如果要申请一块长度为 3 的内存，可以直接申请中间那块。 这比向系统直接申请的方式快得多，并且减少了外部碎片。但因为内存块往往比所需内存大一点，多出来的部分就成了内部碎片。这个问题相比外部碎片好一些，内部碎片可以通过设置以不同单位为块的内存来缓解。 ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:1:0","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":"现代内存分配器 现代内存分配器例如 Linux 的 SLUB Allocator （前身是 SLAB Allocator），此外还有比较常见的： jemalloc 、 TCMalloc 和 ptmalloc 。 TCMalloc = Thread-Caching Malloc ptmalloc = pthreads Malloc 以下举一些例子。 ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:2:0","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":"PHP PHP 的内存分配器参考 jemalloc 和 TCMalloc 的实现。 Zend/zend_alloc.c： /* * zend_alloc is designed to be a modern CPU cache friendly memory manager * for PHP. Most ideas are taken from jemalloc and tcmalloc implementations. * .... */ ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:2:1","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":"Golang Golang 的内存分配器是基于 TCMalloc 实现的。 runtime/malloc.go： // Memory allocator. // // This was originally based on tcmalloc, but has diverged quite a bit. ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:2:2","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":"Redis Redis 把 jemalloc 和 TCMalloc 作为可选项，可以在编译的时候选择用哪种分配器。 src/zmalloc.h： #if defined(USE_TCMALLOC) // 省略 #include \u003cgoogle/tcmalloc.h\u003e // 省略 #endif #elif defined(USE_JEMALLOC) // 省略 #include \u003cjemalloc/jemalloc.h\u003e // 省略 #endif 在 redis 源码中，自带了 jemalloc 源码 deps/jemalloc。 ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:2:3","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":"Memcache 使用的是 SLAB Allocator 的机制。 /* * Slabs memory allocation, based on powers-of-N. * .... */ ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:2:4","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":"扩展阅读 内存优化总结:ptmalloc、tcmalloc和jemalloc： http://www.cnhalo.net/2016/06/13/memory-optimize/ 图解Go内存分配器： https://tonybai.com/2020/02/20/a-visual-guide-to-golang-memory-allocator-from-ground-up/ Memcache-内存模型-源码分析： https://www.jianshu.com/p/a824ae00d9bb Linux slab 分配器剖析： https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/index.html 图解 TCMalloc： https://zhuanlan.zhihu.com/p/29216091 （七）PHP内存管理详解： https://blog.csdn.net/IT_10/article/details/94768679 ","date":"2020-03-23","objectID":"/2020/03/23/memory-allocation/:3:0","tags":["Memcache","Redis","PHP","GO","内存","内存分配"],"title":"内存分配器","uri":"/2020/03/23/memory-allocation/"},{"categories":null,"content":" In computer science, an associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection. —— wikipedia 在计算机科学中，关联数组、映射、符号表或者字典是一种由一系列(键、值)对组成的集合，且集合中的每个键最多出现一次。 由于写代码经常会用到 Map 的结构，经常会很好奇 Map 是怎么实现的。特别是如果只用到 C 语言，它应该怎么实现。 在去看 PHP 数组的源码之前，我自己设想了一下最简单的实现方式。 ","date":"2020-03-23","objectID":"/2020/03/23/simple-map/:0:0","tags":["Map"],"title":"简单的 map 实现","uri":"/2020/03/23/simple-map/"},{"categories":null,"content":"最简单的 map 由两个数组组成，一个存储 key，另一个存储 value。 类似于下面这种： 注意：这里画图的时候把 key 数组的值和下标做了个翻转 +-----+-----+-----+-----+-----+-----+-----+-----+ | | | | | | | | | value | 100 | 250 | 233 | 333 | 123 | 345 | 678 | 111 | | | | | | | | | | key array +-----------------------------------------------+ | | | | | | | | | index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | | | | | | | | | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | | | | | | | | v v v v v v v v +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | | | | | | | | | index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | | | | | | | | | | value array +-----------------------------------------------+ | | | | | | | | | value | 20 | 50 | 12 | 32 | 17 | 55 | 77 | 99 | | | | | | | | | | +-----+-----+-----+-----+-----+-----+-----+-----+ 每次要获取一个 key 对应的值的时候，先遍历 key 数组，找到 key 的下标。然后再用该下标去 value 数组获取下标对应的值，就得到了 value。 例如要获取 key 为 250 对应的值 map_get(map, 250)： 遍历 key array，下标为 0 的值是 100，下标为 1 的值是 250，这个就是想要找的。把下标 1 记到小本本上。 用下标 1 去 value array 找，得到 50 这个值。于是 map_get(map, 250) 的值是 50。 ","date":"2020-03-23","objectID":"/2020/03/23/simple-map/:1:0","tags":["Map"],"title":"简单的 map 实现","uri":"/2020/03/23/simple-map/"},{"categories":null,"content":"改进的方向 降低时间复杂度 key 和 value 对于其他类型的支持 扩容 顺序遍历 ","date":"2020-03-23","objectID":"/2020/03/23/simple-map/:2:0","tags":["Map"],"title":"简单的 map 实现","uri":"/2020/03/23/simple-map/"},{"categories":null,"content":"降低时间复杂度 我们可以知道的是，普遍用于减低时间复杂度的方式是使用 hash 来查找，时间复杂度为 O(1) 。 而使用 hash 的方式又会有 hash 冲突。hash 冲突主要有四种解决方式： 链地址法（常用） 开放定址法 再哈希法 不要和扩容时的 rehash 搞混 建立公共溢出区 ","date":"2020-03-23","objectID":"/2020/03/23/simple-map/:2:1","tags":["Map"],"title":"简单的 map 实现","uri":"/2020/03/23/simple-map/"},{"categories":null,"content":"扩容 通常采取双倍扩容。扩容会涉及到两个方面： 扩容条件 它主要根据负载因子来确定是否扩容。 rehash 扩容后需要对 key 重新做一次 hash，使得数据区分散在更多的 hash 桶内 ","date":"2020-03-23","objectID":"/2020/03/23/simple-map/:2:2","tags":["Map"],"title":"简单的 map 实现","uri":"/2020/03/23/simple-map/"},{"categories":null,"content":" In computer science, an associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection. —— wikipedia 在计算机科学中，关联数组、映射、符号表或者字典是一种由一系列(键、值)对组成的集合，且集合中的每个键最多出现一次。 —— 维基百科 Map 作为一种高效的数据存取数据结构，经常会被用到。在不同的场合，其具体实现上会有一定的差异，但总体上是相似的。其思想甚至可以扩展到很多地方。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:0:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Map 的分类 Map 实现方式总体上分为两类： 哈希表（Hash Table） 红黑树（Red-Black Tree） 大多数的实现都会选择 Hash Table，因为它的平均时间复杂度为 O(1)，而红黑树是 O(log N)。 Hash Table 之所以快，是因为它通过哈希函数和掩码将 key 转化为一个范围内的整数，通过将这个整数作为在数组中的索引，获取存储在数组中的碰撞链。这个查找过程是通过直接计算数组索引的地址偏移量来得到碰撞链的地址，速度极快。 如果哈希函数不是特别差，碰撞链平均元素个数在 8 个左右。最坏的情况下是所有 key 在转化后得到相同的整数，此时只有一条碰撞链，平均时间复杂度为 O(N)。 如果想要知道 Map 具体的不同实现方式，可以看 JAVA 中 java.util.Map 接口的实现。这里列出其中几种： 类型 数据结构 遍历 线程安全 HashMap Hash Table 随机 否 LinkedHashMap Hash Table 按添加顺序 否 ConcurrentHashMap Hash Table 随机 是 Hashtable Hash Table 随机 是 ConcurrentSkipListMap Skip Table 按键升序排序的顺序 是 TreeMap Red Black Tree 按键升序排序的顺序 否 更多实现可看： https://docs.oracle.com/javase/8/docs/api/java/util/Map.html 这里会看到 ConcurrentHashMap 和 Hashtable 似乎是一样的。它们两者的主要区别在于， Hashtable 为了线程安全，所有操作都会锁住整个 Hash Table；而 ConcurrentHashMap 则仅对冲突链（key 有相同的 Hash 值）加锁，即分段锁。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:1:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"各种场景下的实现 其他一些场景下，没有像 JAVA 一样提供多种实现。因此这里将这些实现对应到 JAVA 上来，尽管具体实现细节上会有差异。 场景 类似的 JAVA 实现 PHP5 的数组 LinkedHashMap PHP7 的数组 LinkedHashMap Redis 的字典 HashMap Go 的 map HashMap Go 的 sync.map Hashtable 这里特别备注一下， Go 的 sync.map 的实现是在 map 的基础上加了一层线程安全机制，底层用的还是 map。因此以下说明其他 Map 特性的时候，不再列出 sync.map。 PHP 在 Thread Safe 版本中提供了 Hash Table 的线程安全版本，在读写操作前对整个 Hash Table 加锁。 这些实现都采用 Hash Table，因此以下仅介绍 Hash Table 的几个重点和不同场景在实现上的区别。 先直观地看看它们结构上的区别吧！ JAVA 7： 每个 key-value 对都单独存储一个节点，并链接到下一个节点。 JAVA 8: JAVA 7 的升级，当 table 的长度大于等于 64 且链表的元素个数大于等于 8 时，将链表转换成红黑树。 PHP 5： 和 JAVA 7 的基本思路一样，都是每个 key-value 对都单独存储一个节点，并链接到下一个节点。但是由于要保证数据按加入时的顺序遍历，因此会多出一套指针用于记录元素加入的前后顺序。 PHP 7： 和 PHP 5 不一样，除了 hash 数组外，还增加了数据数组，其全局顺序按照加入顺序放到数据数组中，这样 Bucket 就可以删除用于标识全局顺序的指针了。 Redis 6： 每个 key-value 对都单独存储一个节点，并链接到下一个节点。 Go： 与前面几种都不一样。Go 把多组 key-value 对放在一个 bmap 里面，一个 bmap 可容纳 8 个对。如果不够，则再申请一个 bmap 链接到原来的 bmap 上。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:2:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Hash Table 的四个问题 如何计算哈希值 如何处理哈希碰撞 什么时候扩容 如何扩容 哈希函数用于获取 key 的哈希值。最终哈希值会与掩码执行按位与操作，仅根据数组的长度保留最后几位。哈希算法的好坏影响了冲突的概率，从而影响 Hash Table 的效率。最坏的情况下会使得 Hash Table 退化为链表，从 O(1) 降到 O(N)。 哈希碰撞是指两个不同 key 得到后几位相同的哈希值。碰撞的解决方式就是组织这些 \u003ckey, value\u003e 的方式。 扩容指元素达到一定数量时，申请的内存即将不够用（例如 PHP7 为键值对申请的固定长度数组），或者哈希冲突越来越多导致冲突链变长，此时应该执行扩容以获取更多内存或者减小冲突。 Rehash 是指对所有键值对的 key 重新做一次 Hash 操作，然后和新容量对应的掩码做按位与操作，得到新的冲突链索引。这样在扩容或者其他情况的时候，可以重新组织元素所在的位置。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:3:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"问题一：如何计算哈希值 以下是这部份的参考： 几种常见的hash函数 https://www.jianshu.com/p/bb64cd7593ab 常见的哈希算法和用途 https://blog.cyeam.com/hash/2018/05/28/hash-method 漫谈非加密哈希算法 https://segmentfault.com/a/1190000010990136 hash 函数用于获取 key 的哈希值，即 hash = hash_function(key)。 hash 函数有两种分类： 加密型（md5，sha1，sha256，aes256 …） 非加密型（通常用于查找） Map 实现所用到的 hash 函数通常是非加密型的，它的速度比加密型快，但也更容易产生 hash 碰撞。 hash 碰撞指的是不同的 key 在经过 hash 函数处理后得到二进制后几位相同的 hash 值。 hash 函数涉及到安全问题，主要是避免被攻击者知道 hash 规则，借以生成大量碰撞的 key 执行攻击。 Map 的 key 的类型在有些实现中有限制，另外一些没有限制。这里只取 key 为字符串类型的情况来介绍。 这个知识点基本上属于满足好奇心。其他情况例如面试的时候，面试官不会问。毕竟像最常用的 times33 算法中的 5381 和 33 两个参数都是玄学数字，是基于实验比较出来的，没什么好说。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:4:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Redis 的哈希函数 Redis 使用的是 SipHash 实现，这个实现的特点是解决 Hash-Flooding Attack 这个安全问题。 什么是哈希洪水攻击（Hash-Flooding Attack）？ - Gh0u1L5的回答 - 知乎 https://www.zhihu.com/question/286529973/answer/676290355 src/dict.c /* The default hashing function uses SipHash implementation * in siphash.c. */ // ... uint64_t dictGenHashFunction(const void *key, int len) { return siphash(key,len,dict_hash_function_seed); } SipHash 的具体实现见 src/siphash.c。 除了 SipHash 外，还有一种 Bernstein’s Hash（别名 DJB，DJBX33A，times33）。 下面这个是 Redis 的 C 语言客户端中的： deps/hiredis/dict.c /* Generic hash function (a popular one from Bernstein). * I tested a few and this was the best. */ static unsigned int dictGenHashFunction(const unsigned char *buf, int len) { unsigned int hash = 5381; while (len--) hash = ((hash \u003c\u003c 5) + hash) + (*buf++); /* hash * 33 + c */ return hash; } 这里把 hash * 33 的操作改为位操作 (hash \u003c\u003c 5) + hash，意思是 x*2^5+ x*1 = x*(2^5 + 1)，目的是为了加速。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:4:1","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"JAVA 的哈希函数 JAVA 的实现是基于 times33 的改变，它把 33 改成了 31，并且缓存字符串的 hash 值。 java.lang.String public final class String implements java.io.Serializable, Comparable\u003cString\u003e, CharSequence { // ... /** Cache the hash code for the string */ private int hash; // Default to 0 // ... public int hashCode() { int h = hash; if (h == 0 \u0026\u0026 value.length \u003e 0) { char val[] = value; for (int i = 0; i \u003c value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } // ... } 如果只是单纯地使用这个 hash 函数，那么会遭受到 Hash-Flooding Attack。从 JAVA 8 开始，对哈希碰撞采取了将过长的哈希链转换为红黑树来解决这个问题。后面会详细说明。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:4:2","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP 5 的哈希函数 PHP 5 也是基于 times33 的改变，它主要是减少了循环的次数。存在 Hash-Flooding Attack 安全问题。 src/Zend/zend_string.h static inline ulong zend_inline_hash_func(const char *arKey, uint nKeyLength) { register ulong hash = 5381; /* variant with the hash unrolled eight times */ for (; nKeyLength \u003e= 8; nKeyLength -= 8) { hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; hash = ((hash \u003c\u003c 5) + hash) + *arKey++; } switch (nKeyLength) { case 7: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; /* fallthrough... */ case 6: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; /* fallthrough... */ case 5: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; /* fallthrough... */ case 4: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; /* fallthrough... */ case 3: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; /* fallthrough... */ case 2: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; /* fallthrough... */ case 1: hash = ((hash \u003c\u003c 5) + hash) + *arKey++; break; case 0: break; EMPTY_SWITCH_DEFAULT_CASE() } return hash; } ","date":"2020-03-15","objectID":"/2020/03/15/map/:4:3","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP 7 的哈希函数 PHP 7 的实现比 PHP 5 复杂一些，它针对了不同操作系统以及处理其做了优化，但本质上还是使用 times33。存在 Hash-Flooding Attack 安全问题。 src/Zend/string.h static zend_always_inline zend_ulong zend_inline_hash_func(const char *str, size_t len) { zend_ulong hash = Z_UL(5381); #if defined(_WIN32) || defined(__i386__) || defined(__x86_64__) || defined(__aarch64__) /* Version with multiplication works better on modern CPU */ for (; len \u003e= 8; len -= 8, str += 8) { # if defined(__aarch64__) \u0026\u0026 !defined(WORDS_BIGENDIAN) // ... # else // ... # endif } // ... #else // ... 这里和 PHP 5 一样 #endif } ","date":"2020-03-15","objectID":"/2020/03/15/map/:4:4","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Go 的哈希函数 Go 在 1.17 版本之前使用的是基于 xxhash 和 cityhash 改进的算法。为了解决 Hash-Flooding Attack 这个安全问题，初始化 map 时随机 hash seed，使得不同 map 对相同 key 的 hash 结果不一致。 随机数： // A header for a Go map. type hmap struct { // ... 省略 hash0 uint32 // hash seed // ... 省略 } hash 算法位置： src/runtime/hash64.go // Hashing algorithm inspired by // xxhash: https://code.google.com/p/xxhash/ // cityhash: https://code.google.com/p/cityhash/ 具体实现太长就不贴出来了。 go 1.17 把换成了 wyhash // Hashing algorithm inspired by // wyhash: https://github.com/wangyi-fudan/wyhash ","date":"2020-03-15","objectID":"/2020/03/15/map/:4:5","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"问题二：如何处理哈希碰撞 hash 碰撞的处理方式有几种： 链地址法 开放地址法 线性探测法 二次探测法（平方探测法） 双散列法 伪随机探测法 再哈希法（Rehashing） 公共溢出区法 下表展示了各种 Map 实现使用的碰撞处理方式： 场景 碰撞处理方式 JAVA 的 HashMap 链地址法 PHP5 的数组 链地址法 PHP7 的数组 链地址法 Go 的 map 链地址法 Redis 的 hash 链地址法 尽管都是使用链地址法，但链地址法的具体实现却不相同。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"JAVA HashMap 的冲突链 链表存储在 table 数组中。 每当添加一个元素，就 new 一个 Node，把 key 和 value 存进去。接着链接到冲突链上。 对于 Node 插入到冲突链的位置，JAVA 8 之前和 JAVA 8 开始，有两种不同的方式。 JAVA 8 之前，将 Node 插入到链表头部（头插法）。 JAVA 8 开始，将 Node 插入到链表尾部（尾插法）。 使用尾插法的原因是头插法在扩容的时候有可能会出现环。因为头插法会在扩容时使得链表前后两个节点的位置对调。如果冲突链前后两个节点在扩容后仍然位于同一条冲突链，就有可能出现这种情况。 上面说到 JAVA 8 的 HashMap 解决 Hash-Flooding Attack 的安全问题，是在链表同时满足两个条件的情况下转化为红黑树。 table 的长度大于等于 64 如果不满足该条件，则 table 双倍扩容。 链表的元素个数大于等于 8 在红黑树的情况下，如果元素个数小于等于 6，则将红黑树还原为链表。使用 6 而不是 7 是为了避免频繁地在红黑树和链表之间转换。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:1","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"JAVA LinkedHashMap 的冲突链 LinkedHashMap 继承了 HashMap。并且添加了 head 和 tail 用于存储双向链表。 每当添加一个元素，除了 HashMap 的处理，还会额外地将该元素放到 tail 后面。遍历的时候按添加顺序遍历。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:2","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP 5 数组的冲突链 链表存储在 arBuckets 中。每个 Bucket 存储一个元素。 每当添加一个元素，先申请一个 Bucket 内存，然后再把 key 和 value 拷贝进去，最后把 Bucket 链接到链表的头部（头插法）。 所有元素的内存不是一整块连续的内存。 Bucket 自身维护了全局添加顺序的上下元素的指针。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:3","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP 7 数组的冲突链 链表存储在 arData 中。每个 Bucket 存储一个元素。 内存会预先申请好连续的 nTableSize 个 Bucket 的空间（数组）。 每当添加一个元素，会使用 nNextFreeElement 指向的 Bucket，把 key 和 value 拷贝进去，最后把 Bucket 链接到链表的头部（头插法）。图中 Bucket 是包含了 zval，应看做一个整体而不是冲突链的两个元素。 用于存储元素值的 zval 维护了冲突链下一个 Bucket 的指针。 由于申请连续的空间用于按顺序存储 Bucket，无需为了存储元素添加顺序而加入指针。遍历时直接遍历数组即可。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:4","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"redis Hash 的冲突链 链表存储在 table 中。每个 dictEntry 存储一个元素。 每当添加一个元素，会先申请一个 dictEntry 内存，然后把 dictEntry 链接到链表的头部（头插法），最后把 key 和 value 存储进去。 另一个 dictht 在扩容的时候使用。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:5","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Go map 的冲突链 链表存储在 buckets 中。初始化时，会申请 buckets 数量的 bmap。每个 bmap 由 8 个 bucket 组成。 注意 8 这个数字，JAVA 里面转红黑树也是用 8 作为分界点。 每当添加一个元素，会在 bmap 里面按顺序找到一个空位置，把 key 和 value 复制进去。 需要注意的是，key 和 value 是分内存区块存储的。所有 key 在内存上按顺序紧挨着，而不是每个键值对的 value 都在其 key 的内存位置之后。 如果 bmap 存满了，则申请新的 bmap，并链接到已有 bmap 的 overflow 上面。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:5:6","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"问题二扩展：根据 hash 值找目标元素的过程 获取 key 的 hash 只是第一步，还需要其他步骤才能找到目标元素。 大致涉及三个步骤： 最终 hash 值的获取 对 hash 值做运算获取碰撞链首个 Bucket 在数组中的索引 遍历碰撞链，通过比较找到元素 ","date":"2020-03-15","objectID":"/2020/03/15/map/:6:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"JAVA JAVA 在获取到 hash 值之后，会使用扰动函数对 hash 值再做一次转化。JAVA 7 和 JAVA 8 的扰动函数不一致。JAVA 7 执行了 5 次异或操作， JAVA 8 仅做了一次。 JAVA 7： java.util.HashMap final int hash(Object k) { int h = hashSeed; if (0 != h \u0026\u0026 k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h \u003e\u003e\u003e 20) ^ (h \u003e\u003e\u003e 12); return h ^ (h \u003e\u003e\u003e 7) ^ (h \u003e\u003e\u003e 4); } JAVA 7 的就不详细解释了。 JAVA 8： java.util.HashMap public class HashMap\u003cK,V\u003e extends AbstractMap\u003cK,V\u003e implements Map\u003cK,V\u003e, Cloneable, Serializable { // ... static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u003e\u003e\u003e 16); } // ... } 将 hash 值按位右移 16 位，再跟 hash 值做异或操作。这使得 hash 值在保持高 16 位不变的同时，让低 16 位带上了高 16 位的信息。在字符串随机且 table 数组为 512 个元素的情况下，能减少 10% 的碰撞。 JAVA 的 int 类型占 32 位 经过扰动函数的处理后，最终的 hash 值会用 (table.length - 1) 作为掩码取 hash 值的低位。这个低位的值就是碰撞链的位置。 length = 8 hash: 1111 1111 1111 1111 1111 0000 1110 1010 hash \u003e\u003e\u003e 16: 0000 0000 0000 0000 1111 1111 1111 1111 hash = hash^(hash \u003e\u003e\u003e 16): 1111 1111 1111 1111 0000 1111 0001 0101 length - 1: 0000 0000 0000 0000 0000 0000 0000 0111 index = hash \u0026 (length - 1): 0000 0000 0000 0000 0000 0000 0000 0101 最终得到的 index = 5，于是从 table[index] 中取出碰撞链的头节点。 接着遍历该链表做比较。比较的时候要依次满足两个条件： 经过扰动函数处理后的 hash 值相同 字符串 key 比较结果相等 ","date":"2020-03-15","objectID":"/2020/03/15/map/:6:1","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP 5 的数组 由于 PHP 5 和 PHP 7 的数组内存结构不一样，因此要分开说。 src/Zend/zend_hash.h typedef struct bucket { ulong h; /* Used for numeric indexing */ uint nKeyLength; void *pData; void *pDataPtr; struct bucket *pListNext; struct bucket *pListLast; struct bucket *pNext; struct bucket *pLast; const char *arKey; } Bucket; typedef struct _hashtable { uint nTableSize; uint nTableMask; uint nNumOfElements; ulong nNextFreeElement; Bucket *pInternalPointer; /* Used for element traversal */ Bucket *pListHead; Bucket *pListTail; Bucket **arBuckets; /* \u003c--- */ dtor_func_t pDestructor; zend_bool persistent; unsigned char nApplyCount; zend_bool bApplyProtection; #if ZEND_DEBUG int inconsistent; #endif } HashTable; PHP 5 的 arBuckets 是一个数组，它存储碰撞链的首元素。 获取 hash 值之后，不会像 JAVA 那样经过扰动函数的处理。 用掩码 nTableMask = (nTableSize - 1) 和 hash 值做按位与运算，取得 hash 值的低位作为索引 index。 用 arBuckets[index] 获取碰撞链的首元素。接着遍历该链表比较。比较的时候分为两种类型： 如果字符串 key 是同一个指针，则表示相同 如果不是同一个指针，则比较 hash 值，再比较 key 的长度，最后比较字符串是否完全一致 ","date":"2020-03-15","objectID":"/2020/03/15/map/:6:2","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP 7 的数组 typedef struct _zval_struct zval; struct _zval_struct { zend_value value; // ... 省略 union { uint32_t next; /* 指向 hash 冲突链的下一个元素 */ // ... 省略 } u2; }; typedef struct _Bucket { zval val; zend_ulong h; /* hash value (or numeric index) */ zend_string *key; /* string key or NULL for numerics */ } Bucket; typedef struct _zend_array HashTable; struct _zend_array { zend_refcounted_h gc; union { struct { ZEND_ENDIAN_LOHI_4( zend_uchar flags, zend_uchar _unused, zend_uchar nIteratorsCount, zend_uchar _unused2) } v; uint32_t flags; } u; uint32_t nTableMask; Bucket *arData; /* \u003c--- */ uint32_t nNumUsed; uint32_t nNumOfElements; uint32_t nTableSize; uint32_t nInternalPointer; zend_long nNextFreeElement; dtor_func_t pDestructor; }; /* * HashTable Data Layout * ===================== * * +=============================+ * | HT_HASH(ht, ht-\u003enTableMask) | * | ... | * | HT_HASH(ht, -1) | * +-----------------------------+ * ht-\u003earData ---\u003e | Bucket[0] | * | ... | * | Bucket[ht-\u003enTableSize-1] | * +=============================+ */ PHP 7 的 arData 存储两部分数据，从上面源码画的 Layout 可以看出， arData 指向 Bucket 数组的首元素。画面上面的部分，也就是内存中相对 arData 为低地址的位置，存储 hash 值及其对应碰撞链头部在 Bucket 数组中的下标。 由于结构的关系，碰撞链头部相对于 arData 的位置（索引值）存储在上半部分 Hash 区。Hash 区也是一个数组，数组的索引是碰撞链头部的键值对 key 的 hash 值经过掩码处理后的数字。对于 arData 来说，经过掩码处理的 hash 值应该为负整数才能用 arData 去获取。 这就影响到了作为掩码的 nTableMask 的值，其值为 (-2 * nTableSize)，类型为 uint32_t。 用 nTableMask 与 hash 值做按位或操作，得到负整数。该负整数的范围是 [-2 * nTableSize, -1]。 用该负整数到 arData 获取到碰撞链首个 Bucket 在 Bucket 数组中的正整数下标 index。然后通过 arData[index] 获取到碰撞链首个 Bucket 。 接着遍历该链表比较。和 JAVA 差不多，比较的时候要依次满足两个条件： hash 值相同 字符串 key 比较结果相等 ","date":"2020-03-15","objectID":"/2020/03/15/map/:6:3","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Redis 的 Hash Table typedef struct dictEntry { void *key; union { void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; } dictEntry; // ... 省略 /* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */ typedef struct dictht { dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used; } dictht; typedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */ } dict; Redis 的 hash 值也没有经过扰动函数的处理。 其掩码 sizemask 是正整数，值为 (size - 1)。 注意 dict 结构里面的 dictht，它是一个拥有两个元素的数组。数组第二个元素存储正在执行 Rehash 的 Hash Table。 在使用 sizemask 对 hash 值做按位与得到 idx，用 table[idx] 获取到碰撞链首个 dictEntry 。 接着遍历该链表比较。与 PHP 5 相似，比较的时候分为两种： 如果 key 的指针如果是同一个指针，则表示相同 如果不是同一个指针，则调用函数比较两个 key 是否相等 ","date":"2020-03-15","objectID":"/2020/03/15/map/:6:4","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Go 的 map Go 的 map 的 hash 值也没经过扰动函数处理。 bucketMask 是 buckets 数组容量 - 1。 用 bucketMast 对 hash 值做按位与获得 index，然后用类似于 buckets[index] 的方式取得碰撞链首个 bmap 。 这里的 bmap 可存储 8 个 Bucket。 获取首个 bmap 之后，开始遍历查找。 与其他实现不同的地方在于， Go 引入了 Top Hash 用空间换时间，快速比较确定冲突链中目标值的位置。Top Hash 取 key 的 hash 值的高 8 位。 +-----------+ | bmap | +-----------+ | tophash | +-----------+ | keys | +-----------+ | values | +-----------+ | pad | +-----------+ | *overflow | +-----------+ 其中 tophash 是一个数组，每个 Top Hash 对应一个 key。 遍历的时候，先比较 Top Hash 是否相等。如果相等，则根据 Top Hash 所在的位置找到 keys 数组中对应位置的 key。然后比较 key 是否相等。如果不相等则继续遍历。一个 Bucket 遍历完后，取 overflow 继续遍历。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:6:5","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"问题三：什么时候扩容 随着元素的数量增长，冲突链会变得越来越长。由于查询时会一个个遍历冲突链，因此冲突链变长意味着查询效率降低。 扩容是增加存储冲突链头部的数组的容量，同时改变掩码，使得原先同一个冲突链上的元素分散开来，减少冲突链长度。 由于各种 Hash Table 的实现不同，它们的扩容条件尽管大致相同，但细节上有差异。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:7:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"JAVA HashMap 的扩容条件 // TODO ","date":"2020-03-15","objectID":"/2020/03/15/map/:7:1","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP5 数组的扩容条件 // TODO ","date":"2020-03-15","objectID":"/2020/03/15/map/:7:2","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"PHP7 数组的扩容条件 // TODO ","date":"2020-03-15","objectID":"/2020/03/15/map/:7:3","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Redis 的扩容条件 // TODO ","date":"2020-03-15","objectID":"/2020/03/15/map/:7:4","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"Go 的扩容条件 // TODO 扩容与 Rehash 有很强的关联。做双倍扩容之后，会执行 Rehash 重新计算旧元素的 key 的 hash，然后放到新的 Hash Table 里面。 不过有些情况下不必触发扩容，只需执行 Rehash 来释放部分空间即可。例如 PHP 7 使用数组存储 bucket。如果中间的元素被删除很多，在后续数组空间不够用的时候，会执行 Rehash 将元素往前移动。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:7:5","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"问题四：如何扩容 使用 名称 方式 JAVA HashMap 一次性 PHP 5 数组、关联数组 一次性 PHP 7 数组、关联数组 一次性 Go Map 渐进式 Redis hash 渐进式 JAVA HashMap JAVA 会执行以下函数做双倍扩容： final Node\u003cK,V\u003e[] resize() 双倍扩容的时候，会申请加倍后的容量的 Node 数组。 然后依次遍历旧 Node 数组，及每条碰撞链，重新计算 key 的 hash 值，依次放入到新的数组里面。 PHP 5 PHP 5 会执行以下函数来触发扩容： static void zend_hash_do_resize(HashTable *ht) 首先加倍可容纳元素数量，并申请新的空间挂载到数据区。 接着按添加顺序遍历元素，重新计算元素 key 的 hash 值，然后加入到新的空间。 PHP 7 由于 PHP 7 使用数组存储 Bucket 指针，因此在扩容的时候会一次性申请双倍的数组内存。接着执行 Rehash。 Rehash 分为两种情况： 存放 Bucket 的数组中间没有元素被删除掉 存放 Bucket 的数组中间有元素被删除掉 第一种情况很简单，就是遍历旧数组，然后将元素赋值到新数组。 第二种情况也是遍历旧数组，但是如果碰到元素是无效元素，则会跳过。 Redis Redis 的扩容类型是渐进式扩容。 进入扩容状态时，会启用第二个 hash table（即 ht[1]）。 typedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */ } dict; 触发搬迁的场景是：获取、修改、添加、删除。 每对一个 key 操作，就搬迁一条碰撞链。 实际执行搬迁的逻辑是在： int dictRehash(dict *d, int n) 第二个参数表示要搬迁的碰撞链的条数。 搬迁进度记录在 rehashidx，每次都会搬迁 rehashidx 指向的碰撞链。搬迁完， rehashidx 前进一步。 搬迁的具体过程： 遍历碰撞链，重新计算元素 key 的 hash 值。经过新的掩码处理后放到新 hash table 里面。 搬迁完一条碰撞链后，会把旧的 hash table（即 ht[0]）对应的碰撞链置为 NULL。 判断是否全部搬迁完毕。如果是，则把 ht[1] 覆盖到 ht[0] 上，然后重置 ht[1]。 需要注意的是，如果正在遍历 hash table（即 iterators 不为 0），则不会执行搬迁。 Go 的 map map 的扩容类型是渐进式扩容。 触发搬迁的场景有两个： mapassign：添加或更新 mapdelete：删除 实际执行搬迁的逻辑是在： func evacuate(t *maptype, h *hmap, oldbucket uintptr) 每次搬迁一条碰撞链。其中第三个参数 oldbucket 表示待搬迁的碰撞链所在的位置。 在触发搬迁时，会执行 growWork() 函数。这个函数会先执行一次 evacuate()，如果执行完发现还有其他碰撞链没有搬迁，则再执行一次 evacuate()。 但两次搬迁的冲突链不一样。第一次搬迁的是即将访问的碰撞链，而第二次搬迁的碰撞链所在的位置由一个 nevacuate 参数确定，该参数从 0 逐渐增长。 搬迁的具体过程（假设双倍扩容）： 找到碰撞链，判断该链是否已搬迁。 如果已搬迁则跳过搬迁操作。 如果未搬迁，则创建两条碰撞链。遍历现有碰撞链，将元素分配到这两条碰撞链里面。 除非出现复杂情况，否则判断掩码后的 hash 最高位是否为 1。如果为 1 则放到第二条碰撞链；否则放到第一条碰撞链。 如果搬迁的正好和 nevacuate 相等，则 nevacuate 前进一步。 如果 nevacuate 所指碰撞链已经搬迁，则继续前进一步。直到碰到未搬迁的碰撞链，或者结束。 如果 nevacuate 已经指向最后一条碰撞链，则结束搬迁。 ","date":"2020-03-15","objectID":"/2020/03/15/map/:8:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"问题四扩展：扩容过程中的数据访问 // TODO ","date":"2020-03-15","objectID":"/2020/03/15/map/:9:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":"参考链接 https://www.cnblogs.com/laipimei/p/11282055.html https://www.cnblogs.com/laipimei/p/11275235.html https://blog.csdn.net/g5zhu5896/article/details/82968287 https://mp.weixin.qq.com/s/UWhn1uu401GlbJ0jmpRZmA ","date":"2020-03-15","objectID":"/2020/03/15/map/:10:0","tags":["Map","PHP","JAVA","GO","Redis"],"title":"【数据结构】Map （映射）的各种实现","uri":"/2020/03/15/map/"},{"categories":null,"content":" 本文所用源码为 PHP 7.4.4 的版本。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:0:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"PHP 7 数组概述 PHP 中的数组实际上是一个有序映射。映射是一种把 values 关联到 keys 的类型。此类型在很多方面做了优化，因此可以把它当成真正的数组，或列表（向量），散列表（是映射的一种实现），字典，集合，栈，队列以及更多可能性。由于数组元素的值也可以是另一个数组，树形结构和多维数组也是允许的。 —— PHP 官方文档中文版 这里主要关注两个点： key 可以是整数，也可以是字符串。Float、Bool、Null 类型的 key 会被转换为整数或者字符串存储，其他类型的会报错。 value 可以是任意类型。 遍历数组时，数组元素按照其 key 添加的顺序依次取出。 PHP 7 的数组分为 packed array 和 hash array 两种类型，在满足一定条件时可以互转。 hash array 的 key 可以是整数也可以是字符串，在 hash 冲突时使用链表（冲突链）来解决冲突问题。 packed array 的所有 key 是自然数，且依次添加的元素的 key 逐渐增大（不要求连续）。它的耗时和内存占用都比 hash 数组低。 以下仅介绍 hash array 相关的内容。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:1:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"主要数据类型 下图是数组主要的数据类型： Hash 区 arData Data 区 + | 指 针 指 向 Data 区 的 开 始 v +----------+----------+----------+----------+----------+----------+----------+----------+ | | | | | | | | | |nTableMask|nTableMask| ...... | -1 | 0 | 1 | ...... |nTableSize| | | +1 | | | | | | +1 | +---------------------------------------------------------------------------------------+ | | | | | | | | | | uint32_t | uint32_t | ...... | uint32_t | Bucket | Bucket | ...... | Bucket | | | | | | | | | | +----------+----------+----------+----------+----------+----------+----------+----------+ 从整体看，这是一个数组。但入口是 arData 而不是处于最左侧的一个元素。arData 把数组分为两部分： 左边是 Hash 区，其值为 uint32_t 类型，是冲突链的第一个元素在 Data 区的下标； 右边是 Data 区，其值为 Bucket 类型，用于存储数据及其相关信息。 由于 arData 主要指向 Data 区，因此其默认类型被配置为 Bucket 指针。 在申请内存时，会把 Hash 区所需的内存大小加上 Data 区所需的内存大小，然后一起申请。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:2:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"Bucket 长什么样？ zend_types.h： /* 数组的基本元素 */ typedef struct _Bucket { zval val; /* 值 */ zend_ulong h; /* hash 值（或者整数索引） */ zend_string *key; /* 字符串 key（如果存储时用整数索引，则该值为 NULL） */ } Bucket; Bucket 把 key 和 value 放在一起了。 在冲突链中，Bucket 是一个节点。那么此时心里会有一个疑问：怎么获取冲突链的下一个节点？ ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:2:1","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"冲突链 说到链表，会很自然地想到链表元素的结构体里包含着指向下一个元素的指针 next 。例如单向链表： typedef struct listNode { struct listNode *next; void *value; } listNode; 但 Bucket 却不包含这个指针。 会不会在 Bucket 上一层，也就是数组的结构体定义中有一个专门存放冲突链的地方？ zend_types.h： typedef struct _zend_array HashTable; struct _zend_array { zend_refcounted_h gc; union { struct { ZEND_ENDIAN_LOHI_4( zend_uchar flags, zend_uchar _unused, zend_uchar nIteratorsCount, zend_uchar _unused2) } v; uint32_t flags; } u; uint32_t nTableMask; // 用于把 hash 值转化为 [nTableMask, -1] 区间内的负数。根据 nTableSize 生成。 Bucket *arData; // 指向 Data 区的指针。 uint32_t nNumUsed; // Data 区最后一个有效 Bucket 的下标 + 1。 uint32_t nNumOfElements; // 存在多少个有效 Bucket。删除数组元素时，会使其减一。 uint32_t nTableSize; // 总共有多少空间。 uint32_t nInternalPointer; zend_long nNextFreeElement; dtor_func_t pDestructor; }; 想错了，换个角度想想.jpg 那往 Bucket 下一层看看： zend_types.h： typedef struct _zval_struct zval; struct _zval_struct { zend_value value; // 通用值结构。存储基础类型（double）或指针（数组、对象等等） union { struct { // 省略其他定义 } v; uint32_t type_info; // 值的类型，例如 IS_ARRAY 、IS_UNDEF } u1; union { uint32_t next; // 指向 hash 冲突链的下一个元素 \u003c--- 就是这里 // 省略其他定义 } u2; // u2 表示第二个 union }; 惊！链表元素的 next 居然藏在 PHP 的通用数据类型 zval 里面。 想不到吧？.jpg 补充一点： PHP HashMap 的冲突链始终是一个链表，不会像 JAVA 的 HashMap 那样在达成一定条件时转成红黑树。这会带来一定的问题。后面再详细说明。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:2:2","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"怎么看 HashTable ？ 再看一遍结构体。 zend_types.h： typedef struct _zend_array HashTable; struct _zend_array { zend_refcounted_h gc; union { struct { ZEND_ENDIAN_LOHI_4( zend_uchar flags, zend_uchar _unused, zend_uchar nIteratorsCount, zend_uchar _unused2) } v; uint32_t flags; } u; uint32_t nTableMask; // 根据 nTableSize 生成的负数。用于把 hash 值转化为 [nTableMask, -1] 区间内的负整数，防止越界。 Bucket *arData; // 指向 Data 区的指针。 uint32_t nNumUsed; // Data 区最后一个有效 Bucket 的下标 + 1。 uint32_t nNumOfElements; // 存在多少个有效 Bucket。删除数组元素时，会使其减一。 uint32_t nTableSize; // 总共有多少空间。 uint32_t nInternalPointer; // 内部指针。受到 reset() 、 end() 、 next() 等的影响。 zend_long nNextFreeElement; dtor_func_t pDestructor; }; 有效 Bucket 指的是 Bucket val 的类型不为 IS_UNDEF 。也就是不为未定义的（undefined）值。无效 Bucket 反之。 nNumUsed 、nNumOfElements 、 nTableSize 的区别： nNumUsed = 4 nNumOfElements = 3 nTableSize = 8 +----------+----------+-----------+----------+-----------+-----------+-----------+ | | | | | | | | | 0 | 1 | 2 | 3 | 4 | ...... | 7 | | | | | | | | | +--------------------------------------------------------------------------------+ | | | | | | | | | Bucket | Bucket | Undefined | Bucket | Undefined | Undefined | Undefined | | | | Bucket | | Bucket | Buckets | Bucket | +----------+----------+-----------+----------+-----------+-----------+-----------+ ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:3:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"数组的主要操作 PHP 数组主要用到的基本操作有：查找、添加、更新、删除 PHP 内部操作有：rehash 、扩容 其中查找是较为简单的，添加、更新、删除都包含了查找的动作，因此先看查找。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:4:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"查找 由于 key 有整数和字符串这两种类型，因此查找的实现也分为两种。这里以整数 key 为例。 读源码时要注意 HT_HASH_* 和 HT_DATA_* 开头的函数，分别代表着在 Hash 区和 Data 区的操作。 zend_hash.c static zend_always_inline Bucket *zend_hash_index_find_bucket(const HashTable *ht, zend_ulong h) { uint32_t nIndex; uint32_t idx; Bucket *p, *arData; arData = ht-\u003earData; nIndex = h | ht-\u003enTableMask; // 避免 Hash 区越界 idx = HT_HASH_EX(arData, nIndex); // 在 Hash 区取 nIndex 位置的值，结果是 Data 区某个 Bucket 的下标 while (idx != HT_INVALID_IDX) { ZEND_ASSERT(idx \u003c HT_IDX_TO_HASH(ht-\u003enTableSize)); // 确保 Data 区没有越界 p = HT_HASH_TO_BUCKET_EX(arData, idx); // 用 Data 区下标获取 Bucket，即冲突链的第一个 Bucket if (p-\u003eh == h \u0026\u0026 !p-\u003ekey) { // 整数 key 存到 h，因此比对 h。p-\u003ekey 为 NULL 表示 Bucket 的 key 为整数 key return p; } idx = Z_NEXT(p-\u003eval); // 没有找到的话，从当前的 Bucket 获取冲突链的下一个 Bucket } return NULL; // 链表遍历完也没找到，那就是不存在 } 举个例子： nTableSize = 8 nTableMask = -(nTableSize + nTableSize) = (-16) = (11111111111111111111111111110000) 10 2 h = (100000000) = (00000101111101011110000100000000) 10 2 nIndex = (h | nTableMask) = (11111111111111111111111111110000) = (-16) 2 + 10 | +-------------------------------------------------------------------+ | | Hash arData Data | | + | | +----------------------------+ v v v | | +---------+---------+----------+---------+---------+---------+----------+---------+ | | | | | | | | | | | | -16 | -15 | ...... | -1 | 0 | 1 | ...... | 7 | | | | | | | | | | | | +---------------------------------------------------------------------------------+ | | | | | | | | | | | | 1 | 6 | ...... | 5 | Bucket0 | Bucket1 | ...... | Bucket7 | | | | | | | | | | | | +---------+---------+----------+---------+---------+---------+----------+---------+ | | + + ^ | | | next | | | +---------------------+ | | | +-------------------------------------------------------------------------------+ 至于为什么 nTableMask = -(nTableSize + nTableSize) ，见下文的【负载因子】。 nTableMask 使得无论多大的 uint32_t ，在按位或以及转成有符号整数后，都会变成负整数，并且其值会在 [nTableMask, -1] 这个区间。 介绍完整数 key 的查找，顺便对比一下字符串 key 的查找，不同之处如下： 字符串 key 会存到 p-\u003ekey 里面，而这个字符串的 hash 存到 p-\u003eh 里面。 在比较 key 的时候，整数 key 是比较两个整数是否相等，而字符串 key 会先比较 hash 是否相等，然后比较两个字符串是否相等。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:4:1","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"添加 依然取整数 key 为例。这里不关注更新元素的部分和 packed array 的部分。 zend_hash.c: static zend_always_inline zval *_zend_hash_index_add_or_update_i(HashTable *ht, zend_ulong h, zval *pData, uint32_t flag) { // ... 省略代码 idx = ht-\u003enNumUsed++; // 使用空间 + 1 nIndex = h | ht-\u003enTableMask; // 取 hash 值对应的 Hash 区的下标 p = ht-\u003earData + idx; // 获取指向新元素的指针 Z_NEXT(p-\u003eval) = HT_HASH(ht, nIndex); // 新 Bucket 指向 Hash 区下标所指的冲突链第一个 Bucket HT_HASH(ht, nIndex) = HT_IDX_TO_HASH(idx); // Hash 区下标指向新 Bucket if ((zend_long)h \u003e= (zend_long)ht-\u003enNextFreeElement) { ht-\u003enNextFreeElement = h \u003c ZEND_LONG_MAX ? h + 1 : ZEND_LONG_MAX; } add: ht-\u003enNumOfElements++; // 元素个数 + 1 p-\u003eh = h; // 整数 key 的下标就是 hash p-\u003ekey = NULL; // 整数 key 时，必须把 p-\u003ekey 设置为 NULL ZVAL_COPY_VALUE(\u0026p-\u003eval, pData); // 把要添加的值复制到新 Bucket 里面 return \u0026p-\u003eval; } 小二，上图！ nNumUsed = 1 nNumOfElements = 1 nTableSize = 8 nTableMask = (-16) = (11111111111111111111111111110000) 10 2 h = (100000000) = (00000101111101011110000100000000) 10 2 nIndex = (h + nTableMask) = (11111111111111111111111111110000) = (-16) 2 10 + | +-----------------------------------------------------------------------+ | | Hash arData Data | | + | | +-------------------------------------+ v v v | | +---------+---------+---------+---------+---------+---------+---------+---------+ | | | | | | | | | | | | -16 | -15 | ...... | -1 | 0 | 1 | ...... | 7 | | | | | | | | | | | | +-------------------------------------------------------------------------------+ | | | | | | |Undefined|Undefined|Undefined| | | 0 | -1 | ...... | -1 | Bucket0 | Bucket1 | Buckets | Bucket7 | | | | | | | | | | | | +---------+---------+---------+---------+---------+---------+---------+---------+ | | + | +-----------------------------------------------------------------------------+ ^ + 可 用 的 Bucket nNumUsed = 2 nNumOfElements = 2 Hash arData Data + | +---------------------------+ v v | | +---------+---------+---------+---------+---------+---------+---------+---------+ | | | | | | | | | | | | -16 | -15 | ...... | -1 | 0 | 1 | ...... | 7 | | | | | | | | | | | | +-------------------------------------------------------------------------------+ | | | | | | | |Undefined|undefined| | | 1 | -1 | ...... | -1 | Bucket0 | Bucket1 | Buckets | Bucket7 | | | | | | | | | | | | +---------+---------+---------+---------+---------+---------+---------+---------+ | | + ^ next + | | +----------+ | | | +-----------------------------------------------------------------------------+ 文字表述为： 获取数组 arData 最后一个元素之后的合法位置（这个位置的内存在之前已经申请好了）。把这里的 Bucket 称为 BucketA。 把 BucketA 的下标放入 BucketA 的 h 中，把要添加的元素值放入 BucketA 的 val 。 把 Hash 区 (h | nTableMask) 位置指向的 Data 下标存储的 Bucket 称为 BucketB。 把 BucketA 的 val 的 next 指向 BucketB 。 更新Hash 区 (h | nTableMask) 位置的值为 BucketA 的下标。 Hash 区 -1 表示 HT_INVALID_IDX ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:5:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"更新 在上面的添加部分，可以看到函数的定义是： static zend_always_inline zval *_zend_hash_index_add_or_update_i(HashTable *ht, zend_ulong h, zval *pData, uint32_t flag) 它把添加和更新放在一起处理了。 实际上在添加的时候，会先使用： zend_hash_index_find_bucket(const HashTable *ht, zend_ulong h) 来看 h 这个 key 是否存在。如果存在就执行更新，如果不在就执行添加。 更新的操作就是把 pData 复制到找到的 Bucket 里面，替换掉原先的值。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:6:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"删除 删除分为三种情况： 目标 key 不存在 目标 key 存在，其指向的 Bucket 处于冲突链的第一个位置 目标 key 存在，其指向的 Bucket 不处于冲突链的第一个位置 目标 key 不存在，直接返回就可以了。 目标 key 存在时，包括两个主要的操作： 处理冲突链指针 释放内存 处理冲突链的指针时，分为两种情况： 在第一个位置：直接让 Hash 区的值指向冲突链第二个位置的 Bucket 在 Data 区的下标； 不在第一个位置：同链表删除中间元素的操作。 释放内存时： 如果 key 是字符串，则尝试释放 key 的空间； 把 Bucket 的 val 复制到另一个变量 data，把 Bucket 的 val 的类型设置为 undefined； 尝试释放 data 所占的空间。 做删除动作的入口是： zend_hash_del_bucket(HashTable *ht, Bucket *p) 做核心操作的是： _zend_hash_del_el_ex(HashTable *ht, uint32_t idx, Bucket *p, Bucket *prev) 看一看源码： zend_hash.c: static zend_always_inline void _zend_hash_del_el_ex(HashTable *ht, uint32_t idx, Bucket *p, Bucket *prev) { if (!(HT_FLAGS(ht) \u0026 HASH_FLAG_PACKED)) { if (prev) { // 处于冲突链的中间 Z_NEXT(prev-\u003eval) = Z_NEXT(p-\u003eval); } else { // 处于冲突链的第一个 HT_HASH(ht, p-\u003eh | ht-\u003enTableMask) = Z_NEXT(p-\u003eval); // 让 Hash 区的值指向下一个 Bucket 的 Data 区下标 } } idx = HT_HASH_TO_IDX(idx); ht-\u003enNumOfElements--; // 数组元素计数器减一。此时 nNumUsed 保持不变。 // 如果数组内部指针指向要删除的这个 Bucket ，则让其指向数组下一个有效 Bucket 。 if (ht-\u003enInternalPointer == idx || UNEXPECTED(HT_HAS_ITERATORS(ht))) { uint32_t new_idx; new_idx = idx; while (1) { new_idx++; if (new_idx \u003e= ht-\u003enNumUsed) { break; } else if (Z_TYPE(ht-\u003earData[new_idx].val) != IS_UNDEF) { break; } } if (ht-\u003enInternalPointer == idx) { ht-\u003enInternalPointer = new_idx; } zend_hash_iterators_update(ht, idx, new_idx); } // 如果要删除的元素是数组的最后一个元素，则尝试从后往前多回收几个无效 Bucket if (ht-\u003enNumUsed - 1 == idx) { do { ht-\u003enNumUsed--; } while (ht-\u003enNumUsed \u003e 0 \u0026\u0026 (UNEXPECTED(Z_TYPE(ht-\u003earData[ht-\u003enNumUsed-1].val) == IS_UNDEF))); ht-\u003enInternalPointer = MIN(ht-\u003enInternalPointer, ht-\u003enNumUsed); } // key 为字符串时，释放字符串内存 if (p-\u003ekey) { zend_string_release(p-\u003ekey); } if (ht-\u003epDestructor) { // 如果配置了析构函数，则调用析构函数 zval tmp; ZVAL_COPY_VALUE(\u0026tmp, \u0026p-\u003eval); ZVAL_UNDEF(\u0026p-\u003eval); ht-\u003epDestructor(\u0026tmp); } else { ZVAL_UNDEF(\u0026p-\u003eval); // 没有析构函数，则直接将 zval 的 u1.type_info 配置为 undefind。不用释放空间，因为以后元素可以重用这个空间 } } ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:7:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"PHP 数组可拥有的最大容量 zend_types.h #if SIZEOF_SIZE_T == 4 # define HT_MAX_SIZE 0x04000000 /* small enough to avoid overflow checks */ /* 省略代码 */ #elif SIZEOF_SIZE_T == 8 # define HT_MAX_SIZE 0x80000000 /* 省略代码 */ #else # error \"Unknown SIZEOF_SIZE_T\" #endif 根据 sizeof(size_t) 的执行结果判断应该设置为 67108864 还是 2147483648 。 0x04000000 转为二进制是： 00000100000000000000000000000000 0x80000000 转为二进制是： 10000000000000000000000000000000 当 nNumUsed 大于等于 nTableSize 时，会触发 Resize 操作，以此获取更多可使用的 Bucket 。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:8:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"Resize 策略 Resize 的定义是： zend_hash.c： static void ZEND_FASTCALL zend_hash_do_resize(HashTable *ht) Resize 有两种策略： rehash 双倍扩容 + rehash 之所以有不用双倍扩容的选择，是因为 PHP 在删除元素时，只是将对应 Data 区的 Bucket 的值设置为 undefined，并没有移动后面的元素。 选择的条件主要涉及 HashTable 的三个成员： struct _zend_array { // ...省略 uint32_t nNumUsed; // Data 区最后一个有效 Bucket 的下标 + 1。 uint32_t nNumOfElements; // 存在多少个有效 Bucket。删除数组元素时，会使其减一。 uint32_t nTableSize; // 总共有多少空间。 // ...省略 } 什么情况下只需要 rehash ？ 源码是： ht-\u003enNumUsed \u003e ht-\u003enNumOfElements + (ht-\u003enNumOfElements \u003e\u003e 5) 这里做一个转换，方便理解： ht-\u003enNumUsed - ht-\u003enNumOfElements \u003e (ht-\u003enNumOfElements \u003e\u003e 5) 也就是被设置为 undefined 的 Bucket 数量大于当前元素个数除以 32 向下取整的值。 例如： 当 nNumUsed 为 2048 ， nNumOfElements 为 2000 的时候，得到 2048 - 2000 \u003c 62 ，因此执行扩容。 当 nNumUsed 为 2048 ， nNumOfElements 为 1900 的时候，得到 2048 - 1900 \u003e 59 ，因此执行 rehash。 rehash 做以下操作： 清空 Hash 区； 取两个指针，一个指向当前扫描的位置（叫做 p），一个指向迁移后的位置（叫做 q），遍历直到 p 到达 nNumUsed ； p 在碰到无效 Bucket 时，会继续往前走一步，不做其他事。 p 在碰到有效 Bucket 时，会把 Bucket 的值复制到 q 指向的 Bucket 的值，并且 p 和 q 一起往前走一步。 这种做法的效率会比每次移动有效 Bucket 都把后面的数据一起往前移动来得高。 重新创建冲突链； 更新内部指针，使其指向更新位置后的 Bucket； 更新 nNumUsed，使其等于 nNumOfElements 。 什么情况下双倍扩容 + rehash ？ 满足只 rehash 的条件就只做 rehash，如果不满足条件并且 nTableSize 小于数组可拥有的最大容量（HT_MAX_SIZE），则双倍扩容。 由于 HT_MAX_SIZE 是 0x04000000 或者 0x80000000，并且 nTableSize 始终是 2 的次方，所以最后一次双倍扩容后的容量刚好是 HT_MAX_SIZE 。 0x04000000 转为二进制是： 00000100000000000000000000000000 0x80000000 转为二进制是： 10000000000000000000000000000000 双倍扩容时，做以下操作： nTableSize 变为原先的两倍； 重新申请一次 Hash 区和 Data 区的内存，然后把原先 Data 区的数据以内存拷贝的方式复制到新的 Data 区； 重新计算 nTableMask； 释放掉原先 Data 区的内存； 做 rehash 。主要是为了重建 Hash 区。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:9:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"负载因子（Load Factor） 负载因子会影响 hash 碰撞的概率从而影响到耗时，也会影响 Hash 区的大小来影响内存消耗。 在 PHP 中，用 nTableMask 和 nTableSize 的关系来体现： 负载因子 = |nTableMask / nTableSize| 负载因子为 1 的时候（PHP 5），nTableMask == - (nTableSize) 。 负载因子为 0.5 的时候（PHP 7）， nTableMask == - (nTableSize + nTableSize) 。 上一次修改负载因子的提交是： https://github.com/php/php-src/commit/34ed8e53fea63903f85326ea1d5bd91ece86b7ae 为什么负载因子会影响时间消耗和内存消耗？ 负载因子越大， nTableMask 绝对值就越小（nTableMask 本身受到 nTableSize 的影响），从而导致 Hash 区变小。 Hash 区一旦变小，更容易产生碰撞。也就使得冲突链更长，执行的操作会在冲突链的时间消耗变得更长。 负载因子越小，Hash 区变大，使得内存消耗更多，但冲突链变短，操作耗时变小。 负载因子 时间消耗 内存消耗 大 小 大 小 大 小 所以要根据对内存和时间的要求来做调整。 PHP 的负载因子从 1 （PHP5） 降到 0.5 （PHP7），使得速度变快了，但同时内存消耗变大。 针对内存消耗，PHP 还做了个改进，增加了 packed array。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:10:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"packed array packed array 的所有 key 是自然数，且依次添加的元素的 key 逐渐增大（不要求连续）。 packed array 查询时可以直接根据下标计算目标元素的位置（相当于 c 语言的数组），因此它不需要 Hash 区来加速。 不过由于在某些条件下， packed array 会转成 hash array ，所以它仍然保留 nTableMask 。只是 nTableMask 固定为最小值，当前为 -2 。 Hash 区只有两个位置，其值都是 HT_INVALID_IDX ，也就是 -1 。 ","date":"2020-03-15","objectID":"/2020/03/15/php-array-source-code/:11:0","tags":["Map","Hash","PHP"],"title":"【数据结构】从源码看 PHP 7 数组的实现","uri":"/2020/03/15/php-array-source-code/"},{"categories":null,"content":"sed 替换文本（search 替换为 replace） sed 's/search/replace/g' text-file.txt 删除空行（包括仅包含空格的行） sed '/^\\s*$/d' text-file.txt ","date":"2020-03-13","objectID":"/2020/03/13/text-process/:1:0","tags":null,"title":"文本处理","uri":"/2020/03/13/text-process/"},{"categories":null,"content":"这篇文章所使用代码的完整版见：https://github.com/schaepher/c-objcet-oriented ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:0:0","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"相关背景知识 ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:1:0","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"面向对象 面向对象的三个基本特征是：封装、继承、多态。 封装 隐藏成员变量及成员方法。 继承 子类可以使用现有类（父类）的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 多态 由继承而产生的相关的不同的类，其对象对同一消息会做出不同的响应。 ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:1:1","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"C 语言 C 语言的文件通常分为两种：用于声明的 .h 头文件，用于实现的 .c 文件。 当我们要引用已有的功能时，会使用预编译指令 #include 将 .h 文件包含进来。由于 .h 文件仅包含定义的内容而不包含具体实现，因此客户端（调用者）无法了解和接触到具体的实现细节。 C 语言的结构体在创建时会申请定长的内存空间，并按照结构体内部的结构体成员的声明顺序划分内存。在结构体指针做强制转换的时候，指针指向保持不变，内存保持不变。如果转为第一个成员的类型，则可以将其解释为第一个成员类型的指针。 ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:1:2","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"实现三个基本特征 ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:2:0","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"封装 利用客户端无法访问到 .c 文件的内容来隐藏成员变量和成员方法。下面以隐藏成员变量为例，成员方法类似。 如果不考虑其他条件，那么可以很自然地想到把所有成员变量写在结构体中。例如： struct Animal { char *type; char *name; int age; }; 如果把这段代码放到 .h 文件内部，则客户端引用 .h 后，可以在获取结构体指针后直接访问 type、name 和 age。 如果把这段代码放到 .c 中，.h 仅做声明 struct Animal;，那么客户端尝试访问内部属性时，就会碰到编译错误。 为了方便和简洁，把该结构体的详细定义命名为 _Animal，然后使用 typedef 给结构体一个别名： typedef struct _Animal *Animal; 上面这部分见代码仓库里的 incl/animal.h 和 src/animal.c。 成员变量无法直接访问后，要提供简介访问这些变量的方法。先考虑最简单的方式： src/animal.c: char *animalGetName(Animal this) { return this-\u003ename; } ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:2:1","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"继承 成员变量基于结构体存放元素时是按顺序的来做转换。 Animal Human +------+ +------+ | type | | type | +------+ +------+ | name | | name | +------+ +------+ | age | | age | +------+ +------+ | id | +------+ src/animal.c: struct _Animal { char *type; char *name; int age; }; 而 Human 通过直接把 Animal 作为结构体的第一个成员，继承了 Animal 的属性。 src/human.c: struct _Human { Animal animal; int id; }; 这时 _Human 相当于： struct _Human { char *type; char *name; int age; int id; }; 而对于成员方法，则使用： char *humanGetName(Human this) { return animalGetName((Animal)this); } 因为此时 src/human.c 指 include 了 src/animal.h，所以还是不能直接接触 Animal 里 name 这个属性。 在将指针往父类转换后，实际上不会丢失子类的内容，内存还存在。因此指针还能再向子类转换。 ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:2:2","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"多态 这里要做的是，当子类对象转换为父类的类型时，调用父类方法却得到子类方法的结果。 实现继承后，我们可以根据需要修改方法的实现，达到不同子类对象得到不同的结果。如下： char *humanGetName(Human this) { char *prefix = \"name: \"; char *name = animalGetName((Animal)this); char *result = (char *)malloc(strlen(prefix) + strlen(name)); strcpy(result, prefix); strcat(result, name); return result; } 但是这并没有实现转换为父类类型时也能得到子类方法的结果。 解决方法就是在结构体里面加上一个专门用于存这些方法指针的结构体（以下称之为：虚函数表），父类对象可以使用这些指针来调用子类的方法。 src/animal.h: typedef struct AnimalVtb { void (*say)(void *this); } AnimalVtb; 然后在原先的结构体里面，把虚函数表加到最前面，这样使得其与直接子类互相转换的时候比较方便。 src/animal.c: struct _Animal { AnimalVtb *vptr; char *type; char *name; int age; }; void animalInit(Animal this, AnimalVtb *vptr, char *type, char *name, int age) { this-\u003evptr = vptr; this-\u003etype = type; this-\u003ename = name; this-\u003eage = age; } 子类在创建对象时，将虚函数表传到父类结构体。 src/human.c: void humanSay(void *this) { Human human = (Human)this; printf(\"Hi, my name is %s, and my ID is %d!\\n\", humanGetName(human), humanGetId(human)); } AnimalVtb humanVtb = {humanSay}; Human humanCreate(char *name, int age, int id) { Human this; this = (Human)malloc(sizeof(Human)); animalInit((Animal)this, \u0026humanVtb, \"human\", name, age); this-\u003eid = id; return this; } 这样调用父类方法时，可以直接使用这个函数指针： src/animal.c: void animalSay(Animal this) { (*this-\u003evptr-\u003esay)(this); } 客户端代码为： src/main.c： int main(void) { Animal animal; Human human = humanCreate(\"ZhangSan\", 25, 111); animal = (Animal)human; animalSay(animal); return 0; } 问题 多层级继承的情况下，没法再添加更多虚函数定义 例如有基类 Object，虚函数列表里有 A B C 三个函数指针。类 ObjectA 继承 Object，类 ObjectB 继承 ObjectA。此时类 ObjectA 无法再往虚函数列表里添加更多定义了。 如果要添加接口，转换不了 要解决这些问题，需要有 Map 这种数据结构，将函数指针存放到 Map 里面。 参考资料 C语言：春节回家过年，我发现只有我没有对象！ https://mp.weixin.qq.com/s/2ivQ9hcRvZnhk89jzAppSg 用C实现OOP面向对象编程（1） https://www.cnblogs.com/findumars/p/6350092.html C语言的不完整类型和前置声明 https://blog.csdn.net/astrotycoon/article/details/41286413 C语言实现多态 https://blog.csdn.net/dumpling5232/article/details/52632060 ","date":"2020-03-12","objectID":"/2020/03/12/c-oop/:3:0","tags":["c","oop"],"title":"C 语言实现面向对象（一）：初步实现三个基本特征","uri":"/2020/03/12/c-oop/"},{"categories":null,"content":"HTTP（Hypertext Transfer Protocol，超文本传输协议）是应用层的无状态的请求和响应协议。它使得基于网络的超文本信息系统彼此可以灵活地进行交互。它包含了可拓展的语义（extensible semantics）和自描述的有效报文载荷（self-descriptive message payloads)。 应用层：OSI 七层模型的顶层； 定义请求头； 自描述的：指报文本身描述了处理这个报文的方式，例如 HTML 最开始的 \u003c!DOCTYPE； 有效报文载荷：真正需要的数据，HTML 内容或者 JSON 串； 注：从表示上看，像解释型语言（与编译型语言相对）。其内容可以被所有编程语言处理，因此不受限于特定编程语言和特定平台。基于不同编程语言的系统可以通过 HTTP 进行交互。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:0:0","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"历史 最早对 HTTP/1.1 做出说明的 RFC 文档是 1997 年发布的 RFC2068。在 1999 年发布的 RFC2616 对 RFC2068 做了更新。 当前关于 HTTP/1.1 最新的文档是 2014 年发布的 RFC7230、RFC7231、RFC7232、RFC7233、RFC7234、RFC7235 ，它们将 RFC2616 的内容拆分开来并做详细的解释与更新。 在 RFC INDEX 页面可以看到它们之间的关系： https://www.rfc-editor.org/rfc-index.html 2068 Hypertext Transfer Protocol – HTTP/1.1 R. Fielding, J. Gettys, J. Mogul, H. Frystyk, T. Berners-Lee [ January 1997 ] (TXT, HTML) (Obsoleted-By RFC2616) (Status: PROPOSED STANDARD) (Stream: IETF, Area: app, WG: http) (DOI: 10.17487/RFC2068) 2616 Hypertext Transfer Protocol – HTTP/1.1 R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, T. Berners-Lee [ June 1999 ] (TXT, PS, PDF, HTML) (Obsoletes RFC2068) (Obsoleted-By RFC7230, RFC7231, RFC7232, RFC7233, RFC7234, RFC7235) (Updated-By RFC2817, RFC5785, RFC6266, RFC6585) (Status: DRAFT STANDARD) (Stream: IETF, Area: app, WG: http) (DOI: 10.17487/RFC2616) 7230 Hypertext Transfer Protocol (HTTP/1.1): Message Syntax and Routing R. Fielding, J. Reschke [ June 2014 ] (TXT, HTML) (Obsoletes RFC2145, RFC2616) (Updates RFC2817, RFC2818) (Updated-By RFC8615) (Status: PROPOSED STANDARD) (Stream: IETF, Area: app, WG: httpbis) (DOI: 10.17487/RFC7230) 具体的内容见： https://tools.ietf.org/html/rfc2068：过时的 https://tools.ietf.org/html/rfc2616：过时的 https://tools.ietf.org/html/rfc7230 https://tools.ietf.org/html/rfc7231 https://tools.ietf.org/html/rfc7232 https://tools.ietf.org/html/rfc7233 https://tools.ietf.org/html/rfc7234 https://tools.ietf.org/html/rfc7235 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:1:0","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"内容 本文会对 RFC7230-7235 的内容做个大概的说明（翻译），并且在必要的时候做详细说明（翻译）。 RFC7230：语法和路由 语法：描述了一个 HTTP 请求或者响应长什么样。即第一行写什么怎么写、第二行写什么怎么写… 路由：资源标识（URI）如何确定？通过什么方式获取到想要的内容？是直接从本地缓存获取？还是通过代理（Proxy）获取？还是直接请求？ RFC7231：语义和内容（最需要关注的内容，RESTful-like） 各种请求方法（GET、POST、DELETE 等等）和请求头（Expect、Accept-Language、User-Agent 等等）表达了什么意图？ 响应体的状态（200 OK、201 Created、403 Forbidden 等等）和响应头（Location、Retry-After、Allow 等等）表达什么意思？ RFC7232：条件请求 响应体告知客户端某些数据条件（Last-Modified、ETag 等等），客户端可以在下次请求的时候带上这些信息（If-Modified-Since、If-Match 等等）。在符合条件或者不符合条件的情况下，服务端应该如何处理； RFC7233：范围请求 由于各种因素而只得到部分响应的时候，发起范围请求以获取剩下的内容，避免从头请求而浪费资源； RFC7234：缓存 通过减少请求避免网络资源的浪费； RFC7235：认证 用户认证。Basic Auth、Token 等等。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:2:0","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"RFC7230：语法和路由 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:0","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"客户端和服务端 客户端发起连接请求给服务端，服务端接收来自客户端的请求，并建立连接。 用户代理（User Agent） 用于表示各种客户端程序，例如浏览器、爬虫、命令行工具、手机应用等等。 源服务器（Origin Server） 用于表示对客户端请求的资源生成一个权威性的响应的程序。 HTTP 通过 URI 确定目标资源和资源间的关系。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:1","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"请求的语法 例子： GET /hello.txt HTTP/1.1 User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3 Host: www.example.com Accept-Language: en, mi 第 1 行（请求行）包含三个信息：请求方法、URI、HTTP 版本 第 2-4 行是请求头（Headers） 请求头以一个空行作为结束标志 在空行后面是真正想要发送的报文——有效载荷体（payload body），如果没有就放空。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:2","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"响应的语法 例子： HTTP/1.1 200 OK Date: Mon, 27 Jul 2009 12:28:53 GMT Server: Apache Last-Modified: Wed, 22 Jul 2009 19:15:56 GMT ETag: \"34aa387-d-1568eb00\" Accept-Ranges: bytes Content-Length: 51 Vary: Accept-Encoding Content-Type: text/plain Hello World! My payload includes a trailing CRLF. 第 1 行（状态行）包含三个信息：HTTP 版本、状态码、原因 第 2-9 行是响应头（Header） 响应头以一个空行作为结束标志 在空行后面是客户端想要的报文——有效载荷体（payload body），如果没有就放空。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:3","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"中转（Intermediaries） 从 User Agent 到 Origin Server，中间可以经过各种中转。中转通常有三种：代理（Proxy）、网关（Gateway）和隧道（Tunnel）。 \u003e \u003e \u003e \u003e UA =========== A =========== B =========== C =========== O \u003c \u003c \u003c \u003c 入站（Inbound）和出站（Outbound） 入站表示朝向源服务器，出站表示朝向用户代理。 代理（Proxy） 一种由客户端选择的报文转发代理（message-forwarding agent）。按照一定规则让请求通过同一个中转。 网关（Gateway） 又称为反向代理（Reverse Proxy）。对于出站连接来说，网关就像是源服务器。经常被用于拦截不被信任的服务、提高服务器性能、负载均衡等等。 隧道（Tunnel） 通常被用于建立一条虚拟的连接。通过这条连接的报文不会发生变化。 透明代理（Transparent Proxy） 不是由客户端选择的代理。例如在路由器上建立代理，电脑的浏览器感知不到这个代理。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:4","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"无状态 HTTP 是无状态的协议。这意味着每个请求都能够被独立地理解。 但是由于代理会复用连接或者动态负载均衡的存在，服务端不应该认为来自同一条连接的请求来自于同一个用户代理（User Agent）。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:5","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"缓存 缓存用于存放先前的响应报文，它还作为子系统管理着缓存的抽取和删除。 使用缓存的目的是减少未来发起与先前等价的请求时所带来的响应耗时和网络带宽的消耗。 \u003e \u003e UA =========== A =========== B - - - - - - C - - - - - - O \u003c \u003c 在 B 缓存了先前的响应报文后，UA 发出的请求就不必再经过 C 和 O 了。 一个响应是否会被缓存，由多个因素决定。在 RFC7234 里有详细的说明。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:6","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"协议版本 格式是：\u003cmajor\u003e.\u003cminor\u003e 但从 HTTP/2 开始，仅保留 major 。例如 HTTP/3。 https://www.ruanyifeng.com/blog/2016/08/http.html ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:3:7","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"RFC7231：语义和内容 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:4:0","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"请求方法 请求方法表明客户端发起请求的目的，以及客户端所预期的成功结果。 Representation （表征）： the way that someone or something is shown or described. 人或物被展示或者描述的方式。 +---------+-------------------------------------------------+-------+ | Method | Description | Sec. | +---------+-------------------------------------------------+-------+ | GET | 传输目标资源的一种表征 | 4.3.1 | | HEAD | 和 GET 相同，但只传输状态和头部 | 4.3.2 | | POST | 执行请求有效载荷中特定于资源的处理 | 4.3.3 | | PUT | 用请求有效载荷替换资源的所有表征 | 4.3.4 | | DELETE | 删除目标资源的所有表征 | 4.3.5 | | CONNECT | 建立一条与目标资源指定的服务器之间的隧道 | 4.3.6 | | OPTIONS | 描述与目标资源相关的选项 | 4.3.7 | | TRACE | 执行一个沿着客户端到目标资源路径的消息回送测试 | 4.3.8 | +---------+-------------------------------------------------+-------+ 这些方法可以按照不同属性分类： 安全（safe）和非安全（unsafe） 安全的方法不会造成目标资源的状态改变。方法包括 GET/HEAD/OPTIONS/TRACE。 幂等（idempotent） 同一个请求执行一次或者多次，对服务端资源的影响是一致的。 幂等的方法包括所有的安全方法，还有 PUT/DELETE。例如多次 DELETE 同一个资源，对于服务端来说，最终的结果都是该资源不存在。执行一次或者执行多次都一样。 幂等容易被误解为对客户端来说的幂等，导致难以理解为什么 GET 也是幂等的：毕竟每次 GET 返回的结果都可能不一样。因此要注意站在服务端资源的角度来看。 可缓存（cacheable） 响应结果可存储起来供后续使用。方法包括 GET/HEAD/POST。 大多数的缓存实现都只实现 GET/HEAD，但 RFC 7231 多定义了一个 POST。 这部分内容在 RFC 7234 有详细介绍。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:4:1","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"方法定义 GET HEAD POST POST 根据结果选择合适的状态码： 201 Created 一个或多个资源创建成功后，返回该状态码。同时要在头部加上 Location，指向创建的主要资源。 206 Partial Content 303 See Other 如果 POST 的处理结果与一个已存在资源的某一种表征一致，那么服务端在处理完 POST 后，可以用 303 重定向到已存在的资源。 这是为了共享缓存，但如果客户端代理之前没有缓存该已存在资源的表征，则会因为 303 而额外地发起一次请求。 304 Not Modified 416 Range Not Satisfiable PUT 服务端应校验服务端对目标资源的配置，例如内容的类型。 PUT 根据结果选择合适的状态码： 200 OK 目标资源已存在，此次更新成功 201 Created 目标资源先前不存在，此次执行创建了该资源。 204 No Content 目标资源已存在，此次更新成功 400 Bad Request 客户端请求头包含 Content-Range。部分更新应使用 PATCH。 409 Conflict 415 Unsupported Media Type 服务端对资源配置的 Content-Type 与客户端配置的不一致。例如服务端限定只能是 text/html，而客户端在 HTTP 头部配置的是 image/jpeg。 这只是一种处理方式，其他两种处理方式为： 服务端将客户端传输的类型转换为服务端配置的类型，然后存储 服务端变更配置，将 Content-Type 配置为客户端设置的类型 DELETE DELETE 表达的是源服务器 URL 映射的一种删除操作，而不是一种删除先前关联信息的期望。和 rm 命令类似，只删除映射，并没有将数据清除。 一个资源如果有一种或者多种表征，源服务器可以选择是否清除数据，也可以选择是否回收数据的存储空间。 当使用 PUT 创建资源和 POST 创建资源后，可以用 DELETE 来撤销（undo）这些操作。 DELETE 根据结果选择合适的状态码： 200 OK 操作成功，且响应体包含了描述结果的信息 202 Accepted 操作很可能成功，但还未执行或者还未执行完成。 204 No Content 操作成功，且不需要返回更多的信息给客户端 由于删除操作是幂等的，因此多次删除同一个资源让源服务器对于该资源处于同一个状态。 OPTIONS OPTIONS 用于获取目标资源所支持的选项。OPTIONS 没有对目标资源做出修改。 OPTIONS 支持两种资源类型： * 用于类似 ping 或者空操作。 非 * 用于获取可用的适用于目标资源的选项。 服务端在生成响应的时候，应该发送服务端实现的适用于目标资源的任何可选特性的头部。例如 Allow。此外还包括潜在的没有定义在 RFC 7221 的扩展。 OPTIONS 响应允许包含响应体，用于描述通信选项。如果不包含响应体，其 Content-Length 必须设置为 0。 OPTIONS 请求允许包含请求体。此时请求头必须包含 Content-Type 头部描述请求体的媒体类型。 CONNECT CONNECT 用于建立从参与者到源服务器之间的通道。 TRACE TRACE 请求应用层级别的回环 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:4:2","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"请求头字段 起控制作用的请求头 +-------------------+ | Header Field Name | +-------------------+ | Cache-Control | | Expect | | Host | | Max-Forwards | | Pragma | | Range | | TE | +-------------------+ Max-Forwards 只用于 TRACE 和 HEAD 请求方法。它的值是一个十进制整数，用于表示代理剩余可转发次数。 起条件作用的请求头（RFC 7232） +---------------------+ | Header Field Name | +---------------------+ | If-Match | | If-None-Match | | If-Modified-Since | | If-Unmodified-Since | | If-Range | +---------------------+ 服务端响应一个资源的时候，可以包含响应专用的 ETag 和 Modified-Since 两个响应头，表示资源的版本。 客户端可以在后续请求时，将 ETag 的值放在 If-Match 或者 If-None-Match 上，将 Modified-Since 的值放在 If-Modified-Since 或者 If-Unmodified-Since 上。 If-Match If-Match 的值可以加双引号，值可以有多个，多个值用逗号隔开。值还可以是 *，表示匹配任意 ETag，但如果一个都没有的时候，为 false。匹配方式使用强匹配。 当客户端要更新一个资源，使用了 If-Match 头，它要求当前服务端该资源的 ETag 应该是刚才 GET 资源时所获得的 ETag。避免覆盖掉其他请求对该资源的更新，或者避免更新请求的响应丢失时重试。 当 If-Match 用于安全的请求方法（如 GET）时，如果服务端资源的 ETag 与之不匹配，则终止该请求。 If-None-Match 使用弱匹配。 如果客户端要初始化一个资源，使用了 If-None-Match 头，它要求当前资源的 ETag 不是刚才获得的 ETag 才能更新。例如将 If-None-Match 设置为 *，避免多个初始化请求时后者覆盖前者。 当 If-None-Match 为 false 时： 如果使用 GET 或者 HEAD，返回 304 Not Modified 如果使用其他请求方法，则返回 412 Precondition Failed If-Modified-Since 如果请求头还包含 If-None-Match，则忽略 If-Modified-Since 如果服务端发现资源的最后一次修改时间早于或者等于 If-Modified-Since 指定的时间，则不执行操作而是返回 304 Not Modified If-Unmodified-Since 如果请求头还包含 If-Match，则忽略 If-Unmodified-Since 避免覆盖其他的修改 如果服务端发现资源的最后一次修改时间晚于 If-Modified-Since 指定的时间，则不执行操作，返回 412 Precondition Failed 或者 2xx。返回 2xx 的场景是请求的操作给资源带来的最终状态与当前状态一致。 如果客户端使用 GET 或者 HEAD 时，想要通过请求头来尝试命中缓存，则应该使用以下两个请求头： If-None-Match 表示 “如果不匹配该 ETag，则返回最新的数据”。如果响应的状态码是 304 Not Modified ，则表示客户端传送的 ETag 与服务端的一致。 If-Modified-Since 表示 “这个时间之后资源如果已被修改，则返回最新的数据”。如果响应的状态码是 304 Not Modified ，则表示资源在客户端指定的时间之后没有再被修改了。 起内容协商作用的请求头 主要协商服务端应该返回的数据类型。 +-------------------+ | Header Field Name | +-------------------+ | Accept | | Accept-Charset | | Accept-Encoding | | Accept-Language | +-------------------+ Accept 媒体类型，例如 text/plain。 对应响应头为： Content-Type Accept-Charset 字符集 unicode Accept-Encoding 字符编码，例如 gzip。 对应响应头为： Content-Encoding Accept-Language 语言 协商头的值可以多个，用逗号隔开，表示服务端可以根据情况选择。每种选项都有一个比重（0 ~ 1），用分号与选项隔开，放在分号之后。可以省略比重，此时会被解释为 1。 例如： Accept: text/plain; q=0.5, text/html, text/x-dvi; q=0.8, text/x-c 表示更希望获取 text/html 或者 text/x-c 这两种媒体类型。如果服务器不支持这两种类型，则返回 text/x-dvi 这种类型。如果也不支持 text/x-dvi，则返回 text/plain 这种类型。如果都不支持，则返回 406 Not Acceptable 。 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:4:3","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"响应状态码 1xx 信息 100 Continue 当客户端的 Expect 头部包含 100-continue 的时候，服务端返回该状态码表示知道客户端要发送大量的数据，并且初始数据已被接收，服务端当前不打算拒绝该请求，客户端可以继续发送请求。 RFC 7231 对 Expect 的值只定义了 100-continue 这一种。如果客户端传送了其他值，服务端可能返回 417 Expectation Failed。 101 Switching Protocols 客户端头部包含 Upgrade 时，服务端返回该状态码表示愿意切换协议。 2xx 成功 200 OK 201 Created 成功创建资源后，在响应头 Location 中返回资源的位置。 在实践中，会碰到一个问题：客户端并不总是关心该资源的位置，而是关注查看该资源内容的前端地址。不能将该地址放在响应体，因为 201 要求响应体必须为空。应该把这个链接信息放到响应头 Link 里面。 202 Accepted 服务端接受客户端的请求，但请求未处理完。服务端返回当前处理的状态，以及指向该处理的状态监测器。客户端可使用该监测器获取执行状态。 203 Non-Authoritative Information 由代理返回。表示请求成功，服务端返回了 200，但由于响应有效载荷被转换代理修改，因此返回该状态码。 204 No Content 205 Reset Content 让客户端将导致刚刚请求被发送的那部分页面（例如表单）重置为它的初始状态。 3xx 重定向 表示用户代理需要再做其他动作来满足刚才的请求。比如 Location 字段指定了一个 URI，用户代理可能自动重定向到该 URI。 重定向有四种类型： 资源可能在一个不同的 URI 上可以用。属于这种类型的状态码包括： 301 (Moved Permanently) 302 (Found) 307 (Temporary Redirect) 请求的资源有多种表征，客户端可以选择最合适的表征。属于这种类型的状态码包括： 300 (Multiple Choices) 重定向到一个不同的资源，该资源代表了对请求的间接响应。属于这种类型的状态码包括： 303 (See Other) 重定向到一个已缓存的结果。属于这种类型的状态码包括： 304 (Not Modified) 以下是各个状态码的补充说明： 300 Multiple Choices 请求的资源存在，但是有多种表征。由于客户端没有明确表示需要哪种表征，服务端无法替客户端决定。服务端将这些不同表征的 URI 放在头部 Link 中。 服务端可在 Location 指定服务端倾向的其中一种表征，供客户端自动重定向。 301 Moved Permanently 目标资源被分配了新的 永久 的 URI，后续请求应该使用新的 URI。服务端将新的 URI 放在 Location 中，客户端可以根据这个字段的值自动重定向。 由于历史原因，用户代理一旦收到该状态码，后续请求可能将 POST 转为 GET。如果不能接受这样的转换，则应该使用 307 Temporary Redirect。 302 Found 目标资源 暂时 存放在不同的 URI，后续请求应该使用原先的 URI。服务端将新的 URI 放在 Location 中，客户端可以根据这个字段的值自动重定向。 由于历史原因，用户代理一旦收到该状态码，后续请求可能将 POST 转为 GET。如果不能接受这样的转换，则应该使用 307 Temporary Redirect。 303 See Other 服务端让客户端重定向到一个不同的资源，该资源的 URI 放在 Location 中，目的是为原始请求提供一个间接的响应。这种重定向可能持续多次，并且用户代理应该将最终的结果作为原始请求的响应。 例如 POST 创建资源后，可以使用这个状态码将客户端重定向至新资源的地址。 304 Not Modified（RFC 7232） 当 GET 或者 HEAD 请求附带 Last-Modified 或 ETag 时，如果满足条件，则返回该状态码。表示客户端数据的版本已是最新，服务端不再返回这些数据。 响应头应包含以下头部的任何一个：Cache-Control， Content-Location， Date， ETag， Expires 和 Vary 。 305 Use Proxy（已废弃） 306 Unused（已废弃） 307 Temporary Redirect 目标资源 暂时 存放在不同的 URI，后续请求应该使用原先的 URI。服务端将新的 URI 放在 Location 中，客户端可以根据这个字段的值自动重定向。 用户代理在自动重定向的时候 禁止改变 请求方法。 308 Permanent Redirect（RFC 7238） 目标资源被分配了新的 永久 的 URI，后续请求应该使用新的 URI。服务端将新的 URI 放在 Location 中，客户端可以根据这个字段的值自动重定向。 用户代理在自动重定向的时候 禁止改变 请求方法。 需要特别注意的是： 状态码 暂时/永久 可能改变请求方法 RFC 301 永久 是 7231 302 暂时 是 7231 307 暂时 否 7231 308 永久 否 7238 4xx 客户端错误 凭据（credential）：登录过的证明。如 Cookie，Token。 400 Bad Request 请求的句法有问题（例如请求的第一行不按照标准来）、欺骗性的请求路由、不合法的请求内容等。当其他 4xx 无法表达错误类型的时候，也使用该状态码。 401 Unauthorized（RFC 7235） 不包含有效的凭据。服务端的响应头必须包含 WWW-Authenticate，告知具体原因。无效的凭据：空凭据、过期凭据。 402 Payment Required 仅作为保留状态码，以便未来对此状态码做具体定义。 403 Forbidden 资源存在，但客户端没有权限访问。分为带有效凭据和无效凭据的情况。但最终都要求客户端提供一个有效且具有权限的凭据。 404 Not Found 服务端当前没有找到请求的资源。404 并不意味着资源在未来不会出现。如果服务端通过一些信息知道该资源永远不会再出现，则应返回 410 Gone。 未必找不到资源才返回 404。在实践中，如果服务端不想暴露 “资源存在” 这一信息给客户端，则可以返回该状态码。 405 Method Not Allowed 目标资源不支持该请求方法。例如某个资源不支持 DELETE，而客户端对该资源发起 DELETE 请求。 406 Not Acceptable 客户端请求头包含 Accept、Accept-Charset、Accept-Encoding、Accept-Language 时，服务端发现资源无法满足客户端的要求，且不想返回一个默认的资源表征。 服务端应该在响应的有效载荷中列出可用的选择。 407 Proxy Authentication Required（RFC 7235） 类似于 401，但 407 用于客户端和代理之间。用于表示如果要使用代理，必须提供有效凭据。代理的响应头必须包含 Proxy-Authenticate，告知具体原因。 408 Request Timeout 服务端未能在特定时间内接收一个完整的请求，并且要关闭该连接。 409 Conflict 请求与目标资源当前的状态产生冲突。 常出现在对版本化的资源做 PUT 时，资源已被其他请求变更，当前请求不是基于资源最新版本变更的。服务端可以在响应的有效载荷里面包含帮助客户端合并两个版本的信息。 这个状态码在一些地方被用于 POST 请求的响应，用于表示要创建的资源已存在。 410 Gone 与 404 相似，但 410 表示资源很可能永远都不存在。 411 Length Required 客户端请求头不包含 Content-Length 412 Precondition Failed（RFC 7232） 请求头中一个或者多个测试条件不满足。客户端想要在目标资源处于某种状态的时候才更新该资源，而如果客户端所预期的状态和该资源当前的状态不一致，则返回该状态码。 413 Payload Too Large 请求体（有效载荷）的大小超过了服务端设置的限制。如果这种限制是临时的，则服务端响应头应包含 Retry-After 表示客户端可以过一段时间再尝试。 414 URI Too Long URI 太长。可能是由于重定向导致 POST 转为 GET，也可能是客户端恶意攻击，还可能是客户端不小心传了过多的数据。 415 Unsupported Media Type 客户端请求头 Content-Type 或者 Content-Encoding 指定的值，或者服务端根据请求体的内容来识别，发现其类型不被目标资源所支持。 417 Expectation Failed 跟客户端请求头 Expect 对应（只定义了 100-continue 一种）。如果客户端传送了服务端不支持的 Expect 类型，则报错。 422 Unprocessable Entity（RFC 4918） 导致 400 的条件都不满足的情况下，服务端仍然无法处理该请求。 可以用于业务校验不通过的场景。 426 Upgrade Required 客户端的请求由于 HTTP 版本过低被服务端拒绝执行，如果客户端愿意升级 HTTP 版本，服务端才","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:4:4","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"响应头字段 ","date":"2020-03-08","objectID":"/2020/03/08/http1-1-rfc/:4:5","tags":["rfc"],"title":"【RFC】HTTP/1.1 系列（7230 - 7235）","uri":"/2020/03/08/http1-1-rfc/"},{"categories":null,"content":"这篇主要依据发布于 2002 年的 RFC3305。 https://tools.ietf.org/html/rfc3305 对于这些叫法，有两种观点：传统观点和现代观点。 传统的观点是统一资源标识符（Uniform Resource Identifier）有多层结构，在 URI 底下主要还分为两种 URL（Uniform Resource Location）和 URN（Uniform Resource Name）。任何一个 URI 都尽量地再做一次归类，分到 URL 或者 URN。 +---\u003e URL | | URI +------\u003e URN | | +---\u003e URC 但现代的观点是这种区分并不重要，它们都属于 URI scheme。在 RFC7230 或者 RFC7231 中，见不到 URL 的说法，都是说 URI。 ","date":"2020-03-08","objectID":"/2020/03/08/uri-url/:0:0","tags":["rfc"],"title":"URI/URL 分不清？（rfc3305）","uri":"/2020/03/08/uri-url/"},{"categories":null,"content":"mc mirror -a old new ","date":"2020-03-01","objectID":"/drafts/minio/:0:0","tags":null,"title":"minio","uri":"/drafts/minio/"},{"categories":null,"content":"docker 保存镜像和恢复镜像： docker save myimage:latest | gzip \u003e myimage_latest.tar.gz docker load \u003c busybox.tar.gz ","date":"2020-02-24","objectID":"/2020/02/24/any/:1:0","tags":null,"title":"any","uri":"/2020/02/24/any/"},{"categories":null,"content":"并行 parall https://www.jianshu.com/p/cc54a72616a1 ","date":"2020-02-24","objectID":"/2020/02/24/any/:2:0","tags":null,"title":"any","uri":"/2020/02/24/any/"},{"categories":null,"content":"并行 xargs https://www.cnblogs.com/f-ck-need-u/p/9752365.html ","date":"2020-02-24","objectID":"/2020/02/24/any/:3:0","tags":null,"title":"any","uri":"/2020/02/24/any/"},{"categories":null,"content":"终端复用 tmux https://www.cnblogs.com/kevingrace/p/6496899.html ","date":"2020-02-24","objectID":"/2020/02/24/any/:4:0","tags":null,"title":"any","uri":"/2020/02/24/any/"},{"categories":null,"content":"nginx 连接数判断 https://blog.csdn.net/qq_42303254/article/details/89512198 ","date":"2020-02-24","objectID":"/2020/02/24/any/:5:0","tags":null,"title":"any","uri":"/2020/02/24/any/"},{"categories":null,"content":"移动硬盘被占用无法弹出 可能是 Everything 的后台程序占用了，关闭程序就好了。 ","date":"2020-02-24","objectID":"/2020/02/24/any/:6:0","tags":null,"title":"any","uri":"/2020/02/24/any/"},{"categories":null,"content":"基于较为全面的事实 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:1","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"基于较为片面的事实 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:2","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"基于宏观角度 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:3","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"基于微观角度 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:4","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对主题对象的概念做详细解释 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:5","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对主题对象的概念做简要解释 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:6","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"不对主题对象的概念做解释 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:7","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"缩小讨论的单一对象的范围，没有给出提示 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:8","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调更多权利，或只谈权利 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:9","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调更多责任，或只谈责任 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:10","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调平衡权力和责任 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:11","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"基于事实论据得出结论 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:12","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"基于想象论据得出结论 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:13","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"标注信息来源 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:14","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"不标注信息来源 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:15","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"论据到结论的推理符合逻辑 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:16","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"论据到结论的推理不符合逻辑 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:17","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"讨论严肃论题时将国家人格化 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:18","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"贬义词或褒义词占比大 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:19","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"中性词占比大 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:20","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对事实和推理的重视程度高 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:21","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对于抒发情感的重视程度高 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:22","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"就事论事 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:23","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"适当扩大事件范围（一至二层），添加事件解读角度 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:24","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"任意扩大事件范围（顶层），忽略中间层次的影响 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:25","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"正反双方论点论据的比例以及说服力程度差距较小 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:26","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"过度强调正方观点和论据，少量或不给出反方观点和论据 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:27","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对事件的产生原因基于文化进行分析 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:28","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对事件的产生原因基于自然规律和事实进行分析 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:29","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"过度强调优点，挑选较小缺点或少谈或不谈缺点 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:30","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"过度强调缺点，挑选较小缺点或少谈或不谈优点 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:31","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调一时得失，没有看长期对失去一方的补偿 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:32","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调战术忽视战略 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:33","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调战略忽视战术 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:34","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调个人得失 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:35","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"强调大局 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:36","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"关注所有人 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:37","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"关注当前损失的一部分人，忽略另一种方案带来的其他人的损失 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:38","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"只讨论有无或程度高低，不讨论边界的问题 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:39","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"没有正确推理出主要矛盾 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:40","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"正确推理出主要矛盾 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:41","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"忽略主要矛盾的前提下强调次要矛盾 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:42","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"抓住主要矛盾的情况下提及次要矛盾 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:43","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"如何对待反对意见 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:44","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"所谈对象占其总体的比例 如好地主占所有地主的比例 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:45","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"知道成功的真正原因 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:46","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"知道失败的真正原因 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:47","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"明确要被哪一部分人讨厌 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:48","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"瓜皮教学 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:49","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对于问题第一时间是思考解决方案 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:50","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"对于问题第一时间是思考如何批评 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:51","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"极端评论是为了出气 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:52","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"控制各因素比例，不能一味放开某一方面，也不能一竿子打死 好评与差评 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:53","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"问题之处在于：假设没有第三方参与者，或者认为第三方参与者影响力小 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:54","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"他们的反对的是极端的想法和言论，并似乎眼中只有这些。而忽略了有理由据地乐观的那部分人的说法。 在极端言论中，所挑选的还是逻辑上本身就站不住脚的那部分言论。 ","date":"2020-02-24","objectID":"/drafts/public-knowlege/:0:55","tags":null,"title":"问题","uri":"/drafts/public-knowlege/"},{"categories":null,"content":"pre-determined point of view 预设观点 Cognitive Consistency 认知相符 ","date":"2020-02-24","objectID":"/drafts/cognitive-consistency/:0:0","tags":null,"title":"预设观点与认知相符","uri":"/drafts/cognitive-consistency/"},{"categories":null,"content":" 2020-02-12 第一版完成 接下去的计划： 添加图片 拿一场录像分析 调整内容顺序 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:0:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"铭文与出装 铭文1：5 贪婪 5 狩猎（+5%移速） 10 心眼 10 梦魇 铭文2：10 狩猎 10 心眼 10 梦魇 出装：疾步之靴（+60移速 神行+60移速） 回响之杖（7% 移速） 博学者之怒 虚无法杖 血族之书 辉月 后期出装：血族之书升级到噬神之书；用不好辉月的话卖掉换复活甲（就算辉月还没CD）；把回响之杖换成贤者之书；存钱用于把进入 CD 的复活甲换回辉月 其他情况的出装： 如果对面位移多，容易切到妲己，那么先出辉月 如果对面肉多，那么不出血族之书，可以出贤者之书或者痛苦面具 铭文参照国一妲己的虎牙主播愿望。https://www.huya.com/yuanwang 为什么不用轮回？妲己在出完回响之杖和博学者之怒后，就有一套带走脆皮的能力，有没有装备轮回是一样的。 为什么选狩猎？妲己的移速如果比较快，容易调整走位跑到安全的位置，以及追别人的时候技能容易打到。因此移速是比较重要的，这也是为什么鞋子选疾步之靴。 妲己基础移速为 360。装备铭文、疾步之靴、回响之杖后的移速： (360 + 60) x (1 + 0.05 + 0.07) + 60 = 530.4 或者 (360 + 60) x (1 + 0.1 + 0.07) + 60 = 551.4 （游戏里显示是549） 看直播的时候有人提出能否出冷静之靴。我的观点是：没有必要。妲己靠走位避免自己不死，多等两秒影响不大。如果被切了，冷静之靴带来的减 CD 也没用。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:1:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"打法 妲己的操作过于简单，以至于任何人都能玩。她实际上是一个意识系英雄，意识的好坏决定了能不能玩好妲己。 妲己的打法总结起来就是：猥琐、果断 如果行走的路线有超过 30% 的可能性会死，那就不走那条路线。如果当前位置被杀的可能性超过 30%，那就走到可能性更低的地方。 果断在于不要怕浪费闪现和\"卖队友\"。怕浪费闪现会错过击杀对面的计会，也会容易被对面击杀。如果队友选择打一场赢不了的架，那你果断走，去清他们兵线，这样就算打输了对面也推不了塔。运气好打赢了也有机会推对面塔。 还有两者结合的：一套技能打完就跑，等下一套技能。 如果不能确保自己完全安全，尽量不要用普攻。妲己的普攻有一种“小拳拳锤你胸口”的萌感，锤不锤对局势没有影响。 玩得一般的妲己会认为自己的作用就是单杀，打团的时候就废了。但实际上妲己打团非常强。一场团战妲己的技能如果施放到位，能够打出非常好的效果。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:2:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"妲己的优势 三技能范围非常远，且为指向性技能，没法通过移动躲开。 一技能非常远。 二技能控制时间长，且为指向性技能，没法通过移动躲开。 三技能 CD 短。意味着一场团战可以使用多次大招。 技能远，通常就意味着妲己打得到对面，对面打不到妲己。再加上二技能的控制，想追上妲己的时候会被控住并拉开距离。 同时也意味着，妲己在打团的时候不需要队友保她，甚至她可以保队友。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:3:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"前期 —— 第一波线 妲己前三级的清线能力很弱。不过由于前三级的时间内，塔有额外的护盾，因此不用太担心。 一级的时候有两种可能： 辅助不来中路帮忙 辅助来中路帮忙 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:4:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"1. 辅助不来中路帮忙 不来中路帮忙比较简单。有两种情况： 1.1. 对面清线快 1.2. 对面清线慢 1.1. 对面清线快 如果对面清线快，在妲己一技能 CD 好之前对面就清完兵了。这个时候有两种选择： 1.1.1. 如果对面没有压你的意思，或者你躲掉对面中单的技能，那就把兵卡在塔攻击不到的范围，补刀。这样如果对面刚刚那波兵线没有补到刀，她的经济就比你低。 1.1.2. 如果对面如果技能容易压你血线，那就把兵线拉到塔内，尽量补刀。 如果兵血量少到普攻几下就能打死，那就用普攻打兵，不要浪费蓝。如果普攻三四下打不死，就不要浪费时间，用技能清兵。 如果不使用普攻，则需要三个一技能才能清完兵。 在没有己方小兵帮忙的情况下，会有两个小兵处于普攻一下或者两下就死。 对面清线快必定会去支援，要判断对面法师的走向，给边路信号。 1.2. 对面清线慢 注意补刀即可。 1.3. 总结 清完线基本第二波兵线快到了，继续清线。 第一波兵线不要想着耗对面血，要想着如何不被对面耗血的情况下尽量快地清兵线 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:4:1","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"2. 辅助来中路帮忙 如果辅助来中路帮忙，那么线很快就能清完，这时根据队友情况主要有三个选择： 支援边路 入侵对面野区 保护我方野区 2.1 支援边路 支援边路前要先获取以下信息： 对面打野是红开还是蓝开 对面辅助是跟打野还是跟射手 对面边路是凶还是猥琐 对面边路是不是带了闪现 边路哪边把对面血线压到 50% 以下 去哪一路的优先级是： 血线在 50% 以下且比较凶的那一路 \u003e 没有辅助的那一路 \u003e 射手路 支援边路最好能杀一个，或者混个助攻。能杀就用闪现，否则闪现留着。 如果杀不了，那就二一技能消耗一波，然后立马走。第二波兵线到了。 消耗一波对边路的作用很大，可以让边路对拼的时候我方血量占优。 血线在 50% 以下且比较凶的那一路最有可能拿一血。因为通常边路如果比较凶，两个英雄对打会打到妲己二一这两个技能和队友的一个技能能打死的血量还不会走的地步。这个时候妲己要躲在草丛里，等待这个时机的到来。给队友发信号让他跟对面拼。当血量到达能杀死的地步时，闪现上去放技能。这个闪现用不着省，闪现换一血绝对赚。如果省了闪现结果没杀死，那很亏。因为你等边路拼血量的时候，第二波兵线已经到了中路，你赶过去会少吃一两个兵。对面中路也会更快地去支援。 2.2 入侵对面野区 分为两种情况： 2.2.1. 队友已经入侵野区 2.2.2. 队友没有入侵野区 如果队友已经入侵野区，那就去帮忙，让队友尽快把野打掉。或者帮忙控制对面，避免队友被击杀。 如果队友没有入侵野区，那么就去对面红区小野打小野怪。顺便看对面打野红开还是蓝开。 2.3 保护我方野区 分为两种情况： 2.3.1. 对面入侵我方野区，跟我方打野打起来了 2.3.2. 对面有很强的入侵野区能力或者有两个英雄有带惩戒技能，但没有去找我方打野 2.3.3. 我方入侵对面野区，对面打野跑了 2.3.1. 对面入侵我方野区，跟我方打野打起来了 一定要先等升到二级再赶过去，不然去了也没用。 把控制留给对面控制或者最能够击杀我方英雄的。然后要保证对面打不到妲己。 2.3.2. 对面有很强的入侵野区能力或者有两个英雄有带惩戒技能，但没有去找我方打野 如果我方蓝开，对面可能会入侵我方红区。此时要走过去看一看红是不是没动。 2.3.3. 我方入侵对面野区，对面打野跑了 此时他可能去我方野区。如果我方入侵处于优势，那么妲己应该回去己方野区看视野。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:4:2","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"前期 —— 第二波线 第二波线清线的时候只有一个注意点，也是整场游戏清兵的注意点：不要用二技能清兵和压制对面 二技能是保命技能，用出去就可能被抓。 清完这波线。可以看情况是否支援。 如果之前支援过了，通常这次清兵结束的时间会比原先来的晚。因此不太需要支援，等待第三波兵线，尽早清完。 如果队友实在需要支援，或者有很好的击杀对面的机会，可以选择支援。毕竟第三波兵线后的支援也是为了尽可能击杀对方。 但是如果支援没成功，会影响到后续的节奏。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:5:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"前期 —— 第三波线 清完线后差不多 1 分 30 秒，对面打野也基本打完野了。打野四级，妲己三级。此时有两种选择： 帮助会被对面打野抓的那一路 去对面打野不在的那一路 能够做出这个判断，主要是由于清完第一波线我们知道对面打野的行进路线。 如果对面打野红开，那打野会在打完蓝区的野后就近抓边路；或者蓝开打红那一路。 那具体怎么选择？得看队友和对手的选择。 根据双方打野做基础判断： 对方打野和我方打野同一边开野。两种情况，对方红开我方蓝开；对方蓝开我方红开。 此时双方打完野必有一场大战，因此妲己要跟过去。如果对面法师往另一条边路走，那么就给另一条路的队友信号，让他缩塔。 对方打野和我方打野不同边开野。要再继续观察。有两种情况： 2.1. 打野会去抓的那路，己方很凶。 2.2. 打野会去抓的那路，己方缩塔。 2.1. 打野会去抓的那路，己方很凶 一般意识不够的边路，都会跟对面凶。 那妲己就要去支援，防止那一路队友被杀。 就算赶过去队友被杀了，你也可以把兵线清掉。稳赚不赔。 2.1. 打野会去抓的那路，己方缩塔 一般有意识的边路都会选择缩塔。 那就不必去支援了，打不起来。此时应该往另一条边路走。 这个时候跟清完第一波线不同，多了个变量，就是己方打野也打完了。 那就配合打野击杀对面。能杀就杀，杀不了就耗一波走人。千万不能久留，因为第三波兵线来了。 回去的路线一定要从己方野区绕最大的圈回去，不能走河道。 因为对面打野抓不到人，必定往中路走。此时妲己出现在边路，那对面打野也不会逗留在中路，必定蹲在边路回中路的河道准备抓妲己。或者直接过去跟己方打野打。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:6:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"前期 —— 第四波线 正常来说第四波线清完就四级了。有了大招，就有了很强的击杀能力。对面脆皮的血量如果在 70% 或者稳一点 60% 的时候，找机会一套带走。 首先观察对面法师的血量。如果满足条件，则尝试看有没有在二技能的范围内。有就带走。但是这有风险，对面可能会看到你四级就往后退了。 比较稳的做法是，躲到中路草丛里，等对面出来。这种通常是第五波线出来了，妲己先不要去清兵。 如果对面法师血量超过 70% 呢？可以选择耗一套，让他回城。这时候有两种情况： 对面法师回家 对面法师不回家 如果回家了，那抓紧清线去边路支援。 如果不回家，那么等下一套技能，将其击杀。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:7:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"技能施放 妲己的技能顺序大致有三种：213 、 231 、 321 。根据妲己的被动，这三种顺序打出的伤害递增。321 最高（因为被动），但可能 1 中不了。231 最稳，也是新手的出招顺序。 妲己最关键的操作是学会计算血量，判断一套技能能否击杀对方。多玩一玩就知道了。 如何选择技能顺序 3 技能是必中且范围最大；1 技能范围很大，但对面一个走位就打不中了；2 技能范围最小，2 技能能中的，其他也能中。 如果对面有位移或者闪现，那么 231 比较稳。对于这种，千玩不要怕浪费闪现。闪现除了杀辅助是浪费的，其他都是赚。 原则上是能 321 尽量使用。因为铭文有加移速，装备也加移速，不用想着早早地放技能，多走两步，确保技能都能中。 如果真的杀不死怎么办？赶紧跑呀！难道等着被反杀？ 等技能 CD 好了，再来一套。如果一套杀不死，那就再来一套。 没有多少中路能扛住两套妲己的技能。不过也有例外，对面带吸血铭文，并且恢复技能不在 CD，塔后回复包也还在。 如果两套杀不死那就再来一套。 场景 1 场景 2 场景 3 对面一个脆皮和一个肉站在一起 先二技能选中脆皮 一技能打到脆皮 三技能 此时三技能有很更大的概率打死残血的脆皮。 大招施放时机 分为四种情况： 清兵 消耗 帮队友打输出 击杀 1. 清兵 众所周知，妲己的清兵能力很差。但是一旦到了四级，清兵能力就会大大提高。 清兵的方式通常是：大招接一技能 因为妲己的被动是减法术防御，大招会放出五团狐火。分散到各个小兵，减少他们的法术防御，提高一技能的伤害。 清兵的时候一定不要省大招。妲己的大招 CD 很快。如果不用大招，那么清兵大概需要三个一技能。这比大招的时间还长。用大招清完兵再去支援，不仅节奏快，而且就算大招 CD 没好，也能用二技能打控制。 2. 消耗 消耗分为团战前消耗和对线消耗。 如果边路没有什么机会，大招除了清兵，还可以用来消耗对面中路。 一套技能怼到对面中路身上后，不死也残。如果他自己不回家，等妲己大招 CD 好，就直接送他回家。 团战的消耗通常指团战之前，两边蓄势待发，各种走位。这个时候你可以用大招消耗对面前排。 妲己技能消耗前排是没问题的。基本上妲己一套技能下去，对面的前排就剩下半血了。团战之前把对面前排压到半血甚至残血的情况下，对面如果还不撤退，那就等着团灭吧。 一定要利用好妲己大招 CD 短且距离远的优势。有时候团战前妲己大招可以打两次，这可是很恐怖的。 一定要记得二技能尽量不要跟着大招用于消耗。 3. 帮队友打输出 队友控制住对面，且妲己距离比较远的情况下，直接大招起手帮忙打输出。然后走两步接二技能。 运气好拿到人头，运气不好拿个助攻也不错。 4. 击杀 星耀局很多脆皮在剩下三分之一（有时候甚至四分之一）血的时候不会撤退，而是会抓紧发育。这个血量妲己一个大招下去基本没了。 如果对方周围有草丛，可以想办法钻草丛偷偷过去一个大招。如果没有草丛，也有两个选择： 直接走过去大 闪现过去一套 能直接走过去大是因为有以下条件： 铭文加移速 疾步之靴 如果出了回响之杖就更快了 妲己移速高的好处就在这，对面还没反应过来的情况下，甚至是对方反应过来想往后退的情况下，都可以追上大招的范围。 不一定每次击杀对面都要用完整的一套。如果对面剩下四分之一血或者更少，直接大招就行。要充分利用大招的远距离。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:8:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"单杀 玩妲己不能老想着单杀。除非对面走位失误或者意识太差。 单杀前要计算好血量，不然容易被反杀。 大部分有护盾或者回血技能的都不好单杀，可以直接放弃或者等他们残血。 当出了回响之杖和博学者之怒后，大部分射手和法师都能满血单杀。但是要注意特殊情况： 对方出了抵抗之靴（+100 法术防御），通常是不知火舞 对方出了噬神之书（+800 最大生命） 对方是嫦娥，想都不要想 对方是王昭君（被动 450+52% 法术加成的护盾） 对方是嬴政 （二技能 300+55% 法术加成的护盾） 对方是公孙离（二技能击落妲己的技能） 对方是李元芳（二技能可躲妲己的技能） 如果通过观察发现李元芳已经用过二技能，还在 CD，就可以果断上去单杀 如果碰到这种一次杀不死的，就不要太执着。先用大招消耗（一技能和二技能不要用）。 只用大招的情况下，对面血量还算比较多。他们会以为自己还是安全的。但实际上已经到了妲己能一套带走的血量。十几秒后他一个走位不慎，就送他会泉水。 单杀分为三种情况： 对方有位移或者闪现，自己有闪现：果断闪现上去 231。 对方没有位移或者闪现，或者自己没有闪现可以尝试 321。 对方周围有草丛。妲己在对方察觉不到的角度进入草丛靠近对面，321。如果跟对面英雄靠的比较远， 231。 因为如果距离远用 321，2 可能被躲掉。比如李白的二技能。但是靠得很近的话对方反应不过来。 如果判断对面可能更加靠近草丛，就耐心等待。 当妲己出到魅影面罩（虚无法杖的小件）时，上述的护盾或者加血加法术防御的也基本能单杀了。如果想再稳一点，可以等虚无法杖出出来。 出完虚无法杖的妲己基本已经是完全体了。除非对面脆皮有躲技能或挡技能的能力，或者出了魔女斗篷，否则一套带走没商量。 如果没有护盾和多技能的，虚无法杖后甚至大招和二技能就可以直接带走。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:9:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"团战 不要小瞧妲己的团战能力。妲己在团战开始时，不仅保护自己不死，还可以代替辅助保护射手。 不管是大规模团战还是小规模打架，要记得时刻与对面所有人保持一定的距离。多少的距离呢？就是你技能可以打到对面，但对面打不到你的距离。 团战前消耗 团战之前，对面前排很容易进入妲己的大招范围内。直接用大招消耗即可。 如果对面没有那种可以位移切我方后排的打野和上单，可以用二技能控住对方前排，接一技能消耗多一点。如果对方前排不是很肉，这一套下去可能直接把它打残。这种瞬间将对方前排致残的团战前消耗能力的法师，基本上就妲己一个了。 妲己的二技能很可能左右一场团战的结局。用二技能需要很谨慎，算好时机。这也是为什么说到团战前消耗的时候要加上各种条件才能用二技能消耗。 团战时的站位 不要跟射手站一起，或者双方很靠近 最好站在射手身后且妲己二技能能覆盖到射手的位置 团战打起来的时候，对面会想方设法切我方后排脆皮。例如凯开大闪现上来切射手，这个时候由于妲己的位置让二技能覆盖到射手的位置，因此可以用二技能控制凯，配合队友将其击杀。 什么？为什么不说凯开大闪现上来切妲己？因为妲己不仅没有靠近射手，而且还在射手的后面。凯需要两个闪现才能打到妲己。就算绕过射手，也会被妲己二技能控住。 如果真的有这种憨憨绕过射手怎么办？他绕过射手，也就是主动跟射手拉开距离，射手更安全了。妲己可以先后退，让他跟射手拉开更多距离。然后二技能控住对面，然后往后再拉开距离。这样对面就傻眼了。因为被控制的这段时间内，射手可以走位或者闪现拉开更多距离。现在两个都打不到了。 所以每次对面选出典韦或者凯这种近战或没位移的打野时，就可以无压力点出妲己（又是一场屠杀）。 这种站位不止可以用在团战，还可以用在其他队友不在，切对面打野可能过来抓人时保护射手和反杀。对方只有很小的可能性会绕过射手先杀妲己，因为可能被射手疯狂输出最终一换一，也可能需要绕一大圈失去机会。 也有可能出现对面后排认为他们优势很大，所以跟前排走一起。这个时候妲己要调整站位，让大招的范围只覆盖对方一个脆皮，然后 321 带走。这里就体现了妲己移速的重要性，用于调整走位。 如果团战之前周围有草丛，妲己可以进去草丛。等待对妲己有威胁的敌方英雄先出现，妲己再出现。 技能施放后的选择 打团的时候，妲己不一定要用完一整套技能。特别是一技能可能来不及放。因为对面可能有多个人冲过来了。 通常情况下是 32 技能都用了，但是对面冲过来了。 这时根据所在位置有两种选择： 往后跑 躲进附近的草丛 如果没有草丛，就得往后跑，跑多远呢？跑到对方放弃打妲己为止。然后等技能 CD 好了再去继续参与团战。 没有技能的妲己要确保对方忘记妲己的存在或者认为击杀妲己的代价比对方团战的损失还高。 二技能的施放时机 前面说过，二技能可以用来保护射手不被切。 其实也不用局限于射手，如果己方打野血量危险且被对面追赶，妲己就要控制对面。 有以下几种情况： 对面只有一个人 直接控就行 对面有多个人，但没有控制 控住最前面的那个 对面有多个人，但有一个有控制，对方需要使用控制才能击杀我方队友 控住那个有控制技能的 对面有多个人，但有一个有控制，但即使不使用控制技能，我方队友也会被杀 控住离我方队友最近的 对面有多个人，但多个有控制 控住那个最近没有用过控制的 团战会赢还是会输 在团战之前，要先判断这次团战会赢还是会输。 如果会输，那么在保证自己不被击杀的情况下上去放一套技能，然后走人。去清兵线，防止对方推塔。 通常按照上面的玩法，一场团战下来，妲己被击杀的概率非常小。所以要赶紧去清兵线。 清兵线的时候要记得留着二技能，只用一技能和大招清线。 团战在什么情况下很容易输？ 有一个队友被抓，先被击杀。 我方处于劣势，经济相差很大。 我方队友分散，对方集中（会导致葫芦娃救爷爷的局面）。 我方队友不懂得见好就收，追着对方不放。 你在清塔下兵，队友开团了。 如果你的兵线意识好，队友兵线意识差，很容易造成这种情况。 以上情况下，团战容易输。如果赢了，也可能是因为运气好。如果你想靠运气赢得比赛，那希望就渺茫了。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:10:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"兵线意识 为什么有时候团战打赢了，但推不了对方的塔？甚至还可能己方的塔被推掉？ 这是因为兵线没处理好就开团。 那什么叫兵线处理好了？就是以河道延长线为边界，双方小兵在对方那一边交战。甚至小兵在推对面的塔。 如果你们在小兵推对面塔的时候打起来了，就算打输了，也有机会推对方的塔。 由于王者荣耀地图的对称性，己方小兵和对方小兵的位置以河道为对称轴。通过小地图观看己方小兵的位置就能知道地方小兵的位置。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:11:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"补线意识 补线是指己方守塔队友被击杀，己方其他队友去帮忙清线，防止小兵被防御塔击杀导致经济经验浪费，且被对方推塔的做法。 通常由距离最近的己方队友去补线。整体收益较高的是让中路去补线。当然，距离最近的队友也很有可能是中路。 如果是在中路清完线的情况下，边路被击杀。那么中路补完线后，下一波中路兵线让给刚刚被击杀的边路队友。 如果是在中路小兵都还存活的情况下，边路被击杀。那么中路发信息让打野补中路线或者让辅助把兵线卡在防御塔攻击范围外。如果边路复活赶到中路，可让其吃这波线或者和中路一起吃。 如果边路被击杀，没有补线和让被击杀的队友补上经济经验，会被对方持续滚雪球，导致边路崩得彻底。此时中路也难以逆天。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:12:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"小地图 平均每秒看一次小地图，直到对方五个人中还没有被击杀的人全部出现在小地图上。 根据对方最后一次消失在小地图上的位置及行进方向，推断对方的动向。如果是来抓妲己，那赶紧跑。 如果妲己在上路，对面全在下路，那妲己可安心推塔。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:13:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"妲己的经济 如果采用以上的打法，妲己的经济通常在第一梯队（十人中的前三名）。而且很有可能经济最高并且领先对方 1k 或者更多的经济。 只要妲己的经济在第一梯队，那她就是很恐怖的。 如果因为被针对或者因为失误而导致经济落后，就要多清线以及蹭线把经济拉上来。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:14:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"英雄克制 王者局以下不需要怕这些英雄： 盾山 盾山举盾期间移速变慢，它的队友容易走到盾的前面。 要把妲己看成是团战英雄。妲己技能会被盾山挡住没错，可是妲己有队友啊，她的队友能控制盾山，让盾山的盾放下。 宫本武藏 宫本武藏通常大不到妲己，他大别人后会放一技能。妲己只需等宫本施放一技能，然后等一技能结束或者通过走位（移速的重要性）走到一技能切不到的地方，一套带走。 程咬金 程咬金通常在半血的时候不会开大。前期的妲己可以配合队友在程咬金反应过来前将其击杀。后期妲己可以直接一套带走半血程咬金。 伽罗（提款机） 闪现上去一套，或者躲草丛一套，伽罗能反应过来的时候，妲己大招已经打过去了（移速的重要性）。 李元芳 等他放完二技能，妲己再放二技能控他。但是会玩的李元芳也会等妲己放二技能再自己放二技能。这种情况下妲己会被反杀。 兰陵王 基本切不到妲己。切射手的时候妲己二技能控住，一套带走。 阿珂 基本切不到妲己。切射手的时候妲己二技能控住，一套带走。 上官婉儿 如果婉儿在远的地方起飞，那么妲己往后走到婉儿大招范围外，或者直接闪现。然后追上去一个二技能控住，一套带走。 如果婉儿在近的地方开始准备飞，那么妲己一个二技能控住，一套带走。 庄周 妲己的二技能未必用于控人，也可以用来只打输出。 另外再强调一遍，妲己是有队友的。庄周的大招如果用来防妲己，那么队友的控制就可以发挥效果。 而且庄周和打野一起上来切后排的时候，我方可以无压力控住对方后排。 猪八戒 消耗一套他就慌了，配合队友将其击杀。不需要想着单杀。 李白 已经刷好大的李白就没办法了。如果没刷好大上来，那等他二技能躲队友控制后妲己再控他。李白在没有技能的时候，也容易被妲己击杀。 孙悟空 切射手的时候就会放一技能，看到它一技能结束或者已经被触发的时候，上去控他，一套带走。 妲己需要怕这些英雄： 米莱迪 妲己四级后要用大招快速清兵，但米莱迪的小机器人可以分担大招伤害，使得妲己清兵速度变慢。并且米莱迪可以用小机器人推塔，妲己清不完这些小机器人。最终使得妲己不仅没法支援边路，中路还很容易掉。 在选英雄的时候，一定要等对面法师选了你再选妲己。如果对面选了米莱迪，不要硬着头皮选妲己。可以选一手王昭君出冷静之靴和圣杯克米莱迪。 元歌 两只会分担大招伤害，前期妲己一套技能也杀不死傀儡 司马懿 进来沉默妲己，毫无抵抗能力。队友想帮忙也会被沉默。 云中君 无法选中，只能任其宰割。队友想帮忙也帮不了。 被嫦娥小克制。因为妲己有很强的单杀脆皮的能力，通常会找机会单杀对面射手或者法师。但是嫦娥实在是太肉了，前期妲己拿她没办法。而且她清兵也很快。 那为什么说是被小克制呢？因为只有玩得一般的妲己才会老是想着单杀。如果想玩好妲己，就得懂得跟队友配合击杀对面。单杀只是对面走位不好或者意识不行的时候，妲己顺便做的事情而已。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:15:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"妲己怕魔女斗篷吗 不怕。理由如下： 魔女斗篷只是封掉妲己单杀满血的可能性，残血或者半血的时候照杀不误。 通常经济在第一梯队的妲己会在对方出魔女之前就把虚无法杖出出来。 妲己有队友。出了魔女斗篷意味着对面输出降低（射手和法师），或者物理防御降低（坦克和边路）。 这样就为队友创造了条件。 特别是在打团的时候，妲己并不会去切对方后排，这样就会导致对方后排在打团的时候浪费了一个装备的钱。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:16:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"碰到对方有位移的打野怎么办 思路不太需要变化，只是要有耐心。 举个例子： 对面有赵云，两段位移有机会直接突到妲己。但是按照上面的思路，妲己的面前有射手。一般情况下，赵云会选就近的放大招，否则大空了就尴尬了。 现在有两种情况： 赵云就是要大妲己，绕过射手。按照前面的思路，赵云进，妲己退。这样可以牵制赵云要等团战过了一段时间才放大招，射手可以安全输出。 赵云和我方射手距离很近，只需一技能就可以打到。一旦其打到射手，妲己就可以上去控。此时赵云会选择大妲己。但是赵云如果能大到妲己，妲己二技能也能放出来。这样大过来之后，仍然会被妲己控住。妲己会比赵云先解除控制，此时妲己的大招可以全部放到赵云身上（因为他大招突进来，他的队友跟不上）。可以把赵云的血量压低。同时我方保护射手的辅助也会跟过来控住赵云。配合射手将其击杀。最坏的情况下，如果能跟对方打野一换一，并且保住射手不死，妲己的牺牲是值得的。 ","date":"2020-02-10","objectID":"/2020/02/10/wangzherongyao-daji/:17:0","tags":null,"title":"王者荣耀-妲己","uri":"/2020/02/10/wangzherongyao-daji/"},{"categories":null,"content":"VIP + Keepalived + LVS LVS 使用 VIP + Keepalived 形成一主多备（通常是一主一备）的四层负载均衡（位于 OSI 第四层的传输层）。 VIP（Virtual IP，虚 IP)，用户访问域名时解析到的 IP 地址。用于屏蔽服务的架构。 Keepalived 装在所有 LVS 机器上。 在一主一备的 LVS 架构中，VIP 会设置到主的机器上。当主挂掉，备的 Keepalived 检测到该信息，让 VIP 漂移到备上面。 问题是流量如何从主转移到备。在 LVS 前面还会有一台路由器，这台路由器接收到报文，并且根据 IP 获取到目标 LVS 的 MAC 地址，然后转发出去。 那么这就涉及到 ARP 了。当主还没挂掉的时候，路由器的 ARP 缓存中有 VIP 和主的 MAC 地址（虚拟的 MAC，不是实际上的 MAC）映射关系。 顺便注：这个映射关系有 10-20 分钟的过期时间，但实际上用不到。 当 Keepalived 检测到主挂掉后，让备发一个携带备的虚拟 MAC 和 VIP 映射关系的 ARP 报文，路由器根据这个更新 ARP 缓存。 ","date":"2020-02-06","objectID":"/2020/02/06/lvs-ospf/:1:0","tags":null,"title":"lvs-ospf","uri":"/2020/02/06/lvs-ospf/"},{"categories":null,"content":"VIP + OSPF + Keepalived + LVS 当 LVS 为一主一备的时候，备机处于空闲状态，资源利用率只有 50%。OSPF 则是将这 50% 利用起来。 需要在路由器上配置 VIP 可以转发到不同的 LVS 服务器上。 ","date":"2020-02-06","objectID":"/2020/02/06/lvs-ospf/:2:0","tags":null,"title":"lvs-ospf","uri":"/2020/02/06/lvs-ospf/"},{"categories":null,"content":" apply 失败 添加参数 --whitespace=fix 试试 ","date":"2020-01-14","objectID":"/drafts/git-collect/:0:0","tags":null,"title":"git 问题收集","uri":"/drafts/git-collect/"},{"categories":null,"content":"这个项目是流程系统，有三百条左右的流程。其中一部分流程会共用一些操作，即流程中某几个节点的功能和其他流程一样。但各个流程的原始数据不同，以至于不能完全共用，必须将不同流程的数据转换成功能所需入参。 原来的开发方式让人难以接受，于是我尝试了几种方式。这个过程仿佛是从原始社会到现代社会按顺序走了一遍。（哭了） 原来的开发方式 这个系统的每个节点都有对应的唯一标识。执行某个节点时，会根据这个节点找到对应的代码块执行。其实就是一大堆 if else。例如： if ( $nodeCode === 'code_a' ) { // do_something } else if ( $nodeCode === 'code_b' ) { // do_something } // .... 把它们看成一个个 function 也是没问题的。 由于流程都有一个唯一 ID，在写业务代码的时候，就用这个 ID 来区分获取数据的方式。例如： if ( $processCode === 'proc_code_a' ) { $data1 = get_data('proc_a_data_name1'); $data2 = get_data('proc_a_data_name1'); } else if ( $processCode === 'proc_code_b' ) { $data1 = get_data('proc_b_data_name1'); $data2 = get_data('proc_b_data_name2'); } 这种方式写时一时爽，维护起来可就难受了。业务代码里面混入一大堆获取数据的代码。 在上一篇工作一年的记录中，我开了一个面向对象的入口。在这个路口里面的 Controller 获取数据时，不再是用 if-else 判断，而是利用反射创建对应流程的 DataContainer。 DataContainer 获取数据有两种方法，下面详细说明。 法一：工厂模式 + 场景方法(18年07月) /** $container \\App\\Process\\Container\\Interface\\GetAllDataWhenDoingXXX **/ $container = ProcessInstanceDataContainer::rebuildFrom('proc_inst_id'); $data = $container-\u003egetAllDataWhenDoingXXX(); $data1 = $data['data1']; $data2 = $data['data2']; 如果一个流程有上百个节点（确实有），则该流程对应的 DataContainer 将会有对应数量的方法。并且这些方法需要把数据组装成业务代码调用时所需的格式，因此会很复杂。 鉴于以上问题，该方式用了一阵子就没用了。后面改成法二。 法二：工厂模式 + 获取基本数据 + Controller 根据需要获取和组装 不再组装成业务代码所需的格式，仅提供最基础通用的数据。分为两种情况： 表单的数据 根据表单数据获取的基础数据 每次从 Container 获取基础数据： /** $container \\App\\Process\\Container\\Interface\\GetXXXData **/ $container = ProcessInstanceDataContainer::rebuildFrom('proc_inst_id'); $data1 = $container-\u003egetData1(); $data2 = $container-\u003egetData2(); 所有需要使用相同方法的流程，都实现同一个接口。在 Controller 层根据这些基础数据拼接成最终所需数据。这样就能将 DataContainer 方法的数量减少很多。 业务层调用的时候，可能不同的流程的表单数据不一样。例如流程 A 有 Data1 但没有 Data2，但可以通过其他数据计算出 Data2，流程 B 反之。这时候接口不变，在实现的时候做转换。 ","date":"2020-01-12","objectID":"/2020/01/12/working-with-legacy-code-1-process-form/:0:0","tags":null,"title":"与旧代码相处之一：流程表单数据","uri":"/2020/01/12/working-with-legacy-code-1-process-form/"},{"categories":null,"content":"本来 19 年 7 月写的，结果一拖再拖，拖了半年。以后还是按年份写吧。 这篇写 2018-07 至 2019-12 的事情。 ","date":"2020-01-04","objectID":"/drafts/working-2/:0:0","tags":["工作","阶段总结"],"title":"工作的第二年+第三年上半年","uri":"/drafts/working-2/"},{"categories":null,"content":"公司、部门 19 年 1 月，我在年会上拿到了公司级别的优秀员工。 这一段时间内，特别是半年内，人员变动非常大： 18 年 8 月，来了个新同事，后来被孙总称为小可爱。 19 年 4 月底，飞哥离职。少了个强力战友。 19 年 8 月，原组内女同事 4/5 19 年 9 月，产品经理离职。从此木有霸气的产品经理了。原组内女同事 3/5 19 年 10 月，原组内女同事 2/5 19 年 12 月，原组内女同事 1/5 产品经理换了三个，现在处于没有产品经理的状态。 人员是越来越少了，喜提 N+1 的也没几个。拿到 N+1 的各个开心得不得了（羡慕）。 部门每个离职的同事都会请一顿下午茶。外加最近部门几个关系好的小伙伴会比较经常地出去聚餐，以前一季度一次（称为季度总结），现在一个月最少一次。以至于我胖了 6 斤。 ","date":"2020-01-04","objectID":"/drafts/working-2/:1:0","tags":["工作","阶段总结"],"title":"工作的第二年+第三年上半年","uri":"/drafts/working-2/"},{"categories":null,"content":"项目 分两个时期来写：维护和开发旧系统；设计和开发新系统（重写旧系统）。 旧系统就是 18 年那篇的 A 项目，拥有十年以上的历史。 为了方便，以下在必要时会将旧系统称为 V1，新系统称为 V2。 ","date":"2020-01-04","objectID":"/drafts/working-2/:2:0","tags":["工作","阶段总结"],"title":"工作的第二年+第三年上半年","uri":"/drafts/working-2/"},{"categories":null,"content":"维护和开发旧系统（18-07 ~ 18-12） 这个时期的压力还算不是很大，没有什么任务是非常赶时间的。大部分都在八点半左右就下班回去了（此时的加班标准是达到八点半）。时间点跟宿舍换到离公司很远有关系，在生活篇中有提到。 比较可以一提的有： 写了套简易的异步任务管理器（18-10） V1 原先是这样的：发送一个任务给外部系统，然后流程流转到下一步执行检测。如果任务还没完成，就报错，然后每隔 1 分钟尝试一次，直到任务结束。 但是后来碰到一个要发送的任务是在发送任务后，我们会得到一个发送成功的消息。等任务结束，对方会回调我们这边的接口。如果对方超时了，我方提示超时错误。 于是结合 crontab 写了个处理，当对方调用我方接口后，才让流程执行检测步骤。业务上可容忍一分钟的延迟，所以不需要太复杂的组件。 引入服务容器（18-11） 原先创建一个类是直接 new 或者使用 get/set 注入，而在 11 月的时候我把 Laravel 的 Container 库引入进来。使用服务容器获取服务对象。以便于测试。 优化了个冗长且经常改动的 if-else（18-11） 这是一个根据多个条件判断选取哪些数据的 if-else，它直接嵌在某个业务代码里面，其他地方无法使用。 开早会的时候经常看到需求方又提了个添加条件的需求。于是我请求组长给我点时间把它优化成需求方可以自主配置的形式。 原先需求方提需求的时候，会发一个 Excel 过来，然后把变更点用特殊颜色标记。这个形式就不做变化了。在云云小姐姐的帮助下，与需求方共同确定了数据格式。 我做了个读取 Excel 并转换为 Json 文本的转换器，在需要数据的时候读取 Json 判断。用户上传 Excel 后，会得到新旧的数据差异，确保没有问题。 其实做成界面直接配置更好，然而我们这边没有专门的前端，我也不太想花时间在前端上（Vue 除外）。 ","date":"2020-01-04","objectID":"/drafts/working-2/:2:1","tags":["工作","阶段总结"],"title":"工作的第二年+第三年上半年","uri":"/drafts/working-2/"},{"categories":null,"content":"设计和开发新系统（19-01 ~ 19-12） + 201901 | | 用 户 登 录 （ Token） 和 信 息 获 取 ， 学 习 和 应 用 Restful | | 201902 | | 引 入 流 程 引 擎 （ Camunda）， 实 现 创 建 流 程 实 例 、 表 单 暂 存 、 获 取 流 程 信 息 的 Adapter | | 201903 | | 提 供 流 程 引 擎 查 询 接 口 | | 流 程 实 例 执 行 日 志 记 录 | | 参 考 Java 版 的 Camunda External Task Client 写 了 PHP 版 | | 实 现 External Task Client 根 据 流 程 节 点 ID 计 算 请 求 的 API 地 址 | | 实 现 了 节 点 跳 转 (向 任 意 目 标 节 点 )、 跳 过 当 前 节 点 、暂 停 /继 续 的 Adapter | | 添 加 GraphQL 客 户 端 | | 实 现 流 程 节 点 逻 辑 （ 业 务 ）， 共 15 个 节 点 | | 201904 | | 实 现 流 程 节 点 逻 辑 （ 业 务 ）， 共 23 个 节 点 | | 实 现 流 程 节 点 重 试 的 Adapter | | 实 现 终 止 / 激 活 的 Adapter | | 201905 | | 实 现 流 程 节 点 逻 辑 （ 业 务 ） ， 共 5 个 节 点 | | 使 用 MinIO 作 为 文 件 服 务 器 ， 实 现 了 文 件 版 本 支 持 | | 封 装 脚 本 传 输 和 执 行 工 具 | | 实 现 Elastic Search 查 询 Adapter | | 使 用 Redis 作 为 缓 存 服 务 | | | 201906 | | 实 现 流 程 节 点 逻 辑 （ 业 务 ） ， 共 30 个 节 点 | | 添 加 静 态 代 码 检 测 ( PHPCS )， 使 用 Gitlab CI 执 行 | | 同 步 流 程 流 转 改 为 异 步 ， 添 加 检 测 器 ( 将 循 环 检 测 转 移 到 流 程 外 部 ) | | 流 程 实 例 出 现 非 预 期 错 误 时 ， 自 动 转 交 给 值 班 人 员 ( 涉 及 值 班 轮 班 规 则 ) | | 脚 本 传 输 和 执 行 添 加 批 量 并 发 支 持 | | 201907 | | 实 现 流 程 节 点 逻 辑 （ 业 务 ） ， 共 6 个 节 点 | | 整 理 流 程 的 业 务 文 档 ( 约 28 个 节 点 ) | | | 201908 | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | v 耗时一个月写 Camunda 的 Adapter，使得它能够满足当前业务的需要。 疑问： GraphQL 什么时候引入？ RPC Shell Wrapper 什么时候重写？ 文档什么时候写？ ","date":"2020-01-04","objectID":"/drafts/working-2/:2:2","tags":["工作","阶段总结"],"title":"工作的第二年+第三年上半年","uri":"/drafts/working-2/"},{"categories":null,"content":"将 V2 的东西搬到 V1 日志 难度不大，收益明显。不太值得一提，仅作为记录。 V1 的场景：V1 经常会有问题，后台有经常查当天日志的需求。 我们在内部交流工具里有创建一个服务号，项目内的四五个成员一人一天轮流值班。 查业务日志的做法已经很少有一台台服务器去查的了，通常都是将所有服务器日志统一到一个地方。比较常见的是 ELK。 V2 的日志是用 Filebeat 采集然后放到 ELK，但 V1 不是，导致 V1 查日志的体验很不好。 V1 的做法是：把查询请求发送到每台服务器上，对 grep 日志文件，然后返回并汇总结果展示在界面上。 优点：实现简单 缺点： 没有实现分页查询和多条件查询（但有某个月内时间范围查询） 因此查日志的时候是用某个字符串查出所有相关日志，再用浏览器上的 ctrl + f 查询到想要的数据。 查询缓慢，如果想要查半年前的数据，一般要半小时。有时候甚至要查一年前的数据。等得很痛苦。 有时候会有这样的问题排查需求：系统有个数据有问题，但不知道这个数据是什么时候变更的。结果一查日志发现是一年前的变更。不过这种需求极少，通常只需要查当天的日志。 日志查询中有个指定日志文件的参数，不能传范围。如果想查半年前的数据，得一个个月去查或者查一整年的日志。 我后来用 rsyslog 把这些日志收集起来，丢到 ELK。由于通常只查当天数据，偶尔查 7 天内的数据，所以 E 和 L 不需要做什么特别的优化，只是设置日志的索引粒度为一天。 为什么不用 Filebeat ？因为 V1 这个项目跑在 CentOS 5 上面，Filebeat 是用 Go 写的，Go 不支持 CenOS 5（内核版本过低，至少要 2.6.23）。 如果只是为了收集日志而把系统升上去了，到时就要面对更多问题，搞了半天却发现已经偏离收集日志这项工作了。 结果是： 当天数据查询耗时降低到原来的三分之一 一个月内的日志查询耗时降低到原来的三十五分之一 半年前的数据查询耗时为原来的三分之一 自从把日志收集到 ES 后，大大地提高了值班体验（虽然仍然很痛苦）。 ","date":"2020-01-04","objectID":"/drafts/working-2/:2:3","tags":["工作","阶段总结"],"title":"工作的第二年+第三年上半年","uri":"/drafts/working-2/"},{"categories":null,"content":"其中，PROG变量为所要运行的可执行程序的名称， PROG_PATH为可执行文件所在的目录，PROG_ARGS为执行程序的各个参数。 #!/bin/bash ### BEGIN INIT INFO # # Provides: location_server # Required-Start: $local_fs $remote_fs # Required-Stop: $local_fs $remote_fs # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: initscript # Description: This file should be used to construct scripts to be placed in /etc/init.d. # ### END INIT INFO ## Fill in name of program here. PROG=\"location_server\" PROG_PATH=\"/opt/location_server\" ## Not need, but sometimes helpful (if $PROG resides in /opt for example). PROG_ARGS=\"\" PID_PATH=\"/var/run/\" start() { if [ -e \"$PID_PATH/$PROG.pid\" ]; then ## Program is running, exit with error. echo \"Error! $PROG is currently running!\" 1\u003e\u00262 exit 1 else ## Change from /dev/null to something like /var/log/$PROG if you want to save output. $PROG_PATH/$PROG $PROG_ARGS 2\u003e\u00261 \u003e/var/log/$PROG \u0026 $pid=`ps ax | grep -i 'location_server' | sed 's/^\\([0-9]\\{1,\\}\\).*/\\1/g' | head -n 1` echo \"$PROG started\" echo $pid \u003e \"$PID_PATH/$PROG.pid\" fi } stop() { echo \"begin stop\" if [ -e \"$PID_PATH/$PROG.pid\" ]; then ## Program is running, so stop it pid=`ps ax | grep -i 'location_server' | sed 's/^\\([0-9]\\{1,\\}\\).*/\\1/g' | head -n 1` kill $pid rm -f \"$PID_PATH/$PROG.pid\" echo \"$PROG stopped\" else ## Program is not running, exit with error. echo \"Error! $PROG not started!\" 1\u003e\u00262 exit 1 fi } ## Check to see if we are running as root first. ## Found at http://www.cyberciti.biz/tips/shell-root-user-check-script.html if [ \"$(id -u)\" != \"0\" ]; then echo \"This script must be run as root\" 1\u003e\u00262 exit 1 fi case \"$1\" in start) start exit 0 ;; stop) stop exit 0 ;; reload|restart|force-reload) stop start exit 0 ;; **) echo \"Usage: $0 {start|stop|reload}\" 1\u003e\u00262 exit 1 ;; esac ","date":"2019-12-22","objectID":"/2019/12/22/initd-template/:0:0","tags":null,"title":"init 脚本模板","uri":"/2019/12/22/initd-template/"},{"categories":null,"content":" filename=\"\" base64_script=$(base64 /usr/local/src/${filename}) cmd=\"echo '---run---'; mkdir -p /usr/local/src; echo '${base64_script}' | base64 -di \u003e /usr/local/src/${filename}; bash /usr/local/src/${filename}; echo '---end---'\" bash -c \"${cmd}\" ","date":"2019-12-17","objectID":"/2019/12/17/transfer-script-and-run/:0:0","tags":null,"title":"transfer-script-and-run","uri":"/2019/12/17/transfer-script-and-run/"},{"categories":null,"content":"这是一个较为通用的脚本模板。 #!/bin/bash ############################################################## # Filename : template.sh # Author : xxx(xxx@outlook.com) # Date : xxxx-xx-xx # Function : # Bash Template # Changes : # - 20191217 by xxx # Create Template ############################################################## readonly _filename=$(basename $0) function usage() { cat \u003c\u003cEOF Description: Echo var0 and var1 Usage: template.sh --var0=\u003cvalue\u003e [--var1=\u003cvalue\u003e] Options: -h --help Show this help. --var0=\u003cvalue\u003e Assign \u003cvalue\u003e to var0 --var1=\u003cvalue\u003e Assign \u003cvalue\u003e to var1 EOF exit 0 } if [[ \"$#\" -eq 0 ]]; then usage; fi # Usage: log \u003clog-level\u003e \u003clog-text\u003e... # log-level: DEBUG INFO ERROR function log() { local prefix=\"[$(date +%Y-%m-%d\\ %H:%M:%S)]: \"; echo \"${prefix} $@\" \u003e\u003e /tmp/${_filename}.log; } # Exit if value is empty # Usage: required \"--var1\" \"${var1}\" function required() { if [[ ! -n \"${2}\" ]]; then echo \"${1} is required!\"; exit 1; fi } # Usage: output \"${key}\" \"${value}\" # Double Quote is required to preserve new line character. function output() { local k=$1; shift; local v=$(echo -n \"$@\" | base64 -w 0); [[ \"$debug\" -eq 1 ]] \u0026\u0026 v=\"$@\"; echo ${k}=${v}; } set -e _args=$(getopt --option h --long \"help\",debug,var0:,var1: -- \"$@\") eval set -- \"${_args}\" # common the next line to avoid exit immediately if a simple command exits with a non-zero status set +e while true do case \"$1\" in -h|--help) usage break ;; --var0) var0=$2 shift 2 ;; --var1) var1=$2 shift 2 ;; --) shift break ;; esac done restParams=\"$@\" required \"--var0\" \"${var0}\" function main() { output var0 \"${var0}\" output var1 \"${var1}\" } log 'INFO' 'script start' main log 'INFO' 'script end' ","date":"2019-12-17","objectID":"/2019/12/17/bash-template/:0:0","tags":null,"title":"Bash 脚本模板","uri":"/2019/12/17/bash-template/"},{"categories":null,"content":"执行脚本 $commands = [ ['echo', '\"-----script-start-----\"'], ['mkdir', '-p', '/usr/local/src/scripts'], ['cd', '/usr/local/src/scripts'], [sprintf('echo \"%s\" | base64 -di \u003e %s.env', base64_encode(implode(\"\\n\", $envArr)), $scriptName)], [sprintf('echo \"%s\" | base64 -di \u003e %s', base64_encode($scriptContent), $scriptName)], ['chmod', '+x', $scriptName], array_merge(['bash', $scriptName], array_map(CommandEscapeTool::class, 'escape'], $params), ['2\u003e\u00261']), ['echo', 'script_exit_code=$?'], ['echo', '-----script-end-----'], ]; $commandStr = sprintf('echo \"%s\" | base64 -di | bash', base64_encode($commandStr)); ","date":"2019-12-17","objectID":"/2019/12/17/bash-template/:1:0","tags":null,"title":"Bash 脚本模板","uri":"/2019/12/17/bash-template/"},{"categories":null,"content":"https://blog.csdn.net/suifeng3051/article/details/53992560 ","date":"2019-12-01","objectID":"/2019/12/01/micro-service/:0:0","tags":null,"title":"micro-service","uri":"/2019/12/01/micro-service/"},{"categories":null,"content":"mybatis plus 查询构造器 https://mybatis.plus/guide/wrapper.html#alleq spring boot 配置文件多环境 https://blog.csdn.net/davis2015csdn/article/details/75220046 响应体 Header https://www.baeldung.com/spring-rest-http-headers 注解全介绍 https://cloud.tencent.com/developer/article/1507070 分页 Header 构造 https://www.baeldung.com/rest-api-pagination-in-spring https://www.javadevjournal.com/spring/rest-pagination-in-spring/ 对 Mybatis 的吐槽 https://zhuanlan.zhihu.com/p/45044649 决策： 包名中有多个单词组成的，要用下划线连接吗？ 全部用小写，并且不包含下划线（参考谷歌）。例如 helloworld 配置文件使用 properties 还是 yml？ 使用 yml，结构清晰，易读 如何依据环境来获取不同的配置文件？ application.yml 中有 spring.profiles.active 可以配置。 如配置为 dev，则使用文件 application-dev.yml Bean 使用 xml 配置还是使用注解？ 使用注解。 依赖注入使用何种方式？ Service 使用字段注入，并添加 set 注入方式 Controller 使用方法参数注入 构造器注入会使得在为 Service 添加新的依赖时，也必须修改测试类。 不支持 Controller 方法参数的注入 注解的解释 https://www.cnblogs.com/tanwei81/p/6814022.html 多数据源 使用 dynamic-datasource-spring-boot-starter ，要使用 2.5.6，最新版本配置有问题。使用 2.5.6 必须在配置文件里加上： #去除druid配置 spring.autoconfigure.exclude=com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceAutoConfigure https://www.jianshu.com/p/0b408e4e14a4 mybatis 配置 mapper 路径 mybatis-plus.mapper-locations=classpath:sqlMapperXml/*Mapper.xml 依赖注入 https://www.cnblogs.com/joemsu/p/7688307.html https://spring.io/blog/2007/07/11/setter-injection-versus-constructor-injection-and-the-use-of-required/ 避免使用字段注入。可选构造函数注入和 set 方法注入。 构造函数注入的字段可用 final 修饰，因此该方法通常用于那些必要的注入（或者说几乎每个方法都会用到的那种）。 set 方法注入的字段不能用 final 修饰，因此该方法通常用于那些非必要的注入。 依赖注入对测试的影响： 构造函数的注入每次增加时，都需要修改测试类，在创建服务类的地方添加参数。但通常情况下，一个类不会经常地添加其他依赖，所以这不是个大问题。 set 方法仅需要在使用这个服务类的测试调用 set 。 结论是： 如果是 Controller 这种无需单元测试的，直接使用构造函数注入 对于 Service 这种，优先使用构造函数注入。如果是有多种实现的接口或者那种只有少数地方用到的类，则选择 set 注入。 返回 created 的 path mybatis plus 的 mapper.xml 要放在 resource 里面，并且和 Mapper.java 有相同的目录结构 mybatis 的 ResultMap 支持将表字段映射到另一个类的字段，并且 ResultMap 可以不用处理两者的关联关系 这意味着可以映射到值对象而不是另一个实体 映射方式为： \u003cresultMap id=\"xxxx\" type=\"xxx\"\u003e \u003cid column=\"id\" property=\"id\"\u003e \u003cresult column=\"name\" property=\"name\"\u003e \u003csssociation javaType=\"xxx2\" property=\"displayName\"\u003e \u003cresult column=\"aliases\" property=\"displayName\"\u003e \u003c/association\u003e \u003c/resultMap\u003e mybatis 的 ResultMap 只能在 xml 里面指定使用。如果想用 mybatis plus 的 lambda 表达式查询也能得到相同的映射，需要在 @TableName 里面加上 resultMap = “BaseResultMap” mybatis plus 的 TableField 仅对 lambda 有效，不影响使用 xml 定义的查询结果。 可以完全抛弃 TableField，只要用上 @TableName 的 resultMap 就能得到正确的映射 ","date":"2019-12-01","objectID":"/2019/12/01/java/:0:0","tags":null,"title":"java","uri":"/2019/12/01/java/"},{"categories":null,"content":"https://herbertograca.com/2017/11/16/explicit-architecture-01-ddd-hexagonal-onion-clean-cqrs-how-i-put-it-all-together/ https://www.jianshu.com/p/d3e8b9ac097b https://matthiasnoback.nl/2017/08/layers-ports-and-adapters-part-3-ports-and-adapters/ https://github.com/VaughnVernon/IDDD_Samples https://github.com/citerus/dddsample-core ","date":"2019-11-25","objectID":"/2019/11/25/ddd/:0:0","tags":null,"title":"ddd","uri":"/2019/11/25/ddd/"},{"categories":null,"content":"密码服务器 version: '3.3' services: bitwarden: image: mprasil/bitwarden:latest container_name: bitwarden restart: always volumes: - ./data/bitwarden_rs/data:/data env_file: - config.env nginx: image: nginx ports: - '443:443' - '80:80' volumes: - ./nginx-conf.d:/etc/nginx/conf.d/ links: - \"bitwarden:bitwarden\" SIGNUPS_ALLOWED=false DOMAIN=xxxxx.com DATABASE_URL=/data/bitwarden.db ROCKET_WORKERS=10 WEB_VAULT_ENABLED=false ","date":"2019-11-20","objectID":"/drafts/password-server/:0:0","tags":null,"title":"password-server","uri":"/drafts/password-server/"},{"categories":null,"content":"如果 Bitlock 没有处于关闭状态，就去关闭 Security ，会导致每次 windows 登录时要求输入解锁密码(如果有登录 Windows 账号，则在 Windows 官网可查询)。这个在登录 Windows 后，进入配置界面关闭。 必须在 UEFI 配置中关闭 Security Boot，否则 Windows 无论如何都会使用默认的 Boot Manager。替换了也没用。 使用支持触屏的 rEFInd 来替换系统自带的 Boot Manager。 rEFInd 默认配置没有开启触屏，必须在 refind.conf 里面把 enable_touch 前的注释去掉。 ","date":"2019-11-15","objectID":"/2019/11/15/uefi/:0:0","tags":null,"title":"uefi","uri":"/2019/11/15/uefi/"},{"categories":null,"content":"安装到 SD 卡 在通过 U 盘安装 Android 时，平板电脑的 SD 卡可能识别不到。 在设备列表界面按 CTRL + ALT + F2 打开新命令行界面，执行 modprobe rtsx_pci_sdmmc 之后，按 CTRL + ALT + F1 回到刚才的界面。 选择 Detect Devices ， SD 卡就出现了。选它安装。 https://igordc.com/2017/12/android-x86-on-sdcard-with-ntfs-secure-boot-uefi ","date":"2019-11-15","objectID":"/2019/11/15/uefi/:1:0","tags":null,"title":"uefi","uri":"/2019/11/15/uefi/"},{"categories":null,"content":"Android 启动 编辑启动项。 https://docs.microsoft.com/en-us/windows-hardware/drivers/devtest/adding-boot-entries https://blog.csdn.net/iceteaset/article/details/51855280 ","date":"2019-11-15","objectID":"/2019/11/15/uefi/:2:0","tags":null,"title":"uefi","uri":"/2019/11/15/uefi/"},{"categories":null,"content":"安装 https://docs.docker.com/install/linux/docker-ce/centos/ ","date":"2019-11-04","objectID":"/2019/11/04/docker/:0:1","tags":null,"title":"Docker","uri":"/2019/11/04/docker/"},{"categories":null,"content":"配置国内镜像 vim /etc/docker/daemon.json { \"registry-mirrors\": [\"https://registry.docker-cn.com\"] } Error response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) ","date":"2019-11-04","objectID":"/2019/11/04/docker/:0:2","tags":null,"title":"Docker","uri":"/2019/11/04/docker/"},{"categories":null,"content":"在运行时通过 envsubst 替换配置文件中的变量 CentOS 要安装 gettext 才能执行 envsubst ","date":"2019-11-04","objectID":"/2019/11/04/docker/:0:3","tags":null,"title":"Docker","uri":"/2019/11/04/docker/"},{"categories":null,"content":"构建多种架构的镜像 https://engineering.docker.com/2019/04/multi-arch-images/ https://github.com/docker/buildx/#installing ","date":"2019-11-04","objectID":"/2019/11/04/docker/:0:4","tags":null,"title":"Docker","uri":"/2019/11/04/docker/"},{"categories":null,"content":"docker-compose 服务多实例 https://stackoverflow.com/questions/39663096/docker-compose-creating-multiple-instances-for-the-same-image https://pspdfkit.com/blog/2018/how-to-use-docker-compose-to-run-multiple-instances-of-a-service-in-development/ https://docs.docker.com/compose/reference/up/ ","date":"2019-11-04","objectID":"/2019/11/04/docker/:0:5","tags":null,"title":"Docker","uri":"/2019/11/04/docker/"},{"categories":null,"content":"top -b -n 1 -c -H top -b -n 1 -c -H cat c.txt | sort -k3,3nr | less -S cat c.txt | awk ‘{ sum+=$3 } END { print sum }’ ","date":"2019-10-25","objectID":"/2019/10/25/centos-monitor/:0:0","tags":null,"title":"CentOS Monitor","uri":"/2019/10/25/centos-monitor/"},{"categories":null,"content":" 用网线将两者连接 到 【控制面板\\网络和 Internet\\网络和共享中心\\更改适配器配置】 选择可连外网的网卡（以下称外网网卡），进入属性，切换到共享页面。 注意！是连接外网的网卡，通常是连接 WiFi 的那张，因为有线连接已经被占用去连接 CentOS 了。 点开【允许其他网络….】，然后选择连接台式机的那个网卡（以下称内网网卡） 保存 此时内网网卡会自动设置一个 IP ，我们假设它是 192.168.137.1 。 打开 windows 的路由转发。打开注册表，HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters ，把 IPEnableRouter 设置为 1 进入 centos 的 /etc/sysconfig/network-scripts/ ，修改网卡配置。 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static IPADDR=192.168.137.2 NETMASK=255.255.255.0 BROADCAST=192.168.137.255 GATEWAY=192.168.137.1 DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=enp2s0 UUID=418b805a-b022-4bca-8629-4a7ffac3c2a7 DEVICE=enp2s0 ONBOOT=yes ZONE=public PREFIX=24 DNS1=192.168.50.89 DNS2=192.168.50.87 解释： BOOTPROTO 选 static IPADDR 只要是 192.168.137.0 网段的就行 GATEWAY 设置为 192.168.137.1 DNS 设置和 外网网卡的 DNS 设置一致 参考文档： https://www.cnblogs.com/Bonker/p/4849295.html https://blog.csdn.net/cbuy888/article/details/81362601 ","date":"2019-10-25","objectID":"/2019/10/25/windows-connect-centos-direct-by-wire/:0:0","tags":null,"title":"Windows 主机通过网线直连 CentOS 主机","uri":"/2019/10/25/windows-connect-centos-direct-by-wire/"},{"categories":null,"content":"深入理解，熟练运用； 似懂非懂，其实不懂。 cnblogs: https://www.cnblogs.com/schaepher GitHub: https://github.com/schaepher ","date":"2019-10-23","objectID":"/about/:0:0","tags":null,"title":"about","uri":"/about/"},{"categories":["OpenWRT"],"content":"场景一：自身存储小，通过 U 盘扩展（U 盘挂载到根目录） 仅适用于有 USB 插口的路由器。 注：如果提示软件无法安装，一般是因为没有执行 opkg update ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:0:0","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"支持 U 盘 安装 opkg install kmod-usb-core \\ kmod-usb-uhci \\ kmod-usb-storage \\ kmod-usb2 \\ kmod-usb-ohci \\ block-mount \\ mount-utils \\ fdisk `` 如果 U 盘或移动硬盘是 FAT32 的，就再装个 kmod-fs-vfat；如果是 NTFS 的，就装 ntfs-3g。 挂载 执行 mkdir /mnt/usb 创建挂载目录 插入 U 盘 执行 fdisk -l | grep \"^/\" 会看到与 U 盘容量差不多的分区。这里假设为 /dev/sda4 执行 mount /dev/sda4 /mnt/usb 这样进入 /mnt/usb 就能看到 U 盘的文件了 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:1:0","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"扩容 需要已经按照上面的【支持 U 盘】安装了相应软件。 路由器的存储非常小，装几个软件就没了。例如 小米路由器mini 只有 16MB 的 Flash。 那么其实我们只要把根目录挂载到 U 盘上，那么就能把软件装在 U 盘上了。能存多少东西就取决于 U 盘的容量。此外还可以添加 Swap，扩充内存。用来运行需要比较大内存的程序。 步骤大致有： fdisk 对 U 盘分区 复制当前根目录的所有文件到 U 盘的根目录分区 设置让系统把 U 盘的分区挂载到根目录，并重启（重启后生效） 让 /var 不再指向临时文件系统 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:2:0","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"1. fdisk 对 U 盘分区 我总共分了三个分区： 普通分区，插到 Windows 也能用的。分区类型为 FAT32。这个分区也可以不要。 根目录分区。分区类型为 ext4。除了 1 和 3，剩余容量放这个分区。以下假设为 /dev/sda1。 Swap 分区。分区类型为 Swap。以下假设为 /dev/sda2。 具体容量看你想要多少内存。一两个 GB 就可以了。 我没有使用 fdisk 分区，而是在 Windows 上用 DiskGenius 分区的。所以用 fdisk 分区的方式见： https://www.cnblogs.com/wangkangluo1/archive/2012/06/08/2541161.html 分区后 U 盘插入路由器，用 block info 查看结果。 如果已经有安装 blkid，也可以执行 blkid 查看结果。 以下假设 /dev/sda1 为 U 盘上要作为根目录的分区 opkg install kmod-fs-ext4 e2fsprogs mkfs.ext4 /dev/sda1 命令的含义： kmod-fs-ext4 是对 ext4 文件系统的支持。e2fsprogs 包含了 mkfs 命令，用于格式化分区。 将 /dev/sda1 格式化为 ext4 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:2:1","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"2. 复制当前根目录的所有文件到 U 盘的根目录分区 以下假设 /dev/sda1 为 U 盘上要作为根目录的分区 mkdir /mnt/udisk mount /dev/sda1 /mnt/udisk mkdir /tmp/root mount --bind / /tmp/root tar -C /tmp/root -cvf - . | tar -C /mnt/udisk -xvf - sync umount /tmp/root 命令的含义： 创建下面 U 盘分区要挂载的目录 将 U 盘中要作为根目录分区挂载到 /mnt/udisk 创建一个临时目录，用于拷贝根目录文件 将当前根目录以 bind 的方式挂载到临时目录，此时临时目录里可以看到和根目录一样的文件 将临时目录的内容打包并解压到 /mnt/disk，tar 用于保留文件的属性信息 将所有缓存写入 ROM 取消挂载 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:2:2","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"3. 设置让系统把 U 盘的分区挂载到根目录，并重启（重启后生效） 让系统自己检测分区情况 block detect \u003e /etc/config/fstab 编辑 /etc/config/fstab，编辑后的内容大致如下 config global option auto_swap '1' option auto_mount '1' option delay_root '5' option check_fs '0' option anon_swap '1' option anon_mount '1' config mount option target '/' option uuid '0000-0000' option enabled '1' 第一部分的 global 不需要修改。 第二个部分的 mount 中，target 改为 /，即挂载到根目录；uuid 为 U 盘分区的标识符。 如果不确定哪个 UUID 是对应刚才的分区，可以执行 block info 查看。 执行 reboot 重启 重启后执行 df -h 就能看到 / 对应的容量扩大了 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:2:3","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"4. 让 /var 不再指向临时文件系统 执行：ls -l / 查看 /var 是否指向临时文件系统。如果看到：var -\u003e tmp ，就表示重启路由后，你对 /var 里文件的修改会丢失。比如应用的日志。 执行以下命令： rm /var mkdir /var cp -r /tmp/* /var/ reboot 依次是： 删除当前的软连接 创建文件夹 将 /tmp 里的文件复制到 /var 里面 重启 注：如果 /var 指向 tmp，可能会导致 supervisor 无法正确启动。因为涉及到 /var/run/ 文件夹。 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:2:4","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"扩充内存 上面扩容的时候，有分了一个 Swap 分区。现在我们把它挂载到系统的 Swap。 假设设置为 Swap 的分区为 /dev/sda2，执行以下命令： mkswap /dev/sda2 swapon /dev/sda2 命令的含义： 将 /dev/sda2 设置为 Swap 启用这个 Swap 此时用 free 命令查看就可以看到 Swap 已经加载了。 接下来让路由器启动的时候自动加载这个 Swap。 编辑 /etc/config/fstab。 在下面多添加： config swap option enabled '1' option device '/dev/sda2' 注：swap 无法通过 UUID 挂载，它只有 PARTUUID。只能通过盘号挂载 执行 reboot 重启路由器。重启后执行 free 就能看到效果了。 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:3:0","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":["OpenWRT"],"content":"参考链接 https://segmentfault.com/a/1190000000380233 场景二：自身容量大，但 OpenWRT 只用了一小部分 这里以 Raspberry Pi 4B 为例： 镜像下载地址： https://openwrt.org/toh/hwdata/raspberry_pi_foundation/raspberry_pi_foundation_raspberry_pi_4_b 先把 OpenWRT 装好，网络配置为 DHCP 把存储卡剩余容量格式化为 ext4 给多少容量无所谓，这一步是为了存备份文件 进入到路由，执行命令把系统文件备份到刚刚创建的 ext4 分区(假设为 /dev/sda3) mkdir /mnt/udisk mount /dev/sda3 /mnt/udisk mkdir /tmp/root mount --bind / /tmp/root tar -C /tmp/root -cvf /mnt/udisk/backup.tar . 如果有 Ubuntu 系统桌面版，直接进去 Ubuntu 系统使用下面的操作。 没有的话，刷一个到 U 盘，然后用 U 盘启动。 使用 Ubuntu 自带的 disk 软件，将原先装系统文件的那个分区格式化为 ext4 注意：一定要使用格式化，不能直接删除该分区。因为该分区前面的空白分区需要保留。如果直接删除，会被合并起来。 将上一步格式化为 ext4 分区的容量调大到想要的大小，例如 4GB 把 backup.tar 解压到这个分区里面 在这前面还有一个分区，叫 boot 分区。进去里面编辑 cmdline.txt 把 rootfstype=squashfs,ext4 改为 rootfstype=ext4,squashfs ，并保存 把卡插到 Raspberry 启动 ","date":"2019-10-19","objectID":"/2019/10/19/openwrt-expand-storage/:4:0","tags":["OpenWRT","扩容"],"title":"OpenWRT 扩展容量","uri":"/2019/10/19/openwrt-expand-storage/"},{"categories":null,"content":"客户端配置 配置文件 代理工具（v2rayN） ","date":"2019-10-18","objectID":"/drafts/v2ray/:1:0","tags":["v2ray"],"title":"v2ray","uri":"/drafts/v2ray/"},{"categories":null,"content":"服务端配置 配置文件 开放端口 firewall-cmd --zone=public --add-port=25535/tcp --permanent firewall-cmd --reload ","date":"2019-10-18","objectID":"/drafts/v2ray/:2:0","tags":["v2ray"],"title":"v2ray","uri":"/drafts/v2ray/"},{"categories":null,"content":"too many open files 需要在所有的 outbound 里面添加： \"streamSettings\": { \"sockopt\": { \"mark\": 255 } } 否则会死循环，导致： 2019/10/16 17:53:27 [Warning] v2ray.com/core/transport/internet/tcp: failed to accepted raw connections \u003e accept tcp [::]:1088: accept4: too many open files 来源： https://github.com/v2ray/v2ray-core/issues/1574#issuecomment-539429974 ","date":"2019-10-18","objectID":"/drafts/v2ray/:3:0","tags":["v2ray"],"title":"v2ray","uri":"/drafts/v2ray/"},{"categories":null,"content":"卡在 Read config 日志配置问题。删掉看看。 ","date":"2019-10-18","objectID":"/drafts/v2ray/:4:0","tags":["v2ray"],"title":"v2ray","uri":"/drafts/v2ray/"},{"categories":null,"content":"参考资料 v2ray 完全配置指南： https://ailitonia.com/archives/v2ray%e5%ae%8c%e5%85%a8%e9%85%8d%e7%bd%ae%e6%8c%87%e5%8d%97/ ","date":"2019-10-18","objectID":"/drafts/v2ray/:5:0","tags":["v2ray"],"title":"v2ray","uri":"/drafts/v2ray/"},{"categories":null,"content":"注：如果提示软件无法安装，一般是因为没有执行 opkg update 目录： 支持 U 盘 U 盘扩容 内网穿透 科学学习 Supervisor ","date":"2019-10-18","objectID":"/drafts/openwrt/:0:0","tags":null,"title":"OpenWRT 软件安装和配置","uri":"/drafts/openwrt/"},{"categories":null,"content":"安装 supervisor opkg update opkg install curl opkg install python3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py pip install supervisor mkdir /var/log/supervisor ","date":"2019-10-18","objectID":"/drafts/openwrt/:1:0","tags":null,"title":"OpenWRT 软件安装和配置","uri":"/drafts/openwrt/"},{"categories":["Scripts"],"content":" #!/bin/sh function get_ip() { host=$1 ip=$(nc -vz $host 443 2\u003e\u00261 | grep \"Connect\" | awk -F ' ' '{print $4}' | awk -F ':' '{print $1}') echo $ip $host } get_ip registry-1.docker.io ","date":"2019-10-17","objectID":"/2019/10/17/get-ip/:0:0","tags":["Scripts","hosts","nc"],"title":"获取可连接到某个域名的 IP","uri":"/2019/10/17/get-ip/"},{"categories":null,"content":"https://blog.csdn.net/Roland_Sun/article/details/50266565 ","date":"2019-10-17","objectID":"/2019/10/17/unix-domain-socket/:0:0","tags":null,"title":"进程通信 —— Unix 域套接字","uri":"/2019/10/17/unix-domain-socket/"},{"categories":null,"content":"http://www.voidcn.com/article/p-khtntffn-hy.html https://www.jianshu.com/p/c60ff400ddec ","date":"2019-10-17","objectID":"/2019/10/17/pid/:0:0","tags":null,"title":"pid","uri":"/2019/10/17/pid/"},{"categories":null,"content":"https://github.com/axios/axios/issues/319 const axiosConfig = { headers: { 'content-Type': 'application/json', \"Accept\": \"/\", \"Cache-Control\": \"no-cache\", \"Cookie\": document.cookie }, credentials: \"same-origin\" }; axios.defaults.withCredentials = true; axios.get('/url', axiosConfig) .then((res) =\u003e { // Some result here }) .catch((err) =\u003e { console.log(':('); }); WorkingDirectory ","date":"2019-10-17","objectID":"/drafts/cors/:0:0","tags":null,"title":"cors","uri":"/drafts/cors/"},{"categories":null,"content":"过滤器 配置过滤器 Filter 来对特定类的方法做是否登录的判断 添加 Filter 的方法： 创建过滤器 public class EncodingFilter implements Filter { private static final String CONTENT_TYPE = \"text/html;charset=UTF-8\"; public EncodingFilter() { } public void destroy() { } public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { request.setCharacterEncoding(\"UTF-8\"); response.setCharacterEncoding(\"UTF-8\"); response.setContentType(\"text/html;charset=UTF-8\"); chain.doFilter(request, response); } public void init(FilterConfig filterConfig) throws ServletException { } } 配置类注解 @Configuration 配置类方法注解 @Bean 配置类方法内容： FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setOrder(3); // 设置过滤顺序，越小越先执行 filterRegistrationBean.setFilter(new YourFilter()); // 设置过滤器 filterRegistrationBean.setName(\"customFilter\"); // 设置过滤器名称 filterRegistrationBean.addUrlPatterns(\"*.action\"); // 设置使用该过滤器的 Url 特征 return filterRegistrationBean; 见： https://www.jianshu.com/p/05c8be17c80a ","date":"2019-10-15","objectID":"/2019/10/15/spring-boot/:1:0","tags":null,"title":"Spring Boot","uri":"/2019/10/15/spring-boot/"},{"categories":null,"content":"问题集 如何验证用户是否已登录？ 如何配置过滤器 @Configuration 任意类都可以加上 @Configuration 么 Spring Boot 扫描的规则是什么 @ComponentScan 和 @ServletComponentScan 有什么区别？ 为什么教程说 @ServletComponentScan ，而实际使用自带的 @SpringBootApplication （包含 @ComponentScan）就可以？ @WebServlet, @WebFilter, and @WebListener annotated classes can be automatically registered with an embedded Servlet container by annotating @ServletComponentScan on a @Configuration class and specifying the packages. 为何例子里面没有 @WebFilter 如何读取配置文件 http://idea.medeming.com/jetbrains/ https://www.cnblogs.com/oucbl/p/11664610.html#_label1 https://github.com/ddd-by-examples/factory https://stackoverflow.com/questions/52321971/ddd-ports-and-adapters-with-onion-architecture-what-goes-where 概念数据模型 Conceptual Data Models http://blog.sina.com.cn/s/blog_45eaa01a0102xf3q.html ","date":"2019-10-15","objectID":"/2019/10/15/spring-boot/:2:0","tags":null,"title":"Spring Boot","uri":"/2019/10/15/spring-boot/"},{"categories":["Installation And Configuration"],"content":"方法总览： 国内源 nc + hosts Nginx 代理（需要 VPS） 官方 registry（需要 VPS，可缓存镜像） JFrog Artifactory（需要 VPS，可缓存镜像，需要许可证） ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:0:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":["Installation And Configuration"],"content":"Docker 拉取镜像的流程 见官方： https://docs.docker.com/registry/spec/auth/token/ ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:1:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":["Installation And Configuration"],"content":"国内源 这个网上的介绍一大堆，不再重复。 其中注意的是 Docker 官方在国内的镜像（https://registry.docker-cn.com）已经失效。 ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:2:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":["Installation And Configuration"],"content":"nc + hosts 通过修改本地 hosts，将域名指向可以连接到的 IP。 依次执行下面两条命令： nc -v auth.docker.io 443 nc -v registry-1.docker.io 443 系统会尝试多个 IP ，直到超出重试次数或者找到能连接上的 IP。 成功的例子： nc -v auth.docker.io 443 Ncat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connected to 34.228.211.243:443. 失败的例子： nc -v auth.docker.io 443 Ncat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connection to 52.70.175.131 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection to 52.55.198.220 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection to 54.88.231.116 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection to 52.87.94.70 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection to 34.232.31.24 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection to 52.2.186.244 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection to 54.210.105.17 failed: Connection timed out. Ncat: Trying next address... Ncat: Connection timed out. 然后在 /etc/hosts 里面添加： ip1 auth.docker.io ip2 registry-1.docker.io ip1 和 ip2 替换为找到的 IP。 之后重启 Docker。 ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:3:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":["Installation And Configuration"],"content":"Nginx 代理 没有试过 在 VPS 上搭 Nginx，做代理。 用到的镜像有： nginx:alpine jwilder/nginx-proxy jrcs/letsencrypt-nginx-proxy-companion 可能要换另外一个镜像（paulczar/omgwtfssl），使用自签名证书。let’s encrypt 应该不能申请这种官方的证书。 使用示例见： https://github.com/nextcloud/docker/blob/master/.examples/docker-compose/with-nginx-proxy-self-signed-ssl/mariadb/fpm/docker-compose.yml 在 nginx:alpine 服务里，/etc/nginx/conf.d/docker-proxy.conf 写入： server { listen 80; server_name registry-1.docker.io; location / { proxy_pass https://registry-1.docker.io; } } server { listen 80; server_name auth.docker.io; location / { proxy_pass https://auth.docker.io; } } // 待补充 ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:4:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":["Installation And Configuration"],"content":"官方 registry docker-compose.yml version: '3' services: registry: image: registry:latest ports: - \"5000:5000\" restart: always volumes: - ./config.yml:/etc/docker/registry/config.yml config.yml version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] proxy: remoteurl: https://registry-1.docker.io health: storagedriver: enabled: true interval: 10s threshold: 3 // 待补充 ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:5:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":["Installation And Configuration"],"content":"JFrog Artifactory // 待补充 ","date":"2019-10-15","objectID":"/2019/10/15/docker-registry/:6:0","tags":["Linux","Docker","Registry"],"title":"Docker 镜像拉取失败的几种解决方法","uri":"/2019/10/15/docker-registry/"},{"categories":null,"content":"只要硬件不损坏，都有办法救砖！ https://0x00rick.com/hardware%20hacking/2018/05/04/Xiaomi_mi_router_3.html https://openwrt.org/toh/xiaomi/mir3 http://www.lyyyuna.com/2015/12/19/xiaomiluyou01/ https://blog.csdn.net/aggresss/article/details/52694051 改成 SPI 模式，然后刷： https://www.right.com.cn/forum/thread-310776-1-1.html http://www.xcx1024.com/ArtInfo/2604245.html ","date":"2019-10-15","objectID":"/2019/10/15/bootloader-error/:0:0","tags":null,"title":"刷错 BootLoader 导致硬件启动错误的修复方法","uri":"/2019/10/15/bootloader-error/"},{"categories":["Router","OpenWRT"],"content":"步骤： 开启 SSH 用 XShell 登录 挂载 U 盘 备份 MTD 刷 Breed （Bootloader） 不使用 Breed 刷 OpenWRT ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:0:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Router","OpenWRT"],"content":"开启 SSH 登录 进入官网： https://d.miwifi.com/rom/ssh 登录并绑定路由器。此时可以得到 账号和密码。 点击【下载工具包】。 之后按官方的说法操作： 请将下载的工具包bin文件复制到U盘（FAT/FAT32格式）的根目录下，保证文件名为miwifi_ssh.bin； 断开小米路由器的电源，将U盘插入USB接口； 按住reset按钮之后重新接入电源，指示灯变为黄色闪烁状态即可松开reset键； 等待3-5秒后安装完成之后，小米路由器会自动重启，之后您就可以尽情折腾啦 ：） ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:1:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Router","OpenWRT"],"content":"用 XShell 登录 路由器默认的地址为 192.168.31.1。如果曾经修改过，按照修改后的值登录。 端口是 22，账号密码用上面步骤获取到的输入。 ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:2:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Router","OpenWRT"],"content":"挂载 U 盘 查看 U 盘可能的位置：ls /dev/sd* 得到：/dev/sda /dev/sda1 取数字最高的挂载到 /mnt 文件夹：mount /dev/sda1 /mnt ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:3:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Router","OpenWRT"],"content":"备份 MTD 备份后就算刷坏也能还原。 for name in $(grep -v 'dev' /proc/mtd | awk -F ':' '{print $1}'); do dd if=/dev/$name of=/mnt/$name.bin; done 如果用这种方式都不能还原，可以用 TTL 救回来 ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:4:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Router","OpenWRT"],"content":"刷 Breed 需要改硬件，我就暂时不尝试了。 有两个版本： 按套路的版本： http://bbs.mydigit.cn/read.php?tid=2325027 不按套路的版本（不需要专业工具）： http://tieba.baidu.com/p/6078302343 Breed 的刷入命令是： mtd -r write /mnt/breed-mt7620-xiaomi-mini.bin Bootloader 其中 /mnt/breed-mt7620-xiaomi-mini.bin 这个是文件位置。这里假设把 U 盘挂载到 /mnt 上。 如果刷了 Breed，那后面直接用 Breed 刷就行了。见另一篇在 2019-07-25 写的小米路由 mini 刷 OpenWRT 教程。 要刷入的 bin 在以下地址中找到以 xiaomi_miwifi-r3-squashfs-breed-factory.bin 结尾的文件： https://dl.x-wrt.com:4443/rom/ ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:5:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Router","OpenWRT"],"content":"不使用 Breed 刷 OpenWRT 由于 OpenWRT 不想支持小米路由3，因此要刷 ptpt52 大神基于 OpenWRT 改的 X-Wrt。 在 Github 上说不支持： https://github.com/openwrt/openwrt/pull/597#issuecomment-407720718 X-Wrt 官网： https://x-wrt.com 点【固件下载】进入下载界面 要下载的文件有两个： 以 xiaomi_miwifi-r3-squashfs-kernel1.bin 结尾的文件 以 xiaomi_miwifi-r3-squashfs-rootfs0.bin 结尾的文件 回到 XShell，执行以下命令（bin 文件的名称根据下载的文件名而定，这里只是举个例子）： nvram set flag_last_success=1 nvram set boot_wait=on nvram set uart_en=1 nvram commit mtd write xxxxx-kernel1.bin kernel1 mtd write xxxxx-rootfs0.bin rootfs0 # 如果你刷了 Breed，要再执行以下命令（记得去掉前面的 #）： # mtd write xxxxx-kernel1.bin kernel0 reboot 前面 4 条命令开启了串口，非常重要。小米默认锁死串口。如果不开启，万一刷机失败或者出现意外，再也救不回来了。 ","date":"2019-10-12","objectID":"/2019/10/12/xiaomi-router-r3-openwrt/:6:0","tags":["Router","OpenWRT","Breed","XiaoMi Router R3"],"title":"小米路由3 刷 OpenWRT","uri":"/2019/10/12/xiaomi-router-r3-openwrt/"},{"categories":["Learning Docker"],"content":"Docker Compose 是用来做什么的 最直接的就是把原先要在命令行跑容器所需的参数整合到一个文件（docker-compose.yaml）里面，组织起来。这样就不用怕忘记某个参数了。 可以使用一行简单的命令（docker-compose）同时启动（up）、重启（restart）、关闭（stop）多个服务。 开发和测试环境使用另外安装的 docker-compose 命令，单机的生产环境也可用。 生产环境的集群如果是 Docker Swarm，则使用 Docker 自带的 docker stack deploy 。 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:1:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"Docker Compose 和 Docker Stack 的区别和共同点 最显著的区别是：docker-compose 可以构建镜像，也可以使用已构建镜像，而 docker stack deploy 只能使用已构建镜像。 另外还有一些配置上的区别，后面会讲到。 他们有一个共同点，那就是服务的配置文件都是用 yaml （发音 /ˈjæməl/ ）语言写的。并且如果他们发现传入的配置文件中的配置不是自己支持的，会选择忽略。这样我们就可以把 Compose 和 Stack 的配置写在同一个文件。 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:2:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"简单介绍 YAML 语法 YAML 语言教程 http://www.ruanyifeng.com/blog/2016/07/yaml.html YAML 包含三种数据类型：标量、数组、对象 使用缩进来区分配置块（类似于 Python） 缩进只能使用空格 标量类型有：字符串，布尔值，整数，浮点数，Null，时间（ISO8601），日期（ISO8601） version: '3' services: web: build: . ports: - \"5000:5000\" volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redis volumes: logvolume01: {} 上面是 Docker 官方给出的 Compose 文件示例。 其中 version: '3' 表示字段 version 的值为字符串的 3。 docker-compose 在读取 version 这个字段时，限制其类型必须为字符串，否则报错。 继续往下看，字段值可以是一个 对象： services: web: build: . build 的值写的是一个标量：点，表示当前目录。 像这种没有明显特征的都会被当做 字符串。 继续往下看： ports: - \"5000:5000\" 这里 ports 的值是一个 数组。数组元素用一个减号开头。 继续往下： volumes: logvolume01: {} {} 表示空对象。顺便一提，[] 表示空数组。 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:3:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"Docker Compose 文件的结构 version: '3' services: service-name1: build: . service-name2: image: redis volumes: logvolume01: {} networks: hostnet: configs: my_first_config: secrets: my_first_secret: 一个 docker-compose.yaml 文件中，通常至少包含： version ：用于标识该文件使用的配置版本。不同版本的配置项有一些差异。 services ：用于配置服务 version、services、volumes、networks、configs、secrets 这几个都属于顶层（top-level）配置，当前总共也就这几个。 具体 service 可以使用 volumes、networks、configs、secrets 定义的资源，不能在 service 里定义这些资源。 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:4:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"举个例子 // 待补充 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:5:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"Docker Compose 和命令行参数的对应关系 命令行 Docker Compose 值类型 -v volumes 对象数组或者字符串数组 -p ports 对象数组或者字符串数组 -l labels 对象或数组 --expose expose 字符串数组 -e environment 对象或数组 COMMAND command 字符串或数组 补充个命令行没有的： entrypoint 字符串或数组 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:6:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"Docker Compose 独用的 Service 选项 官方文档： https://docs.docker.com/compose/compose-file/#not-supported-for-docker-stack-deploy 选项 作用 Stack类似功能 build 指定构建文件夹 无 container_name 指定启动后的容器名 无 restart 重启策略。no、always、on-failure、unless-stopped。设置为 always 会使得该服务在宿主机开机后启动。 restart_policy tmpfs 映射目录到容器 tmp 目录 devices 映射外部设备（例如 USB） links 连接到某个服务，会影响服务启动顺序。类似于修改 hosts 给依赖的服务添加别名。推荐用 network 代替 links external_links 外部链接 cgroup_parent network_mode security_opt userns_mode ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:7:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"Docker Stack 独用的 Service 选项 只有一个：deploy 官方文档： https://docs.docker.com/compose/compose-file/#deploy 其子选项有： 选项 类型 作用 restart_policy Object 重启策略。condition: none、on-failure、any（默认，总是重启） mode String 设置服务部署模式。global 表示所有节点都部署，replicated 表示仅部署指定数量的节点 replicas Integer 配置改服务部署多少个实例 placement Object 见下 placement.constraints Array\u003cString\u003e 只有满足条件的节点才会部署。例如：engine.labels.operatingsystem == ubuntu 14.04 placement.preferences Object 分部模式。针对某个节点 label 的不同值做分部。当前策略只有 spread 一种，即平均分部 resources Object 服务使用宿主机资源的限制。有 limit 和 reservations，分别代表和最小值 resources.limits Object 允许使用的资源最大值 resources.reservations Object 允许使用的资源最小值 labels Array\u003cString\u003e 和 Dockerfile 的 LABLE 相似，但仅作用于服务的容器 endpoint_mode String 设置 Virtual IP 或者 DNSRR rollback_config Object 服务升级失败时的回滚策略 update_config Object 服务升级时的策略 placement.constraints 选项具体见： https://docs.docker.com/engine/reference/commandline/service_create/#specify-service-constraints—constraint placement.preferences 示例： 集群有 6 个节点，他们都有一个 lable 叫做 zone。条件形式为 node.labels.zone 。 其中三个节点的 node.labels.zone=A 两个节点的 node.labels.zone=B 一个节点的 node.labels.zone=C 要发布 9 个实例，设置 spread=node.labels.zone，就会给 A B C 每个分配三个实例。 对于 A，会给每个节点一个实例。 对于 B，会给一个节点两个实例，另一个节点一个实例。 对于 C，会给这个节点三个实例。 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:8:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"很有用的配置 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:9:0","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":["Learning Docker"],"content":"服务启动先后顺序 —— depends_on 类型：Array\u003cString\u003e 如果 A depends_on B ,那么 B 就会先启动，之后再启动 A。 ","date":"2019-10-11","objectID":"/2019/10/11/docker-series-e2/:9:1","tags":["Docker","Docker Compose","YAML"],"title":"Docker Compose","uri":"/2019/10/11/docker-series-e2/"},{"categories":null,"content":"配置国内源 cp /etc/apt/sources.list /etc/apt/sources.list.default sudo cat \u003e /etc/apt/sources.list \u003c\u003c DELIM deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ buster main non-free contrib deb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ buster main non-free contrib DELIM cp /etc/apt/sources.list.d/raspi.list /etc/apt/sources.list.d/raspi.list.default sudo cat \u003e /etc/apt/sources.list.d/raspi.list \u003c\u003c DELIM deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ buster main ui DELIM sudo apt-get update ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:1:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"从 Windows 连接到 Raspberry 的远程桌面 法一：自带的 VNC Server raspbian 桌面版自带 VNC，默认不启动。执行以下命令启动： sudo systemctl start vncserver-x11-serviced.service 然后下载 VNC Viewer： https://www.realvnc.com/download/viewer/ 用 IP 登录即可。 法二：Windows 的远程桌面 执行以下命令： sudo apt-get install xrdp 安装完成后，用 Windows 的远程桌面连接即可。 ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:2:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"从 Raspberry 连接到 Windows 的远程桌面 用了 xrdp 和 freerdp 后，选择用 freerdp 执行以下命令： sudo apt-get install freerdp2-x11 xfreerdp /u:邮箱 /v:地址 安装 Docker curl -sSL https://get.docker.com | sh 安装 tmux sudo apt-get install tmux 安装 mariadb sudo apt-get install mariadb-server ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:3:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"连接无线 法一：通过桌面配置 这种方式会自动设置为开机自动连接。 法二：命令配置 wpa_supplicant -B -i wlan0 -c \u003c(wpa_passphrase \"你要连接的 WiFi 名称（SSID）\" \"你的密码\") 只需执行一次，重启后会自动扫描和连接。 wlan0 表示 WiFi 的网卡，可能根据不同情况名称不同。网卡名称通过 ifconfig -s 看第一列得到。 SSID 可通过以下命令扫描： iwlist wlan0 scan | grep SSID 详细的请参考： https://www.jianshu.com/p/e22331d62a16 ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:4:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"添加用户并设置为可登录 root 将用户加入组 sudo 即可登录 root。 ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:5:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"基本软件安装 sudo apt-get install -y vim git lrzsz ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:6:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"安装 Docker curl -fsSL https://get.docker.com | sh ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:7:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"外接硬盘 要把 Docker 的目录放到外部硬盘，外部硬盘最好实用 xfs 文件系统。这样可以直接实用 overlay2。 ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:8:0","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"XFS 如果想使用 xfs 文件系统，则需要对硬盘格式化。 安装 xfsprogs yum install -y xfsprogs 或者 apt-get install -y xfsprogs 找到硬盘 fdisk -l | grep dev 找到容量和外置硬盘差不多的。例如 /dev/sda 格式化 mkfs -t xfs -f /dev/sda https://www.cyberciti.biz/faq/how-to-install-xfs-and-create-xfs-file-system-on-debianubuntu-linux/ ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:8:1","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"NTFS 如果外部硬盘是 NTFS 系统，则会报错： docker: error creating overlay mount to invalid argument 如果实在想用 NTFS，则把 Docker 的 storage-driver 设置为 vfs。虽然慢，但是能用。 见： https://github.com/moby/moby/issues/23930#issuecomment-358367075 ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:8:2","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"设置为开机自动挂载硬盘 获取 PARTUUID blkid -p /dev/sda 使用 PARTUUID 是为了防止标识变化。 用 UUID 也可以 https://raspberrypi.stackexchange.com/questions/75027/whats-the-difference-between-uuid-and-partuuid/75030#75030 编辑 /etc/fstab PARTUUID=XXXXX /mount-dest-dir xfs defaults,nofail,x-systemd.device-timeout=5 0 0 https://askubuntu.com/questions/14365/mount-an-external-drive-at-boot-time-only-if-it-is-plugged-in/1027273#1027273 ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:8:3","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":null,"content":"设置时区 timedatectl set-timezone Asia/Shanghai ","date":"2019-10-11","objectID":"/2019/10/11/raspberry-pi-4-raspbian/:8:4","tags":null,"title":"raspberry-pi-4-raspbian","uri":"/2019/10/11/raspberry-pi-4-raspbian/"},{"categories":["Installation And Configuration"],"content":" yum install -y make autoconf gcc zlib-devel gettext-devel make config ./configure --prefix=/usr make all make install https://git-scm.com/book/en/v2/Getting-Started-Installing-Git ","date":"2019-10-09","objectID":"/2019/10/09/git-compiling/:0:0","tags":["Linux","CentOS7","Compiling","Git"],"title":"CentOS7 编译 Git","uri":"/2019/10/09/git-compiling/"},{"categories":null,"content":"Pro 破解： https://ciphers.pw/resources/artifactory-pro-license-crack-6-6-0.119/ java -jar artifactory-injector-1.1.jar 先选 2，注入到目录：/opt/jfrog/artifactory 再选 1，得到 license 然后访问 web，输入账号密码。 用户名：admin 密码：password 会提示没有 license，按提示点进去，把刚刚生成的 license 放进去就行了。 问题： Timed out waiting for join.key file to be made available at /var/opt/jfrog/artifactory/etc/security/join.key 解决： # Upload the access.war and artifactory.war via the Tomcat Manager webapp. # As soon as these are uploaded, stop tomcat and delete the automatically-created artifactory folder. # Create artifactory folder. mkdir /usr/share/artifactory chown tomcat.tomcat /usr/share/artifactory cd /usr/share/artifactory # Start tomcat. service tomcat8 start # Monitor the etc/security folder repeatedly until it has been automatically created by the artifactory webapp (a few seconds): ls etc/security ls etc/security ls etc/security # Create a new master key for artifactory: openssl rand -hex 16 \u003e etc/security/master.key chown tomcat.tomcat etc/security/master.key chmod 600 etc/security/master.key # Monitor the access/etc/keys folder repeatedly until it has been automatically created by the access webapp (about 20 seconds): ls access/etc/keys ls access/etc/keys ls access/etc/keys # Create a new join key for access: openssl rand -hex 16 \u003e access/etc/keys/join.key chown tomcat.tomcat access/etc/keys/join.key chmod 600 access/etc/keys/join.key cp -a access/etc/keys/join.key etc/security/join.key # Check the logs to confirm artifactory was able to connect to the access server: tail logs/artifactory.log 2019-06-03 15:47:51,644 [art-init] [INFO ] (o.a.w.s.ArtifactoryContextConfigListener:215) - ########################################################### ### Artifactory successfully started (53.527 seconds) ### ########################################################### 参考链接： https://stackoverflow.com/questions/55142890/join-key-file-not-created-or-deleted-on-upgrade-to-6-8-x-artifactory-pro/56430896#56430896 docker-compose.yml version: '2' services: postgresql: image: docker.bintray.io/postgres:9.6.11 container_name: postgresql ports: - 5432:5432 environment: - POSTGRES_DB=artifactory # The following must match the DB_USER and DB_PASSWORD values passed to Artifactory - POSTGRES_USER=jfrog - POSTGRES_PASSWORD=9f2X3Bg3ZeEMINlgTgrx volumes: - postgresql:/var/lib/postgresql/data restart: always ulimits: nproc: 65535 nofile: soft: 32000 hard: 40000 artifactory: image: docker.bintray.io/jfrog/artifactory-pro:6.13.1 container_name: artifactory ports: - 9003:8081 depends_on: - postgresql links: - postgresql volumes: - artifactory:/var/opt/jfrog/artifactory environment: - DB_TYPE=postgresql # The following must match the POSTGRES_USER and POSTGRES_PASSWORD values passed to PostgreSQL - DB_USER=jfrog - DB_PASSWORD=9f2X3Bg3ZeEMINlgTgrx # Add extra Java options by uncommenting the following line - EXTRA_JAVA_OPTIONS=-Xms512m -Xmx4g restart: always ulimits: nproc: 65535 nofile: soft: 32000 hard: 40000 volumes: artifactory: postgresql: 要先注入，否则无法启动。注入后会自动重启。注入后要立即让工具生成 License 启动期间要生成 join key 会有提示： join.key not found, waiting for sync openssl rand -hex 16 \u003e etc/security/master.key chown tomcat.tomcat etc/security/master.key chmod 600 etc/security/master.key openssl rand -hex 16 \u003e access/etc/keys/join.key chown tomcat.tomcat access/etc/keys/join.key chmod 600 access/etc/keys/join.key cp -a access/etc/keys/join.key etc/security/join.key ","date":"2019-10-09","objectID":"/drafts/jfrog-artifactory/:0:0","tags":null,"title":"jfrog-artifactory","uri":"/drafts/jfrog-artifactory/"},{"categories":null,"content":"TTL 救砖和刷机看这篇： https://linan.blog/2019/N1-Burn-img/ 注意！！ 擦除 flash 和 擦除 bootloader 是要 取消勾选！！！眼瞎看成 勾选 导致多花了些时间。 见：https://www.right.com.cn/forum/thread-308485-2-1.html 官改固件作者发的贴子： https://www.znds.com/tv-1118656-1-1.html OPENWRT： https://www.znds.com/tv-1151977-1-1.html https://github.com/coolsnowwolf/lede https://www.znds.com/tv-1034311-1-1.html http://www.epinv.com/post/12289.html https://www.mivm.cn/phicomm-n1-unofficial/ 工具包： https://share.weiyun.com/5t9zqaO 刷 armbian ： 下载包： https://yadi.sk/d/pHxaRAs-tZiei 配置时间： https://yuerblog.cc/2019/10/23/%e6%96%90%e8%ae%afn1-%e5%ae%8c%e7%be%8e%e5%88%b7%e6%9c%baarmbian%e6%95%99%e7%a8%8b/ 看起来舒服： https://mivm.cn/phicomm-n1-linux 参数和统计： https://github.com/kasicass/blog/blob/master/debian/2018_11_19_armbian_on_n1_box.md 软件安装： https://post.smzdm.com/p/a25gpgx7/ freerdp： https://blog.csdn.net/Pipcie/article/details/84955347 xfreerdp –sound alsa -f –audio-mode 0 –disable-wallpaper –disable-themes –enable-fonts –disable-menu-anims –network lan –enable-clipboard –sec nla -w 1920 -h 1080 -u 用户邮箱 -v ip地址 xfreerdp -wallpaper -themes +clipboard +fonts -menu-anims /compression-level:0 /sound:alsa /sec:nla /w:1920 /h:1080 /network:lan /u:用户邮箱 /v:192.168.1.102:3389 ","date":"2019-10-09","objectID":"/2019/10/09/phicomm-n1/:0:0","tags":null,"title":"phicomm-n1","uri":"/2019/10/09/phicomm-n1/"},{"categories":["Installation And Configuration"],"content":"基础操作 添加用户： useradd -m username -m 参数是为了给用户创建 home 目录。例如 /home/username 设置密码或修改密码： passwd username 用户加组： gpasswd -a username groupname 组移除用户： gpasswd -d username groupname 删除用户： userdel username ","date":"2019-10-07","objectID":"/2019/10/07/centos-user-management/:1:0","tags":["Linux","CentOS7","UserManagement"],"title":"CentOS7 用户管理","uri":"/2019/10/07/centos-user-management/"},{"categories":["Installation And Configuration"],"content":"允许用户执行 sudo 系统允许在 wheel 用户组内的用户执行 sudo： gpasswd -a username wheel ","date":"2019-10-07","objectID":"/2019/10/07/centos-user-management/:2:0","tags":["Linux","CentOS7","UserManagement"],"title":"CentOS7 用户管理","uri":"/2019/10/07/centos-user-management/"},{"categories":["Installation And Configuration"],"content":"工具 yum install -y NetworkManager-wifi ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:0","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"查看网卡状态 nmcli dev status 结果示例： DEVICE TYPE STATE CONNECTION eth0 ethernet connected Wired connection 1 wlan0 wifi disconnected -- p2p-dev-wlan0 wifi-p2p disconnected -- lo loopback unmanaged -- 如果是 unavailable，则重启系统。 ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:1","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"查看 WiFi 列表 nmcli dev wifi 结果示例： IN-USE SSID MODE CHAN RATE SIGNAL BARS SECURITY CMCC-5xJW Infra 6 130 Mbit/s 100 ▂▄▆█ WPA1 WPA2 CMCC-CsJc Infra 13 130 Mbit/s 55 ▂▄__ WPA1 WPA2 TP-LINK_3F82 Infra 11 405 Mbit/s 29 ▂___ WPA1 WPA2 ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:2","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"连接 WiFi nmcli dev wifi con \"SSID\" password \"密码\" 注： 如果之前有通过其他方式连接或者创建配置文件到 /etc/sysconfig/network-scripts/ 里面，则要先删除。 否则会提示：Secrets were required, but not provided ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:3","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"启动已经连接过的 WiFi nmcli con up \"SSID\" ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:4","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"设置开机自动连接 WiFi nmcli con mod \"SSID\" connection.autoconnect yes ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:5","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"如果发现无法通过 WiFi 连接 SSH 重启路由器 ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:1:6","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":["Installation And Configuration"],"content":"参考链接 https://www.cnblogs.com/asker009/p/10212045.html ","date":"2019-10-07","objectID":"/2019/10/07/centos-network/:2:0","tags":["Linux","CentOS"],"title":"CentOS7 NetworkManager 管理网络","uri":"/2019/10/07/centos-network/"},{"categories":null,"content":"下载与安装 https://openwrt.org/toh/hwdata/raspberry_pi_foundation/raspberry_pi_foundation_raspberry_pi_4_b 当前只有 snapshot 版本。找到【Firmware OpenWrt snapshot Install URL】，后面是下载地址。下载下来解压得到 .img 文件。 使用 Win32DiskManager 将 img 文件刷入到 SD 卡。 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:1:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"配置 将设备接入电脑，电脑使用 Xshell 或者其他工具连接 192.168.1.1，用户名为 root，无密码。 执行以下命令： uci set network.lan.proto=dhcp uci delete network.lan.ipaddr service network restart 将设备接入以前的路由器。进入路由器界面，找到路由器分配给 Raspberry 的 IP。然后用 Xshell 连接。此时可以下载软件，以及做其他配置。 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:2:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"Snapshot 版本 需要自己安装界面管理 LuCI。 opkg update opkg install luci ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:3:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"https 支持 opkg update opkg install libustream-openssl20150806 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:4:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"安装 USB 转千兆网卡驱动 购买 USB 转千兆网卡。例如绿联20255，其芯片为 AX88179，安装命令为： opkg update opkg install kmod-usb-net-asix-ax88179 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:5:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"配置拨号连接 将设备连接到家庭网络的入口，电脑连接 Raspberry，进入 http://openwrt.lan/cgi-bin/luci 。 进入 [Network | Interfaces] ，[Add new interface] 添加接口： 选项 填写 Protocol PPPoE username 宽带账号 password 宽带密码 firewall-zone WAN 然后编辑原先的 DHCP 的接口： 选项 填写 Protocol Static address address 192.168.100.1 netmask 255.255.255.0 firewall-zone LAN Bridge interfaces 打勾 Interface eth1 和 wlan0 点 [Save \u0026 Apply] 保存并应用。 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:6:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"扩容 以下内容引用自： https://schaepher.github.io/2019/10/19/openwrt-expand-storage/#more 先把 OpenWRT 装好，网络配置为 DHCP 把存储卡剩余容量格式化为 ext4 给多少容量无所谓，这一步是为了存备份文件 进入到路由，执行命令把系统文件备份到刚刚创建的 ext4 分区(假设为 /dev/sda3) mkdir /mnt/udisk mount /dev/sda3 /mnt/udisk mkdir /tmp/root mount --bind / /tmp/root tar -C /tmp/root -cvf /mnt/udisk/backup.tar . 如果有 Ubuntu 系统桌面版，直接进去 Ubuntu 系统使用下面的操作。 没有的话，刷一个到 U 盘，然后用 U 盘启动。 使用 Ubuntu 自带的 disk 软件，将原先装系统文件的那个分区格式化为 ext4 注意：一定要使用格式化，不能直接删除该分区。因为该分区前面的空白分区需要保留。如果直接删除，会被合并起来。 将上一步格式化为 ext4 分区的容量调大到想要的大小，例如 4GB 把 backup.tar 解压到这个分区里面 在这前面还有一个分区，叫 boot 分区。进去里面编辑 cmdline.txt 把 rootfstype=squashfs,ext4 改为 rootfstype=ext4,squashfs ，并保存 把卡插到 Raspberry 启动 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:7:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"安装 DDNS 如果家里网络是有外网 IP 的，且申请了域名，那么可以给域名设置 DDNS。 opkg update opkg install ddns-scripts luci-app-ddns 配置： namecheap 会提供一个 [Dynamic DNS Password]，这个用作密码。 http://electropit.com/index.php/2015/10/16/openwrt-ddns-setup-for-namecheap/ ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:8:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"其他软件 opkg update opkg install lrzsz vim curl ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:9:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"注意点 WiFi 无法发送信号，只能用于接收信号。 WiFi 可接收 2.4G 和 5G 信号，5G 信号仅能接收 100 信道以下的。 ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-openwrt/:10:0","tags":null,"title":"树莓派4 安装 OpenWRT 作为路由器","uri":"/2019/10/07/raspberry-openwrt/"},{"categories":null,"content":"工作温度： 无风扇：56℃ 带风扇：42℃ 软件：Screen，多标签 Shell，断开不会中断任务，可恢复 https://www.chenhe.cc/p/139 https://www.ibm.com/developerworks/cn/linux/l-cn-screen/ http://www.raspigeek.com/index.php?c=home CentOS7 32 bit getconf LONG_BIT http://shumeipai.nxez.com/2019/08/27/add-the-open-and-shutdown-keys-to-the-raspberry-pi.html ","date":"2019-10-07","objectID":"/2019/10/07/raspberry-pi-4/:0:0","tags":null,"title":"树莓派4 笔记","uri":"/2019/10/07/raspberry-pi-4/"},{"categories":["Installation And Configuration"],"content":"主从 官方文档 https://mariadb.com/kb/en/library/setting-up-replication/ 主从配置其实很简单。有以下几个步骤： 步骤 Primary Replica 1 开启 bin-log，设置服务器 ID，重启 Mariadb 2 设置服务器 ID，重启 Mariadb 3 创建 Replica 连接 Primary 所需的用户名密码，并授予 replication 权限 4 把内存脏页刷到磁盘并加上读锁，防止其他事务修改数据 5 查看 bin-log 文件名和 bin 位置，并记录 6 Replica 找 Primary 同步所有表及数据 7 释放读锁 8 创建到 Primary 的连接，并附上刚才记录的 bin-log 文件名和 bin 位置 9 启动 replication 服务 启动后查看服务状态，如果报错，则停掉 replication 服务，并解决问题。然后重置 replication 链接，并从 4 开始。 ","date":"2019-10-07","objectID":"/2019/10/07/mariadb-replication/:1:0","tags":["Mariadb","Replication"],"title":"Mariadb 配置","uri":"/2019/10/07/mariadb-replication/"},{"categories":["Router","OpenWRT"],"content":"背景 之前往 小米路由3 刷入小米路由 MINI 的 OpenWRT 包，结果发现无法安装软件。于是就想刷回来，但是 MTD 信息已经被清空，无法通过 mtd write 来重新写入其他固件。 也无法通过官方的方法（U盘放入官方固件 miwifi.bin）来恢复。因为系统压根就不理这个文件，毕竟 bootloader 不一样。 这时候跟变砖已经没什么两样了。 一开始折腾到绝望，然后买了台二手小米 MINI，勉强先用着。直到今天才又开始继续折腾。幸运的是刷回来了，否则今晚会睡不着。 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:1:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"准备 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:2:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"硬件 一台被刷坏的 小米路由3 和电源 一个带有 小米路由3 固件二进制文件的 U 盘 一个 USB 转 TTL 转换器 我用的是 CH340G RS232升USB转TTL模块转串口，12 块钱。 公对母杜邦线 3 条（10 条 6 块钱） 买转换器的时候顺便买几条。实在没有就用导线代替。 十字螺丝刀 网线一条 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:2:1","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"软件 PuTTY 或者 XShell，用于读写串口 Windows 自带的 tftp ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:2:2","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"路由器系统相关文件 uboot_md5-b7a74e9668289dd68f1c637cd99dcc5c.bin Bootloader miwifi_r3_firmware_e9f31_2.27.120.bin 官方固件 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:2:3","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"大概要做啥 把 小米路由3 的电路板拆出来 将 USB 转 TTL 转换器的 TTL 端连接到电路板上的 TTL 接口，USB 端连 Windows 小米路由3 通过网线连接到 Windows 上 Windows 上通过软件与路由器 TTL 交互 路由器接入电源，进入命令行模式，做一些配置 路由器重启，进入【通过 TFTP 写入 Flash】模式 通过 TFTP 将 uboot 和 miwifi 这两个二进制文件写入路由器 重启路由器 如果提示 Press reset button to enter USB recovery，则按照官方刷机教程刷机 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:3:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"把 小米路由3 的电路板拆出来 后盖商品信息条的中间部分有一颗螺丝，把它转出来。然后就可以撬开外壳了。 把天线连接到电路板的那个地方拔起来。这样就可以把整块板取出来。 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:4:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"TTL 转换器连接 连接路由电路板 在电路板靠电源的一边，有四个小孔。分别是 3.3V、RX、GND、TX。 转换器有五个口，分别是 5V、3.3V、TXD、RXD、GND。 以下是连线关系，左边连接到右边（公的部分直接插到电路板的孔里就可以用，不需要焊接）： 转换器 电路板 TXD RX RXD TX GND GND 其他不需要连接 把转换器的 USB 端连接到 WIndows 上 通过 Windows 的【设备管理器】得到刚刚插入的串口号：COMn。这里的 n 是一个数字，在不同电脑上可能不同。 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:5:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"小米路由3 通过网线连接到 Windows 上 用于传输数据 路由端连 LAN 口 Windows 连接后，要在网络配置里面，做以下配置： IP 获取模式： DHCP 改为手动 IP： 192.168.1.3 子网掩码：255.255.255.0 或者子网前缀长度：24 网关：192.168.1.1（路由器的地址） Windows 如果有启动 WiFi，则关掉 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:6:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"Windows 上通过软件与路由器 TTL 交互 用于发送指令 这里以 PuTTY 为例： |配置项|配置| |Connection type|Serial| |Serial line|COMn（记得根据实际情况把 n 替换为某个数字）| |Speed|115200| 配置完后点【Open】，打开交互界面，继续下面的操作 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:7:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"路由器接入电源，进入命令行模式 要先在上一步 Open 之后才接入电源，这样才不会让路由器直接加载系统。 接入电源后，能从交互界面上看到一些信息，直到展示下面这些： Please choose the operation: 1: Load system code to SDRAM via TFTP. 2: Load system code then write to Flash via TFTP. 3: Boot system code via Flash (default). 4: Entr boot command line interface. 7: Load Boot Loader code then write to Flash via Serial. 9: Load Boot Loader code then write to Flash via TFTP. 先选择 4，进入命令行界面。配置启动选项等待和开启写入。 执行以下命令： setenv boot_wait on setenv uart_en 1 saveenv 这些命令相当于在官方或者潘多拉固件里执行： nvram set boot_wait=on nvram set uart_en=1 nvram commit 或者相当于 OpenWrt 里执行： fw_setenv boot_wait on fw_setenv uart_en 1 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:8:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"Windows 开启 TFTP 打开 Windows 命令行界面（CMD 或者 PowerShell），执行 tftp。如果提示无法执行，则表示没有开启该功能。 按以下步骤开启： WIN+R 打开 Run，输入 appwiz.cpl，并打开 点击左侧的【启用或关闭 Windows 功能】 找到 【TFTP Client】，前面打勾 确定 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:9:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"路由器重启，进入【通过 TFTP 写入 Flash】模式 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:10:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"写 BootLoader 拔掉电源，再插入。这次选择 9。 选 Y 填入 192.168.1.1（默认就是） 填入 192.168.1.3（我们在上面设置的 Windows IP） 文件名填入 uboot.bin 我们下面假设 uboot_md5-b7a74e9668289dd68f1c637cd99dcc5c.bin 存放在 E:\\uboot.bin 此时会进入等待状态。 在 Windows 上打开命令行界面，执行： tftp -i 192.168.1.1 PUT E:\\uboot.bin 执行过程中可能会出现 N 次失败，比如 Retry count exceeded; 或者 Timeout，不断重试直到成功就行了。 如果看到 Writing image to，就表示很快要成功了。 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:10:1","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"写 Firmware 同理，拔电源重启，这次选 2。 其余和上面差不多，就是把 uboot 换成 miwifi_r3_firmware_e9f31_2.27.120.bin。 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:10:2","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"重启路由器 写入成功后，会自动重启路由器。看着交互界面读取的信息就有一股成功的气息。 路由器会做一些初始化，需要等待一段时间。这段时间灯会常亮黄色灯。 路由器会多次自动重启。 如果看到 Booting up finished，那么就是成功了。通过 192.168.31.1 就可以看到界面。 如果一直提示 Press reset button to enter USB recovery，则把 miwifi 那个文件重命名为 miwifi.bin 放到 U 盘根目录，然后插入 USB 口。 按住 Reset 按键不放，然后拔电源重启。当黄灯慢闪烁后，放开 Reset 键。等待安装完成。从交互界面可以看到安装的进度。 安装完经过初始化，就能够运行啦！ ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:11:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"参考文档 非常感谢这些文档的作者~没有这些文档，我是不可能在短时间内救活我的路由器的。 主要参考文档： https://blog.csdn.net/flyhorstar/article/details/95729059 提供了非常关键的 mi3_uboot： https://aisoa.cn/post-2213.html OpenWRT： https://openwrt.org/toh/xiaomi/mir3 LEDE 全球首发，支持小米路由3（Xiaomi Mi Router R3）: https://www.right.com.cn/forum/thread-261964-1-1.html 小米路由器3潘多拉固件刷机教程： https://blog.csdn.net/u011054333/article/details/88564078 ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:12:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":["Router","OpenWRT"],"content":"资源 小米路由器历史固件集合：https://www.right.com.cn/FORUM/thread-706545-1-1.html 没想到这篇居然写了两个小时 \u003e \u003c ","date":"2019-10-07","objectID":"/2019/10/07/xiaomi-router-r3-recovery/:13:0","tags":["Router","OpenWRT","XiaoMi Router R3","TTL"],"title":"用 TTL 救活变砖的小米路由3","uri":"/2019/10/07/xiaomi-router-r3-recovery/"},{"categories":null,"content":"ESP 8266 是带蓝牙功能的芯片 ","date":"2019-10-06","objectID":"/drafts/esp8266/:0:0","tags":null,"title":"esp8266","uri":"/drafts/esp8266/"},{"categories":null,"content":"芯片 ","date":"2019-10-06","objectID":"/drafts/esp8266/:1:0","tags":null,"title":"esp8266","uri":"/drafts/esp8266/"},{"categories":null,"content":"ESP8266-01 这是最核心的部分，把程序烧录进去后，就可以跑。 但问题是，如果想要开发，只有这一块是不够的。 有两种选择： 购买自带开发板的版本 购买 USB 转 TTL 串口模块 ","date":"2019-10-06","objectID":"/drafts/esp8266/:2:0","tags":null,"title":"esp8266","uri":"/drafts/esp8266/"},{"categories":null,"content":"开发工具 ","date":"2019-10-06","objectID":"/drafts/esp8266/:3:0","tags":null,"title":"esp8266","uri":"/drafts/esp8266/"},{"categories":null,"content":"GPIO 基础 在不依赖 SDK 的情况下操作 GPIO： https://www.cnblogs.com/vamei/p/6751992.html ","date":"2019-10-06","objectID":"/2019/10/06/raspberry-gpio/:1:0","tags":null,"title":"树莓派编程","uri":"/2019/10/06/raspberry-gpio/"},{"categories":null,"content":"GPIO 库 —— Wiring PI http://wiringpi.com/ 这是个 C 语言库。 Wiring PI 的官方库没有维护在 Github 上，但是有镜像仓库： https://github.com/WiringPi/WiringPi 目前还没有添加树莓派 4 的识别，可参照： https://github.com/WiringPi/WiringPi/pull/65 你可以在下面这个链接里面找到其他语言对 Wiring PI 的包装： https://github.com/WiringPi C 语言和 Python 的示例： https://www.oschina.net/question/1425530_140979 ","date":"2019-10-06","objectID":"/2019/10/06/raspberry-gpio/:2:0","tags":null,"title":"树莓派编程","uri":"/2019/10/06/raspberry-gpio/"},{"categories":null,"content":"编程要点 树莓派有 40 个针脚（PIN），每个针脚的作用见针脚图 特别注意：5V 电源、接地针脚、GPIO 针脚 使用 GPIO 针脚时，要先设置该针脚是用于读还是用于写 GPIO 有两种模式：板上（BOARD）模式和 BCM 模式。 两者的区别是 GPIO 编号和板上编号的对应关系不同。 非 GPIO 针脚不受影响。 ","date":"2019-10-06","objectID":"/2019/10/06/raspberry-gpio/:3:0","tags":null,"title":"树莓派编程","uri":"/2019/10/06/raspberry-gpio/"},{"categories":["Installation And Configuration"],"content":"简介 Syncthing 是一款同步功能简洁但提供 P2P 功能的基于网络的多端（IOS除外）文件同步的开源软件。 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:1:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"要点 基于网络同步。不是用来将本机的某个硬盘同步到另一个硬盘内。这种需求请使用 FreeFileSync 这种软件。 自带支持两台无公网 IP 的设备进行同步。不需要自己申请公网机器，使用社区贡献的中继服务器即可。当然，也可以自己申请公网机器来做中继服务器。 有 WEB UI 。自带 Basic Auth 认证方式，根据需要开启。 有文件版本管理功能。 自动同步。 可以添加多个文件夹到同步列表，并控制对哪个设备共享该文件夹。 可设置文件夹为仅发送、仅接收、发送和接收（默认）。 没有列出文件对比的功能。 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:2:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"两台要同步的设备如何发现对方？ 每个启用了 Syncthing 的设备都会得到一个 ID。Syncthing 在启动后会向默认的发现服务器（Syncthing Discovery Server）发送自己的信息。 当添加远程设备时，把另一个设备的 ID 复制进去。此时 Syncthing 会向默认的发现服务器请求这个 ID 的设备信息。这样就可以连上了。 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:3:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"两台设备如何传输数据？ 如果两台设备的其中一台拥有公网 IP，那么 Syncthing 会使用逆向链接的 P2P 通信方式传输数据。 如果两台设备都没有公网 IP，那么 Syncthing 会寻找一台 社区贡献的 拥有公网 IP 的中继服务器（Syncthing Relay Server），使用中继的 P2P 通信方式让两台设备通过中继服务器传输数据。 中继服务器列表： https://relays.syncthing.net/ ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:4:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"发现服务器和中继服务器的问题 使用默认的发现服务器的问题是隐私。因为使用时会把设备和 ID 信息发送给发现服务器，这样这一台发现服务器的拥有者就可以得知这些信息。 使用社区贡献的中继服务器的问题除了隐私外，还有速度的问题。 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:5:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"自己搭建发现服务器和中继服务器 参考这篇博客： https://segmentfault.com/a/1190000017273107 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:6:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"P2P 通信方式 中继（Relaying） 两台机器都没有公网 IP 时，使用该方式通过公网的中继服务器传输数据。传输速度受中继服务器的传输速度影响。 逆向链接（Connection reversal） 其中一台机器拥有公网 IP 时，内网机器发起连接到公网机器。传输速度受两台连接的机器的带宽影响。 UDP 打洞（UDP hole punching） 两台机器都没有公网 IP 时，使用该方式通过公网的服务器建立连接。传输速度受两台连接的机器的带宽影响。 要求两台内网机器都处于锥形 NAT 下。如果是对称 NAT，则无法建立连接。 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:7:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"能否让 Syncthing 使用 UDP 打洞方式连接 试试 FRP ？ ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:8:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":["Installation And Configuration"],"content":"参考链接 P2P通信原理与实现(C++)： https://www.cnblogs.com/pannengzhi/p/4800526.html CONE NAT 和 Symmetric NAT： https://www.cnblogs.com/dyufei/p/7466924.html Syncthing - 文件同步工具: https://zhuanlan.zhihu.com/p/69267020 ","date":"2019-10-05","objectID":"/2019/10/05/syncthing/:9:0","tags":["File Sync"],"title":"Syncthing 通过网络同步文件","uri":"/2019/10/05/syncthing/"},{"categories":null,"content":"硬件知识：http://madaimeng.com/article/NAS-Assemble-1-hardware-ref/# 选购：https://post.smzdm.com/p/120480/ 软件：https://www.zhihu.com/question/21359049/answer/34375825 烧录软件：https://sourceforge.net/projects/win32diskimager/ 一大堆相关网站集合：https://post.smzdm.com/p/andg4g90/ ","date":"2019-10-03","objectID":"/2019/10/03/nas/:0:0","tags":null,"title":"NAS 硬件选购","uri":"/2019/10/03/nas/"},{"categories":null,"content":"选硬件 两个重要因素： 安静 低功耗 安静是因为 7x24 小时都在运作，太吵了影响心情。低功耗也是因为 7x24 小时，耗电量不小。 要安静就得尽量选择无风扇的方案。 挑选硬件时，电源放最后选。 防护方案： UPS 用于断电防护。分在线式和后备式。 在线式切换无延迟，价格贵 4~5 倍。后备式有 10 毫秒左右延迟，NAS 供电电源质量不差的情况下不影响。 UPS 内置的电池只能撑一小段时间。因此真正断电后，要尽快将 UPS 接到备用电源（电池）上。100W 功率能撑 22 分钟。 APC BK500-CH 如果不能保证在 UPS 电池用完之前给 UPS 供电，那么要接入自动关机的方案。否则 UPS 电池耗尽，对寿命有影响。 思路是让路由器来控制给 NAS 和 UPS 关机。因此要求路由器能刷系统（OpenWRT）。 https://post.smzdm.com/p/508308/ AMD 速龙 3000G 处理器 2核4线程 搭载Radeon Vega Graphic 3.5GHz AM4接口 盒装CPU https://item.jd.com/100005955855.html 桌面CPU性能排行榜 https://rank.kkj.cn/dcpu.shtml 神舟K580C-i5 D1参数 https://product.pconline.com.cn/notebook/hasee/553230_detail.html nas系列 篇三：nas主板新选择-华擎E350M1 https://post.smzdm.com/p/a5k67ee3/ 正版 Windows 10 https://www.microsoftstore.com.cn/software/windows 重新激活 https://support.microsoft.com/zh-cn/windows/%E5%9C%A8%E6%9B%B4%E6%8D%A2%E7%A1%AC%E4%BB%B6%E5%90%8E%E9%87%8D%E6%96%B0%E6%BF%80%E6%B4%BB-windows-10-2c0e962a-f04c-145b-6ead-fb3fc72b6665 基于 AMD 平台的家用 NAS 方案综述 https://post.smzdm.com/p/a6lrz7v0/ 浅谈家庭NAS的组装与应用 篇一：硬件选购篇 https://post.smzdm.com/p/120480/ CPU 比较 https://cpu.userbenchmark.com/Compare/Intel-Core-i5-4200M-vs-AMD-Athlon-3000G/m2341vsm968952 文菌装NAS E1：手把手教您组装2021全能NAS，ALL IN ONE 配置清单 https://www.bilibili.com/read/cv10629149 ","date":"2019-10-03","objectID":"/2019/10/03/nas/:1:0","tags":null,"title":"NAS 硬件选购","uri":"/2019/10/03/nas/"},{"categories":null,"content":"待看 https://detail.zol.com.cn/motherboard/s1280/ https://mall.jd.com/index-1000000225.html?from=pc https://mall.jd.com/index-1000000266.html?from=pc https://asus.jd.com/view_search-416883-2981961-99-1-20-1.html https://msigaming.jd.com/view_search-392505-14790148-99-1-20-1.html ","date":"2019-10-03","objectID":"/2019/10/03/nas/:2:0","tags":null,"title":"NAS 硬件选购","uri":"/2019/10/03/nas/"},{"categories":null,"content":"定位 安装一些不想安装在 pad 上的软件 下载文件 同步和备份文件 装一些 Windows 软件，供在外远程使用 Windows 管理工具 ","date":"2019-10-03","objectID":"/2019/10/03/nas/:3:0","tags":null,"title":"NAS 硬件选购","uri":"/2019/10/03/nas/"},{"categories":null,"content":"前期准备 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:1:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"硬件准备 树莓派 4B（5V 3A Type-C 电源、class10 TF 闪存卡、读卡器） 闪存卡用来装系统，读卡器用于通过 Windows 把系统烧录到闪存卡上 （硬盘盒 + 硬盘）或者（移动硬盘） USB 3.0 带电源接口的分线器（可选） 如果不想用硬盘盒/移动硬盘自带的电源或者有其他用途才选择这个 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:1:1","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"注意点 树莓派的 CPU 是 ARM v8 架构 64 位，因此从系统到软件都要选择支持 ARM 64 的版本 详细参数： https://www.raspberrypi.org/products/raspberry-pi-4-model-b/specifications/ 树莓派的电流有限（3A），接太多设备或者电流要求高的设备时，会发生故障。因此可以选择购买带电源接口的 USB 分线器。 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:1:2","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"软件准备 Windows 下的烧录工具： win32diskimager 用于烧录 CentOS 7 到树莓派。下载后要安装。 https://sourceforge.net/projects/win32diskimager/ CentOS 7 ARM 点下面链接进入官方的镜像列表，选择 CentOS-Userland-7-armv7hl-RaspberryPI-Minimal-4-1908-sda.raw.xz http://isoredirect.centos.org/altarch/7/isos/armhfp/ Docker ARM curl -fsSL https://get.docker.com | sh 源自于： https://docs.docker.com/install/linux/docker-ce/centos/ Nextcloud ARM 使用上和 x86 没什么不同 https://hub.docker.com/_/nextcloud ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:1:3","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"备份已有系统 把 TF 闪存卡放入读卡器，插到电脑。 打开 win32diskimager ，设备选择闪存卡 在电脑上随便一个地方新建一个文件，命名为 Raspberry4B.img （随便命名都行） win32diskimager 上选择刚刚创建的文件，点击【读取（Read）】按钮读取 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:2:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"安装 CentOS 7 把 TF 闪存卡放入读卡器，插到电脑。 解压下载的 .xz 文件，得到 .raw 文件。 启动 win32diskimager，选择解压出来的 .raw 文件，目标设备选择闪存卡，点击【写入（Write）】按钮写入，等待写入完成。 闪存卡放树莓派，启动树莓派。用默认账户 root 和默认密码 centos 登录。 执行 rootfs-expand 来扩展根文件夹大小，充分利用 TF 卡的空间。否则会发现空间不够用。 可参考： https://zhuanlan.zhihu.com/p/33030757 同步时间： yum -y install ntp ntpdate ntpdate cn.pool.ntp.org CPU 温度： CUR_TEMP=$(cat /sys/class/thermal/thermal_zone0/temp); echo $[$CUR_TEMP / 1000]\".\"$[$CUR_TEMP % 1000]°C ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:3:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"安装 Docker curl -o docker.tgz https://download.docker.com/linux/static/stable/armhf/docker-19.03.3.tgz tar xvzf docker.tgz mv docker/* /usr/bin/ rmdir docker https://docs.docker.com/install/linux/docker-ce/binaries/#install-daemon-and-client-binaries-on-linux https://download.docker.com/linux/static/stable/armhf/ 如果要使用 systemctl 管理 docker，那么需要三个文件： /usr/lib/systemd/system/docker.service /usr/lib/systemd/system/docker.socket /usr/lib/systemd/system/containerd.service systemctl enable containerd.service \u0026\u0026 systemctl enable docker.socket \u0026\u0026 systemctl enable docker.service ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:4:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"安装 Docker Compose 确保已安装 pip。如果没有安装，则执行以下命令安装： curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py 使用 pip 安装： pip3 install docker-compose 该方式来自： https://docs.docker.com/compose/install/#install-using-pip 注意： 如果安装时出现错误 fatal error: Python.h: No such file or directory，则要安装 python3-devel。 如果出现 fatal error: openssl/opensslv.h: No such file or directory，则要安装 openssl-devel。 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:4:1","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"安装 Nextcloud public-proxy(443) -\u003e frps(443) –穿透–\u003e frpc(443) -\u003e inner-proxy(443) -\u003e nextcloud(80) public-proxy 和 frps 都放外网机器 frpc 放路由器 inner-proxy 和 nextcloud 放内网服务器 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:5:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"frpc 配置的影响 注意 frpc 会指定 inner-proxy 的地址（local_ip）。如果 inner-proxy 地址变更，会导致请求得到 502 的返回。 但由于 local_ip 可设置为域名，因此避免 inner-proxy 地址变更造成的影响。 路由器上可设置主机名对应 IP，这样可直接绑定。在 OpenWRT 管理界面的 【 网络 | 主机名 】 菜单可找到。将主机名设置为 nextcloud 的域名。 相同主机名可设置多个 IP，这样就可以同时设置内网服务器的 WiFi 网卡和网线连接的 IP。 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:6:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"Public Proxy 如果要结合 Frp 使用，一定要将以下内容保存为 proxy.conf ，并放到 /etc/nginx/ 底下。 # HTTP 1.1 support proxy_http_version 1.1; proxy_buffering off; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $proxy_connection; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $proxy_x_forwarded_proto; proxy_set_header X-Forwarded-Ssl $proxy_x_forwarded_ssl; proxy_set_header X-Forwarded-Port $proxy_x_forwarded_port; proxy_ssl_server_name on; # Mitigate httpoxy attack (see README for details) proxy_set_header Proxy \"\"; ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:7:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"增加最大上传文件限制 Nextcloud 官方示例的配置文件已经包含了该限制的配置，因此无需另外修改。 https://github.com/nextcloud/docker/blob/master/.examples/docker-compose/with-nginx-proxy/postgres/fpm/web/nginx.conf 以下内容需要修改： inner-proxy 的 Nginx 需要设置 client_max_body_size 10G; 具体做法是，新增个 max_body.conf 文件，把 client_max_body_size 10G; 写进去。然后把文件放到 /etc/nginx/conf.d 里面。 Nextcloud fpm 的限制 在 Nextcloud 的数据目录下，有个 .user.ini 的文件。在文件里面添加以下内容： php_value upload_max_filesize 16G php_value post_max_size 16G php_value max_input_time 3600 php_value max_execution_time 3600 https://docs.nextcloud.com/server/17/admin_manual/configuration_files/big_file_upload_configuration.html#configuring-your-web-server ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:8:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"如果应用商店没有展示 查看日志我们可以看到： Operation timed out after 10001 milliseconds with 0 out of 0 bytes received\",\"url\":\"https:\\/\\/apps.nextcloud.com\\/api\\/v1\\/apps.json\" 默认超时时间为 10 秒。 如果在容器里面请求 https://apps.nextcloud.com/api/v1/apps.json 能够得到数据，只是比较久。那么我们就得把超时时间设置长一点。 进入 Nextcloud 容器，编辑 /var/www/html/lib/private/App/AppStore/Fetcher/Fetcher.php ，把 fetch 方法里面 timeout 改久一点，比如 60 秒。 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:9:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"存储方案 要确保任何一个节点挂掉之后， ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:10:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"Nextcloud 挂载外部存储 Nextcloud 自身支持挂载 http://www.bujarra.com/nextcloud-anadiendo-acceso-a-datos-externos/?lang=en 修复文件删除出错，或者备份恢复文件出错： docker exec --user www-data nextcloud php occ files:scan --all docker exec --user www-data nextcloud php occ maintenance:mode --on // 进入数据库执行： delete from oc_file_locks where lock = 1; docker exec --user www-data nextcloud php occ maintenance:mode --off https://villekaaria.eu/2019/03/10/hosting-nextcloud-with-docker/ 修改 Docker 数据存放文件夹： sudo vi /etc/docker/daemon.json { \"data-root\":\"/NEWLOCATION\" } ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:11:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":null,"content":"其他 安装 OpenWRT 把树莓派作为路由器使用： https://www.shuyz.com/posts/install-openwrt-on-raspberry-as-a-wireless-router/ 安装 Windows 10 IoT 系统： https://docs.microsoft.com/en-us/windows/iot-core/downloads https://docs.nextcloud.com//server/14/admin_manual/configuration_files/external_storage_configuration_gui.html#available-storage-backends https://docs.nextcloud.com//server/14/admin_manual/configuration_files/external_storage_configuration_gui.html https://hub.docker.com/_/gitlab-community-edition/plans/6a33c5d4-c1cc-48f4-ae30-e033126ffd7f?tab=instructions https://villekaaria.eu/2019/04/13/nextcloud-filesscan-with-docker/ https://github.com/kahing/goofys https://www.shuyz.com/posts/install-openwrt-on-raspberry-as-a-wireless-router/ config.txt http://shumeipai.nxez.com/2015/11/23/raspberry-pi-configuration-file-config-txt-nstructions.html https://www.raspberrypi.org/documentation/configuration/config-txt/video.md VGA: https://blog.csdn.net/bona020/article/details/79027409 挂载： https://techwiztime.com/guide/auto-mount-ntfs-usb-drive-raspberry-pi/ 关闭 MAC 随机生成（如果安装了 Network Manager）： https://raspberrypi.stackexchange.com/questions/68513/pi-using-a-random-mac-address-after-every-reboot-how-do-i-stop-this-behavior/75497#75497 蓝牙连接： https://www.embbnux.com/2016/04/10/raspberry_pi_3_wifi_and_bluetooth_setting_on_console/ CentOS7 arm 中科大源 # CentOS-Base.repo # # The mirror system uses the connecting IP address of the client and the # update status of each mirror to pick mirrors that are updated to and # geographically close to the client. You should use this for CentOS updates # unless you are manually picking other mirrors. # # If the mirrorlist= does not work for you, as a fall back you can try the # remarked out baseurl= line instead. # # [base] name=CentOS-$releasever - Base #mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026arch=$basearch\u0026repo=os baseurl=http://mirrors.ustc.edu.cn/centos-altarch/$releasever/os/$basearch/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-AltArch-Arm32 #released updates [updates] name=CentOS-$releasever - Updates # mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026arch=$basearch\u0026repo=updates baseurl=http://mirrors.ustc.edu.cn/centos-altarch/$releasever/updates/$basearch/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-AltArch-Arm32 #additional packages that may be useful [extras] name=CentOS-$releasever - Extras # mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026arch=$basearch\u0026repo=extras baseurl=http://mirrors.ustc.edu.cn/centos-altarch/$releasever/extras/$basearch/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-AltArch-Arm32 #additional packages that extend functionality of existing packages [centosplus] name=CentOS-$releasever - Plus # mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026arch=$basearch\u0026repo=centosplus baseurl=http://mirrors.ustc.edu.cn/centos-altarch/$releasever/centosplus/$basearch/ gpgcheck=1 enabled=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-AltArch-Arm32 ","date":"2019-10-03","objectID":"/2019/10/03/raspberry4b-nas/:12:0","tags":null,"title":"树莓派4B + CentOS 7 + Nextcloud","uri":"/2019/10/03/raspberry4b-nas/"},{"categories":["Learning Docker"],"content":"Docker 是用来干嘛的？ 帮助开发者和系统管理员使用容器开发、部署和运行应用。 对于开发者而言，最直观的感受就是原先我们要跑起来一个已有项目，必须装一大堆依赖。 在没有 Docker 的时候，可以有两种方式： 写一个自动化脚本。安装的时候去执行这个脚本。 将依赖安装到虚拟机，导出镜像。需要的时候去下载镜像。 第一种方式可能会因为网络原因或者版本变更导致安装失败。第二种方式的虚拟机会占用大量资源，而且使用上还要对虚拟机做配置。 而如果使用 Docker，这些依赖的安装命令将会由运维人员定义在一个脚本（Dockerfile）。并且通过脚本构建镜像。开发人员安装 Docker 引擎后，拉取镜像并运行。 跟虚拟机很像，但是也有区别。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:1:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"Docker 和虚拟机的区别 主要有两点：宿主机磁盘占用和资源占用 先说宿主机磁盘占用： 虚拟机由于包含了一个完整的系统，因此镜像会比较大。例如 CentOS 7 的镜像大小为 1GB 左右。 CentOS 7 虚拟机镜像下载地址： https://www.osboxes.org/centos/ 而 CentOS 7 的 Docker 镜像大小为：70 MB 左右。 CentOS 7 的 Docker 镜像： https://hub.docker.com/_/centos?tab=tags 能否再小一点？能！Alpine 镜像的大小不到 3 MB。 Alpine 的 Docker 镜像： https://hub.docker.com/_/alpine?tab=tags 再从复用性来讲。每启动一个新的虚拟机，就得再消耗一份镜像大小的空间。而每启动一个 Docker 容器，就只需要加一个容器层。这个我们后面再说。 从资源占用的角度讲两者的区别： 由于虚拟机是装一个完整的系统，因此系统内核的运行消耗会增加宿主机的资源消耗。另外一个显著的问题就是系统启动时，很慢。可达分钟级别。 在分配资源时，虚拟机需要通过一个虚拟机管理系统(Hypervisor)分配硬件资源。 而 Docker 镜像不是一个完整的操作系统，它使用宿主机的内核。Docker 容器的启动时间是秒级别，甚至可以是毫秒级别。 Docker 容器的资源分配是通过 Docker 直接向宿主机获取的。 虚拟机：[ App -\u003e Bins/Libs -\u003e Guest OS ] -\u003e Hypervisor -\u003e Host OS -\u003e Infrastructure Docker：[ App -\u003e Bins/Libs ] -\u003e Docker -\u003e Host OS -\u003e Infrastructure 你甚至可以在镜像内只安装一个小应用，然后把这个镜像当做一个软件。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:2:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"Docker 基于什么样的技术 Linux Kernel 3.10+：Namespace 和 Control Groups Windows 10：Hyper-V Docker 现在要求如果是 Linux 系统，内核版本要大于 3.10。对应就是 CentOS 7。如果感兴趣我们可以讲讲为什么。 Docker 用到的 Linux 内核技术是命名空间（Namespace）和控制组（Control Groups）。总结起来就是资源隔离和资源控制。 命名空间就跟我们 Java 包名或者 C++ 和 PHP 里面的那个命名空间有点相似。 命名空间可以隔离系统资源，包括：文件系统挂接点、nodename 和 domainname、进程间通信资源、进程 ID 数字空间 、网络相关的系统资源、用户和组 ID 空间。 举个例子，在各个容器里面都可以有进程号为 1 的 root 进程。就像不同命名空间可以有同名的类。 但不同的是，这里的命名空间还使得处于不同空间的对象无法直接互相访问。这样就提供了一定程度的安全性。 控制组的功能包括对硬件资源的：资源限制、优先级分配、资源统计、进程控制。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:3:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"Hello World 我们试着下载镜像并启动。 执行命令： docker run hello-world 会有以下输出： Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:b8ba256769a0ac28dd126d584e0a2011cd2877f3f76e093a7ae560f2a5301c00 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 执行 run 的时候，Docker 会在本地找 hello-world 镜像。由于我们没有指定镜像版本，因此默认使用 latest 版本。 接着因为 Docker 没有找到要执行的镜像，它会到远程镜像仓库里面找这个镜像。如果找到镜像，则下载并执行；找不到就报错。 https://github.com/docker-library/hello-world https://github.com/docker-library/hello-world/blob/master/arm64v8/hello-world/Dockerfile https://github.com/docker-library/hello-world/blob/master/hello.c ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:4:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"轻量基础镜像 Alpine 前面提到 Alpine 的大小只有 3 MB。如果想尽可能地使构建出来的镜像小的话，则优先考虑使用 Alpine。 如果能够确定不需要依赖于任何系统，连这 3 MB 都省去。例如前面一节的 hello-world 就不基于任何基础镜像，但是这样一来就得自己做好安全控制和增加必要的便捷性。除非必要，否则都会基于一个基础镜像。 拉取 Alpine 镜像： docker pull alpine 如果你 run 这个 alpine，它会立即退出。因为没有什么命令使得它必须处于运行状态。 如何使它保持运行状态？等说完镜像层和容器层，会详细说明。 你可以把它当成一个执行 Linux 命令的工具： docker run alpine echo \"hello\" 如果你想要有交互，可以加上 -it： docker run -it alpine sh 这样就进入了交互状态，就好像进入了另一台机器。 关于 -it 选项： -i 是 –interactive 的缩写，表示将当前输入连接到容器执行的进程的标准输入（STDIN）里面。 -t 是 –tty 的缩写。表示申请一个伪终端（PTY），并连接到容器。可以接受 STDOUT 和 STDERR。 关于 i 和 t 的详细解释见： https://stackoverflow.com/questions/30137135/confused-about-docker-t-option-to-allocate-a-pseudo-tty/54254380#54254380 当你执行 exit 退出时，容器就结束了。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:5:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"镜像层和容器层 先说镜像层。 一个镜像通常由多个只读（read only）的镜像层组成，每个镜像层表示构建这个镜像时的每一步操作。 +------------------------+ | | | Image Layer 1 (ro) | | | +------------------------+ | | | Image Layer 2 (ro) | | | +------------------------+ 执行 run 的时候，发生了什么？ run 包含了两个步骤：create 和 start。如果还加了 -i -t 两个参数，那还有第三个步骤 attach。 create 时， Docker 会在镜像的基础上创建一个可读写(read and write)的容器层。 +------------------------+ | | | Container Layer (rw) | | | +------------------------+ | | | Image Layer 1 (ro) | | | +------------------------+ | | | Image Layer 2 (ro) | | | +------------------------+ start 启动容器。可以加入 -i -a 来进入交互模式。-i 会连接标准输入，-a 会连接标准输出和标准错误。 attach 会将你的终端接入正在执行中的容器，连接到标准输入输出和错误。STDIN,STDOUT,STDERR。 创建多少容器，就有多少份容器层。但它们没有依赖关系，共用镜像。 当我们销毁容器的时候，会把这一个读写层完全删除。里面的数据会全部被删除掉（这点很重要）。 +------------------------+ +------------------------+ +------------------------+ | | | | | | | Container Layer (rw) | | Container Layer (rw) | | Container Layer (rw) | | | | | | | +------------------------+ +------------------------+ +------------------------+ X X X XXXXXXXXXXXXXXXXXXXXXXX X XXXXXXXXXXXXXXXXXXXXX X X X X X X +------------------------+ | | | Image Layer 1 (ro) | | | +------------------------+ | | | Image Layer 2 (ro) | | | +------------------------+ ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:6:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"如何保持运行状态？ 当从镜像创建容器后，这个容器相当于一个程序。你执行这个程序，等执行完后，它就退出了。就像最开始的 hello-world ，当它打印完说明后，就结束了。然后容器退出。 我们在创建容器的时候，会指定一个程序来执行。启动容器时，会启动这个程序，分配一个进程（主进程）。容器将会跟踪这个进程的状态。 当进程退出时，容器也会随之退出。例如我们在执行 docker run -it alpine sh 后进入了 sh 的交互状态。当我们继续执行 exit 时，退出了 sh 的进程，容器也跟着退出了。 如果想让容器不退出，就得将进程保持在执行状态，不让它退出。 最简单的一种方式就是让容器启动时执行 tail -f /dev/null 这个命令。这个命令表示不断读取 /dev/null，这样它永远都不会退出，并且不会产生多余的输出。 但是如果所有的镜像都指定容器启动时执行上面这个命令，那就不合适了。 这是由于我们让容器提供的服务有可能会因为各种异常而退出，这时我们希望它可以自动重启。 我们可以让 Docker 在发现容器退出时自动重启容器，但是 tail -f /dev/null 这个命令使得容器不会因为其他进程的退出而退出。 后面会详细说明其他保活方法。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:7:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"定制镜像 上文在对比 Docker 和虚拟机的时候说过。虚拟机可以进入安装所有依赖，然后保存镜像。Docker 也可以。Docker 可以将容器层（可读写）转换为镜像层（只读）。 我们可以通过 docker run -it alpine sh 进入 alpine ，然后做一些依赖的安装操作。 这里举个不是安装依赖的简单例子：在 home 底下创建一个 test 文件夹。然后退出。 如果你再执行 docker run -it alpine sh 进入 alpine 。你会发现这个文件夹不在了。在【镜像层和容器层】这一节有提到，每次 run 都会生成新的容器。 该如何找到刚刚创建的容器？可以用 docker ps -a。-a 表示显示所有容器，包括启动的和已退出的。因为我们没有给容器加上保活的机制，所以在退出主进程的时候就会变为退出状态。 如果我们想把对这个容器的更改保存起来，防止被删除，可以执行： docker commit \u003ccontainer-id\u003e \u003ctag\u003e 例如对 id 为 33ebad6f23d5 的容器，在宿主机执行： docker commit 33ebad6f23d5 test 然后再去 run 这个名字为 tag 的新镜像，就可以在 home 底下看到 test 文件夹了。 docker run test ls /home ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:8:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"使用 Dockerfile 基于基础镜像构建新镜像 使用 commit 定制镜像有一个问题。 通过 docker history 镜像名 来查看上一节中 commit 的镜像的历史，可以看到最顶层是这样的： IMAGE CREATED CREATED BY SIZE COMMENT 25b2ad442d41 6 days ago sh 31 B 这样看不出来这一层究竟执行了哪些命令，增加了维护的难度。 有的同学就会说了：那写到脚本里面不就行了？ 写到脚本里面也是可以的，但这不是最优的做法。 在【镜像层和容器层】一节中，我们知道镜像是分层的。这样做有什么好处呢？ 当 Docker 直到你即将构建的这一层与原先已构建的一层所执行的命令一样，那它就直接使用原先的镜像层。这样加快了构建的速度。 当镜像传输时，本地已有的镜像层不会再下载。加快了镜像下载的速度。 比如你已经下载了一个最新版的 alpine 镜像，然后又下载两个都是基于当前最新版本的 alpine 构建的镜像。那么下载的时候会跳过 alpine 镜像的下载。 如果使用脚本去构建，总是会只生成一层镜像层。那么每次构建时都得从头开始。传输时也是全部重新传一次。 我们可以像写 bash 脚本那样把命令都放在一个文件。这就是 Dockerfile。 在使用 Dockerfile 构建镜像时，FROM 以外的命令都会生成一个镜像层。这样就能享受到分层的好处了。 对于刚才使用 commit 的定制，我们可以换成 Dockerfile 的方式： FROM alpine:latest RUN mkdir /home/test 这样我们就可以通过查看这个文件得知这个镜像都做了哪些修改。 这里的 FROM 的意思是基于哪个镜像。在那个镜像上执行操作。 RUN 后面跟上相关命令。 文件以 Dockerfile 为名并保存。然后在宿主机执行构建： # docker build . -t testv2 Sending build context to Docker daemon 2.048kB Step 1/2 : FROM alpine:latest ---\u003e 965ea09ff2eb Step 2/2 : RUN mkdir /home/test ---\u003e Running in e2548722ecbe Removing intermediate container e2548722ecbe ---\u003e 9678e6e6b110 Successfully built 9678e6e6b110 Successfully tagged testv2:latest 从 Step 1/2 可以看到使用的基础镜像 ID 为 965ea09ff2eb。 从 Step 2/2 可以看到 Docker 在构建镜像的时候启动了一个容器 e2548722ecbe，然后执行 mkdir /home/test。在 commit 后生成镜像 9678e6e6b110，并且删除刚刚创建的容器 e2548722ecbe。 镜像已经创建了。由于执行构建的时候，还指定了镜像的 tag，因此会将 9678e6e6b110 的名称设置为 testv2:latest。 执行下面的命令可以看到镜像 testv2 在 home 底下有个 test 文件夹： docker run testv2 ls /home ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:9:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"端口映射 由于命名空间对网络资源的隔离，宿主机无法直接访问容器内进程的端口。 我们要做的就是分配一个宿主机的端口，让它与容器的端口关联起来。 下面的命令将宿主机的 60044 端口与容器的 80 端口关联起来： docker run -d -p 60044:80 nginx:alpine 这样当我们访问宿主机的 60044 端口时，就相当于在访问容器的 80 端口。 有的同学会问：那我多个容器内部能不能使用一样的端口？比如启动两个 nginx 容器，都监听 80 端口。 当然可以。由于命名空间对网络资源的隔离，容器之间使用的端口都不影响。但是当它们与宿主机端口建立关联时，不能使用同一个宿主机端口。 有时候下载了一个镜像，不知道这个镜像的作者提供了哪些可用的端口怎么办？ 可以找到构建这个镜像的 Dockerfile，里面会有 EXPOSE 命令： EXPOSE 80 EXPOSE 仅用于提示使用该 Dockerfile 构建出的镜像的用户有哪些端口可以映射，它不会自己映射端口。 没有 Dockerfile 怎么办？docker history 镜像名。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:10:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"数据卷映射 在【镜像层和容器层】一节提到，删除容器时会把容器里面的数据全删除掉。那么如何才能让想要的数据不被删除呢？ 可以选择映射到宿主机的目录或者放到数据卷里面。 你可以把数据卷想象成一个专门存放数据的空间，将多个数据卷挂载到容器的不同目录下。容器被删除时，这些数据卷不会被删除。 执行 docker volume create 数据卷名称 来创建数据卷。然后执行下面命令将数据卷映射到 /home/test。 docker run -it -v 数据卷名称:/home/test testv2 sh 其中第一步创建数据卷可以省略，第二步在数据卷未创建时，会自动创建。 下面命令将当前所在的宿主机目录映射到容器的 /home/test 目录。 docker run -it -v \"${PWD}:/home/test\" testv2 sh ${PWD} 可以替换为相对路径或者绝对路径。 Dockerfile 的 VOLUME 类似于 EXPOSE： VOLUME /home/test 但它不仅有提示用户的作用。如果用户没有在创建容器的时候指定映射目标，它会创建一个随机命名的数据卷，并挂载到 /home/test。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:11:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"更多的 Dockerfile 构建命令 看另一篇： Dockerfile ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:12:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":["Learning Docker"],"content":"直观地感受 Docker 资源 Docker 的数据存放在一个特定的目录里面。例如 CentOS 7 的 Docker 数据默认存放在 /var/lib/docker 里面。 # ls /var/lib/docker builder buildkit containers image network overlay2 plugins runtimes swarm tmp trust volumes 之前创建的数据卷在 volumes 里面。 这个存放位置可以更改（这在将树莓派作为 NAS 的时候非常有用）： vim /etc/docker/daemon.json { \"data-root\": \"/mnt/docker-data\" } ","date":"2019-09-19","objectID":"/2019/09/19/docker-series-e1/:13:0","tags":["Docker"],"title":"Docker 介绍","uri":"/2019/09/19/docker-series-e1/"},{"categories":null,"content":"为什么以前没有 Docker 在 Docker 安装文档的 CentOS 部分，要求最低要使用 CentOS 7。 安装文档： https://docs.docker.com/install/linux/docker-ce/centos/ 这是因为 CentOS 从 7 开始使用 Linux 3.10 以上版本的内核。从 Docker 的 FAQ 中可以看到对于 Linux 的要求： Any distribution running version 3.10+ of the Linux kernel FAQ： https://docs.docker.com/engine/faq/#what-platforms-does-docker-run-on ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:1:0","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"Linux 3.10 有什么不同？ Docker 依赖 Linux 的两个内核特性： Namespaces：命名空间 Control groups（cgroups）：控制组 而 Namespace 特性在 Linux 3.8 （2013年02月）才完成。 文章： https://www.cnblogs.com/sammyliu/p/5878973.html Linux 内核版本发布时间表： https://kernelnewbies.org/LinuxVersions ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:1:1","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"为什么 CentOS 6 可以装 Docker CentOS 6 的内核是 2.6 但是也能装 Docker 1.8.0 之前的版本。这是因为 CentOS 会移植一些功能到旧内核。 但自从 Docker 1.8.0 开始就不再支持了。 为什么？可以见：issue#14365 简单来说就是兼容性问题。新版 Docker 使用了新内核的特性。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:2:0","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"为什么 Windows 10 可以直接装 Docker Windows 10 引入了 Hyper-V 技术。 https://docs.microsoft.com/zh-cn/virtualization/hyper-v-on-windows/reference/hyper-v-architecture ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:3:0","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"为什么 Windos 7 可以装 Docker 实际上 Windows 7 本身是不支持虚拟化的。但是可以通过在 Windows 7 上安装虚拟机，然后在虚拟机里面安装 Docker。再使用 docker-machine 创建主机并关联到虚拟机。 https://docs.docker.com/toolbox/toolbox_install_windows/ ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:4:0","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"Docker 与 Go 语言 Go 编译的结果是一个单独的文件，这样的好处是我们可以基于一个非常小的基础镜像（如 Alpine），然后将编译后的文件复制进去。这样可以使得整个镜像非常小。 ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:5:0","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"减小镜像大小，多阶段构建 在一个 Dockerfile 内部可以有多个 From xxx 用于构建多个镜像，这使得我们可以在前一个 From 编译，然后后一个 From 把编译的结果拷贝过来。 https://docs.docker.com/develop/develop-images/multistage-build/ 例如： FROM ubuntu:latest as builder RUN apt-get update RUN apt-get install curl -y RUN curl -L -o /tmp/go.sh https://install.direct/go.sh RUN chmod +x /tmp/go.sh RUN /tmp/go.sh FROM alpine:latest LABEL maintainer \"Darian Raymond \u003cadmin@v2ray.com\u003e\" COPY --from=builder /usr/bin/v2ray/v2ray /usr/bin/v2ray/ COPY --from=builder /usr/bin/v2ray/v2ctl /usr/bin/v2ray/ COPY --from=builder /usr/bin/v2ray/geoip.dat /usr/bin/v2ray/ COPY --from=builder /usr/bin/v2ray/geosite.dat /usr/bin/v2ray/ COPY config.json /etc/v2ray/config.json RUN set -ex \u0026\u0026 \\ apk --no-cache add ca-certificates \u0026\u0026 \\ mkdir /var/log/v2ray/ \u0026\u0026\\ chmod +x /usr/bin/v2ray/v2ctl \u0026\u0026 \\ chmod +x /usr/bin/v2ray/v2ray ENV PATH /usr/bin/v2ray:$PATH CMD [\"v2ray\", \"-config=/etc/v2ray/config.json\"] ","date":"2019-09-19","objectID":"/2019/09/19/docker-training/:6:0","tags":null,"title":"Docker 介绍","uri":"/2019/09/19/docker-training/"},{"categories":null,"content":"如果是覆盖文件，repo 不会生效。执行以下命令使其生效 yum-config-manager –add-repo /etc/yum.repo.d/CentOS-Base.repo http://huqunxing.site/2017/03/31/linux%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E5%88%86%E6%9E%90/ https://bobcares.com/blog/error-137-docker/ https://success.docker.com/article/what-causes-a-container-to-exit-with-code-137 yum 出错时： https://cloudlinux.zendesk.com/hc/en-us/articles/115004075294-Fix-rpmdb-Thread-died-in-Berkeley-DB-library ","date":"2019-09-19","objectID":"/2019/09/19/centos/:0:0","tags":null,"title":"centos","uri":"/2019/09/19/centos/"},{"categories":null,"content":"设置时区 https://www.cnblogs.com/zhangeamon/p/5500744.html ","date":"2019-09-19","objectID":"/2019/09/19/centos/:1:0","tags":null,"title":"centos","uri":"/2019/09/19/centos/"},{"categories":null,"content":"VIM 不使用鼠标选择 set mouse-=a ","date":"2019-09-19","objectID":"/2019/09/19/centos/:2:0","tags":null,"title":"centos","uri":"/2019/09/19/centos/"},{"categories":["Installation And Configuration"],"content":" cd / fallocate -l 1G swapfile chmod 600 swapfile mkswap swapfile swapon swapfile 如果执行 swapon 的时候，提示 swapon failed: Invalid argument 则把 fallocate -l 2G swapfile 替换为 dd if=/dev/zero of=/swapfile count=4096 bs=1MiB cd / dd if=/dev/zero of=/swapfile count=1024 bs=1MiB chmod 600 swapfile mkswap swapfile swapon swapfile 参考： https://blog.csdn.net/zstack_org/article/details/53258588 https://unix.stackexchange.com/questions/294600/i-cant-enable-swap-space-on-centos-7 ","date":"2019-09-19","objectID":"/2019/09/19/centos-swap/:0:0","tags":["Linux","CentOS7","Swap"],"title":"CentOS7 添加 Swap","uri":"/2019/09/19/centos-swap/"},{"categories":null,"content":"https://github.com/tideways/php-xhprof-extension https://github.com/laynefyc/xhgui-branch https://blog.it2048.cn/article-tideways-xhgui/ 获取当前机器 IP： gethostbyname(gethostname()) ","date":"2019-09-18","objectID":"/drafts/php/:0:0","tags":null,"title":"PHP 性能优化","uri":"/drafts/php/"},{"categories":null,"content":"从模型的角度看问题。站在调用 Repository 的用户的角度去看待这个东西。 从代码结构来看： Repository -\u003e ORM(Store) 例如 Laravel 中的 Cache 用到的 Store。每个 Store 的存储介质不一样，但提供统一的接口。 Cache::store(\"redis\")-\u003eget(\"\") Repository 就像是一个 Collection，用户不知道它底层是用什么存储的。Repository 应该让使用者感觉对象就好像驻留在内存中一样。 ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:0:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":null,"content":"Repository 用于管理对象 可以在 Repository 层给数据加缓存。用于把查询的细节放到中间层，客户端无需关心查询细节。 ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:1:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":null,"content":"面向集合资源库和面向持久化资源库 面向集合资源库没有“保存(save)”这一操作，而是有“添加(add)”这一操作，面向持久化资源库反之。 当对象被加入到面向集合资源库后，对于对象的直接修改，会影响到资源库中的对象。find -\u003e modify -\u003e find。第二次 find 可以查看到修改后的数据。 面向持久化资源库则是 find -\u003e modify -\u003e save -\u003e find。 ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:2:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":null,"content":"为不同存储介质创建不同的 Repository 实现 与 DAO 冲突。 ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:3:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":null,"content":"Repository 和 DAO 的区别 DAO 只负责将数据存储到特定存储介质中，不负责管理对象。 同一个对象在不同存储介质对应不同的 DAO，此时的 DAO 为 Store。如果直接用 DAO 而不是注册到 Repository 里面，则在 DAO 更换时，需要在所有用到的地方进行替换。而如果使用 Repository 模式，则只需替换 Repository 里面的 DAO。 直接用 ORM： ORM(Repository/Store) 如果想要在多个 Store 之间切换，则需要把 Repository 里面的单一逻辑下放到 Store，两者拥有相同接口。Repository 仅作为代理，此时编程三层：Repository -\u003e Store -\u003e ORM。 一开始可以先使用两层的方式，等到有切换的需求时，再切换到三层。如果没有切换的需求，则保持在第二层就行。毕竟如果只是更换，则两者的修改量是一样的。 什么情况下只使用一层？单文件项目之类的短期小项目。 其他时候首先考虑二层。有需要再转三层。 ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:4:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":null,"content":"Repository 和 Factory 的区别 Factory 负责制造新对象，Repository 负责查找已有对象。 Client –create_obj–\u003e Factory Client –add_obj–\u003e Repository –insert–\u003e Database ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:5:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":null,"content":"参考文档 https://developer.aliyun.com/article/758292 殷浩详解DDD系列 第三讲 - Repository模式 https://www.baeldung.com/java-dao-vs-repository DAO vs Repository Patterns https://github.com/eugenp/tutorials/tree/master/patterns/design-patterns-architectural/src/main/java/com/baeldung/repositoryvsdaopattern ","date":"2019-09-07","objectID":"/2019/09/07/repository-patten/:6:0","tags":null,"title":"Repository 模式","uri":"/2019/09/07/repository-patten/"},{"categories":["Installation And Configuration"],"content":"执行：pkill -o -USR2 php-fpm https://stackoverflow.com/questions/37806188/how-to-restart-php-fpm-inside-a-docker-container For me PID 1 is not always correct (especially after killing it once). What helps is pkill -o -USR2 php-fpm, because the option -o searches for the oldest process (the master) and kills it. ","date":"2019-08-30","objectID":"/2019/08/30/docker-reload-php-fpm/:0:0","tags":["Linux","Docker","PHP-FPM"],"title":"Docker 在不重启容器的情况下重新加载 PHP-FPM 配置","uri":"/2019/08/30/docker-reload-php-fpm/"},{"categories":null,"content":"https://stackoverflow.com/questions/18215389/how-do-i-measure-request-and-response-times-at-once-using-curl curl-format.txt 域名解析结束时间: %{time_namelookup}\\n 与远程主机建立连接完成时间: %{time_connect}\\n SSL/SSH握手结束时间: %{time_appconnect}\\n 数据发送开始时间: %{time_pretransfer}\\n 发送结束前所有重定向所需时间: %{time_redirect}\\n 接收返回的第一个字节的时间: %{time_starttransfer}\\n ----------\\n 总耗时: %{time_total}\\n curl -w \"@curl-format.txt\" -o /dev/null -s \"https://www.baidu.com\" ","date":"2019-08-29","objectID":"/2019/08/29/curl-analyze/:0:0","tags":["Linux","Network","OPS"],"title":"用 CURL 命令分析请求时间","uri":"/2019/08/29/curl-analyze/"},{"categories":null,"content":"简介 Keepalived 实现 VIP 漂移到正常的机器 Nginx 用 stream 模块实现 TCP 层（四层）的负载均衡 Keepalived是一款高可用软件，它的功能主要包括两方面： 1）通过IP漂移，实现服务的高可用：服务器集群共享一个虚拟IP，同一时间只有一个服务器占有虚拟IP并对外提供服务，若该服务器不可用，则虚拟IP漂移至另一台服务器并对外提供服务； 2）对LVS应用服务层的应用服务器集群进行状态监控：若应用服务器不可用，则keepalived将其从集群中摘除，若应用服务器恢复，则keepalived将其重新加入集群中。 ","date":"2019-08-26","objectID":"/2019/08/26/load-balance/:1:0","tags":null,"title":"load-balance","uri":"/2019/08/26/load-balance/"},{"categories":null,"content":"链接 https://blog.51cto.com/zephiruswt/1235852 https://www.jianshu.com/p/b6bc24a1201f https://cloud.tencent.com/developer/article/1027563 https://www.cnblogs.com/kevingrace/p/8290452.html https://www.chainnews.com/articles/868724777800.htm https://zhuanlan.zhihu.com/p/34246665 https://www.cnblogs.com/liwei0526vip/p/6370103.html https://my.oschina.net/leeypp1/blog/294807 ","date":"2019-08-26","objectID":"/2019/08/26/load-balance/:2:0","tags":null,"title":"load-balance","uri":"/2019/08/26/load-balance/"},{"categories":null,"content":"日志打印响应时间： https://segmentfault.com/a/1190000007903143 https://segmentfault.com/q/1010000004034343/a-1020000004036067 有两种响应时间 从接到请求，到发送给客户端 从请求发送给 upstream 到收到 upstream 的回复 ","date":"2019-08-24","objectID":"/2019/08/24/nginx-log/:0:0","tags":null,"title":"nginx-log","uri":"/2019/08/24/nginx-log/"},{"categories":null,"content":"JFrog Artifactory 可以代理多个仓库 ","date":"2019-08-24","objectID":"/drafts/repository/:0:0","tags":null,"title":"repository","uri":"/drafts/repository/"},{"categories":null,"content":"pip pip 是 EPEL 的一个包，所以需要安装 EPEL。 yum install -y epel-release 如果装不了 EPEL 或者系统版本过低没有搜索到 pip，该怎么办？ ","date":"2019-08-24","objectID":"/drafts/python-installation/:0:1","tags":null,"title":"python-installation(CentOS)","uri":"/drafts/python-installation/"},{"categories":null,"content":"问题：Laravel 的配置里面域名只能配置一个，但是现实要求多个域名能够访问。 因此使用 Nginx 做转发。 server { listen 80; server_name another-api.mydomain.com; location / { proxy_read_timeout 600s; fastcgi_read_timeout 600s; proxy_set_header Host api.mydomain.com; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://api.mydomain.com; } } 注意 header 里面的 Host 要设置为代理的域名 ","date":"2019-08-24","objectID":"/2019/08/24/nginx-multiple-domain/:0:0","tags":null,"title":"nginx-multiple-domain","uri":"/2019/08/24/nginx-multiple-domain/"},{"categories":null,"content":"https://www.infoq.cn/article/ZgAAVBZZaoo4I0-pkgV8 ","date":"2019-08-19","objectID":"/2019/08/19/api-design/:0:0","tags":null,"title":"api-design","uri":"/2019/08/19/api-design/"},{"categories":["MySQL"],"content":" 创建用户 CREATE USER \"username\"@\"host\" IDENTIFIED BY 'password'; 授权 GRANT ALL PRIVILEGES ON dbname.* TO \"username\"@\"host\"; 回收 REVOKE ALL PRIVILEGES ON dbname.* FROM \"username\"@\"host\"; ","date":"2019-08-18","objectID":"/2019/08/18/mysql-grant/:0:0","tags":["MySQL","Grant"],"title":"Mariadb 授权与回收","uri":"/2019/08/18/mysql-grant/"},{"categories":["PHP"],"content":"https://hub.docker.com/r/sroze/tideways https://github.com/sroze/dockerfiles/tree/master/tideways https://blog.it2048.cn/article-tideways-xhgui/ 两种方式： Nginx 指定入口脚本 开放端口，在插件里指定 tcp 连接 ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:0:0","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"步骤 安装 MongoDB 安装 PHP tideways 模块并配置 安装 PHP MongoDB 模块并配置 安装 Xhgui Nginx 配置将性能分析脚本附加到执行 Nginx 配置 Xhgui 网页访问 ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:0","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"安装 PHP 源 https://rpms.remirepo.net/wizard/ yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install -y https://rpms.remirepo.net/enterprise/remi-release-7.rpm yum install -y yum-utils yum-config-manager --enable remi-php73 yum makecache fast 如果发现其他 repo 被关闭了。先执行 yum repolist all，看要开启哪些。然后执行 yum-config-manager --enable xxx 。 ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:1","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"安装 PHP tideways 模块并配置 yum install -y php-devel cd /tmp curl -o xhprof.zip -L https://github.com/tideways/php-xhprof-extension/archive/master.zip unzip xhprof.zip cd php-xhprof-extension-master phpize ./configure make \u0026\u0026 make install ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:2","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"安装 PHP MongoDB 模块并配置 安装 MongoDB 模块： yum install php-mongodb -y 不需要额外配置 ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:3","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"安装 Xhgui cd /tmp curl -o xhgui.zip -L https://github.com/laynefyc/xhgui-branch/archive/master.zip unzip -d / xhgui.zip cd /xhgui-branch-master php install.php 如果有问题，用 composer 安装 alcaeus/mongo-php-adapter 。 ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:4","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"Nginx 配置将性能分析脚本附加到执行 将这一句加到 server 块： fastcgi_param PHP_VALUE \"auto_prepend_file=/xhgui-branch-master/external/header.php\"; ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:5","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"Nginx 配置 Xhgui 网页访问 xhgui.conf server { listen 80; server_name xhgui.mydomain.net; root /xhgui-branch-master/webroot; location / { index index.php; if (!-e $request_filename) { rewrite . /index.php last; } } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:6","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["PHP"],"content":"初始化 mongo use xhprof db.results.ensureIndex( { 'meta.SERVER.REQUEST_TIME' : -1 } ) db.results.ensureIndex( { 'profile.main().wt' : -1 } ) db.results.ensureIndex( { 'profile.main().mu' : -1 } ) db.results.ensureIndex( { 'profile.main().cpu' : -1 } ) db.results.ensureIndex( { 'meta.url' : 1 } ) ","date":"2019-08-16","objectID":"/2019/08/16/php-profiler-tideways/:1:7","tags":["PHP","PHP7","Profile"],"title":"PHP7 非侵入式性能分析工具套件 tideways \u0026 xhgui","uri":"/2019/08/16/php-profiler-tideways/"},{"categories":["Record"],"content":"背景 我们组负责维护一个内部系统，这个系统里面有一个功能：允许管理员在上面执行 SQL 语句，没有任何限制。 14 号下午，同事在这个系统的生产环境执行了一条 SQL 语句，导致部分功能异常。四十分钟后恢复服务。但数据在 15 号早上才完全恢复。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:1","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":["Record"],"content":"处理过程 在接受到用户反馈后，我知道了这是数据库的问题，但不知道具体问题在哪。 如果继续提供服务，可能会扩大影响范围。所以我询问组长和其他人看能否先停掉服务。得知有一个功能可以关闭部分系统，但没有操作文档，一时也找不到功能在哪。我就说还是先把服务停掉吧。 停掉服务后，好多人问我们服务号（刚好我值班）为什么 404 了。我一一道歉并说正在处理中。同事也在各大群里说系统先停止服务。 开始排查问题。 先确定当前影响的数据范围。查看表，了解到这是对全表的操作。 然后有个同事帮忙查找代码里面是否有不包含 WHERE 的 UPDATE，发现没有这样的语句。 然后另一个同事说有没有可能是他刚刚执行的一个 SQL。此时我们并没有注意到这一点。 （原因可能是因为他后面把语句改正确了，再执行了一次。然后他认为导致问题的那条 SQL 不应该生效，因为在他看来是个语法错误） 此时有两种做法： 查找原因，通过处理根本原因来解决问题 想办法在不知道原因前解决或者缓解当前问题，再去找原因 以往的经验告诉我，先解决当前的问题，减少影响，再去查找根本原因。 我首先去联系联系了 DBA，简要告诉他事情大概是怎么回事。以便等会儿做什么应急操作时，可以避免产生无法恢复的影响。 然后思考处理方案。 有人提出使用备份数据，恢复到凌晨的数据。DBA 也这么说。然而这是不行的，因为在这期间创建了很多新数据。原来那些 instance 的状态也可能变为结束。如果完全恢复，也会出问题。 此时无法确定是哪条 SQL 导致的，只能有一个大概的执行时间。当时告诉 DBA 的时间是用户反馈的时间（后来发现 SQL 是这个时间点的五分钟前执行的）。 之前已经了解到影响只有这张表。业务上这个字段的问题不会导致太大问题，只是因为某个条件限制，导致无法执行了。 要解除限制，可以把字段置为空字符串。这样主要服务就可以正常。 我先询问组内同事能否把字段值置空。得到确认后，问 DBA ，如果把字段置空会不会影响到后续数据恢复。DBA 说可以先执行。 于是我就执行了一句把那个值为 0 的字段设置为空字符串的 SQL。然后恢复服务。此时比较明显的那个影响也只是小问题。但由于涉及到某些定时任务（后面会说明为什么），所以仍然需要 DBA 帮忙恢复那部分不应该设置为空字符串的数据。 此时已经过去 40 分钟。在这期间，有个大问题，就是外部也有一部分人在使用这套系统。他们要求后续给个故障报告。这时我们才想起来没有通知他们。 目前系统主要功能已经恢复，接下来就是分析原因。 我想到了之前系统有个接口，参数是 SQL 语句，然后无条件执行。估计是认为自己系统调用，也不会有什么人会恶意攻击。我也觉得不是有人恶意攻击，但还是先跟组内其他人说一声。他们有人去看请求日志，没有发现这种请求。 有同事就说，自己排除下是否是在生产环境的那个功能执行了什么语句。有日志功能。自己看了一下也没什么问题。 另一个同事就说是不是有把执行日志记录到数据库里面，有的话直接搜索看看，说不定有符合特征的语句。然后过会儿就放一张截图出来。 同事看了一眼截图，惊讶地说：“xx，是你啊”。没错，指的就是我。我：？？？。吓得我差点心脏承受不住。结果仔细一看，那是我刚刚为了恢复服务执行的语句。虚惊一场。Orz 过一会儿，我们组长把那张截图的范围缩小到了一条记录。发出了截图。一看，终于找到了原因。 后续 DBA 恢复的时候，取 binlog 中 UPDATE 前后数据，并过滤掉已经不需要更新的部分，这只能恢复一小部分数据（这个我得研究看看为啥少了那么多数据）。最后还是通过备份数据加上当前数据的状态，提取出用于恢复的 SQL 并执行，这才成功地恢复了所有数据。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:2","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":["Record"],"content":"操作内容及影响 MySQL 版本：5.6.41 执行的语句（表名和字段名做了替换）： UPDATE instances SET operator = \"某人\" AND id = 123456; 影响：该表所有行（100万条）的 operator 字段的值都被设置为 0。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:3","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":["Record"],"content":"解释 SQL 语句的问题在于操作符的优先级 MySQL 操作符优先级： https://dev.mysql.com/doc/refman/5.6/en/operator-precedence.html 从高到低为： INTERVAL BINARY, COLLATE ! - (unary minus), ~ (unary bit inversion) ^ *, /, DIV, %, MOD -, + \u003c\u003c, \u003e\u003e \u0026 | = (comparison), \u003c=\u003e, \u003e=, \u003e, \u003c=, \u003c, \u003c\u003e, !=, IS, LIKE, REGEXP, IN BETWEEN, CASE, WHEN, THEN, ELSE NOT AND, \u0026\u0026 XOR OR, || = (assignment), := 可以看到 AND 的优先级高于 = (assignment)，这样在 MySQL 看来，上面的语句就是： UPDATE instances SET operator = (\"某人\" AND id = 123456); 这里的 \"某人\" 在做 AND 运算时，会被转换为 false。 SELECT IF(\"某人\", TRUE, FALSE); 得到的结果是 0，也就是 FALSE。 https://dev.mysql.com/doc/refman/5.6/en/boolean-literals.html 这样虽然后面的 id = 123456 在某一条会得到 TRUE 的结果，但由于 AND 的结果必然为 FALSE，最终还是会是 FALSE，然后被转为 0 。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:4","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":["Record"],"content":"为何会影响定时任务 对于 instance 表来说，每条记录都是一个任务的相关信息。里面包括了这个任务预定在什么时间执行。 由于系统设计时，没有考虑到这点。所以后来要加上预定执行的功能时，就直接把时间放入 operator 。 例如： xxxx@2019-08-13 15:00:00 如果把这个字段置空，定时的这些任务就没办法到指定时间执行，而是直接不执行了。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:5","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":["Record"],"content":"影响到外部 系统并不是由同一批人使用的，用术语来说就是多租户。 由于系统设计之初（十年前）并没有考虑到多租户的需求，这是去年才有的需求，并且由于数据和业务各种复杂的原因，导致多个租户用同一套服务和数据库。 因此数据库数据出错以及在暂停服务的时候，都会影响到其他租户。 这次我没做好的地方就是在停服务之前，没有事先通知这些租户。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:6","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":["Record"],"content":"教训 这次大家也总结了一些处理方案。 涉及到更新操作的，都要审核。 这个其实之前就有这个规范，只是这次同事因为觉得这语句太简单，就给忽略了。 我认为如果实在要做，也应该做成一个人提交要执行的语句，另一个人点击审核通过之后才能执行。这样就避免了某个人可以不走审核流程就能执行的问题。 UPDATE 之前先用 SELECT 获取影响范围，再从 SELECT 语句的基础上改为 UPDATE。 这个其实还可以用 EXPLAIN 来确定影响范围。配合上面在审核过程中自动执行 EXPLAIN 让审核人得知影响范围，能更有效地避免问题。 UPDATE 如果不带 WHERE 语句，执行之前提示用户。 这个可以直接禁掉不包含 WHERE 的 SQL。实在想全表操作，用 WHERE 1 = 1 就行了。 但其实还是应该避免线上直接执行 SQL。最好对想要的操作做个单独页面，通过界面化操作来从根本上解决问题。 服务停止时应通知所有租户。 好像还少了一条：出现问题时的处理方案。比如说这次出现问题，没有一个文档可以指导我们正确地停止服务，减小影响范围。 ","date":"2019-08-15","objectID":"/2019/08/15/mysql-and-error/:0:7","tags":["MySQL"],"title":"记一个 SQL 导致的问题的处理","uri":"/2019/08/15/mysql-and-error/"},{"categories":null,"content":"https://linux.cn/article-10164-1.html ","date":"2019-08-13","objectID":"/2019/08/13/build-rpm/:0:0","tags":null,"title":"build-rpm","uri":"/2019/08/13/build-rpm/"},{"categories":null,"content":"安装和配置：https://www.cnblogs.com/Eivll0m/p/6700828.html CentOS5 可用最新版 GitHub: https://github.com/rsyslog/rsyslog 官方文档：https://www.rsyslog.com/doc/master/index.html 配置：https://www.karlzhou.com/articles/center-log-with-rsyslog/ https://www.rsyslog.com/doc/v8-stable/configuration/modules/imfile.html 服务端：https://linux.cn/article-5023-1.html 客户端：https://linux.cn/article-4835-1-rel.html https://www.loggly.com/docs/troubleshooting-rsyslog/ ","date":"2019-08-13","objectID":"/2019/08/13/syslog/:0:0","tags":null,"title":"syslog","uri":"/2019/08/13/syslog/"},{"categories":null,"content":"多个 Filebeat 源 根据文件名区分 ","date":"2019-08-13","objectID":"/2019/08/13/logstash/:0:1","tags":null,"title":"logstash","uri":"/2019/08/13/logstash/"},{"categories":null,"content":"多个源的解决方案 加一层 MQ，通过 MQ 不同的队列来获取 ","date":"2019-08-13","objectID":"/2019/08/13/logstash/:0:2","tags":null,"title":"logstash","uri":"/2019/08/13/logstash/"},{"categories":null,"content":"多个配置文件 https://stackoverflow.com/questions/27146032/make-logstash-add-different-inputs-to-different-indices Logstash 的事件会被分发到 pipeline 所有配置文件里的 input 要区分多个的时候，要在 input 里面设置 type ，并通过条件语句区分。 input { udp { ... type =\u003e \"foo\" } file { ... type =\u003e \"bar\" } } output { if [type] == \"foo\" { elasticsearch { ... index =\u003e \"foo-index\" } } else { elasticsearch { ... index =\u003e \"bar-index\" } } } ","date":"2019-08-13","objectID":"/2019/08/13/logstash/:0:3","tags":null,"title":"logstash","uri":"/2019/08/13/logstash/"},{"categories":null,"content":"解析 HTML https://stackoverflow.com/questions/1600526/how-do-i-encode-decode-html-entities-in-ruby https://stackoverflow.com/questions/51412579/logstash-not-producing-output-although-pipeline-main-starts ","date":"2019-08-13","objectID":"/2019/08/13/logstash/:0:4","tags":null,"title":"logstash","uri":"/2019/08/13/logstash/"},{"categories":null,"content":"错误 https://discuss.elastic.co/t/invalid-frame-type-received-69-84/130234 https://stackoverflow.com/questions/50693683/filebeat-to-logstash-invalidframeprotocolexception ","date":"2019-08-13","objectID":"/2019/08/13/logstash/:0:5","tags":null,"title":"logstash","uri":"/2019/08/13/logstash/"},{"categories":["Installation And Configuration"],"content":"查看磁盘 fdisk -l ","date":"2019-08-13","objectID":"/2019/08/13/centos-ntfs/:0:1","tags":["Linux","CentOS7","NTFS"],"title":"CentOS7 读取 NTFS 格式硬盘","uri":"/2019/08/13/centos-ntfs/"},{"categories":["Installation And Configuration"],"content":"挂载 安装支持程序： yum install -y ntfs-3g mkdir /mnt/hd mount -t ntfs /dev/sdb /mnt/hd ","date":"2019-08-13","objectID":"/2019/08/13/centos-ntfs/:0:2","tags":["Linux","CentOS7","NTFS"],"title":"CentOS7 读取 NTFS 格式硬盘","uri":"/2019/08/13/centos-ntfs/"},{"categories":["Installation And Configuration"],"content":"格式化磁盘 yum install -y ntfs-3g ntfsprogs mkntfs -F --fast --label myDisk /dev/sdb ","date":"2019-08-13","objectID":"/2019/08/13/centos-ntfs/:0:3","tags":["Linux","CentOS7","NTFS"],"title":"CentOS7 读取 NTFS 格式硬盘","uri":"/2019/08/13/centos-ntfs/"},{"categories":["Installation And Configuration"],"content":"从源码安装 https://www.tuxera.com/community/open-source-ntfs-3g/ https://blog.csdn.net/qiushisoftware/article/details/79520869 ","date":"2019-08-13","objectID":"/2019/08/13/centos-ntfs/:0:4","tags":["Linux","CentOS7","NTFS"],"title":"CentOS7 读取 NTFS 格式硬盘","uri":"/2019/08/13/centos-ntfs/"},{"categories":null,"content":"CentOS 5：https://shazi.info/centos-5-i386-x86_64-%E7%94%A8-rpmbuild-%E5%AE%89%E8%A3%9D-filebeat-6-x/ ","date":"2019-08-13","objectID":"/2019/08/13/filebeats/:0:0","tags":null,"title":"filebeats","uri":"/2019/08/13/filebeats/"},{"categories":null,"content":"high disk watermark 问题 Logstash 报： index read-only / allow delete (api) https://www.elastic.co/guide/en/elasticsearch/reference/2.1/disk-allocator.html PUT _settings { “index”: { “blocks”: { “read_only_allow_delete”: “false” } } } PUT _cluster/settings { “persistent”: { “cluster.routing.allocation.disk.threshold_enabled”: false } } ","date":"2019-08-13","objectID":"/2019/08/13/elasticsearch/:0:1","tags":null,"title":"elasticsearch","uri":"/2019/08/13/elasticsearch/"},{"categories":null,"content":"the type event field won’t be used to determine the document _type https://www.elastic.co/guide/en/elasticsearch/reference/6.2/removal-of-types.html https://discuss.elastic.co/t/detected-a-6-x-and-above-cluster-the-type-event-field-wont-be-used-to-determine-the-document-type-es-version-6/183471 ","date":"2019-08-13","objectID":"/2019/08/13/elasticsearch/:0:2","tags":null,"title":"elasticsearch","uri":"/2019/08/13/elasticsearch/"},{"categories":["OpenWRT"],"content":" 安装 安装 smb 服务 opkg install samba36-server 安装 LuCI 配置界面 opkg install luci-i18n-samba-zh-cn 配置服务器 创建账号并设置密码 创建组 给 Samba 添加账号密码 启动 Samba 服务 /etc/init.d/samba start 设置开启自启动 /etc/init.d/samba enable 打开路由器 WEB 界面。进入【服务】-\u003e【网络共享】。 添加一个共享目录。 名字随便填 目录选择路由器里的目录。这个目录要给刚才创建的账号读写权限。 允许的用户填刚才创建的用户 权限掩码都填 777 保存 Windows 10 配置 在【启用或关闭 Windows 功能】，在【SMB 1.0/CIFS 文件共享支持】-\u003e【SMB 1.0/CIFS 客户端】选项前面打勾 重启 Windows 10 在【控制面板\\用户帐户\\凭据管理器】里点击【添加 Windows 凭据】 填完地址、用户名、密码后，点确定即可。 这里的地址是路由器地址。或者打开 Windows 文件浏览器里左边菜单栏的【网络】里看到的名称。 参考： win10 无法访问samba，没有权限，登录会话解决: https://jingyan.baidu.com/article/c146541382b6950bfcfc4ca5.html ","date":"2019-08-09","objectID":"/2019/08/09/openwrt-nas/:0:0","tags":["OpenWRT","Samba"],"title":"OpenWRT NAS","uri":"/2019/08/09/openwrt-nas/"},{"categories":null,"content":"Docker 网络 查看当前主机 Docker Daemon 已创建的网络： docker network ls。 Docker 网络驱动（driver）主要由五种： bridge （默认）、 host 、 overlay 、 macvlan 、 none 。 bridge 桥接网络。加入同一个 bridge 的容器可以互相通信。用于同一个 Docker Daemon 内容器通信。 host 使用主机的网络。对使用 host 网络的容器不做网络隔离，把容器暴露在公网中。 overlay 跨主机通信使用的网络。连接多个 Docker Daemon ，多用于 Docker Swarm 集群。 Swarm 会默认创建一个 ingress 的 overlay。 macvlan MAC 级别网络。会给容器分配 MAC 地址，使其在网络中看起来像是一个独立的物理设备。 none 隔绝网络。当使用自定义网络驱动时，使用 none。 Network - Docker Document: https://docs.docker.com/network/ 浅聊几种主流 Docker 网络的实现原理 - InfoQ: https://www.infoq.cn/article/9vfPPfZPrXLM4ssLlxSR MPLS L3 VPN - Zhihu: https://zhuanlan.zhihu.com/p/27539826 需要注意的问题： 使用 bridge 和 overlay 网络时，需要注意分配的网段。 例如在内网中，如果使用了 192.168.1.0/24 这个网段，就不能让这两个 driver 使用这个网段。否则会出现容器请求了某个 IP，但被 Docker 内部网络拦截而导致无法请求到外部机器的问题。 ","date":"2019-08-09","objectID":"/2019/08/09/docker-swarm/:1:0","tags":null,"title":"docker-swarm","uri":"/2019/08/09/docker-swarm/"},{"categories":null,"content":"集群基础 节点 节点类型 说明 Worker 工作节点。只用于提供服务 Leader 主控节点。用于管理其他节点 Manager 管理节点。一个 Leader 节点必须是一个 Manager 节点。Leader 节点由所有 Manager 选举产生。管理节点也可以作为 Worker Leader 节点选举 当 Leader 节点故障时，Manager 节点会举行一次投票，在存活的 Manager 节点中选举出一个新的 Leader。 （票数）过半原则：Leader 选举时，投票数要超过 Manager 总数（包括故障的 Manager）的一半才能成功。 如果没有过半的节点参与投票，那么集群会变为不可用。 因此如果总共有 3 个 Manager 节点，挂掉一个节点没事。挂掉两个节点时，集群变为不可用。 正是因为这个过半原则，我们通常使用奇数个 Manager 节点。如果用偶数个节点会是什么情况呢？ 假设有 4 个 Manager 节点，挂掉一个节点没事。挂掉两各节点时，剩余的两个节点无论怎么投票，都不会超过总 Manager 数量的一半。集群也会变得不可用。 https://juejin.im/post/5cb6d5a0e51d456e51614a88 ","date":"2019-08-09","objectID":"/2019/08/09/docker-swarm/:2:0","tags":null,"title":"docker-swarm","uri":"/2019/08/09/docker-swarm/"},{"categories":null,"content":" https://zhoujinl.github.io/2018/10/19/docker-swarm-manager-ha/ https://severalnines.com/database-blog/introduction-docker-swarm-mode-and-multi-host-networking 构建请求变更域名解析的镜像，设置为只在 manager 开启。 Portainer Agent 在部署后，应通过 Portainer 应用连接 Agent 地址。 docker login -u username -p password https://registry.schaepher.com docker build –tag=itask_test . \u0026\u0026 docker tag itask_test registry.schaepher.com/task/itask_test \u0026\u0026 docker push registry.schaepher.com/task/itask_test supervisord priority 默认 999 ，越低越早启动 http://supervisord.org/configuration.html 集群同步数据及主备切换： https://zhuanlan.zhihu.com/p/46675331 MySQL 主备切换： https://www.cnblogs.com/gomysql/p/3663146.html https://www.jianshu.com/p/41b700ebf1c1 ","date":"2019-08-09","objectID":"/2019/08/09/docker-swarm/:3:0","tags":null,"title":"docker-swarm","uri":"/2019/08/09/docker-swarm/"},{"categories":["Installation And Configuration"],"content":"在脚本里面使用 getopts 获取参数。 ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopts/:0:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopts）","uri":"/2019/08/08/bash-params-getopts/"},{"categories":["Installation And Configuration"],"content":"示例 test2.sh #!/bin/bash function main() { local OPTIND count=0 while getopts ab:-: OPT do echo \"current -${OPT}${OPTARG}\" case \"${OPT}\" in -) case \"${OPTARG%%=*}\" in atest) echo \"option: --${OPTARG}\" echo \"\" let count++ ;; btest) echo \"option: --${OPTARG%%=*}\" echo \"value: ${OPTARG#*=}\" echo \"\" let count=count+2 ;; esac ;; a) echo \"option: ${OPTIND}\" echo \"\" let count++ ;; b) echo \"option: ${OPTIND}\" echo \"value: ${OPTARG}\" echo \"\" let count=count+2 ;; esac done shift $count echo \"rest option(s): $@\" } main \"$@\" 执行： bash test2.sh --atest --btest=\"2 4\" -- --ctest=3 --dtest=4 结果： current --atest option: --atest current --btest=2 4 option: --btest value: 2 4 rest option(s): --ctest=3 --dtest=4 ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopts/:1:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopts）","uri":"/2019/08/08/bash-params-getopts/"},{"categories":["Installation And Configuration"],"content":"解释 与 getopt 相比： while 里面不能使用 shift，必须在结束之后统一 shift 长选项不能用纯空格隔开 放在 function 里面的时候，必须先执行 local OPTIND 不必另外处理 -- 的情况 ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopts/:2:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopts）","uri":"/2019/08/08/bash-params-getopts/"},{"categories":["Scripts"],"content":"在脚本里面使用 getopt 获取参数。 ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopt/:0:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopt）","uri":"/2019/08/08/bash-params-getopt/"},{"categories":["Scripts"],"content":"示例 test.sh #!/bin/bash ARGS=$(getopt --option ab: --long atest,btest: -- \"$@\") eval set -- \"${ARGS}\" while true do echo \"current $1\" case \"$1\" in -a|--atest) echo \"option: $1\" echo \"\" shift 1 ;; -b|--btest) echo \"option: $1\" echo \"value: $2\" echo \"\" shift 2 ;; --) shift echo \"break\" echo \"\" break ;; esac done echo \"rest option(s): $@\" 如果执行： bash test.sh --atest --btest=2 -- --ctest=3 --dtest=4 则会得到： current --atest option: --atest current --btest option: --btest value: 2 current -- break rest option(s): --ctest=3 --dtest=4 ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopt/:1:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopt）","uri":"/2019/08/08/bash-params-getopt/"},{"categories":["Scripts"],"content":"解释 getopt --option ab: --long atest,btest: -- \"$@\" 当想使用 getopt 获取长长选项时，必须带 --option 或其简写 -o，虽然指定的选项未必和长选项一一对应。 ab: 是短选项 a 和 b b 后面加一个冒号，表示 b 这个参数必须携带值。例如 -b 2 或者省略空格 -b2 如果加两个冒号，则表示值是可选的。例如对于 c::，则可以传 -c 也可以 -c 3 atest,btest: 是长选项，选项间用英文逗号隔开。冒号的作用同短选项 传参时，可用空格或等号隔开选项和参数值 -- 表示其后内容就算是以 - 开头，也不作为选项解析，当做纯文本 \"$@\" 表示传入脚本除了路径 $0 外的所有参数。加双引号是为了处理参数值包含空格的情况。 例如如果不加双引号， --btest=\"2 4\" 解析出来的值就是 option: --btest value: 2 如果加引号，结果就是： option: --btest value: 2 4 eval set -- \"${ARGS}\" 使用 set 命令刷新参数列表。直观上就是等号被替换为空格。例如 --btest=2 被替换为 --btest 2。 后面就是普通的 switch-case 了。加上 shift [num] 使得每次都从 $1 开始，不用担心参数顺序问题。 参数的解析是从左到右按顺序的，所以我们也可以处理 -- 的情况。直接 break 停止解析即可。 由于使用了 shift，所以 -- 之后的内容都保存在 $@ 里面。 ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopt/:2:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopt）","uri":"/2019/08/08/bash-params-getopt/"},{"categories":["Scripts"],"content":"长选项对单横杠的支持 如果使用上面的解析，--btest=2 没有问题。但是 -btest=2 就会报错。 可以给 getopt 加一个 --alternative 选项： getopt --alternative --option ab: --long atest,btest: -- \"$@\" ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopt/:3:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopt）","uri":"/2019/08/08/bash-params-getopt/"},{"categories":["Scripts"],"content":"选项简写 上述命令可简写为： getopt -a -o ab: -l atest,btest: -- \"$@\" ","date":"2019-08-08","objectID":"/2019/08/08/bash-params-getopt/:4:0","tags":["Shell","Bash","Linux"],"title":"Bash 脚本长参数（getopt）","uri":"/2019/08/08/bash-params-getopt/"},{"categories":null,"content":" ext3grep ext3undel debugfs 和 dd 单个文件 extundelete ext 3 4 –superblock –joural 时间段 文件、文件夹 grep grep -F -A100 -B100 关键字 /dev/sda4 会找到被删的文件 先 umount ","date":"2019-08-08","objectID":"/2019/08/08/rm_rf/:0:0","tags":null,"title":"Linux 执行文件删除后的恢复","uri":"/2019/08/08/rm_rf/"},{"categories":null,"content":"避免 rm -rf $dir/ 检测 $dir 是否为空 rm -rf ./ 可能手抖没写 . 机器重装，注意看数据库是否还有连接 https://mp.weixin.qq.com/s/-TLfLSbnIIbYnKRfBYCkTA ","date":"2019-08-08","objectID":"/2019/08/08/rm_rf/:1:0","tags":null,"title":"Linux 执行文件删除后的恢复","uri":"/2019/08/08/rm_rf/"},{"categories":null,"content":"https://phpartisan.cn/news/55.html https://www.zybuluo.com/phper/note/89081 https://linuxeye.com/380.html https://www.kancloud.cn/digest/php-src/136260 ab -t 100 -c 10 -p post_data.txt -T ‘application/json’ http://localhost:8083/xxxxx ","date":"2019-08-08","objectID":"/2019/08/08/nginx_php-fpm_concurrency/:0:0","tags":null,"title":"Nginx 和 PHP-FPM 并发优化","uri":"/2019/08/08/nginx_php-fpm_concurrency/"},{"categories":null,"content":"https://ibcl.us/HC5962-V2Ray_20190518/ ","date":"2019-08-08","objectID":"/drafts/openwrt_proxy/:0:0","tags":null,"title":"V2Ray 透明代理","uri":"/drafts/openwrt_proxy/"},{"categories":null,"content":"mipsle https://forum.openwrt.org/t/xiaomi-mi-wifi-3-support/2252/692 https://openwrt.org/toh/xiaomi/mir3 https://downloads.openwrt.org/snapshots/targets/ramips/mt7620/ ","date":"2019-08-08","objectID":"/drafts/v2ray-for-xiaomi-route3/:0:0","tags":null,"title":"小米路由3 安装 V2ray","uri":"/drafts/v2ray-for-xiaomi-route3/"},{"categories":null,"content":"字符串转整形排序 法一：CAST SELECT val FROM test ORDER BY CAST(val AS unsigned); MySQL 5.6 不支持直接 cast 到 integer，所以使用 signed 或者 unsinged。 法二：加减法 SELECT val FROM test ORDER BY 0 + val; SELECT val FROM test ORDER BY 0 - val; SELECT val FROM test ORDER BY -val; 同 0 - val SELECT val FROM test ORDER BY --val; 同 0 + val ","date":"2019-08-04","objectID":"/2019/08/04/mysql/:0:1","tags":null,"title":"MySQL 笔记","uri":"/2019/08/04/mysql/"},{"categories":["Router","OpenWRT"],"content":"本文包括三个部分： 连接路由器 Shell 下载和写入 OpenWrt 升级包 配置 OpenWrt ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:0:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"连接路由器 Shell 由于现在小米禁止刷官方的 SSH 开启工具，因此需要使用另外的方法进入 Shell。 官方的 SSH 开启工具： https://d.miwifi.com/rom/ssh 这里使用 openwrt 提供的方法。 openwrt xiaomi mini： https://openwrt.org/toh/xiaomi/mini 路由器固件降级 下载官方旧版本固件（新版的固件可能没法用以下方法）： http://bigota.miwifi.com/xiaoqiang/rom/r1cm/miwifi_r1cm_firmware_b9d56_2.7.11.bin 连接路由器 WIFI 或者网线直连 打开浏览器进入 192.168.31.1 ，这个是路由的管理界面 选择【常用设置】-\u003e 【系统信息】 -\u003e 【升级】，选择刚才下载的文件，确定。 此时会提示系统降级最好删除配置文件，勾选并继续。 等执行完，路由器会重启，并闪烁黄灯。一直等到蓝灯常亮，表示降级完毕。 开启 telnet （现在还开不了 SSH） 打开浏览器进入 192.168.31.1 ，配置并等待初始化完成。 此时 url 会包含 stok=xxxxx，把 xxxx 复制出来，以下用 表示复制出来的部分。 注：一定要等上面初始化完成，否则下面的命令无法执行。 复制下面这串： http://192.168.31.1/cgi-bin/luci/;stok=\u003cSTOK\u003e/api/xqnetwork/set_wifi_ap?ssid=whatever\u0026encryption=NONE\u0026enctype=NONE\u0026channel=1%3B%2Fusr%2Fsbin%2Ftelnetd 贴到浏览器（注意替换 ），回车。这个用来开启 telnet。 执行结束会提示：{“msg”:“未能連線到指定Wi-Fi(Probe timeout)”,“code”:1616} 虽然信息是错误，但实际上是成功。 复制下面这串： http://192.168.31.1/cgi-bin/luci/;stok=\u003cSTOK\u003e/api/xqsystem/set_name_password?oldPwd=\u003cCURRENTPASS\u003e\u0026newPwd=\u003cNEWPASS\u003e 贴到浏览器（注意替换 （当前路由登陆密码） （新的登陆密码）），回车。 这个用来重新设置密码。 执行结束会提示：{“code”:0}。 此时 telnet 已开启。 打开 Windows 的 cmd，并连接路由器： telnet 192.168.31.1 23 如果提示找不到 telnet，需要在控制面板的【程序和功能】-\u003e【启用或关闭 Windows 功能】里面找到 【Telnet 客户端】，前面的打勾，并点【确定】。 连接时的用户名为 root，密码为刚才的 ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:1:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"备份 MTD 插入 U 盘，在 telnet 里面进入 U 盘文件夹。在 /extxxxx/ext4 里面。 这里的 xxxx 根据不同情况可能不同，你可以 cd / \u0026\u0026 ls 看到以 ext 开头的文件夹 执行以下命令： for name in $(grep -v 'dev' /proc/mtd | awk -F ':' '{print $1}'); do dd if=/dev/$name of=/extxxxx/ext4/$name.bin; done ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:2:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"刷引导 BootLoader（不死 Breed） 主要是为了避免把路由器刷坏，没法恢复。只要刷成功，以后就不用怕了。而且刷固件用界面操作也比较方便。 进入官网，找到 breed-mt7620-xiaomi-mini.bin 官方网站： https://breed.hackpascal.net/ 将文件下载到 U 盘 进入 U 盘，执行命令写入： mtd -r write /extxxxx/ext4/breed-mt7620-xiaomi-mini.bin Bootloader 等待路由器重启 重启后会亮红灯闪烁，等待红灯常亮，即表示成功。 进入 Bread 的方式： 关掉路由器电源 按住 reset 接通路由器电源，等待 3 秒，灯闪烁，再放开 reset 打开 192.168.1.1 ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:3:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"刷 OpenWrt ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:4:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"法一：Bread 刷 OpenWrt 获取 OpenWrt 升级包下载地址 回到 OpenWrt 的页面 https://openwrt.org/toh/xiaomi/mini 找到【OpenWrt support】这一块，复制【Firmware OpenWrt Upgrade】下面的链接 下载这个以 -ramips-mt7620-miwifi-mini-squashfs-sysupgrade.bin 结尾的文件 进入 Bread 在【固件启动设置】里，将类型设置为【小米 Mini】 在【固件备份】里，都点一遍 在【固件更新】里，在【固件】一栏选择刚才下载的 bin。 点上传，等待上传完毕，路由器会自动重启。 重启后会亮红灯闪烁，等待红灯常亮，即表示成功。 ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:4:1","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"法二：下载和写入 OpenWrt 升级包 获取 OpenWrt 升级包下载地址 回到 OpenWrt 的页面 https://openwrt.org/toh/xiaomi/mini 找到【OpenWrt support】这一块，复制【Firmware OpenWrt Upgrade】下面的链接 升级 刚刚连接的 telnet 执行： cd /tmp wget http://downloads.openwrt.org/releases/18.06.4/targets/ramips/mt7620/openwrt-18.06.4-ramips-mt7620-miwifi-mini-squashfs-sysupgrade.bin `` 检查 MTD cat /proc/mtd 你可以看到有一行包含 OS1 写入升级包 mtd -r write 刚刚下载的文件名 OS1 写入完成后，路由器会自动重启 重启后会亮红灯闪烁，等待红灯常亮，即表示成功。 ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:4:2","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"配置 OpenWrt 以下配置完成后，记得在【系统】-\u003e【备份/升级】-\u003e【动作】-\u003e【生成备份】创建备份。以后重新刷 OpenWrt 的时候，就可以直接导入。不用再做配置。 进入 OpenWrt 界面启用 WIFI 刚装完后，WIFI 没有默认开启。所以需要用网线连接路由器的 LAN 口。如果电脑连了其他路由器的 WIFI，则先断掉。 进入 192.168.1.1 。刚开始会要求设置密码。点【Login】按钮，再点【Go to password configuration…】进入设置界面。 在 Password 和 Confirmation 输入密码，点 【Save \u0026 Apply】。 进入顶部的 【Network】-\u003e【Wireless】，选择一个，点击 【Enable】就开启 WIFI 了。 点【Edit】进去设置 ESSID（WIFI连接名称）。 在【Wireless Security】一栏的 Encryption 选择 WPA2-PSK，然后在 Key 一栏填入密码。点【Save \u0026 Apply】。 开启 SSH 进入顶部的【System】-\u003e【Administration】 在 Dropbear Instance 下面的 Interface 选择 lan Port 设置一个，比如 55555 确保勾选了【Password authentication】和【Allow root logins with password】 点【Save \u0026 Apply】 修改路由地址 我这里是路由器用线连接光猫，光猫的地址是 192.168.1.1，无法修改。OpenWrt 也是 192.168.1.1。 如果输入 192.168.1.1 会进入 OpenWrt 管理界面，而不是光猫的。所以可以选择将 OpenWrt 的地址改掉。 第一种方法是在管理界面的【Network】-\u003e【Interface】-\u003e【LAN】，把【IPv4 address】这一栏改成类似 192.168.23.1。点【Save \u0026 Apply】 第二种方法是进入 SSH，编辑文件：vim /etc/config/network 修改 config interface 'lan' 这一区块的 option ipaddr，修改后类似于： config interface 'lan' option type 'bridge' option ifname 'eth0.1' option proto 'static' option ipaddr '192.168.23.1' option netmask '255.255.255.0' option ip6assign '60' 然后执行 /etc/init.d/network reload，等一会儿就可以通过 192.168.23.1 访问路由器了。 我试了好多次第一种方法，总是保存不了，用这个方法才成功。 ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:5:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"安装管理界面中文包 网上说的安装方法大多过时，在 OpenWrt 18.06.4 版本，需要执行以下命令安装： opkg update opkg install luci-i18n-base-zh-cn 从下面这个文档找到的： https://openwrt.org/packages/pkgdata/luci-i18n-base-lang ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:6:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"USB 支持 安装 opkg install kmod-usb-core \\ kmod-usb-uhci \\ kmod-usb-storage \\ kmod-usb2 \\ kmod-usb-ohci \\ block-mount \\ mount-utils \\ fdisk `` 如果 U 盘或移动硬盘是 FAT32 的，就装 kmod-fs-vfat；如果是 NTFS 的，就装 ntfs-3g。 挂载 执行 mkdir /mnt/usb 创建等下的挂载目录 插入 U 盘 执行 fdisk -l | grep \"^/\" 会看到与 U 盘容量差不多的分区。这里假设为 /dev/sda4 执行 mount /dev/sda4 /mnt/usb 这样进入 /mnt/usb 就能看到 U 盘的文件了 ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:7:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Router","OpenWRT"],"content":"主要参考文档 基础知识：https://www.zoulei.net/2016/05/05/openwrt_recovery_you_need_to_know/ https://openwrt.org/toh/xiaomi/mini https://leamtrop.com/2017/05/11/flash-openwrt-squashfs/ http://bbs.xiaomi.cn/t-13391060-1-o0 修改路由地址：https://www.cnblogs.com/double-win/p/3841017.html Breed：https://www.jianshu.com/p/cab3062ef920 U 盘：https://jingyan.baidu.com/article/5225f26b6b273fe6fa090829.html U 盘：https://www.cnblogs.com/double-win/p/3841801.html MTD：http://blog.chinaunix.net/uid-28790518-id-5082378.html ","date":"2019-07-25","objectID":"/2019/07/25/xiaomi-router-mini-openwrt/:8:0","tags":["Router","OpenWRT","XiaoMi Router Mini"],"title":"小米路由MINI（R1C）刷 OpenWrt","uri":"/2019/07/25/xiaomi-router-mini-openwrt/"},{"categories":["Life"],"content":"七月十号这一天，刚好是我入职两年整，我们几个同事出去烤串。我也想起来该回顾过去的一年了。 去年差不多这个时间，写了一篇《毕业一年啦》。在写这篇之前，我也去回顾了这篇文章。感觉自己这一年的生活没有什么进步，反而退步了。 写作格式暂不做变更，还是按原来的“住、食、行、娱、健、书”来写。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:0:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"住 这一年换了两个地方。一是在《毕业一年啦》写的刚好泊寓到期，就搬到离公司更近的地方住下了。住了一个多月，在八月中下旬又搬了一次。 这是因为得到消息，说公司十一月要搬，所以就先搬过去。然后宿舍到公司单程从步行 20 分钟变为 80 分钟。不过公司如果也搬了，就变成 5 分钟。最后公司到十二月底才搬。 宿舍是全新的，面积四十几平方米，带阳台和卫生间，在六楼。房租比之前那间还便宜一点，物业水电宽带费另算。一个月下来差不多 1k。不过由于是单间，就没法像以前那样和其他人玩耍了。好在大家都在同一栋楼，有两个小伙伴和我在同一层。 十二月初，同事送了我一套沙发。几个人可以一起躺在沙发上玩游戏，很舒服。 但是今年三月 12 号楼上一个住户在早上六点多剁肉开始，我就因为能听到楼上时不时一声“咚”而没法好好入睡。这种声音特别大声，通过墙体传播，戴耳塞+隔音耳罩都没用。我到楼上问了一遍，都说没有做什么。其他小伙伴来我房间也能听到，我在他们房间也能听到。很奇怪。 六月初，同事离职，我换到他那间。其实就是从六楼搬到五楼。发现还是有那个声音，不过睡觉的时候没有怎么听到了，也可能跟我更晚睡有关。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:1:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"食 泊寓到期后的那段时间，因为没有地方煮饭，就先停下。大概从十月初开始，陆陆续续地把厨房用具买足，才恢复了做饭的生活。不过也基本是在周末做。 之前立了个目标：每周至少做一道没有做过的菜。一直保持到今年春节。 目前最大成就还是炒鸡肉能做得和我妈妈做的接近了很多，不过还是没她做的好吃。这个进步主要还是来自于把酒换成了我妈妈自己酿的酒哈哈。 印象比较深的一道菜还是炒花蛤，拍照效果不错，也很好吃。 尝试做了一次酸菜鱼。是那种调料都准备好，只需要自己把鱼肉切好就能做的。吃起来不比去外面吃的差。 今年三月底，公司组织了一次水煮鱼教学，我赶紧报名。主要的几个收获：片鱼片的方法、材料的顺序、花椒。之前还傻傻的把鱼片一片一片地夹出来放到锅里面。 雯雯推荐的张云雷泡面煮法，小伙伴吃了之后连连叫好。感觉我以后吃泡面都会这样煮了。哈哈哈。给雯雯点赞。 其他的还有紫薯泥、西红柿厚蛋烧、土豆焖饭、日本豆腐金针菇、汁烧酿茄子、蛋包肉、毛氏红烧肉。 后来因为买菜不方便，而且加班得比较多，就没有怎么做饭了。有几次是和小伙伴们一起，大家各自做一道菜，然后我们一起吃饭。 去年冬天的时候，偶尔会大家一起在宿舍吃火锅。最近几个月开始在宿舍做烧烤，小伙伴自己买了个电烧烤炉。烧烤的过程中，发现烧烤调料调出来的不好吃，这也使得我们发现了耗油这个神器。 按照惯例，还是经常去蛙小侠。 今年二月中旬，我买了个小冰箱。这几个月我妈妈偶尔会给我寄鸡肉鸭肉，让我自己炒鸡肉和炖鸭汤。最近还寄了她包的肉粽和做的包子。这样我早上就可以自己蒸包子吃。很好吃！ 说完菜，补个水果。五月下旬开始，开始比较经常地吃水果了，一周至少一次。相较于以前几个月吃一次，有很大进步。另外有时候吃饭，也会有水果。 吃了次油桃之后，就爱上它了。黄肉小西瓜也不错。不过比较常吃的还是哈密瓜。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:2:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"行 去年说要去日本玩，不过没有规划好，就没有去。不过先把护照办了。没想到办护照这么方便。 后来也没去哪。过年的时候去了一趟市里，见了不少高中同学。 母校 80 周年校庆回去了一趟算不算？ ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:3:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"娱 去年十一月，公寓旁边举办了个国际动漫节，我也跟着进去瞎逛。有很多 Coser。有个 cos 玩偶的，让我印象很深刻。 部门活动有一次去游戏室，我还是玩 VR 和体感游戏。VR 滑雪好有趣。体感游戏还是玩的水果忍者，但是这次大开脑洞：拿着刀不切，两把刀并在一起接水果，接了好几个再向上扔，然后一刀切掉~ 去看了几部电影：流浪地球、惊奇队长、复仇者联盟4、蜘蛛侠。 有段时间，小伙伴的同事把投影仪寄放在他那。我们几个就一起在房间里看日剧《轮到你了》。以后我也要买个投影仪！ 然后好像就没有什么值得一提的娱乐活动了。像和小伙伴游戏开黑这种，就比较没有什么可说的了。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:4:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"健 锻炼的跨度为从去年的 2 月底，到今年的三月底的 13 个月。 平均每两天一次。共运动 141 天，总运动时间为 3101 分钟。 一开始基本是热身和各种入门。然后是 HIT 做了几遍，还有有氧操。反正就是各种尝试。 从去年十一月初开始，基本上就都是徒手胸肌初级训练。别看是初级，可把我给累坏了。一开始一整套都做不完。直到今年一月初做第 20 次的时候，才能做完一整套，但还是很费力。 锻炼真的让人舒服，也有成就感，在一些需要用力的地方也能轻松一些。当时我还笑着说，这让我可以放心的加班而不用担心身体问题。另外体重曾一度跟最开始比降了 3.5 公斤，腹肌也开始展现，胸肌也练出来了。 从一月初开始就一直保持着做完一整套。二月初有一次隔了几天没做，就没做完整。直到二月底才可以不用很费力就能做完。三月初能比较轻松做完。然后三月底就停了。总共做了 46 次徒手胸肌初级训练。 以前早一点的时候，九点就开始做，晚一点也是十一点半。但后来越来越晚。直到三月底，加班完都十二点多了，也就不想做了。有时候会加到一点，也有一次加到两点。 有时候胸口会难受，自己对自己开玩笑说，这是不是就是猝死前的感受。不过机智的我都会在感觉不对劲的时候赶紧回去。最近得找个时间去做一次全身体检。 已经三个多月没有锻炼了。不过最近公寓旁边的体育馆开业了。这周五晚上我还和小伙伴们一起去游泳。农村人的我表示第一次看到游泳馆里面还有可以泡热水的地方。以后每周去游一次。 除了健身，也有其他跟健康相关的。 去年双十一那天，买了个电动牙刷。用了之后感觉很不一样，刷完很舒服。有时候牙疼，刷完之后就会好很多。 去年年底，会经常牙疼，特别是吃了花生之后。实在受不了，就去补牙了。医生说上下都有牙齿要补，我就觉得牙疼主要是下面，就先补下面。结果后来发现其实是上面的牙在疼。哈哈哈。 今年五月底，买了个华为手环。主要是记录睡眠时间和质量，也用来测心率。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:5:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"书 在刚开始搬到新宿舍到公司也搬过来这段时间内，由于路上坐公交有大量的时间，我就拿来看书。 不过看的主要是《领域驱动设计》，这段时间差不多把这部书看完。不过看完了也不知道如何将其应用到项目中。原因主要有两个：一是没有做笔记，二是没有看其他参考材料。可能在车上看也有影响。后来也有在网上查阅资料，增加理解。但还是领会不到其精神和做法。我决定再看一遍。 《人类群星闪耀时》看了：滑铁卢决定胜利的一瞬、飞越大洋的一句话、南极争夺战、逃向上帝。逃向上帝没看完，这一篇对于我现在的水平来说，还是很吃力。 二月初，利用吃饭的时间看完《杨早·简说中国史》。具体的细节除非提到，不然我现在也想不起来。不过“不同民族融合”我倒是记得。我总觉得我在看历史相关的文章时，少了什么东西。比如文章里的地理位置我就不清楚。还有一些时间线也没理清楚。我个人觉得历史很有趣，但是没有怎么花时间在这上面。得找个时间补一补历史基础了。 偶尔会看一两篇鲁迅的《热风》，不过不多。 最近主要在看张大春的《大唐李白·少年游》，看了五分之一。基本是在吃午饭和午休之前的时间看。 除了这些，平时比较多的是从公众号上面看一些技术文章，看完后觉得不错的，也会分享到微信的看一看。不过这类知识都比较碎片化。 看书这一块，相对于去年没有什么进步。还是看得太少了。看看我这次由《大唐李白·少年游》开始，能坚持多久。 除了文字，也有影视。不过这些算半娱乐性质的吧。 之前看 BBC 的《王朝》系列。动物世界也很有趣。看黑猩猩，帝企鹅，狮子，杂色狼，老虎。猩猩之间权力的斗争，还有示好的举动，都让人印象深刻。帝企鹅繁殖和养育小企鹅真是不容易，而且也有很残酷的抢其他母亲的小企鹅这种情节。小企鹅也好可爱，把我萌到了。不过当看到它们被困在冰坑里，甚至有些小企鹅被抛弃冻死的时候，也会猛男落泪。看到一只企鹅妈妈带着小宝宝艰难地一步步前进，终于爬上去的时候，真的让人很感动！ 最近看完 BBC 的《行星》系列，很震撼，也很有趣。有时候就像在看一部家庭剧。在讲到木星向内太阳系迁移的时候，会想象火星在“你不要过来啊”（动漫语气），想想就觉得好笑。不过更多的是感叹宇宙之大，人之渺小。同时也很敬佩和感谢那些探索宇宙的科学家们。能用探测器去了解太阳系的各个行星，并解读它们的历史，真是太厉害了。很期待以后有新的发现。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:6:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"物 推荐三种提升生活质量的东西。 一次性毛巾 一开始是我妈妈在家里使用，我用着也觉得很不错，也自己买了些。有个小伙伴一看很不错，也跟着买，结果就停不下来了。 这种还可以带到公司。想要洗脸的时候，抽一条去洗手间就能洗。 湿巾 好像是在部门的趣味运动会中得到的安慰奖，后来有一次出去吃饭，就带出去。 当时没注意包装上有一个专门的开口用来抽湿巾，于是整个撕开，只用了一张。其他的放着很快就干了。后来发现有个专门的口，就开始大量使用。 很常用的场景是：坐在电脑前面的时间一长，脸部容易变油。如果不想去洗脸，直接拿出一包湿巾抽一张出来擦脸就行了。 目前用的是心心相印的《我超迷你》湿巾。小包便携，出去吃饭也很容易带。 医用酒精棉片 我买的是 12cm*12cm 的，酒精浓度 75%。一开始用来给冰箱消毒用，后来发现还能用到其他地方。 比如擦鼠标和键盘。键盘上如果有油油的感觉，一擦就没掉。还可以擦手机。 去年发现我买的台式机只被我用来看视频，实在是太浪费。堂弟刚好想买台式机，就便宜点卖个他了。而我则把大学用的那台笔记本从家里带过来用，到现在都还很好用。过了一个月，我买了台低配 iPad，配上笔。超好用。感觉有笔和没有笔的 iPad 完全不一样。同事在我的安利下，也买了。（他们买的都是高配版 -.- 再装上微软的 RD Client，就可以远程连接到我宿舍电脑。然后买个带键盘的 iPad 保护套，就可以当做电脑用了。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:7:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"技 去年花了一百多从同事那里买了​块笔记本主板和电源。然后买了个红酒箱和一些工具，在十月初把主板装到红酒箱里面，搭了自己的 NAS。功率非常低，开机后保持在 15W 左右。 装上 CentOS 7 系统，再装个 Docker，然后装 Nextcloud。Nextcloud 很好用，除了网页访问外，Android 和 IOS 都有应用。 用 FRP 做内网穿透，用 Let’s Encrypt 生成证书，可以用 HTTPS 访问。 后来另外买了块大容量硬盘，想做​备份。但是备份方案还没确定，就​一直没动手。 另外还发现了 chocolatey。这个可以用来在 Windows 上用命令行安装软件。比如 choco install googlechrome。不过只是稍微玩一玩，并没有继续使用。 在 GitHub Pages 上面创建了博客​。用 Hexo 框架，结合 Travis CI，做到 git push 之后自动构建。这篇也是先在本地用 Markdown 编辑后，push 到 GitHub，然后​从 GitHub Pages 上复制到微信公众号文章编辑界面。样式都不用改，不过图片​是另外加上去的。​ 博客地址： https://schaepher.github.io 现在，我写东西可以发到三个地方。首先是比较随意的 GitHub Pages，然后是偏技术方面的博客园，还有微信公众号。 最近在服务器上搭了个 Code Server，就是网页版的 VS Code。最主要的目的是用来随时随地查看开源项目的源码。不过有了这个，我就能随时随地打开网页写东西了，比如这篇。写完之后直接 commit + push。这个比 Nextcloud 好点，而且因为是布到云上面的，所以不怕断电。 Code Server: https://github.com/cdr/code-server 上面这些，除了 FRP 有分享过，其他的还没写点东西分享。不过 GitHub Pages 的教程网上一大把，随便搜就有了。之前还说过要分享我搭 NAS 的这一套工具，我给拖到现在还没写。不过最近有在整理文件，可能过一阵子就会分享出来。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:8:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"情 （现在六个毒瘤里面，万年单身的有两个：我和大苏。大苏在他的 2018 年总结的时候说了脱单的愿景，然而现在还没找到女朋友 233 还有半年，加油） 其实我不太愿意谈情感方面的内容，不过说出来可能会得到不错的建议。而且最近也有跟同事说过我的看法，感觉说出来也无妨。 这是我首次将这类型的内容写出来发布到网上。尽量还原我的想法，但没法说全。 最近其他人跟我谈起这个话题，基本都是从“还好你没有女朋友”开始的。哈哈哈。我觉得这句话有理。 为什么这么说呢？因为从春节后开始，就进入了疯狂加班的状态。能沟通交流的时间不多，这要是有女朋友，估计也会很快分手吧。 也有人问，为什么你这么优秀还没有女朋友？我说我一点也不优秀，同一个工作岗位，能替代我的人太多了。 而且根据我自创的人品守恒定理（雾），在某一方面越优秀，在另外的一个方面就会越糟糕。只是因为这“另一方面”的影响范围的大小不一，导致有些别人不容易发现。 跟我关系很好的那几个小伙伴，交流多了，自然都知道我的那些问题。而其他人就会有一种我算是比较优秀的那种错觉。 自从 17 年九月底的一次尬聊后，我发现自己处理事情的能力非常欠缺。重要的事情不应该在微信这种文字聊天工具上说。今年春节也让我反思很多。 我曾经学其他小伙伴说“94 省内单身可加的 158 萌妹子”。让其他人一度认为我是因为标准太高。其实这些都是可选项，单身除外。 18 年后期，我才开始思考什么类型的女生适合我。我把过去喜欢的女生的优点都列出来，看看哪些可以去掉，哪些不能去掉。 就拿“萌”来说，去掉也无所谓。只要不那么无趣，也挺不错。 我觉得重要的是：认真。对自己认真，对生活认真。 对自己认真，一般来说就是不拒绝学习、总结和反思。偶尔看书，看的书的数量不少于我看的书（编程书除外），且其中一半的书的质量不低于我看的这些书，另一半看言情小说之类的也无所谓。参照上面的内容，就知道这个要求很容易达到。 还有一个加分项（程序员视角），就是在问别人问题的时候，不求直接得到答案，而是想知道方向，然后自己去查阅资料。在问问题前就有做一定准备，那就更吸引人了。 提问的智慧：https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/master/README-zh_CN.md 对生活认真，会做家务，会做饭，不是特别宅。 虽然家务和做饭我都会，由我承包也没关系。但是这两件事能拓展出很多乐趣，我觉得很重要。不要求做得比我好。不是特别宅这一点，就是会经常和别人面对面聊天。不拒绝出去旅游。 该笑就笑，该哭就哭，该生气就生气。虽然我也在努力的了解别人的情绪，但是我不能保证只看眼神就能了解一切。虽然我会经常反思自己哪些事情做错了，应该怎么改善，但是我不能保证我能察觉出自己所有的错误。但是有些缺点很难改，大缺点我一定改，小缺点尽量改。如果只会强忍而不表达，最终也只会伤害到双方。 对生活持至少不是悲观的态度，不经常地随便地以恶意揣测别人的想法，对于小损失不计较。有些小事一笑置之。 虽然我也会说“看脸”，但其实我看着舒服就行。不要求很漂亮，也不要求漂亮，这两个按别人的标准来看，不按照我的审美标准。 这么一梳理，这几个要求好像都蛮高的？我也深知自己想要追到满足这些要求的女生，还要不断地提高自己。 所幸我父母没有怎么催我找女朋友，我还可以花一些时间先提高自己。 ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:9:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":["Life"],"content":"总结 总的来说，这一年的生活没有实质性的提升。对阅读还不够重视，思想上认识到读书很重要，但却没有付诸行动。接下去一年重点提高阅读和写作。 饮食方面，学着做蛋糕。芒果千层、椰蓉奶冻都试试看。持续改善水煮鱼，做得更好吃一些。然后尝试水煮肉。 锻炼这方面每周去一次游泳馆游泳，或者做三次 Keep。先恢复徒手胸肌训练，再测试看看短板在哪，然后去提高。 希望明年的这个时候，能写出一份更让自己满意的回顾。 加油鸭！ ","date":"2019-07-13","objectID":"/2019/07/13/second-year-after-graduation/:10:0","tags":["Life","After Graduation"],"title":"毕业两年啦","uri":"/2019/07/13/second-year-after-graduation/"},{"categories":null,"content":"在经过了一年半的项目开发之后，今年年初我开始负责我们项目的重构工作。虽然是叫重构，但实际上项目已经无法重构，只能重写。以下都称之为重写。 在这之后的半年时间，我犯了很多错误。而这些错误实际上大多数都是可以避免的。我现在都不好意思说我曾经学过软件工程了。 我会分成项目管理和所涉及的技术这两个方面来写。这一篇先讲项目管理，后续技术部分再分几次写。 ","date":"2019-07-05","objectID":"/drafts/project_refactoring/:0:0","tags":null,"title":"项目重构（重写）的问题","uri":"/drafts/project_refactoring/"},{"categories":null,"content":"项目介绍 这是一个强业务弱技术的流程项目。原先项目涉及到的技术点非常少，进不到几个类，都是面向过程。整个项目都是业务的堆砌。 没有专职前端。因为基本都只需要配置流程里的表单信息，且很多都是简单的文本框，所以都是在后端人员需要的时候自己写。原先框架允许开发人员快速配出简单的表单。 这是一个 PHP 项目，整个项目的代码量大致为 57万 行。 ","date":"2019-07-05","objectID":"/drafts/project_refactoring/:1:0","tags":null,"title":"项目重构（重写）的问题","uri":"/drafts/project_refactoring/"},{"categories":null,"content":"人员 在定计划的时候，今年上半年将会有三个开发，下半年会再加入两到三个。所有开发人员中，只有一个前端，其余后端。 在最初定计划的时候，我就 ","date":"2019-07-05","objectID":"/drafts/project_refactoring/:2:0","tags":null,"title":"项目重构（重写）的问题","uri":"/drafts/project_refactoring/"},{"categories":null,"content":"编译时命令 所有以 # 开头的命令都是预处理命令，在编译时处理。 ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:1:0","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":null,"content":"定义 ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:1:1","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":null,"content":"#define 用于替换 #define X1 X2 将所有出现 X1 的地方替换为 X2。 #define 是可覆盖的，以最近一次出现的定义为准。 纯替换 函数替换 替换为字符串 ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:1:2","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":null,"content":"条件编译： #ifdef 出现在 server.c 的 main() 里面。 #ifdef XXXX // codes #endif 如果定义了 XXXX，则编译这段代码，否则忽略。 ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:1:3","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":null,"content":"\u003cstring.h\u003e ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:2:0","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":null,"content":"strcasecmp(str1, str2) 出现在 server.c 的 main() 里面。 忽略大小写的字符串比较。 若参数 str1 和 str2 字符串相同则返回0。 str1 长度大于 str2 长度则返回大于 0 的值，str1 长度若小于 str2 长度则返回小于 0 的值。 ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:2:1","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":null,"content":"“zmalloc.h” 封装了多平台的内存申请。 zstrdup() 复制字符串。strcpy() 只有遇到 \\0 才会结束，容易内存溢出。 ","date":"2019-05-28","objectID":"/2019/05/28/c_basic_in_redis/:3:0","tags":null,"title":"Redis 用到的 C 语言","uri":"/2019/05/28/c_basic_in_redis/"},{"categories":["Redis"],"content":"服务端 首先找到服务端主入口，在 server.c 文件的 int main(int argc, char **argv)。 首先定义了时间变量 tv，类型是结构体 timeval，来源于 #include \u003csys/time.h\u003e。 #ifdef 是编译用的，如果满足条件就编译；否则不编译。 设置本地化参数。 设置 zmalloc （申请内存的工具，代替 malloc） 内存溢出的错误处理函数。 srand() 用来设置 rand() 产生随机数时的随机数种子 gettimeofday 给 tv 变量设置当前时间。 用 hashseed 设置 Dic Set 的 Seed 检查服务器启动参数是否开启了哨兵模式，并设置服务器的哨兵模式。 ","date":"2019-05-27","objectID":"/drafts/redis_read/:0:0","tags":null,"title":"阅读 Redis 源码","uri":"/drafts/redis_read/"},{"categories":["Redis"],"content":"初始化服务器配置 设置时间到 server 结构体，缓存起来。 变量 配置常量 配置 默认值 作用 runid 无 configfile 无 NULL 配置文件路径 executable 无 NULL 可执行文件路径 timezone 无 设置时区。如果是 LINUX，则使用 time.h 的 timezone。否则使用 gettimeofday 获取时区。 hz 无 设置 serverCron() 调用频率，单位 hz（hertz）。默认十次/秒。 dynamic_hz CONFIG_DEFAULT_DYNAMIC_HZ 客户端设置 hz 变量。 arch_bits 无 CPU 位数 port CONFIG_DEFAULT_SERVER_PORT 设置端口 tcp_backlog CONFIG_DEFAULT_TCP_BACKLOG bindaddr_count 无 0 绑定地址个数 unix_socket 无 NULL unix_socketperm CONFIG_DEFAULT_UNIX_SOCKET_PERM 0 权限 ipfd_count 无 ip 文件描述符数量 sofd 无 -1 socket 文件描述符 protected_mode CONFIG_DEFAULT_PROTECTED_MODE 1 根据配置设置保护模式，不接收外部连接 gopher_enabled CONFIG_DEFAULT_GOPHER_ENABLED 0 不知道是啥 dbnum CONFIG_DEFAULT_DBNUM 16 设置数据库个数，用于存储 key-value verbosity CONFIG_DEFAULT_VERBOSITY LL_NOTICE 设置日志是否记录更多内容 maxidletime CONFIG_DEFAULT_CLIENT_TIMEOUT 0 客户端最大空闲时间（超时时间） tcpkeepalive CONFIG_DEFAULT_TCP_KEEPALIVE 300 TCP 连接保活时间 active_expire_enabled 无 1 active_defrag_enabled CONFIG_DEFAULT_ACTIVE_DEFRAG 0 active_defrag_ignore_bytes CONFIG_DEFAULT_DEFRAG_IGNORE_BYTES 100«20 active_defrag_threshold_lower CONFIG_DEFAULT_DEFRAG_THRESHOLD_LOWER 10 active_defrag_threshold_upper CONFIG_DEFAULT_DEFRAG_THRESHOLD_UPPER 100 active_defrag_cycle_min CONFIG_DEFAULT_DEFRAG_CYCLE_MIN 5 active_defrag_cycle_max CONFIG_DEFAULT_DEFRAG_CYCLE_MAX 75 active_defrag_max_scan_fields CONFIG_DEFAULT_DEFRAG_MAX_SCAN_FIELDS 1000 proto_max_bulk_len CONFIG_DEFAULT_PROTO_MAX_BULK_LEN 21211*1024*1024 协议桶最大长度 client_max_querybuf_len PROTO_MAX_QUERYBUF_LEN 102410241024 客户端最大查询缓冲区长度 saveparams 无 NULL 为RDB保存点数组 loading 无 0 是否正在从磁盘加载数据 logfile CONFIG_DEFAULT_LOGFILE \"\" 日志文件路径 syslog_enabled CONFIG_DEFAULT_SYSLOG_ENABLED 0 使用系统日志 syslog_ident CONFIG_DEFAULT_SYSLOG_IDENT “redis” syslog_facility LOG_LOCAL0 16«3 daemonize CONFIG_DEFAULT_DAEMONIZE 0 是否放后台运行 Redis supervised 无 0 supervised_mode SUPERVISED_NONE 0 AOF(Append Only File)，持久化方案之一。将每一个数据操作记录到文件中。体积过大时会对其进行重写。 官方文档： https://redis.io/topics/persistence 变量 配置常量 配置 默认值 作用 aof_state AOF_(ON|OFF|WAIT_REWRITE) 0(AOF_OFF) 是否开启 AOF aof_fsync CONFIG_DEFAULT_AOF_FSYNC 2(AOF_FSYNC_EVERYSEC) 或者 0(AOF_FSYNC_NO) 或者 1(AOF_FSYNC_ALWAYS) aof_no_fsync_on_rewrite CONFIG_DEFAULT_AOF_NO_FSYNC_ON_REWRITE aof_rewrite_perc AOF_REWRITE_PERC aof_rewrite_min_size AOF_REWRITE_MIN_SIZE aof_rewrite_base_size 无 aof_rewrite_scheduled 无 aof_last_fsync 无 aof_rewrite_time_last 无 aof_rewrite_time_start 无 aof_lastbgrewrite_status aof_delayed_fsync 无 aof_fd 无 aof_selected_db 无 aof_flush_postponed_start 无 aof_rewrite_incremental_fsync CONFIG_DEFAULT_AOF_REWRITE_INCREMENTAL_FSYNC aof_load_truncated CONFIG_DEFAULT_AOF_LOAD_TRUNCATED aof_use_rdb_preamble CONFIG_DEFAULT_AOF_USE_RDB_PREAMBLE aof_filename CONFIG_DEFAULT_AOF_FILENAME RDB(Redis Database)，持久化方案之一。将 Redis 在内存中的数据建立一份快照（Snapshot），存储到文件中。 官方文档： https://redis.io/topics/persistence 变量 配置常量 配置 默认值 作用 rdb_save_incremental_fsync CONFIG_DEFAULT_RDB_SAVE_INCREMENTAL_FSYNC 1 增量存储数据。不是一次性把所有数据准备好再写入文件，而是每生成 32MB 数据，就写一次文件。 rdb_filename CONFIG_DEFAULT_RDB_FILENAME “dump.rdb” RDB 保存的文件名 rdb_compression CONFIG_DEFAULT_RDB_COMPRESSION 1 是否对数据启用压缩 rdb_checksum CONFIG_DEFAULT_RDB_CHECKSUM 1 是否开启校验和 ","date":"2019-05-27","objectID":"/drafts/redis_read/:1:0","tags":null,"title":"阅读 Redis 源码","uri":"/drafts/redis_read/"},{"categories":null,"content":"minio Minio是一个非常轻量的对象存储服务。 Github： minio 它本身不支持文件的版本管理。如果有这个需求，可以用 s3git 搭配使用。 Github： s3git ","date":"2019-03-05","objectID":"/2019/03/05/minio/:0:0","tags":["MinIO"],"title":"轻量对象存储服务——minio","uri":"/2019/03/05/minio/"},{"categories":null,"content":"安装 minio 文档有列出各平台的安装方式。这里只说 docker 的方式。 docker-compose.yml version: \"3\" services: minio: image: minio/minio volumes: - minio-data:/data ports: - \"9080:9000\" environment: MINIO_ACCESS_KEY: minio MINIO_SECRET_KEY: minio123 command: server /data volumes: minio-data: 将上面内容保存为 docker-compose.yml 文件。然后在这个文件所在的文件夹内执行 docker-compose up -d。minio 服务就启动了。 ","date":"2019-03-05","objectID":"/2019/03/05/minio/:1:0","tags":["MinIO"],"title":"轻量对象存储服务——minio","uri":"/2019/03/05/minio/"},{"categories":null,"content":"minio 界面 服务启动后，访问 http://127.0.0.1:9080 进入登录界面： 输入上面设置的 access key：minio 和 secret key：minio123，登录。 图中 1 是上传一个文件；图中 2 是创建一个 bucket （储存区）。 文件必须上传到某一个存储区里面，因此必须先创建一个 bucket。 文件上传后，一旦选择文件，就会在顶部出现删除和下载的操作按钮。 在 Laravel 里使用 ","date":"2019-03-05","objectID":"/2019/03/05/minio/:2:0","tags":["MinIO"],"title":"轻量对象存储服务——minio","uri":"/2019/03/05/minio/"},{"categories":null,"content":"配置 引入包 composer require league/flysystem-aws-s3-v3 修改 config/filesystems.php ... 'cloud' =\u003e env('FILESYSTEM_CLOUD', 'minio'), ... 'disks' =\u003e [ ... 'minio' =\u003e [ 'driver' =\u003e 's3', 'endpoint' =\u003e env('MINIO_ENDPOINT'), 'use_path_style_endpoint' =\u003e true, 'key' =\u003e env('MINIO_ACCESS_KEY_ID'), 'secret' =\u003e env('MINIO_SECRET_ACCESS_KEY'), 'region' =\u003e env('MINIO_DEFAULT_REGION'), 'bucket' =\u003e env('MINIO_BUCKET'), ], ... ] 修改 .env FILESYSTEM_CLOUD=minio MINIO_ENDPOINT=\"http://127.0.0.1:9080\" MINIO_ACCESS_KEY_ID=minio MINIO_SECRET_ACCESS_KEY=minio123 MINIO_DEFAULT_REGION=cn-north-1 MINIO_BUCKET=刚创建的bucket名称 ","date":"2019-03-05","objectID":"/2019/03/05/minio/:3:0","tags":["MinIO"],"title":"轻量对象存储服务——minio","uri":"/2019/03/05/minio/"},{"categories":null,"content":"尝试 打开 tinker php artisan tinker 存储 Storage::cloud()-\u003eput('hello.json', '{\"hello\": \"world\"}'); 结果：true 取出 Storage::cloud()-\u003eget('hello.json'); 结果：{“hello”: “world”} ","date":"2019-03-05","objectID":"/2019/03/05/minio/:4:0","tags":["MinIO"],"title":"轻量对象存储服务——minio","uri":"/2019/03/05/minio/"},{"categories":["Learning Docker"],"content":"Dockerfile 的官方文档：https://docs.docker.com/engine/reference/builder/ 1 示例 当你想要一个镜像，但它没有办法满足你的所有要求时，就得在它的基础上做一些定制化的修改。此时就得用到 Dockerfile 。 你有两种选择：一种是获取这个镜像的原始 Dockerfile 文件，并在上面修改，从头开始构建镜像；另一种是直接指定已存在的镜像，并添加内容。 例如 schaepher/frps 这个镜像： FROM alpine:latest ENV FRP_VER 0.20.0 ENV FRP_FULL_VER frp_${FRP_VER}_linux_amd64 RUN cd /root \\ \u0026\u0026 wget https://github.com/fatedier/frp/releases/download/v${FRP_VER}/${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 tar vxzf ${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 rm ${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 mv ${FRP_FULL_VER} frps \\ \u0026\u0026 rm -rf ./frps/frpc* ./frps/frps.ini WORKDIR /root/frps CMD [ \"./frps\", \"-c\", \"./frps.ini\" ] 由于 frp 自身不支持在读取配置文件的时候解析环境变量，因此无法在运行 Docker 镜像的时候指定环境变量来完成配置。 有四种方式可以选择，最常用的一种就是写个 Dockerfile 把 frps.ini 文件复制到 /root/frps/ 这个文件夹。如下所示： FROM schaepher/frps COPY frps.ini /root/frps/frps.ini 写完 Dockerfile 后，要把自己写好的 frps.ini 放到这个 Dockerfile 所在的文件夹。然后执行下面这个命令来构建一个新的镜像： docker build . -t frps 记住这个 frps 镜像，后面会用到 2 Dockerfile 解释 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:0:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.1 基于哪个基础镜像 —— FROM 定义：FROM \u003c镜像\u003e 用于指明在哪个镜像的基础上继续添加镜像层。 例如第一个 Dockerfile 是基于 alpine:latest 这个镜像，并在构建后生成 schaepher/frps 这个镜像。 接着第二个 Dockerfile 是基于 schaepher/frps 镜像，再继续构建生成 frp 这个镜像。 我们可以通过下面这个命令来查看这个镜像的修改历史： $ docker history schaepher/frps IMAGE CREATED CREATED BY SIZE COMMENT ed08390ad317 4 months ago /bin/sh -c #(nop) CMD [\"./frps\" \"-c\" \"./frp… 0B \u003cmissing\u003e 4 months ago /bin/sh -c #(nop) WORKDIR /root/frps 0B \u003cmissing\u003e 4 months ago /bin/sh -c cd /root \u0026\u0026 wget https://gith… 8.89MB \u003cmissing\u003e 4 months ago /bin/sh -c #(nop) ENV FRP_FULL_VER=frp_0.20… 0B \u003cmissing\u003e 4 months ago /bin/sh -c #(nop) ENV FRP_VER=0.20.0 0B \u003cmissing\u003e 5 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B \u003cmissing\u003e 5 months ago /bin/sh -c #(nop) ADD file:25f61d70254b9807a… 4.41MB Dockerfile 中的每个指令都会创建一个新的镜像层。 其中最底下两层是 alpine:latest ，再往上就和第一个 Dockerfile 所执行的一样了。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:1:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.2 环境变量 —— ENV 定义：ENV \u003cvarname\u003e \u003cvalue\u003e 用于设置环境变量。也可以同时设置多个： ENV \u003cvarname1\u003e=\u003cvalue1\u003e \u003cvarname2\u003e=\u003cvalue2\u003e... 如果进入 schaepher/frps 这个镜像的容器执行 env 这个命令，就能看到 ENV 设置的环境变量： ~/frps # env ...省略无关内容... FRP_VER=0.20.0 FRP_FULL_VER=frp_0.20.0_linux_amd64 当然，ENV 用在这里并不是很合适。因为实际上这两个 ENV 只在构建的时候用到，后续不会再需要。因此可以把 ENV 改为 ARG ，ARG 的变量不会在容器中出现。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:2:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"构建时用的环境变量 —— ARG 定义：ARG \u003cvarname\u003e[=\u003cdefault value\u003e] 这意味着如果有值，必须用等号连接变量名和值，且不能多个。 值是可选的，因为这可以在构建的时候，加上下面这个参数来指定： --build-arg \u003cvarname\u003e=\u003cvalue\u003e 如果填写了默认值，也可以通过这个选项来覆盖默认值。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:2:1","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.3 基于基础镜像执行命令 —— RUN 定义：RUN \u003ccommand\u003e 用于执行 shell 命令。 在第一个 Dockerfile 中有如下内容： RUN cd /root \\ \u0026\u0026 wget https://github.com/fatedier/frp/releases/download/v${FRP_VER}/${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 tar vxzf ${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 rm ${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 mv ${FRP_FULL_VER} frps \\ \u0026\u0026 rm -rf ./frps/frpc* ./frps/frps.ini 每一行后面的 \\ 用于分行 为什么写成多行的呢？上面有提到 Dockerfile 中的每个指令都会创建一个新的镜像层 ，如果拆成很多个 RUN 的话，会有很多层。而层数是有限制的，例如默认的 overlay2 这种 Stroage Driver，最多支持 128 层。 但是层数不是问题最大的地方。之前提到过，就算你在上一层删了下一层的文件，这个文件仍然会存在。如果你在下一层安装了一些编译工具，这些文件就会一直存在于这一层。这会导致镜像的大小比所需的还大。 回到上面的 RUN 命令，这个命令在最后把一些不再需要的文件删除，这样就减少了这一层的大小。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:3:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.4 sh 默认工作目录 —— WORKDIR 定义：WORKDIR \u003c/path/to/workdir\u003e 用于指定进入镜像的容器后所在的文件夹。 如果指定 WORKDIR /root/frps，那么当用户进入 schaepher/frps 的镜像后，执行 pwd 会得到以下结果： ~/frps # pwd /root/frps ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:4:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.5 容器启动时的默认命令 —— CMD 定义：CMD [\"executable\",\"param1\",\"param2\"] 用于指定启动镜像时默认执行的命令。 CMD [ \"./frps\", \"-c\", \"./frps.ini\" ] 上面这一行其实就是去执行： ./frps -c ./frps.ini 下面这一条是新建并启动容器时的命令格式： docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 如果指定了 COMMAND ，则会覆盖 Dockerfile 里 CMD 指定的内容。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:5:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"默认的可执行文件或脚本 —— ENTRYPOINT 定义：ENTRYPOINT [\"executable\", \"param1\", \"param2\"] 与 CMD 相似。 CMD 和 ENTRYPOINT 可以同时存在，此时 CMD 变为 CMD [\"param1\",\"param2\"] ，也就是 CMD 的内容都会作为 ENTRYPOINT 的参数。 例如： ENTRYPOINT [ \"./frps\" ] CMD [ \"-c\", \"./frps.ini\" ] 也是执行 ./frps -c ./frps.ini。 从上面对 docker run 的解释，结合有 ENTRYPOINT 之后 CMD 的作用，可以这样使用： 在 Dockerfile 里面这样写： ENTRYPOINT [ \"./frps\" ] 然后 docker run -it frps -c ./frps.ini。 如果换做是一次性执行的应用，就可以让镜像变成命令一样使用。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:5:1","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.6 复制宿主机文件到容器 —— COPY 定义：COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc_dir1\u003e... \u003cdest_dir\u003e 用于将 Dockerfile 所在文件夹内的文件或文件夹复制到镜像内。镜像内目录如果不存在，会自动创建。 源路径可以使用通配符。 文件的元信息会被保留，但可以通过 --chown 参数修改权限。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:6:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"解压宿主机压缩包到容器 —— ADD 定义：ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc_dir1\u003e... \u003cdest_dir\u003e 在添加压缩文件时使用，会自动解压。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:6:1","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.7 ONBUILD 定义：ONBUILD [INSTRUCTION] 当该镜像作为其他镜像的基础镜像时，执行 INSTRUCTION 指定的内容。在作为多个类似镜像的基础镜像时很有用处。 对于第二个 Dockerfile 中的内容： FROM schaepher/frps COPY frps.ini /root/frps/frps.ini 可以将 COPY 写到第一个 Dockerfile： FROM alpine:latest ENV FRP_VER 0.20.0 ENV FRP_FULL_VER frp_${FRP_VER}_linux_amd64 RUN cd /root \\ \u0026\u0026 wget https://github.com/fatedier/frp/releases/download/v${FRP_VER}/${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 tar vxzf ${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 rm ${FRP_FULL_VER}.tar.gz \\ \u0026\u0026 mv ${FRP_FULL_VER} frps \\ \u0026\u0026 rm -rf ./frps/frpc* ./frps/frps.ini WORKDIR /root/frps ONBUILD COPY frps.ini /root/frps/frps.ini CMD [ \"./frps\", \"-c\", \"./frps.ini\" ] 这样第二个 Dockerfile 就简化成： FROM schaepher/frps 没错，就这么简单的一行！ ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:7:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.8 容器执行时的用户 —— USER 定义：USER \u003cuser\u003e[:\u003cgroup\u003e] 或者 USER \u003cUID\\\u003e[:\u003cGID\u003e] 用于切换到指定用户。会影响 Dockerfile 这一行之后的命令。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:8:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.9 数据卷声明 —— VOLUME 定义：VOLUME \u003cpath\u003e 用于防止用户忘记挂载目录导致动态文件被写到容器存储层。 这个命令将指定 path 挂载到匿名卷。 在执行 docker run 的时候可以用 -v 参数将其覆盖。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:9:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.10 端口声明 —— EXPOSE 定义：EXPOSE \u003cport\u003e [\u003cport\u003e/\u003cprotocol\u003e...] 用于让镜像使用者知道这个镜像用到了哪几个端口，方便镜像使用者配置端口映射。 EXPOSE 实际上不会开启端口。 ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:10:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":["Learning Docker"],"content":"2.11 为镜像添加说明 —— LABEL 定义：LABEL \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e ... 用于说明镜像的一些信息。例如维护者、版本、镜像描述等等。 如果有空格，需要加引号。例如： LABEL name=\"a b\" 如果不包含空格，可以不加引号。 原来还有个 MAINTAINER \u003cname\u003e 用于指定维护者，但现在已经被弃用了，应该使用下面这个来代替： LABEL maintainer=\u003cname\u003e ","date":"2018-12-09","objectID":"/2018/12/09/dockerfile/:11:0","tags":["Docker","Dockerfile"],"title":"Dockerfile","uri":"/2018/12/09/dockerfile/"},{"categories":null,"content":" https://apereo.github.io/cas/development/images/cas_flow_diagram.png ","date":"2018-10-10","objectID":"/2018/10/10/cas/:0:0","tags":null,"title":"CAS","uri":"/2018/10/10/cas/"},{"categories":["RESTful"],"content":"本文内容主要参考 Github 上的 NationalBankBelgium/REST-API-Design-Guide 项目以及 MDN https://github.com/NationalBankBelgium/REST-API-Design-Guide/wiki/HTTP-Status-Codes https://developer.mozilla.org/zh-CN/docs/Web/HTTP ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:0:0","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"2xx Success 成功 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:1:0","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"201 Created 当 POST 成功时返回。并且头部要带上 Location ，其值为该资源的地址。 例如： Request： POST /apples Response： HEADER： Location: /apples/1 场景： 资源创建成功 误用的场景： 暂无 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:1:1","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"202 Accepted 当服务端无法立即返回处理结果的时候使用。 例如：耗时操作，批量操作，删除操作无法立即返回 头部需要带上 Location， 其值为获取该结果的地址。 如果请求 Location 指向的地址时，资源还没有处理完毕，则仍然返回 202。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:1:2","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"204 No content 当删除操作能够立即返回时使用。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:1:3","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"3xx Redirection 重定向 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:2:0","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"301 Moved permanently 本次请求的资源已经被关联到新的 url 上，客户端以后的请求都应该向新的 url 发送。可以用于 API 的版本管理。 新的 url 会放在 Header 的 Location 。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:2:1","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"303 See other 服务端已经处理完客户端的请求，但是响应体并不包含任何内容。因为服务端认为客户端很可能不需要这些内容。 在响应的头部中 Location 带有这些内容的 URL ，客户端如果需要，可以向该 URL 发起请求。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:2:2","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"304 Not modified 客户端已经拥有所请求资源的最新版本。当客户端用 GET 或 HEAD 发起请求，且可以使用缓存的内容时。 请求的要求： 在 Header 中带上 If-None-Match 或 If-Modified-Since 参数 响应的要求： 响应体应为空 场景： 条件请求 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:2:3","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"4xx Client Error 客户端错误 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:0","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"400 Bad request 请求异常、请求是不完整的时候使用。或一次请求有多个错误时使用。 这要求客户端不能重复发送该请求，应该在修改请求后再发送。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:1","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"401 Unauthorized 客户端没有登录，但请求需要登录才能获取的数据或操作。 客户端应该在请求头加上服务端要求的身份信息。如 token。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:2","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"403 Forbidden 客户端有登录，但没有访问或修改该资源的权限。 如果某个资源存在，但是不能让没有权限的客户端知道该资源存在，则应该使用 404。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:3","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"404 Not Found 请求的资源不存在。 当客户端请求资源的集合，但资源不存在时，不应返回 404，而是 200 加上空数组。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:4","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"405 Method Not Allowed 客户端使用了不被允许的 HTTP 方法（GET POST PUT PATCH DELETE）。 场景： 该资源尚未支持这种 HTTP 方法 不允许使用这个操作 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:5","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"406 Not Acceptable 无法按照客户端所要求的响应体格式生成数据。 请求的要求： Header 必须带上 Accept 参数，用于表示使用哪种格式。例如 Accept: application/json Header 可以带上 Accept-Language 参数，用于表示使用哪种语言。例如简体中文： Accept-Language: zh_CN;q=0.8 场景： 不支持 Accept 指定的格式 不支持 Accept-Language 指定的语言 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:6","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"422 Unprocessable Entity 请求没有问题，但由于业务上不满足条件，不给予处理。用于业务检查。 ","date":"2018-10-08","objectID":"/2018/10/08/restful-http-code/:3:7","tags":["RESTful"],"title":"RESTful API HTTP Code","uri":"/2018/10/08/restful-http-code/"},{"categories":["RESTful"],"content":"REST = REpresentational State Transfer ","date":"2018-10-08","objectID":"/2018/10/08/restful/:0:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"逐步接近 REST Martin Fowler 在博客《steps toward the glory of REST》中提到 https://martinfowler.com/articles/richardsonMaturityModel.html 简单的说： level 0 ： 没有资源的概念。url 上面最后一个部分经常是个动词。例如：POST /appointmentService 。 甚至只有一个 HTTP 头部不涉及业务逻辑，相当于降级为传输层协议。 level 1 ： 有了资源的概念，在 url 上面使用名词。但只用到了两个 web 操作：GET、POST。 当发生错误时，仍然返回 200 OK 并附上错误信息。例如某个资源已被其他人创建（医生的预约名额），这是一个不相容（incompatible）的资源，你无法再创建。 level 2： 除了 POST 之外，还使用更多的操作：DELETE、PUT、HEAD、OPTIONS、PATCH 出现错误时，不再使用 200 OK。例如上面的错误会返回 409 Conflict ，不用带上错误信息调用方也能知道是什么错误。 当资源创建成功时，不再返回 200 OK，而是返回 201 CREATED 。 这样就让 HTTP code 有更多的表达空间。 level 3： 返回的结果不仅包含了所请求的信息，还包含了该资源其他操作的链接。 http://olivergierke.de/2016/04/benefits-of-hypermedia/ 客户端在请求的时候，从这些链接中取出想要的链接，并执行操作。客户端不应将这样的链接硬编码到代码里面，否则将来接口变动会很麻烦。 如果没有这些信息，它不能叫做 REST API，只能称之为 REST-like API。 https://github.com/NationalBankBelgium/REST-API-Design-Guide/issues/8 不能把所有操作的链接都暴露出来，而是根据用户的访问权限暴露出他所能访问的所有连接。 https://github.com/NationalBankBelgium/REST-API-Design-Guide/issues/9 ","date":"2018-10-08","objectID":"/2018/10/08/restful/:1:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"PUT 和 POST 的区别 https://stackoverflow.com/a/630475 两者都可以用来创建资源，但是场景不同。 POST 让服务端决定资源的名称 POST /questions 服务器返回这样的链接： /questions/\u003cquestion_id\u003e PUT 由客户端决定资源的名称 PUT /questions/\u003cnew_question\u003e 服务器返回这样的链接： /questions/\u003cnew_question\u003e PUT 是幂等的。这意味着执行多次 PUT 的结果都一致。但是如果用 POST，就会产生很多个资源。 例如：x=5 是幂等的，这个赋值运算执行一百次， x 都是等于 5。但 x++ 不是幂等的。 https://stackoverflow.com/a/2691891 除了 PUT ， GET 和 DELETE 也是幂等的。 PUT 可以用于替换已有的资源。而如果是部分更新，则使用 PATCH。 用 POST 还是用 PUT，主要还是看是否要求幂等。 其他的可以去看 RFC 7231。 http://www.rfcreader.com/#rfc7231 2014 年 RFC 7230 - 7235 将原来的 RFC 2616 拆成六个单独的协议说明。 https://www.infoq.cn/article/2014%2F06%2Fhttp-11-updated http://www.rfcreader.com/#rfc7230 The fundamental difference between the POST and PUT methods is highlighted by the different intent for the enclosed representation. The target resource in a POST request is intended to handle the enclosed representation according to the resource’s own semantics, whereas the enclosed representation in a PUT request is defined as replacing the state of the target resource. Hence, the intent of PUT is idempotent and visible to intermediaries, even though the exact effect is only known by the origin server. POST 和 PUT 之间的根本区别突出在封装表征（enclosed representation，即请求体）具有不一样的目的。 POST 请求中的目标资源根据资源自身的语义处理封装表征，而 PUT 请求中的封装表征则是用于替代目标资源的状态。因此， PUT 的目的是幂等和对中介可见，即使确切的影响只有服务端知道。 ","date":"2018-10-08","objectID":"/2018/10/08/restful/:2:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"无状态 RESTful API 必须是无状态的。这意味着服务器不保存客户端的会话信息，而是由客户端来保存。 例如：JWT（JSON Web Token） https://en.wikipedia.org/wiki/JSON_Web_Token 服务端有一串密钥，将用户的登录信息加密，生成密文，并发给客户端。客户端在请求的时候，将密文放到 HTTP Header。服务端用密钥解密客户端发来的密文。如果无法解密，则说明这个密文不是由服务端的密钥加密的，是伪造的；如果能够解密，则表示该密文是由服务端的密钥加密的。 如果服务端有多台机器，那么每台机器只要用相同的密钥加密，其他机器就能解密。这样就能愉快地使用负载均衡了。 ","date":"2018-10-08","objectID":"/2018/10/08/restful/:3:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"url 形式 如果一个资源的名称由超过一个单词组成，那么单词间用减号（-)连接。例如：/contract-types。 资源总是用复数命名，并且每个资源都有一个 ID 当 url 带 ID 时，获取的是单个资源；不带 ID 时，获取所有资源（或者说想要获取）。 ","date":"2018-10-08","objectID":"/2018/10/08/restful/:4:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"ID 在URL上使用 UUID ，但数据库里面的主键还是要用自增 ID。 https://www.zhihu.com/question/43500172/answer/95903244 理由是： MySQL 的 InnoDB 对主键采用聚集索引。如果使用 UUID 作为主键，索引插入的位置是乱序的，会导致索引树节点的频繁分裂。同时由于相邻主键的数据存放在磁盘同一个或者相邻的页中，也会导致页的分裂，影响 IO。 InnoDB 的非主键索引叶子节点存储的是主键值。 UUID 比自增 ID 所占的存储空间大，使用存储占用大的主键会导致非主键索引的总体空间变大。 ","date":"2018-10-08","objectID":"/2018/10/08/restful/:5:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"资源间的关系 如果资源 A 必须依赖于资源 B，那么应该设计成这样： GET /tickets/12/messages 表明 messages 必须依赖于 tickets ","date":"2018-10-08","objectID":"/2018/10/08/restful/:6:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":["RESTful"],"content":"名词 URL 搜索：Search 执行：Execution 批量执行： Bulk 批量的另一种：例如创建一个删除的资源 POST /xxx/delete-requests Delete multiple records using REST https://stackoverflow.com/questions/21863326/delete-multiple-records-using-rest ","date":"2018-10-08","objectID":"/2018/10/08/restful/:7:0","tags":["RESTful"],"title":"RESTful 程度","uri":"/2018/10/08/restful/"},{"categories":null,"content":" ref: https://docs.stackstorm.com/overview.html ref: https://docs.stackstorm.com/reference/ha.html ","date":"2018-10-08","objectID":"/2018/10/08/stackstorm_0/:0:0","tags":null,"title":"Stackstorm","uri":"/2018/10/08/stackstorm_0/"},{"categories":null,"content":" 以下用 st 表示 Stackstorm。 可以到 st 官方网站查看 API 文档。 https://api.stackstorm.com/ 如果你已经学过命令行的使用，可以加上 --debug 参数来输出与命令等价的 curl 及输出。 st 默认用 Nginx 来处理 API 请求，默认端口为 9101 。API 的地址为：https://$ST2_HOSTNAME/api。 ref: https://docs.stackstorm.com/reference/ha.html st 会默认生成自签名的证书。 ","date":"2018-10-08","objectID":"/2018/10/08/stackstorm_api/:0:0","tags":null,"title":"Stackstorm API","uri":"/2018/10/08/stackstorm_api/"},{"categories":["Development"],"content":"https://stackoverflow.com/questions/20342058/which-uuid-version-to-use version 3/5： 根据某个命名空间和名称生成，名称相同则会产生相同的 UUID。可用于设置用户的 UUID（跟用户名唯一绑定）。一般用 version 5。 version 4: 获取随机数。用得最多的一种。会重复的概率几乎可以忽略不计。 https://en.wikipedia.org/wiki/Universally_unique_identifier#Collisions “除非你每秒生成十亿个 UUID，并且持续一个世纪” ","date":"2018-10-08","objectID":"/2018/10/08/uuid/:0:0","tags":["UUID"],"title":"UUID","uri":"/2018/10/08/uuid/"},{"categories":["Development"],"content":"同时使用 UUID 和自增 ID 接口提供以 UUID 为条件的查询方式。自增 ID 作为后端特定的场景使用，接口不提供这个参数，避免被顺序遍历。 ","date":"2018-10-08","objectID":"/2018/10/08/uuid/:1:0","tags":["UUID"],"title":"UUID","uri":"/2018/10/08/uuid/"},{"categories":["Development"],"content":"分布式 UUID 一次生成多个 ID，放到 Redis 里面。需要 UUID 的时候，从 Redis 里取。这样可以加快速度。 ","date":"2018-10-08","objectID":"/2018/10/08/uuid/:2:0","tags":["UUID"],"title":"UUID","uri":"/2018/10/08/uuid/"},{"categories":["Development"],"content":"分布式顺序 ID 用数据库。数据库只有一列自增 ID。一次插入多行，获取这些行的 ID，放入 Redis。 ","date":"2018-10-08","objectID":"/2018/10/08/uuid/:3:0","tags":["UUID"],"title":"UUID","uri":"/2018/10/08/uuid/"},{"categories":null,"content":"安装地址：C:\\Users\\Username\\AppData\\Local\\Programs ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:0:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"截图 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:1:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Snipaste 下载地址：https://dl.snipaste.com/win-x64-cn 每次有人看到我使用贴图功能的时候，都会问我这是什么软件，他也想下载一个用。 最让我喜欢的功能就是贴图了。主要用于三个场景： 将可能忘记的事情贴到屏幕上，由于贴图会展示在屏幕最顶层，就会一直看到，避免忘记。 有时候想把另一个软件里展示的内容输入到另一个软件，但不方便切换窗口，就可以把内容截出来托到一边。 对展示的内容想做对比的时候，可以截多张图，放在一起。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:1:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"文件管理 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:2:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Everything 下载页面：http://www.voidtools.com/downloads/ 根据文件名搜索硬盘里的文件，速度很快。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:2:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"AnyText 下载页面：https://anytxt.net/download/ 根据文件内容搜索硬盘里的文件。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:2:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Q-Dir 下载地址：http://www.softwareok.com/?Download=Q-Dir\u0026goto=../Download/Q-Dir_Installer_x64.zip 多窗口文件管理。整理文件的时候用。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:2:3","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"下载 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:3:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Free Download Manager 下载页面：https://www.freedownloadmanager.org/download.htm ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:3:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"网页 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:4:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Chrome 下载地址：https://www.google.cn/chrome/thank-you.html?statcb=1\u0026installdataindex=empty 离线包列表：https://api.shuax.com/tools/getchrome ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:4:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"MicroSoft Edge（Chromium 版） 下载地址：https://www.microsoftedgeinsider.com/zh-cn/download/ 用了一段时间的 Windows 版和 Android 版，感觉已经非常好用了。刚安装的时候还会将 Chrome 已安装的插件自动在 Edge 上安装。 以前觉得不好用是因为当时收藏夹的数据会丢失，现在没有这个问题。收藏夹数据可以多端同步（不像 Chrome 需要科学上网）。 Linux 版本：https://www.microsoftedgeinsider.com/zh-cn/download/?platform=linux 购物党（插件） 在京东、淘宝等购物网站上添加一个对比价格的一块区域。我主要用来查看某个商品的历史价格。 I’m a Gentleman（插件） 鼠标点图片拖拽的时候，会将该图片下载下来。省的右键点击+图片另存为。 Tampermonkey（插件） 这是一个脚本管理器。你可以从 https://greasyfork.org/zh-CN 之类的网站下载脚本来执行。 比如百度网盘界面获取不限速的下载链接的脚本。 Vimium（插件） 如果经常在 Linux 上使用 Vim ，那么可以尝试安装这个插件，在 Chrome 上使用 Vim 快捷键浏览网页。 Octotree（插件） Github 目录树。用于浏览源代码。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:4:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"文档编辑 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"NotePad++ 下载页面：https://notepad-plus-plus.org/download/v7.5.8.html 代替系统自带的文本编辑器 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Virtual Studio Code 下载地址：https://code.visualstudio.com/docs/?dv=win64user 文本编辑器增强版。可当作 IDE 使用。 Markdown Python、Go、PHP … ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Vim 命令行下的文本编辑。原先是在 PowerShell 里执行一些操作的时候，懒得用其他软件打开，直接 Vim 比较方便。 替代： 如果有安装 VSCode，可以在命令行执行 code 要打开的文件。 例如：code C:\\Windows\\System32\\drivers\\etc\\hosts 注意：要确保 VSCode 安装目录/bin 这个路径在系统环境变量 Path 里面。 如果有安装 NotePad++，可以在命令行执行 notepad++ 要打开的文件。 例如：code C:\\Windows\\System32\\drivers\\etc\\hosts 注意：要确保 NotePad++ 安装目录 这个路径在系统环境变量 Path 里面。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:3","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Office 365 购买页面：https://products.office.com/zh-CN/compare-all-microsoft-office-products?tab=1 替代： LibreOffice： 免费、开源，经常被安装在 Linux 发行版中。 下载：https://mirrors.cloud.tencent.com/libreoffice/libreoffice/stable/7.0.1/win/x86_64/LibreOffice_7.0.1_Win_x64.msi ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:4","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Evernote 笔记 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:5","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Amazon Kindle 阅读 Kindle 上的文件 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:6","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Calibre Kindle 格式转换 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:7","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Pandoc 文档格式转换 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:8","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"XMind 思维导图 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:5:9","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"影音 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"PotPlayer 下载地址：https://t1.daumcdn.net/potplayer/PotPlayer/Version/Latest/PotPlayerSetup64.exe ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"网易云音乐 下载地址：https://music.163.com/api/pc/download/latest ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"CRadio 听广播 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:3","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"GifCam Gif 录制工具 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:4","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"rayying图片压缩 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:5","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Bandicam 下载地址：https://dl.bandicam.cn/bdcamsetup.exe 屏幕录像 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:6","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Boilsoft Video Splitter 下载页面：https://www.boilsoft.com/videosplitter/ 快速截取视频 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:7","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Format Factory 视频、音频、图片格式转换 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:8","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Ulead GIF Animator Gif 编辑 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:6:9","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"磁盘管理 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:7:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"DiskGenius 下载页面：http://www.diskgenius.cn/download.php ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:7:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"垃圾清理 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:8:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"软媒魔方 下载页面：http://mofang.ruanmei.com ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:8:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"程序员 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"IntelliJ IDEA 下载地址：https://www.jetbrains.com/idea/download/download-thanks.html?platform=windows Go Java JavaScript, TypeScript Kotlin PHP Python Ruby Scala SQL ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"CLion C C++ ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Git 下载地址：https://git-scm.com/download/win 代码/文档版本管理 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:3","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Xshell 下载页面：http://www.netsarang.com/download/down_form.html?code=622 连接 Linux ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:4","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Fiddler 下载地址：https://telerik-fiddler.s3.amazonaws.com/fiddler/FiddlerSetup.exe 网络抓包 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:5","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Postman 下载页面：https://dl.pstmn.io/download/latest/win64 发送 HTTP 请求 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:6","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Frp 下载页面：https://github.com/fatedier/frp/releases 内网穿透 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:7","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Docker 下载页面：https://download.docker.com/win/stable/Docker%20for%20Windows%20Installer.exe ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:8","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Navicat 数据库管理 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:9","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Disk2vhd 将当前 Windows 系统打包成虚拟机文件 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:9:10","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"工具 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Logitech Options 下载页面：https://support.logitech.com.cn/zh_cn/software/options 罗技键盘 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"UltraISO 将 iso 镜像写入到 U 盘 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"f.lux 保护眼睛 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:3","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"USB Network Joystick 手柄 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:4","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"AutoHotkey 快捷键 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:5","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"Chocolatey 类似于 Linux 里的包管理工具。可以用命令行安装 Windows 软件。 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:10:6","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"聊天 ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:11:0","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"微信 下载地址：https://dldir1.qq.com/weixin/Windows/WeChatSetup.exe 官网：https://pc.weixin.qq.com/ ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:11:1","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"QQ Tim 下载地址：https://qd.myapp.com/myapp/qqteam/tim/down/tim_pc.exe ","date":"2018-09-02","objectID":"/2018/09/02/windows-softwares/:11:2","tags":["Windows 软件"],"title":"Windows 好用的软件","uri":"/2018/09/02/windows-softwares/"},{"categories":null,"content":"NPM npm config set registry https://registry.npm.taobao.org 换回来： npm config set registry https://registry.npmjs.org 如果只想生效一次： npm --registry=https://registry.npm.taobao.org install xxxxx ","date":"2018-08-12","objectID":"/2018/08/12/china-mirror/:1:0","tags":["NPM","Docker","Alpine"],"title":"一些国内镜像源设置","uri":"/2018/08/12/china-mirror/"},{"categories":null,"content":"Docker 编辑文件： C:\\Users\\chensf1\\.docker\\daemon.json 在 registry-mirrors 里添加 https://registry.docker-cn.com，如下： {\"registry-mirrors\":[\"https://registry.docker-cn.com\"],\"insecure-registries\":[], \"debug\":true, \"experimental\": false} ","date":"2018-08-12","objectID":"/2018/08/12/china-mirror/:2:0","tags":["NPM","Docker","Alpine"],"title":"一些国内镜像源设置","uri":"/2018/08/12/china-mirror/"},{"categories":null,"content":"alpine sed -i 's/http:\\/\\/dl-cdn.alpinelinux.org/https:\\/\\/mirror.tuna.tsinghua.edu.cn/' /etc/apk/repositories ","date":"2018-08-12","objectID":"/2018/08/12/china-mirror/:3:0","tags":["NPM","Docker","Alpine"],"title":"一些国内镜像源设置","uri":"/2018/08/12/china-mirror/"},{"categories":["Life"],"content":"从毕业到现在，一年过去了。趁着刚写完《入职一年啦》，也来回顾回顾这一年的生活吧。 现在跟同事谈起的时候，他们还会感到羡慕。因为无论是刚毕业还是后来换地方，我们几个同学都住在一起。 我们部门里就有四个新人是同一个学院的，其中三个是同班同学，我就是其中一个。 ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:0:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"住 在我们入职前就说再过不久公司就要搬到其他地方。嗯，到现在还没开始。 我还记得我们几个同学一起去找房子，找了一天没什么进展。我还记得那天下午我爸妈要去找我外婆玩，他们来接我一起去。后来他们继续找，在泊寓订了下来。我和其中两个同学住三房一厅的套房，剩下三个人各自租单间，其中有一个租了 1700 的单间（算上水电费）。 套房感觉挺好的，我们是第一批住户，一切都很新。有个厨房可以让我们学做饭。在这年代，做饭这种基础技能还是要有的。我们自己买了个冰箱，可以放菜或者其他需要保鲜的东西。 平时我们会在客厅里聊天、玩游戏、看电视。一般晚上会在这吃完饭，有时候会一起吃火锅。 上周因为一年的合同到期了，我们搬到离公司更近的民房。两人住一间，每人 400，对比一下上面的 1700 看看？相当于我那同学一个月省了 1300 左右啊！暂时先这样住着，可能会等公司搬了我们再一起搬过去。 ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:1:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"食 刚入职那会儿，没有多少事情做，没有加班。每天下班就买点菜回到宿舍做晚饭。 最开始只有一个会炒菜，有厨师证！不过倒没从他那学到什么就对了，因为没做过什么高大上的菜。后来我们几个都会做一些家常菜。 比如几乎天天吃的炒包菜。这个真是会让人吃出阴影……几乎每餐都有。每个人炒出来的都有自己的特色。他们都不喜欢加味精，导致有时候盐加多了很难吃。这时得安利个： 味精对人体健康有危害吗？ - 知乎 。 除了包菜，我偶尔会炒个西蓝花，做道韭黄炒蛋、番茄炒蛋、茄子等等。很家常对吧！ 除了这种特别家常的，还是要学点有特色的菜。比如我有一道非常喜欢吃的菜，那是我妈炒的鸡腿肉。有一次回家就去跟她学，后来这道菜成为了我们宿舍的特色菜。因为同公司也有一个新入职的同事住在我们楼下，有时会邀请她过来吃饭，我们也去过她们那（也是三房一厅）吃饭。两边手艺比起来，自然是我们这边 low 了很多，只能靠我的炒鸡腿肉撑撑场面。吃过都说好（噗）。哈哈哈。 后来有了高压锅，也会经常买五花肉回来做。 我偶尔会买龙骨来煲汤，算是加餐。他们都不喜欢喝汤，夏天的时候总是买饮料来喝。我倒是会比较经常煮点什么，有时候是紫菜蛋汤，有时候是云吞，有时候连汤圆都煮！ 我个人蛮喜欢吃咖喱饭，大概每周会做一次。以前以为咖喱很难做，其实买盒咖喱块，再买点鸡胸肉、萝卜、土豆就可以开始做。真是简单便宜又好吃！ 早餐也还算丰盛。一开始是煮粥，自配的八宝粥。有几次出现某些配料没熟，根本没法吃。后来干脆就不加那种配料了，哈哈。中间有段时间是买包子来蒸，通常我们会买三种口味的包子，奶黄包是必备的（吃到怕为止），其他两种会换：豆沙包、紫薯包、肉包、小馒头、杂粮馒头、云吞……后来博饼博到豆浆机，每天早上又能加杯豆浆，美滋滋。过一段时间又换回粥，配上榨菜或者腌萝卜。吃了一段时间又换回包子。哈哈。 除了正餐，还会做点其他的。比如自从博饼拿到了个烤箱，我们就开始做蛋糕吃。不过做了几次就没做了，一是经常加班，二是懒（这个才是最主要的！）。还有，因为有冰箱，我买过冰淇淋粉来做冰淇淋。 ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:2:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"行 公司有上班班车，我们那是首站。不过从住的地方到站点，快一点的话要走十分钟。班车八点半出发，有时候起晚了（经常），得跑着过去赶班车。我们美其名曰：晨跑。 有时候赶上了，心里美滋滋；有时候，班车在人行道那头，我们在这头，眼睁睁地看班车开走；更绝望的是，绿灯了，我们到了那头，车走了。 从我们那到公司坐班车要半个小时，早上比较堵。有时候加班比较晚，打的回去只要十几分钟。上周搬到公司附近，走路慢点也只要大概二十分钟。 中间有段时间是自己开车上班的哈哈。大概 2017 年九月底报考了驾校，十一月初考科一，2018 年一月初终于预约上科二，中旬考完科二，月底考科三，二月初考科四。终于赶在过年前拿到驾照~ 练科二的那段时间，每天六点多起来做早饭，吃完去坐半小时公交到练车点。从快八点开始练到九点，然后走路到公司。当时就因为练车点离公司近才选了它。教练也挺不错的。 回家之后就开始直接开了，第一次是和爸妈一起去外婆那。夜晚加上山路，挺稳。就是我爸坐在副驾驶座一路上怕怕的（他怕），抓着手刹没放松过。那一次右边留空比较大。后来我专门找时间到人少的地方练练距离感，感觉一下右车轮的距离。 很快春节结束，我直接开两个多小时的车到工作的地方。这个时候开得还是特别小心谨慎的那种，毕竟还不太稳。当我两个月后开车回家，就快了很多。 两个月间，每次上班都开车。再也不用晨跑了。但是比较不好的是下班的时候，快累死了还得开车！而我的伙伴们坐在后面休息。o(╥﹏╥)o 中间有一次和公司其他人一起开车去赏樱花，好几辆车一起过去。但没有同时到，也没一起回来，哈哈。来回路上七八个小时，赏花只赏了半个小时。挺累的。 后来开回家就没有再开过来了。感觉还是不如坐班车上班和公交车回去爽。而且停车费惊人：小区一个月 300，公司这边一个月 260。对刚出来工作的萌新来说，还是蛮贵的！ ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:3:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"娱 大部分娱乐挺单调的，就是几个人坐在客厅开黑，有时候不开黑就自己在房间里看动漫。 此外，有上面提到的赏樱花。还有一起去游戏室体验 VR 游戏，Xbox，还有打桌球。我们楼下也有桌球，有一阵子比较经常去，后来就没去过了。 有一次突然想去外面走走，就骑着小黄车到处逛。后来遇到大雨，就赶紧回去了。 周末会约在同城的老同学出去吃饭，或者看电影什么的。 有一次和舍友坐动车回学校，去见了栋哥，还有老同学。栋哥还是挺忙的，不过还是抽出一些时间出来跟我们聊。下次还要找个时间去找他。 龙眼成熟那时候，还去同事家摘，摘了好几袋。带了一袋子回宿舍和舍友一起吃。 部门有一次组织去打沙滩排球。我的排球技术还是不错的。想当初高中时候，在实践中锻炼的技术可以打得过不少人。可惜那时候只有女子排球赛，我们只能当陪练。那时候一个对她们六个完全没问题，哈哈。 还有其他的一些活动。这么算起来，娱乐活动也还算蛮丰富。 ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:4:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"健 从 18 年春节后，我就开始锻炼身体了，下载了个 keep 跟着做。两天锻炼一次，都是在客厅里面做。到了夏天，每次锻炼都会出一身汗。直到现在还保持着。不过这一两周少锻炼了两次，要警惕，不能把这习惯断掉。 锻炼身体开始前去体检了一次，大部分都正常。就血脂和尿酸偏离正常值，但都是轻微。最近公司的体检已经开始了，我打算再过一两个月去体检。有锻炼身体以及比较注意饮食，应该会好很多！ ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:5:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"书 心痛！心痛啊！知道看书很重要的我，还是没有怎么去看书。重视程度不够高。 这一年间，我把《红顶商人胡雪岩》听完了。嗯，是听。大学的时候下载了喜马拉雅，那时候就开始断断续续地听这个。基本上是走在路上的时候听。出来工作后，我在路上也会听。有时候回家要两三个小时，可以听好几集。王刚讲的是真的好！通过声音能够把每个人物的性格表现出来。然而听完没有写什么听后感，这是不足的地方。 后来在喜马拉雅上买了吴军老师的《文明之光》音频版，也听完了。补了一波历史。貌似温吞的声音好听！很适合这种类型的内容。还是没写听后感。 打开多看阅读，发现除了大四看完的《异类》和《褚时健传》。其他的诸如《美国种族简史》和《人类群星闪耀时》都只看了一小部分。Kindle 上的《What if?》也只看了 6%。 技术类的书籍也没怎么看。大部分阅读都是在看各种英文技术文档吧。 前一段时间学了点日语，后来停了一段时间，最近又恢复了。打算国庆去一趟日本。 得想办法把读书的习惯培养起来。还有多写点东西。 春节那会儿，还跟周老师和邹老师说要多写博客。几个月过去了，都没什么动作…… ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:6:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":["Life"],"content":"结 就先这样。今天两篇博客先写上，以后争取多发写博文。以前总觉得有些东西太过于简单，发出来岂不是贻笑大方？现在觉得虽然不是什么高级的东西，但聊胜于无。 ","date":"2018-07-14","objectID":"/2018/07/14/first-year-after-graduation/:7:0","tags":["Life","After Graduation"],"title":"毕业一年啦","uri":"/2018/07/14/first-year-after-graduation/"},{"categories":null,"content":"从入职到现在已经过了一年，然而博客几乎没有更新。本来毕业时和过年前后是打算写点总结什么的，写着写着就没有继续。 但不写点回顾的话，心里总感觉难受。以后要是想回忆一下当初都做了啥，结果没想起什么，感觉都是在浪费时间，这就不好了。 这几天刚好是我们这批新人入职满一年，脑中“写点什么回顾一下吧”的想法频繁浮现。索性就把周末时间拿来写这篇博客了。 ","date":"2018-07-14","objectID":"/2018/07/14/working-1/:0:0","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"公司、部门 在同班同学的安利下，来到现在就职的这家公司面试。现在我们在同一个小组。 当初是冲着说不用加班来的（然而后来却经常加班，虽然是自找的）。此外，待遇好也是非常吸引我的地方~ 同事之前的氛围很好。大家人都挺好的。此外我们老大（我们都叫他大大~）经常会请我们吃下午茶，有时候会组织部门聚餐，上周六还一起去看了《我不是药神》。小组的聚餐也组织过几次。前面说过，这几天我们这批新人入职满一年，大家会买下午茶请部门的同事。于是我们就这样吃了一周下午茶，哈哈哈。在这样的部门里面，感觉很幸福~ 也有不好的地方。我所在的小组就编程而言，我的水平是最高的了，其他技术方面的也差不多。这也就意味着在这个小组里面，没有大腿可以抱，只能通过自学来提高自己。自学的进步速度比不上有人带的速度。这也是我经常感到压力的原因——进步速度太慢了。 所幸的是，从去年十二月开始，我们部门有同事组织起了知识分享会，成立了学习小组。每周五下午会抽出两个小时的时间给知识分享会。期初是针对特定议题，大家各自学习，并在会上针对议题讲述所学内容或发表看法。后来则是由每个人自定主题，然后上台演讲。主题的范围很广，非技术相关的例如：时间管理、如何做好演讲、团队合作等；技术相关的例如：代理服务、分布式、存储组件等；不知道怎么分类的例如：人工神经网络介绍（我讲的），三维地图重建，密码学入门等。 总的来说，能学到东西，但还是不够。通常只有讲师学到的东西最多，其他人能学到多少，还是要看讲师讲得如何。像我讲的人工神经网络，虽然全程都是有画图讲解的（完全没有PPT），但是到了后半部分往深讲的时候，还是有一半同事懵圈了。 ","date":"2018-07-14","objectID":"/2018/07/14/working-1/:1:0","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"项目 一年间接触的项目总共有三个，姑且称作项目 A B C。用的语言都是 PHP。 主要参与的项目是 A ，是一个用 PHP 写的至少有十年历史的任务系统（是不是有点可怕？）。 项目 B 是参与了其中一个模块的重构，但重构得不彻底。因为这次重构的目的主要是加个流程的流转引擎进去。 项目 C 是个聊天机器人，用到了人工神经网络。然而我并没有负责核心部分（泪）。 项目 A 放后面讲，另外两个内容比较少。 ","date":"2018-07-14","objectID":"/2018/07/14/working-1/:2:0","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"项目 B 项目用的是 Zend Framework 框架，1.10.8 版本，2010 年 8 月发布的。现在最新版本是 3。 旧代码很少用到框架已经写好的东西，还有不少东西是框架已经写好，但还是自己写的。应该只是为了用上 MVC 和部分功能吧。 和其他模块的重构是把原来的代码拷一份，然后在上面修修补补的方式不同。我基本重写了所有代码。把业务相关的东西抽出来，简化复杂的逻辑，把大块的代码拆解成几个部分。然而这是有代价的，我为此加班了好多天。不过最后还是按期交付了。如果不重写的话，我可以不加班也能完成同样的功能，但是过不了自己这一关。自己经手过的代码，让它们保持以前那种糟糕的状态，实在不能接受。 不过我也知道自己这样做的风险。从公司的角度讲，自然是项目越快完成越好。万一要是因为我想把代码写好一些，导致项目进度变慢，那问题就大了。虽然在不少地方我克制了冲动，但还是经常忍不住去重构代码。幸运的是，项目的进度要求都没有紧张到没有时间来做优化。这也是受到公司氛围的影响吧~ ","date":"2018-07-14","objectID":"/2018/07/14/working-1/:2:1","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"项目 C 一开始冲着机器学习来的，兴冲冲地加入到项目里面，想着学点有深度的东西。但果然还是太年轻了。自身并没有相关的技术积累，想通过加入项目来提高技术的想法，是不是太幼稚了呢？ 在这个项目组里面，到目前为止我都是负责后台的简单工作。例如提供知识库的录入界面等各种界面及其配套的后台逻辑代码，大部分工作都是前端相关。后端框架用的 EasySwoole ，因为涉及到通信，需要用 swoole 扩展。不过我这一部分的工作跟这个框架其实没什么关系。前后端接口用的 Restful API 标准，但是 GET 查询的支持不完善。 其他同事有的负责模型训练，有的是负责通讯模块。虽然原理我懂不少，但没有实践。后面我们会交换工作内容，我到时争取换到模型训练去。 说到我在这个项目所做的事情，可以引出一个话题： 前端 VS 后端 其实我只想好好地做一个后端开发人员。嗯！前端是要了解一点，但是也不必要这么早吧。等这次做完，以后再有其他前端的事情，我都推掉。这次没有早点推掉，把自己给坑了。 待我成为后端大佬，需要拓展技术面的时候再去多了解和实践前端。 ","date":"2018-07-14","objectID":"/2018/07/14/working-1/:2:2","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"项目 A 由于是主要参与的项目，所以可以说的东西就比较多了。这是一个流程系统，用于 CDN 相关的运维操作。包括机器上下架，故障登记、发起部署等流程。涉及了公司几乎所有主要业务相关系统。 事先说明，这是个内部系统，用户量不会大到哪去，但使用频率很高，而且跟绝大部分提供给用户的线上的机器有关。 前面说过，这个项目的历史有十年以上。想象一下十年前的开发吧，现在流行的几个框架中，最早的是在 2007 年左右发布第一个版本。没错，也就是说这个项目没有用到任何框架。最常见的 MVC 也是不存在的，基本所有东西都写在一起，面向过程。据说这个项目的开发人员基本都是从运维转到开发，包括目前组内的几个成员都是从运维转到开发的。 不过神奇的是，我参与的这部分（也是整个系统最有业务价值的部分）可以说是隔离的还不错。前辈们已经把前端相关的东西封装好了，只需要到配置界面配置一下，就能组合成一个简单的表单界面。剩下的就是写后端部分，你可以把它看成是一个接近纯 API 形式的系统，最终只要 return 一个 json 字符串就行了。这点实在是令人佩服。这也为我接下来的改造提供了便利，因为在整个请求中，会有一个地方成为主入口，只要在这个地方做个拦截，把部分请求重定向到另一个位置，最后返回的时候是一个符合规定的 json 字符串就行。 这个系统的开发面临几个问题： 代码管理方式落后 本地环境搭建复杂 单个代码文件过大 难以测试 维护成本高 没有文档 据说在我来之前的一年，已经有说要重构项目了，但一直没有行动。因为维护本身就要花去很多时间，再加上需求一直做不完。这周才又开始提起这件事，因为组内目前有几个同事，他们负责的部分没有需求了。而且我在最近的半年间也做了不少事情，对他们也会有帮助。 接下来说说我怎么逐个解决上面那几个问题吧。 1. 代码管理方式落后 原本可以说是没有什么管理方式。事实上我们改代码基本都是直接到线上用 vim 改（导致我现在 vim 几个快捷键用得 66 的）。一旦保存，就会自动生成一份备份文件到指定的备份目录。然后我们会有代码审核，要调用 linux 的 diff 比较两个文件的不同，然后截图发到讨论组让其他人审核。有时候改的地方多，要截图好几屏幕。 连 svn 都没有，更不用说 git 了。 今年三月，我开始推动代码管理方式的变化，改成用 git 管理代码。当然，老大是支持滴。我做了几件事： 准备工作 找其他组了解到我们这边内部有搭了一个 Gitlab，于是到上面建了个仓库。 线上代码整份备份，拷贝一份到我本地。把线上代码目录下，一些没用的东西都移动到一个备份文件夹。例如文件名为：！或者`这一类奇奇怪怪的文件；还有一些开发者个人的备份文件、测试文件等等，统统移出去。这些在移动之前都会把列表发出来，让其他人确认过才移动。 找到程序运行时产生的文件夹，这些是要放到 .gitignore 里面的。 原本超过 100G 的项目文件夹，只有大约 80 MB 的有用文件。大部分都是 log。这 80 MB 的文件里面，还有可以排除掉的，但可能有危险，就算了。这样以后要搭建新环境的时候，只要下载 80 MB 的文件就行了。之后把这些文件提交到 Gitlab 仓库上。 代码同步 如果使用 Gitlab ，那么要怎么把代码同步到线上机器呢？最简单的就是在线上目录下直接用 git pull 了，只是要记得把 .git 目录屏蔽掉，不然别人直接把你 .git 下载过去，就能得到你整套代码了。当然，也可以在另外一个目录 pull ，然后用 rsync 同步到线上目录。后一种方式会比较安全，不过没有采用，可能是因为要支持线上修改代码？ 线上修改代码是为了在上线代码后出现问题时能够立即在线上修改代码，然后不能影响到后续代码上线。要补充一点，这个项目没有定期发布版本的概念，一旦完成需求，就立即上线。既然是这样，就不能直接 git pull 。于是我写了个脚本专门用来同步。当线上代码和要更新的代码有冲突时，自动处理冲突——其实就是自动用冲突部分的线上代码覆盖上线的代码。 培训 上面准备工作做完了，就得向组内其他人推广了。组内只有我那同学有接触过git，但也是那种 push 和 pull 的程度。这次用的是 GitHub Flow 的方式，就是 Pull Request + 审核，没有人可以直接 push 代码上去。 我总共组织了三次培训，每次半个小时到一个小时。把 git 的基本使用和 GitHub Flow 的方式都介绍了。然后还写了篇教程。之后大家就开始用起来。 前后大概一个月的时间，前期准备花去大概三周（毕竟还有需求要做）。培训前后跨度一周。培训完的下一周开始使用。算起来应该要从四月初开始算起，到目前为止合并了 760 个 Pull Request。 我们组四月份的时候来了个新人。当后来谈起 git 的时候，我说了在三月底才逐渐开始使用 git 以及以前是怎样管理代码的时候，她都惊呆了。 2. 测试机 让我们回到还没使用 git 的时候。 上面说了修改代码时通常要在线上修改，为什么不在测试机上修改呢？ 测试机代码跟线上代码有很多不同的地方。数据库也有很多不同。我那个同学就曾经栽在数据库表字段不统一上。 我们的系统依赖于其他系统提供的数据，在测试机上难以获取所需数据，虽然他们也有提供测试机，但不好找到合适的数据。 不过我也有一段时间是在测试机上修改，然后复制到线上。但是 vim 编辑虽然挺熟悉的，但还是不舒服。后来就下载 PHPStorm ，在本地电脑上修改，然后通过它内置的 ftp 传到测试机上测试。 为啥不在本地搭建个环境呢？这其实也挺困难的。 代码有很多依赖，甚至于有些代码写的是用 linux 的路径。你在 windows 下根本没法跑。 把系统跑起来的数据库表要先建好，数据要先准备好，但不好找到需要哪些表。 第一点还好解决，毕竟有虚拟机呀。第二点就比较麻烦，全部数据都同步的话，要几十个 G 吧。 于是本地搭建环境这点就先搁置了。要等到接下来说的这件事做完才有继续的可能。 3. 调试 先说之前的开发是如何调试代码的。首先肯定是在线上调试，调试的手段是 echo var_dump print_r 这些，直接把数据打印到界面上。是的，你没看错。用户都能看到他们打印出来的信息。 大量的数据暴露给用户，而且我之前还看到把一个接口请求的 key 暴露出来。更惨的是，这是已经调试完，没有删掉的。也就是说存在相当长的一段时间。事实上代码里面有大量的这种输出信息没有删除。所幸直接用户都是公司内部员工，这要是外部人员使用，早炸了。 对于开发者而言，如果是自己写的代码，echo 看一下可能很快就找到问题。但如果是别人写的代码，可能要 echo 一大堆才知道问题在哪。更可怕的是，这如果是系统基础层面的问题，根本不知道在哪找 bug，那代码一坨坨（来自同事的描述）的。 有个比较好的地方是，这个系统有备机。我在备机上装了个 Xdebug，这才使得断点调试成为可能。当然，为了支持多人能同时调试，还去装了个 dbgp。这些在我之前的博客中有写了：php+xdebug+dbgp远程调试（多人）。当时发到博客园首页，还被管理员踢掉哈哈哈。 此后我都是使用 IDE 来断点调试，这才能知道系统到底是怎么运行的。 4. 本地环境 能够断点调试意味着我能够知道它连接了哪些数据库，查了哪些表。这样就可以开始做配置了。 为了方便以后新搭建系统，我还写了个半自动初始化的脚本。把要修改的配置组织好，然后转化成原系统可用的数据。还提示了要把系统跑起来，需要同步哪几个表格。但是碰到路径依赖的代码时，仍然要再去手动修改代码，然后重试。 这种方式持续很长一段时间，直到我最近接触 Docker 才得到解决。我在 Windows 10 上面装了 Docker。然后对 PHP 镜像做了定制，其实就是额外装了 xdebug 、 composer（PHP 的包管理工具）、一些项目会用到的 php 插件。 经过一些配置的调整，总算是跑起来了。效果还不错~不过还有几个地方需要调整和添加的，后续再继续处理。 5. 单个文件太大 看以下几个数字： 19264 17622 13223 12160 6621 6067 4655 这些是几个主要文件的行数，最大一个将近两万行了。用 PHPStorm 打开后有点吃力，解析要好一会儿。 里面其实是很多个 if-elseif-else 组合成的，每个分支完成一件事情。其实可以把它们看为一个个 function，事实上把它们抽取为 function 也能正常执行。最大的一个文件由大概 200 个分支组成，平均每个分支 100 行代码。但是也有不少分支有 5~6 百行代码，3 百行的 foreach 循环了解一下？ 后来我做了个改变。前面有提到，它在某一个地方会有一个统一的入口，然后也有一个统一的出口。我另外建了个文件夹，然后建了一个文件作为新的入口，把满足条件的请求转发到这个入口。就像是进入一个子系统。 这个子系统可以直接使用 PHP 的一些特性，例如命名空间。在里面写代码直接用面向对象的方式，只要最后返回的结果是 json 就行了。这样可以为以后的重构（其实是重写）做准备，提前写好一些工具类，积累一些需要注意的点。 在实现上面，没有直接使用开源框架。一个是编码问题，我们项目是 GBK 编码（其实也不是很难解决）；另一个是以后项目重写的成本问题。目前不用框架，把骨架搭建起来还是蛮快的。在一些实现上面会去参考开源框架，像 Yii2 和 Laravel ，也会往 PSR 上面靠。 这样单个文件的大小就变小了，变成多个小文件","date":"2018-07-14","objectID":"/2018/07/14/working-1/:2:3","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"结束 你可能会问，哪来时间做这些事情？不用做需求吗？我说一个数据。我们加班是没有加班费的，但是有一个误餐补贴。每天加班到八点半后就有 20 块钱，而我上个季度的补贴是 1k+。 这一年间做的事情看起来不是很多，也不是很少，唯一能确定的是学到的东西不多。这是最让我忧虑的地方。这一点之前我们部门的一个大佬跟我聊的时候有说过，孤军奋战进步很慢。其实部门里面的高手也是有的，但我不善于使用已有资源这一点是硬伤，要想办法有所突破。 希望接下去能有更多进步的机会~我也会去多争取这样的机会。 ","date":"2018-07-14","objectID":"/2018/07/14/working-1/:3:0","tags":null,"title":"入职一年啦","uri":"/2018/07/14/working-1/"},{"categories":null,"content":"目录 服务器上安装 XDebug 及配置 客户端 PHPstorm 配置 浏览器安装插件 ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:1:0","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"服务器上安装 XDebug 及配置 ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:2:0","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"XDebug 安装 略 ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:2:1","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"配置： 打开 php.ini 配置文件： vim /etc/php.ini 在最后加上以下内容： [Xdebug] zend_extension=\"/usr/lib64/php/modules/xdebug.so\" xdebug.remote_enable=1 xdebug.remote_host=\"客户端IP地址\" xdebug.remote_port=\"客户端开启的端口\" 端口可以自己选，例如选择 5566 端口。 设置完毕后，重启 web 服务。 注：这种方式不支持多人调试，是因为 remote_host 只能填一个 IP 地址。如果需要让团队内其他人也可以调试，参考： php+xdebug+dbgp远程调试（多人） ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:2:2","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"客户端 PHPstorm 配置 设置端口，这里确保和 php.ini 里设置的端口号一致。如果端口没有打开，请按照 该链接 打开。 设置服务器。要记得先在服务器上安装 FTP（例如 vsftpd），并配置好。 例如这里是假设创建了 xdebug 用户，并用该账号登录 192.168.1.100 这台机器。 Root path 设置为你的项目（这里假设为 test）的根目录。 还是设置服务器，选择 Mappings 这个选项。在 Deployment path on server 这一栏填入斜杠即可。 开始监听 debug： ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:3:0","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"浏览器安装插件 这里以 chrome 为例。 进入 chrome 商店，搜索 Xdebug helper，安装该插件。或者点击直达链接：Xdebug helper 重启浏览器。 右键点击 chrome 工具栏上的 Xdebug helper，选择 选项 。在 IDE key 那里选择 PHPstorm，点右边的 save。 ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:4:0","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"加断点调试 打开 PHPstorm ，在想要调试的地方打上断点。 进入想要调试的页面，左键点击 chrome 工具栏上的 Xdebug helper，选择 Debug。 刷新页面或者点击按钮触发请求，一旦有执行到打断点的那一行，就会停下来。如果是第一次， PHPstorm 会跳出一个窗口。 在 Configure local file path 里选择 Import mappings from deployment ，并在 Deployment 那里选择刚才配置的服务器。 点击 Accept。 ","date":"2018-04-25","objectID":"/2018/04/25/php-debug-single/:5:0","tags":["PHP","Debug"],"title":"php+xdebug远程调试（单人）","uri":"/2018/04/25/php-debug-single/"},{"categories":null,"content":"结果 收到结果后，有三种方式设置结果变量到变量仓库。 Input/Output 每个变量一个 Output Input/Output 一个变量内同时设置多个变量 Listener 里面设置 end 类型，并同时设置多个变量 Response 是 Json，用 JavaScript 处理。 ","date":"2017-04-02","objectID":"/2017/04/02/camunda-basic/:1:0","tags":["Camunda"],"title":"Camunda 流程图配置","uri":"/2017/04/02/camunda-basic/"},{"categories":null,"content":"Input/Output 每个变量一个 Output Script 内容为： S(response).prop('字段名').boolValue() ","date":"2017-04-02","objectID":"/2017/04/02/camunda-basic/:1:1","tags":["Camunda"],"title":"Camunda 流程图配置","uri":"/2017/04/02/camunda-basic/"},{"categories":null,"content":"Input/Output 一个变量内同时设置多个变量 Script 内容为： resp = S(response) var1 = resp.prop('字段名').boolValue() execution.setVariable('字段名', var1) ","date":"2017-04-02","objectID":"/2017/04/02/camunda-basic/:1:2","tags":["Camunda"],"title":"Camunda 流程图配置","uri":"/2017/04/02/camunda-basic/"},{"categories":null,"content":"Listener 里面设置 end 类型，并同时设置多个变量 Script 内容为： resp = S(response) var1 = resp.prop('字段名').boolValue() execution.setVariable('字段名', var1) ","date":"2017-04-02","objectID":"/2017/04/02/camunda-basic/:1:3","tags":["Camunda"],"title":"Camunda 流程图配置","uri":"/2017/04/02/camunda-basic/"},{"categories":null,"content":"软件 安装 Vim echo \"y\" | sudo apt-get install vim 安装搜狗输入法 这个我在虚拟机里面尝试了好多遍，不断恢复备份然后重试。终于有了这个纯靠命令安装的做法。 wget -O $HOME/Downloads/sogou.deb http://cdn2.ime.sogou.com/dl/index/1475147394/sogoupinyin_2.1.0.0082_amd64.deb?st=bt76tTmCNyR7z8AF3TSnuQ\u0026e=1491123967\u0026fn=sogoupinyin_2.1.0.0082_amd64.deb echo \"y\" | sudo apt install $HOME/Downloads/sogou.deb gnome-session-quit --no-prompt 此时会退出当前用户，需要重新输入密码进入桌面 sed -i 's/sogoupinyin:False/sogoupinyin:True/g' $HOME/.config/fcitx/profile fcitx-remote -r fcitx-remote -s sogoupinyin 安装网易云音乐 wget -O $HOME/Downloads/netease-cloud-music.deb http://s1.music.126.net/download/pc/netease-cloud-music_1.0.0_amd64_ubuntu16.04.deb sudo apt install $HOME/Downloads/netease-cloud-music.deb netease-cloud-music \u0026 这一句是启动网易云音乐 连上 google 下载 hosts 文件到 $HOME/Downloads 下面 -\u003e hosts 文件自己找（逃 sudo cp $HOME/Downloads/hosts /etc/ sudo systemctl restart NetWorkManager 下载 chrome wget -O $HOME/Downloads/google-chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb echo \"y\" | sudo apt install $HOME/Downloads/google-chrome.deb google-chrome-stable \u0026 此时弹出对话框，询问是否设置 chrome 为默认浏览器(default brower) 接下去就是登陆(sign in) 安装 git echo \"y\" | sudo apt install git tmux 多窗口 配置 ","date":"2017-04-02","objectID":"/2017/04/02/ubuntu-softwares/:0:0","tags":["Ubuntu"],"title":"Ubuntu一些常用的软件安装及配置","uri":"/2017/04/02/ubuntu-softwares/"},{"categories":null,"content":"显示配置 launcher 放到底部 gsettings set com.canonical.Unity.Launcher launcher-position Bottom 如果参数为 Left 则还原为默认 修改 launcher 的大小 bash 中执行： unity-control-center 打开系统设置 选择 Appearance ，这个页面包含 Look 和 Behavior 两个子页面 Look 页面最底下的 Launcher icon size 改为 34 SystemSettings/Appearance/Behavior 勾选 Add show desktop icon to the launcher ：在 launcher 上显示一个图标，和 windows 的 “显示桌面” 按钮一样。 勾选 Always displayed ：总是显示当前窗口的菜单 勾选 In the windows's title bar ： 将菜单显示在当前窗口的顶部。另一项则是显示在系统桌面顶部那一栏。 ","date":"2017-04-02","objectID":"/2017/04/02/ubuntu-softwares/:1:0","tags":["Ubuntu"],"title":"Ubuntu一些常用的软件安装及配置","uri":"/2017/04/02/ubuntu-softwares/"},{"categories":["Linux"],"content":" 下载锐捷程序包 点此下载 没有连接到锐捷里就进不了这个安装包的官方下载界面（好矛盾啊这个），所以我把它上传到博客园了。 解压文件 schaepher:~$ cd Downloads/ schaepher:~/Downloads$ ls Linux_V1.30.rar schaepher:~/Downloads$ rar x Linux_V1.30.rar RAR 5.30 beta 2 Copyright (c) 1993-2015 Alexander Roshal 4 Aug 2015 Trial version Type RAR -? for help Extracting from Linux_V1.30.rar // ... 中间太长省略 All OK schaepher:~/Downloads$ ls Linux_V1.30 Linux_V1.30.rar schaepher:~/Downloads$ cd Linux_V1.30/ schaepher:~/Downloads/Linux_V1.30$ ls rjsupplicant schaepher:~/Downloads/Linux_V1.30$ cd rjsupplicant/ schaepher:~/Downloads/Linux_V1.30/rjsupplicant$ ls README rjsupplicant.sh x64 x86 修改脚本的可执行属性 一开始 rjsupplicant.sh 是无法执行的，需要使用 chmod a+x ./rjsupplicant.sh 来修改可执行属性。 如果你的 bash 是有着色的，你会发现使用这个命令之前这个文件是白色的，而使用之后变为绿色。 schaepher:~/Downloads/Linux_V1.30/rjsupplicant$ sudo chmod a+x ./rjsupplicant.sh [sudo] password for schaepher: schaepher:~/Downloads/Linux_V1.30/rjsupplicant$ ls README rjsupplicant.sh x64 x86 获取网络适配器名称 schaepher:~/Downloads/Linux_V1.30/rjsupplicant$ ifconfig -s Iface 后面还有一大堆 enp8s0 后面还有一大堆 lo 后面还有一大堆 在 Iface 这一列下面选择一个。比如我选的是 enp8s0 第一次登录 schaepher:~/Downloads/Linux_V1.30/rjsupplicant$ sudo ./rjsupplicant.sh -a 1 -n enp8s0 -d 1 -u 输入你的用户名 -p 输入你的密码 -S 1 AuthMode Wired Authentication Adapter enp8s0 UserName 显示你的用户名 2017-03-16 10:07:26 Initializing..... 2017-03-16 10:07:26 Lookup the switch... 2017-03-16 10:07:26 Connecting radius server... 2017-03-16 10:07:26 Authenticating now... 2017-03-16 10:07:26 Geting IP Address... 2017-03-16 10:07:29 Lookup the switch... 2017-03-16 10:07:29 Connecting radius server... 2017-03-16 10:07:29 Authenticating now... 2017-03-16 10:07:29 Success 2017-03-16 10:07:31 Management Tips: 福州大学官方论坛： bbs.fzu.edu.cn 福州大学校园网报修电话:38 201299,38201295,网上报修http://59.77.132.125 -a 1 ：表示使用有线的方式接入 -n enp8s0 ：这个就是刚才获取的名称 -d 1 ：设置为 1 表示从 dhcp 服务器获取 ip -u 输入你的用户名 -p 输入你的密码 -S 1 :设置为 1 表示保存密码 可以执行 sudo ./rjsupplicant.sh --help 来获取更详细的内容 以后登录 schaepher:~/Downloads/Linux_V1.30/rjsupplicant$ sudo ./rjsupplicant.sh -u 你的用户名 退出登录 按一下 q 再按回车键 在后台运行锐捷 如果按照上面的方法执行锐捷，它会占用一个 terminal 。如何在关闭 terminal 的情况下还能保持锐捷客户端的执行呢？ 只需要在命令的最后面加上 \u0026 就行了。 免去每次输入用户名的麻烦 创建一个脚本来执行。例如创建 rjApplication.sh 。 文件内容为： #!/bin/bash rjsupplicant的绝对路径/rjsupplicant.sh -u 你的用户名 \u0026 你可以先进入 rjsupplicant 文件夹，执行 pwd 来获取绝对路径。 在任何地方都可以启动锐捷 将 rjApplication.sh 脚本所在的目录添加到 PATH 里面。 执行 PATH=$PATH:脚本所在目录 。 添加完毕后，在任何地方执行 sudo rjApplication.sh 就能启动了。 ","date":"2017-03-16","objectID":"/2017/03/16/linux-ruijie/:0:0","tags":["Linux","Software"],"title":"锐捷Linux版的下载和使用（福大客户端）","uri":"/2017/03/16/linux-ruijie/"},{"categories":["前端"],"content":"jQuery 是一个 JavaScript 库，简化了 JavaScript 的编程。 语法：$(selector).action() selector 是字符串，表示HTML元素。 对象 符号 例子 效果 当前整个HTML this this 选择整个HTML 标签 标签名 “p” 选择所有\u003cp\u003e元素 id # “#test” 选择所有 id = “test\"的元素 class . “.test” 选择所有 class = “test\"的元素 链接 [] [href=“test”] 选择所有 href 等于 “test\"的元素 [href!=“test”] 选择所有 href 不等于 “test\"的元素 [href$=“test”] 选择所有 href 以 “test” 结尾的元素 表格 ul li: “ul li:first” 选择每个 \u003cul\u003e 的第一个 \u003cli\u003e 元素 以上三者可以合起来用：$(“p#test.test”) action()列表： 隶属 名称 功能 document ready(function) HTML文档加载完毕时执行function window 浏览器视口 所有元素 hide() 隐藏所选元素 show() 显示所选元素 toggle() 以上两者切换 fadeIn() 渐渐隐藏 fadeOut() 渐渐显示 fadeToggle() 以上两者切换 slideUp() 向上滑动隐藏元素 slideDown() 向下滑动显示元素 slideToggle() 以上两者切换 addClass() 添加一个或多个类 removeClass() 删除一个或多个类 toggleClass() 以上两者切换 mouseover(function) 鼠标悬停在元素上时执行function focus(function) 元素获得焦点时执行function click(function) 点击元素时执行function dblclick(function) 双击元素时执行function attr() 改变元素的属性 css() 改变或返回元素的css属性 animate({dict参数}) 执行动画以使元素符合参数指定内容（属性名用骆驼命名法） stop(bool,bool) 停止动画 text() 设置或返回所选元素的文本内容 html() 设置或返回所选元素带 HTML 标记的内容 val() 设置或返回表单字段的值 append(string) 元素内容结尾添加 after(string) 元素结束之后添加 remove() 删除整个元素 empty() 删除子元素 width() 不包括 padding 和 border height() 不包括 padding 和 border innerWidth() 包括 padding innerHeight() 包括 padding outerWidth() 包括 padding 和 border outerHeight() 包括 padding 和 border 当出现动画动作时，可以传入两个参数： 毫秒表示动作过渡时长。 回调，在动画完成后执行 可在数值前使用 += 和 -= 表示相对值。 在不重载整个网页的情况下，AJAX 通过后台加载数据，并在网页上进行显示。 AJAX: load() get() post() ","date":"2017-03-13","objectID":"/2017/03/13/jquery/:0:0","tags":["前端","jQuery"],"title":"jQuery简单笔记","uri":"/2017/03/13/jquery/"},{"categories":["Server"],"content":"下载 wampserver官网： http://www.wampserver.com/en/ 花生壳官网： http://hsk.oray.com/download/ 下载花生壳3最新版 ","date":"2017-02-03","objectID":"/2017/02/03/wampserver/:0:1","tags":["Server"],"title":"wampserver3.0.6+花生壳（Windows端）","uri":"/2017/02/03/wampserver/"},{"categories":["Server"],"content":"花生壳的配置 花生壳可注册和使用“免费版”，加引号是因为，整个过程下来仍然需要8块钱还是9块钱的费用（我注册的时候） 注册完账号后，到软件界面点“域名列表”，注册个域名。可以选择免费的 .imwork.net 域名。 注册完域名后，确保域名列表的界面“开启花生壳”是打开的状态。 如果软件不能登录，则按以下步骤操作： 控制面板 -\u003e 网络和 Internet -\u003e 网络连接 选择当前连接的网络，右键 -\u003e 属性 -\u003e 双击图左红框部分，修改图右部分： 如果能登录，那么继续： 点击软件界面中间的“内网穿透”，进入网页 点击右上的“添加映射” 使用windows时，可以通过cmd的ipconfig命令查看当前机器的ip地址 如果是给wampserver用的，上面不选“自定义端口”，而是选择“网站80端口” ","date":"2017-02-03","objectID":"/2017/02/03/wampserver/:0:2","tags":["Server"],"title":"wampserver3.0.6+花生壳（Windows端）","uri":"/2017/02/03/wampserver/"},{"categories":["Server"],"content":"wampserver的配置 打开其他机器对服务器的访问权限 打开wampserver后，右下角单击wampserver图标： 打开httpd-vhosts.conf后，添加一行： # Virtual Hosts # \u003cVirtualHost *:80\u003e ServerName localhost DocumentRoot D:/software/wamp64/www \u003cDirectory \"D:/software/wamp64/www/\"\u003e Options +Indexes +Includes +FollowSymLinks +MultiViews AllowOverride All Require local # 添加下面这行 Require all granted \u003c/Directory\u003e \u003c/VirtualHost\u003e # 安装Apache服务（如果未安装的话） 如果提示80端口被占用，则按以下步骤： 打开cmd，执行 netstat -aon | findstr :80 调出任务管理器： 右键关闭这个进程，再尝试安装服务。 如果仍然不行，则把所有名称为 httpd.exe 的进程关掉。 安装MySQL服务（如果未安装的话） 略。 启动服务 左键wampserver图标，“重新启动所有服务” ","date":"2017-02-03","objectID":"/2017/02/03/wampserver/:0:3","tags":["Server"],"title":"wampserver3.0.6+花生壳（Windows端）","uri":"/2017/02/03/wampserver/"},{"categories":["Server"],"content":"网页存放 打开目录： 新建文件： 文件名：index.html 内容：hello world 打开浏览器访问： ","date":"2017-02-03","objectID":"/2017/02/03/wampserver/:0:4","tags":["Server"],"title":"wampserver3.0.6+花生壳（Windows端）","uri":"/2017/02/03/wampserver/"},{"categories":["VIM"],"content":"写这篇文章是因为在更新我的一篇博客 Git的其他用法 的时候，里面的修改已经提交的commit说明这一部分需要用到vim。 在使用git config --global --edit或者git rebase -i commiteId^的时候，git会进入文本编辑模式。默认的编辑器是vim，你可以在Git安装的上层目录\\Git\\usr\\bin里找到vim.exe。 这让我想起以前使用VI（Visual Interface）和VIM（VI IMproved）编辑器时的懵逼。现在干脆把vim的基本使用整理出来。 vim编辑器有两种模式，分别为命令模式和编辑模式。 ","date":"2017-01-13","objectID":"/2017/01/13/vim-simple/:0:0","tags":["VIM"],"title":"vim编辑器的简单使用","uri":"/2017/01/13/vim-simple/"},{"categories":["VIM"],"content":"一、命令模式 当你刚进入文本编辑器的时候，处于编辑器的命令模式。这个命令模式可以做很多事情。这里介绍几个常用的： 命令 对应英文单词 说明 i insert 进入编辑模式，将光标定位在当前字符的前面 v visual mode 按一下v相当于你平时在MS Word里面按住shift，用来选择（高亮）一段文本 y yank 和复制的功能一样（英文意思为：猛拉） p paste 粘贴到当前字符前面 x x就是\"叉\"（或者“干掉”）的意思 删除被高亮的字符（光标所在的字符也算是被高亮的字符） yy yank 复制光标所在行 dd delete 删除光标所在行 u undo 撤销上一个修改 ctrl + r redo 不小心撤销过多的时候使用 /想搜索的字符串 /之后无空格，按Enter键开始搜索。按n（即next）往下搜索，按N往上搜索 :1,$ s /text1 /text2 /c substitute 把text1替换成text2。1,$表示行数范围，其中 $ 表示文档末尾。当你把数字1换成小数点. 时，表示从当前位置开始搜索（跟 bash 中用 . 表示当前位置一样）。/c表示让你选择找到之后的动作：y（yes）表示替换当前所选；n（next）表示跳过当前所选；a（all）表示当前所选及剩下的全部替换，不再确认；q（quit）表示停止替换。注意前面的冒号，与下面的命令类似。 :q! quit discard 舍弃修改并退出 :wq write then quit 保存修改并退出 看到:q!这个命令，可能有点懵圈。你没看错，得先输入一个冒号:，再去输入q!。最开始的时候我不知道要输入冒号，结果半天退不出来。 重要的说明： 在i的说明中，你可能不太理解为什么说“将光标定位在当前字符的前面”。在vim编辑器的命令模式下，光标是覆盖在字符上的。当你按i，它就将光标定位到当前字符的前面。与此相对的，按a（即append）时，光标就定位到当前字符的后面。 如图所示： 找了半天终于找到一个不错的在线vim编辑器：Vim.js 还有一款加载比较慢的：Interactive Vim tutorial - sandbox 点进去后你能直观地看到光标是覆盖到字符上的。你可以分别按i和a来查看效果。 至于其他命令，还是看图比较直观： 中文版图源链接 英文版图源链接 当然，你也可以下载vim的文档。这里是官方文档：vimbook-OPL.pdf ","date":"2017-01-13","objectID":"/2017/01/13/vim-simple/:1:0","tags":["VIM"],"title":"vim编辑器的简单使用","uri":"/2017/01/13/vim-simple/"},{"categories":["VIM"],"content":"二、编辑模式 当你按i进入编辑模式的时候，基本上就可以照常编辑文本了。 除了正常的输入外，这些按键也可以正常使用：del（往光标后删除），back（也就是键盘上的←，往光标前删除），enter（回车键），tab（制表符） 但是注意，想要选择字符或者复制粘贴等的时候，必须退出编辑模式，到命令行模式去执行操作。 当你想要退出编辑模式的时候，按esc键。注意，这时是退到命令模式，不是完全退出。你得在命令模式输入命令来完全退出编辑。 知道了以上这些介绍，你可以进行基本的编辑了。 ","date":"2017-01-13","objectID":"/2017/01/13/vim-simple/:2:0","tags":["VIM"],"title":"vim编辑器的简单使用","uri":"/2017/01/13/vim-simple/"},{"categories":["VIM"],"content":"三、正则表达式 在命令模式点击 / 后，会进入搜索模式。它会搜索 / 之后的 pattern 。 在搜索和替换的时候，如果不能用正则表达式，极可能耗费大量时间。Vim 是支持正则表达式的，不过 Vim 正则表达式的写法跟通常的写法有点不一样。 比如你要匹配 dekkkl 和 detttl ，你可以这么写： /de.\\+l 这里的 / 是之前输入的，表示搜索功能。后面的 de.\\+l 是正则表达式。这里与普通的正则表达式不同的地方在于用 \\+ 来表示匹配一次或多次，而不是 +。 详细的规则可以看：VIM 正则参考 除了搜索之外，替换也能使用正则表达式。替换的语法和上面表格中的一致， text1 这个部分可以是正则表达式。 ","date":"2017-01-13","objectID":"/2017/01/13/vim-simple/:3:0","tags":["VIM"],"title":"vim编辑器的简单使用","uri":"/2017/01/13/vim-simple/"},{"categories":null,"content":" 一、Navigation Drawer 二、ToolBar 三、参考链接 写Android的时候，可能有多个界面。在风格统一的软件中，写Activity时会有很多重复。例如我所在软工课程小组的项目：Github链接 ，里面的TaskListActivity和TeacherListActivity就在Navigation的处理上有重复。还有一个双击退出APP的方法onBackPressed()也重复实现了。之前让负责界面的同学把这些代码放到一个BaseActivity里面，让其他Activity继承它。他说不好做，他尝试过，但失败了。 于是这次我独自做 英语词典APP 的时候 ，经过在Google上的一番搜索和实践探索，写出一个还可以的BaseActivity。现在做个记录，以后还会用得到。 BaseActivity项目的Github链接 这个BaseActivity包括侧滑菜单（Navigation Drawer）和工具栏（Tool Bar）。 先看最终效果： ","date":"2016-07-09","objectID":"/2016/07/09/android-base-activity-navi-and-toolbar/:0:0","tags":["Android"],"title":"Android带侧滑菜单和ToolBar的BaseActivity","uri":"/2016/07/09/android-base-activity-navi-and-toolbar/"},{"categories":null,"content":"一、Navigation Drawer 由于Navigation Drawer涉及到BaseActivity的主要布局，所以先说明。 Activity的布局文件 先看Android官方Navigation Drawer说明：Creating a Navigation Drawer 根据说明，将 layout/activity_base.xml 设置为以下内容： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003candroid.support.v4.widget.DrawerLayout android:id=\"@+id/drawer_layout\" xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\"\u003e \u003c!-- 主内容布局 --\u003e \u003cFrameLayout android:id=\"@+id/content_frame\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\"/\u003e \u003c!-- 侧滑菜单 --\u003e \u003cListView android:id=\"@+id/left_drawer\" android:layout_width=\"240dp\" android:layout_height=\"match_parent\" android:layout_gravity=\"start\" android:background=\"#111\" android:choiceMode=\"singleChoice\" android:divider=\"@android:color/transparent\" android:dividerHeight=\"0dp\"/\u003e \u003c/android.support.v4.widget.DrawerLayout\u003e Android Studio不会提示android:layout_gravity这一项，但是整行敲完之后，可以正常运行。 侧滑菜单列表布局 这里只做简单的布局，所以侧滑菜单项都是TextView。 创建 layout/list_item_drawer.xml： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cTextView android:id=\"@+id/tv_na_draw\" xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:textSize=\"30sp\" android:textColor=\"#fff\"/\u003e 因为等会儿要用ArrayAdapter，所以这里最外层一定要是TextView。 侧滑菜单项的文本 StringArray 创建 values/string_array_test.xml ： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cresources\u003e \u003cstring-array name=\"planets_array\"\u003e \u003citem\u003e主界面\u003c/item\u003e \u003citem\u003e关于\u003c/item\u003e \u003citem\u003e设置\u003c/item\u003e \u003citem\u003e退出\u003c/item\u003e \u003c/string-array\u003e \u003c/resources\u003e Activity初始化 Initialize the Drawer List 这里由于要将Activity做成BaseActivity，所以和官方文档上的代码不太一样。 public class BaseActivity extends AppCompatActivity { protected String[] planetTitles; protected DrawerLayout drawerLayout; protected ListView drawerList; protected FrameLayout frameLayout; @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); } /** * 重写setContentView，以便于在保留侧滑菜单的同时，让子Activity根据需要加载不同的界面布局 */ @Override public void setContentView(@LayoutRes int layoutResID) { drawerLayout = (DrawerLayout) getLayoutInflater().inflate(R.layout.activity_base, null); frameLayout = (FrameLayout) drawerLayout.findViewById(R.id.content_frame); // 将传入的layout加载到activity_base的content_frame里面 getLayoutInflater().inflate(layoutResID, frameLayout, true); super.setContentView(drawerLayout); setUpNavigation(); } private void setUpNavigation() { planetTitles = getResources().getStringArray(R.array.planets_array); drawerList = (ListView) findViewById(R.id.left_drawer); drawerList.setAdapter(new ArrayAdapter\u003c\u003e(BaseActivity.this, R.layout.list_item_drawer, planetTitles)); } } MainActivity 先创建个Activity查看效果。 创建MainActivity继承BaseActivity ： public class MainActivity extends BaseActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); } } layout_main用的是默认布局。 在启动前要更改 AndroidManifest.xml ，将MainActivity设置成Launcher： \u003cactivity android:name=\".MainActivity\"\u003e \u003cintent-filter\u003e \u003caction android:name=\"android.intent.action.MAIN\"/\u003e \u003ccategory android:name=\"android.intent.category.LAUNCHER\"/\u003e \u003c/intent-filter\u003e \u003c/activity\u003e 启动后，手指从最左往右划，打开侧滑菜单。 添加侧滑菜单点击事件 Handle Navigation Click Events 先创建ClickListener，这里简单地设置点击后显示所点击的文本。 在BaseActivity中添加： private class DrawerItemClickListener implements ListView.OnItemClickListener { @Override public void onItemClick(AdapterView\u003c?\u003e parent, View view, int position, long id) { selectItem(position); } } private void selectItem(int position) { Toast.makeText(BaseActivity.this, planetTitles[position], Toast.LENGTH_SHORT).show(); } 在BaseActivity的 setUpNavigation() 中添加： drawerList.setOnItemClickListener(new DrawerItemClickListener()); ","date":"2016-07-09","objectID":"/2016/07/09/android-base-activity-navi-and-toolbar/:1:0","tags":["Android"],"title":"Android带侧滑菜单和ToolBar的BaseActivity","uri":"/2016/07/09/android-base-activity-navi-and-toolbar/"},{"categories":null,"content":"二、ToolBar ToolBar 布局 ToolBar的各个成分： Setting Up the App Bar 打开AndroidManifest.xml，将里面的 android:theme=\"@style/AppTheme\" 替换成 android:theme=\"@style/Theme.AppCompat.Light.NoActionBar\"，如下： \u003capplication android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:label=\"@string/app_name\" android:supportsRtl=\"true\" android:theme=\"@style/Theme.AppCompat.Light.NoActionBar\"\u003e ...（三点表示省略，下同） 创建 layout/toolbar.xml ： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cmerge xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\"\u003e \u003candroid.support.v7.widget.Toolbar android:id=\"@+id/toolbar\" android:layout_width=\"match_parent\" android:layout_height=\"?attr/actionBarSize\" android:background=\"?attr/colorPrimary\" android:elevation=\"4dp\" android:theme=\"@style/ThemeOverlay.AppCompat.ActionBar\" app:popupTheme=\"@style/ThemeOverlay.AppCompat.Light\"/\u003e \u003c/merge\u003e ToolBar 初始化 在BaseActivity里添加： public class BaseActivity extends AppCompatActivity { ... private Toolbar toolbar; ... private void setUpToolBar() { toolbar = (Toolbar) findViewById(R.id.toolbar); setSupportActionBar(toolbar); } } 在BaseActivity的 setContentView() 里的 setUpNavigation();下面添加setUpToolBar();，如下： @Override public void setContentView(@LayoutRes int layoutResID) { ... setUpNavigation(); setUpToolBar(); } 如果想要在MainActivity里显示ToolBar，还需要在 activity_main.xml 里include一个ToolBar的布局： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cRelativeLayout ...\u003e \u003cinclude layout=\"@layout/toolbar\"/\u003e \u003c/RelativeLayout\u003e 显示的效果如下： 加入侧滑菜单打开键 到 Material Design Icons 下载图标。搜索menu，下载PNG版本。 解压后，将 ic_menu_black_24dp/android 文件夹下面的所有文件夹复制到 res/ 里面。 在BaseActivity的 setUpToolBar() 中添加： toolbar.setNavigationIcon(R.drawable.ic_menu_black_24dp); 虽然有图标，但是还没设置点击事件，所以点击的时候没有任何反应。 点击ToolBar的home按钮打开Navigation Drawer 在BaseActivity里添加： @Override public boolean onOptionsItemSelected(MenuItem item) { switch (item.getItemId()) { case android.R.id.home: drawerLayout.openDrawer(GravityCompat.START); return true; default: break; } return super.onOptionsItemSelected(item); } 这里就省略ActionBarDrawerToggle的部分了，影响不是特别大。实际应用中再去添加。 添加Option Menu 在 values/strings.xml 里添加等会儿会用到的字符串： \u003cresources\u003e ... \u003cstring name=\"edit\"\u003e编辑\u003c/string\u003e \u003cstring name=\"search\"\u003e搜索\u003c/string\u003e \u003cstring name=\"change_history\"\u003e变更记录\u003c/string\u003e \u003c/resources\u003e 去 Material Design Icons 下载相关图标。 创建 res/menu/menu_main.xml ： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cmenu android:id=\"@+id/expanded_menu\" xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\"\u003e \u003citem android:id=\"@+id/menu_edit\" android:icon=\"@drawable/ic_mode_edit_black_24dp\" android:title=\"@string/edit\" app:showAsAction=\"ifRoom\"/\u003e \u003citem android:id=\"@+id/menu_search\" android:icon=\"@drawable/ic_search_black_24dp\" android:title=\"@string/search\" app:showAsAction=\"ifRoom\"/\u003e \u003citem android:id=\"@+id/menu_change_history\" android:icon=\"@drawable/ic_change_history_black_24dp\" android:title=\"@string/change_history\" app:showAsAction=\"never\"/\u003e \u003c/menu\u003e 其中ifRoom表示如果ToolBar空间足够，则将其图标显示在ToolBar上面。never表示永不显示在ToolBar上面，只有当点击option menu按钮的时候，才会出现。 在BaseActivity中添加： @Override public boolean onCreateOptionsMenu(Menu menu) { getMenuInflater().inflate(R.menu.menu_main, menu); super.onCreateOptionsMenu(menu); return true; } 启动APP： 点击menu按钮后： Option Menu的点击事件 在BaseActivity里的 onOptionsItemSelected() 增加case即可。 隐藏ActionBar的Option Menu 有时候不想在某个Activity中显示Option Menu，于是修改BaseActivity： public class BaseActivity extends AppCompatActivity { ... private boolean showOptionMenu = true; ... protected void hideOptionMenu() { showOptionMenu = false; } @Override public boolean onCreateOptionsMenu(Menu menu) { if (showOptionMenu) { getMenuInflater().inflate(R.menu.menu_main, menu); } super.onCreateOptionsMenu(menu); return true; } } ","date":"2016-07-09","objectID":"/2016/07/09/android-base-activity-navi-and-toolbar/:2:0","tags":["Android"],"title":"Android带侧滑菜单和ToolBar的BaseActivity","uri":"/2016/07/09/android-base-activity-navi-and-toolbar/"},{"categories":null,"content":"三、参考链接 ToolBar https://developer.android.com/training/appbar/setting-up.html https://developer.android.com/training/appbar/actions.html http://stackoverflow.com/questions/32367041/calling-toolbar-on-each-activity http://stackoverflow.com/questions/30824324/clicking-hamburger-icon-on-toolbar-does-not-open-navigation-drawer http://stackoverflow.com/questions/19724567/how-to-add-menu-indicator-next-to-action-bars-app-icon http://stackoverflow.com/questions/26582075/cannot-catch-toolbar-home-button-click-event Navigation Drawer https://developer.android.com/training/implementing-navigation/nav-drawer.html http://stackoverflow.com/questions/33009469/baseactivity-for-navigation http://stackoverflow.com/questions/22652556/creating-base-activity-with-navigation-drawer-in-android ArrayAdapter http://stackoverflow.com/questions/9280965/arrayadapter-requires-the-resource-id-to-be-a-textview-xml-problems http://stackoverflow.com/questions/29591546/unable-to-use-layout-gravity-for-listview-inside-drawerlayout String Array https://developer.android.com/guide/topics/string-resource.html#StringArray Material icons https://design.google.com/icons/ ","date":"2016-07-09","objectID":"/2016/07/09/android-base-activity-navi-and-toolbar/:3:0","tags":["Android"],"title":"Android带侧滑菜单和ToolBar的BaseActivity","uri":"/2016/07/09/android-base-activity-navi-and-toolbar/"},{"categories":null,"content":"面向对象方法是一种运用对象、类、封装、继承、多态和消息等概念来构造、测试、重构软件的方法。 UML主要提供了五类十种图形: 用例图(Use case diagram) 从用户角度描述系统功能,并指出各功能的操作者。 静态图(Static diagram) 表示系统的静态结构,包括类图、对象图、包图。 行为图(Behavior diagram) 描述系统的动态模型和组成对象间的交互关系。包括状态图、活动图。 交互图(Interactive diagram) 描述对象间的交互关系。包括顺序图、合作图。 实现图( Implementation diagram ) 用于描述系统的物理实现,包括构件图、配置图。 面向对象的基本概念： 对象是一个拥有属性,行为和标志符的实体 方法的类型：属性过程，服务函数，接口函数，对象控制函数 对象控制函数:实现对象生命周期的典型功能,控制对象的创建和销毁. 传统方法将数据和功能分开封装 面向对象技术则是把功能和数据封装进入对象 类之间的关系：普通关联，层次结构 在类的继承层次结构中，位于较高抽象层次的类称为父类，位于较低抽象层次的类称为子类。 isa进行继承关系的测试 an A(子类) is a B(父类) (A是一个B) 继承,重载,多态是为了提高系统的灵活性,降低类之间的耦合性 UML用例图中，用例之间的关系可分为：include，extend 活动图活动之间的连线表示：控制转移 状态图状态之间的连线表示：动作 内部转移域是可选的。其中所列的动作将在对象处于该状态时执行且该动作的执行并不改变对象的状态。 对象形式的设计模式一般采用组合/聚合的方法，而类形式的设计模式一般采用继承的方法。 什么是多继承？分析其利弊 什么是多态？ 简单工厂模式解决什么问题？ 什么是“面向接口编程”，分析其作用 适配器模式使用情况： (1)系统需要使用现有的类，而此类的接口不符合系统的需要。 (2)想要建立一个可以重复使用的类．用于一些彼此之间没有太大关联的一些类，包括一些可能在将来引进的类一起工作，这些源类不一定有很复杂的接口。 (3)(对对象的适配器模式而言)在设计里，需要改变多个已存在的子类的接口．如果使用类的适配器模式、就要针对每一个子类做一个适配器类，而这不太实际。 ##合成模式（树枝，树叶）： 在下面的情况下应当考虑使用合成模式： （1）需要描述对象的部分和整体的等级结构 （2）需要客户端忽略掉整体构件和组合构件的区别。 合成模式的优点： （1）可以很容易地增加新种类的构件 （2）使客户端变得很容易设计，因为客户端不需知道是树叶还是树枝。 缺点： （1）确定树枝构件的接口不太容易 （2）使用继承的方法来增加新的行为很困难。 ##装饰模式 优点: (1)装饰模式与继承关系的目的都是要扩展对象的功能, 但是装饰模式可以提供比继承更多的灵活性. —装饰模式允许系统动态地决定“贴上”一个需要的“装饰”,或者除掉一个不需要的“装饰”.继承关系则不同,继承关系是静态的,它在系统运行前就决定了. (2)通过使用不同的具体装饰类以及这些装饰类的排列组合,设计师可以创造出很多不同的行为的组合. (3)这种比继承更加灵活的特性； 缺点: 产生出较多的对象; 比继承更易出错； ##代理模式优缺点 远程代理 —-可以将网络的细节隐藏起来，使得客户端不必考虑网络的存在。客户完全可以认为被代理的对象是局域的而不是远程的，而代理对象承担了大部分的网络通信工作。 虚拟代理 —–使用虚拟代理模式的优点就是代理对象可以在必要的时候才将被代理的对象加载。代理可以对加载的过程加以必要的优化。当一个模块的加载十分耗费资源的时候，虚拟代理的优点就非常明显。 保护代理 保护代理可以在运行时间对用户的有关权限进行检查，然后在核实后决定将调用传递给被代理的对象。 智能引用代理 在访问一个对象时可以执行一些内务处理（Housekeeping）操作，比如计数操作等。 ##简单工厂模式 优点：该模式的核心是工厂类。这个类含有必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例（通过参数的传入）。而客户端则可以免除直接创建产品对象的责任，而仅仅负责“消费”产品。对于消费者角色来说，任何时候需要某种产品，只需要向工厂角色（下订单）请求即可，而无需知道产品创建细节。简单工厂模式通过这种做法实现了对责任的分割 缺点：当产品类有复杂的多层次等级结构时，工厂类只有它自己。以不变应万变，就是模式的缺点。 ##原型模式： 通过复制（克隆）实现 优点 1、Prototype模式允许动态增加或减少产品类。由于创建产品类实例的方法是产品类内部具有的，因此增加新产品对整个结构没有影响。 2、Prototype模式提供了简化的创建结构。工厂方法模式常常需要有一个与产品类等级结构相同的等级结构，而Prototype模式就不需要这样。 3、Portotype模式具有给一个应用软件动态加载新功能的能力。由于Prototype的独立性较高，可以很容易动态加载新功能而不影响老系统。 4、产品类不需要非得有任何事先确定的等级结构，因为Prototype模式适用于任何的等级结构。 缺点 每一个类必须配备一个克隆方法。而且这个克隆方法需要对类的功能进行通盘考虑，这对全新的类来说不是很难，但对已有的类进行改造时，不一定是件容易的事 ","date":"2016-07-08","objectID":"/2016/07/08/uml/:0:0","tags":["UML"],"title":"UML 课程","uri":"/2016/07/08/uml/"},{"categories":null,"content":"###Chapter One Begin ####学院大道 最开始的一个段落就把家里的四个人写全了，也交代了背景。同时又留了个疑问：她父亲是因为什么进了监狱的？ 接着过度到描述这个时期的生活。回到父母的年轻时期，介绍父母。讲了母亲，然后承接父母的相遇，通过母亲对父亲的描述，过度到对父亲的介绍。都是从他们小时候开始说起。两人的共同点是家庭都不幸福，后来染上毒瘾。 父亲好一点，他学习能力强，而且他母亲尽力让他接受最好的教育：高中上名校，毕业进纽约市中心的大学。而她母亲13岁便离开家人，可见受教育程度不怎么样。 他们俩是在双方共同朋友的家中相遇的。而正式交往时，母亲22岁，父亲34岁。他们没有正式登记结婚，就生下孩子。 这里由相遇过度到介绍父亲，介绍完父亲又接回到相遇的时候。由相遇自然地讲到恋爱，然后同居，直至生子。此时又点出了孩子还是婴儿时的事情，说明时间。 接下去的介绍是解答最开始的那个段落提到的问题。讲她父亲如何与她母亲合伙贩卖止痛药以及如何被发现并且被警察逮捕的。俩人都被逮捕了，但是她母亲因为怀着她，最终被释放了。 之后她母亲生下她。三个人靠着政府的资助过着生活。但还好，至少母亲还勤做家务，为什么这么说呢？她父亲在她3岁时出狱了。此后，母亲变得越来越懒散，很少做家务。他们又开始一起吸毒了。 到了5岁，家里全部经济来源就是政府救济金。这时道出母亲有先天眼疾。 既然说到了救济金，就讲到每月一号领救济金的种种。她因为负责在快递员到来的时，第一时间通知父母，这也让她感觉到自己在家里的重要性。接着讲拿到支票后去支票兑换点换取现金的事。每次在排队的时候，她都会抓住机会多问母亲一些问题。领到救济金的当天就是几顿大餐。 开始介绍她姐姐。从几个事件中，可以看出她姐姐与她大不相同。以姐姐因为不想吃鸡蛋而吵闹讲到姐姐性格为何与她不同——原来母亲怀她的时候把她姐姐送到一个寄养家庭生活了近8个月。 开始介绍她姥姥。写姥姥要来家里。姥姥来之前，母亲有哪些准备——各种清理。通常，除了她之外，没人愿意和她姥姥交谈。她姥姥是虔诚的天主教徒，所以接着描述她姥姥“与上帝对话”。 讲公立学校派发免费午餐。 讲她和母亲独处，看娱乐节目。讲她父亲带她去图书馆。 讲她要开始上学了。上学之前，她母亲帮她剪头发——结果剪得太短。 ###Chapter One End ","date":"2016-07-08","objectID":"/2016/07/08/reading-breaking-night/:0:0","tags":["Books"],"title":"读《Breaking Night》","uri":"/2016/07/08/reading-breaking-night/"},{"categories":null,"content":" 隐式Intent Activity活动周期 整个周期 三个分段 onSaveInstanceState()方法 可以将数据放在Bundle，再将Bundle存放在Intent里 活动的四种启动方式 知道当前是在哪一个活动 getClass().getSimpleName() 新建一个活动，放入该代码到Log，其他活动都继承该活动。即建立一个专门的类，把所有活动都会出现的代码都放在这里（与集合类不同）。 static 集合类管理器 public static List\u003cActivity\u003e activities = new ArrayList\u003cActivity\u003e(); public static void addActivity(Activity activity) { activity.add(activity); } 启动活动 public clasee SecondActivity extends Activity { public static void actionStart(Context context, String data1, String data2) { Intent intent = new Intent(context,SecondActivity.class); intent.putExtra(“param1”,data1); intent.putExtra(“param2”,data2); context.startActivity(intent); } } 需要启动该SecondActivity时，直接调用： SecondActivity.actionStart(this, “data1”,“data2”); android:maxLines=“2” 指定EditText最大行数为2行，超过2行时文本向上滚动。 ImageView.setImageResource(R.drawable.picture); ProgressBar ProgressBar.getProgress() ProgressBar.setProgress(int) ProgressDialog RelativeLayout android:layout_centerInParent=“true” android:layout_alignLeft表示让一个控件的左边缘和另一个控件的左边缘对齐 引入布局 \u003cinclude layout=\"@layout/title\" /\u003e 自定义标题栏 public class TitleLayout extends LinearLayout { public TitleLayout(Context context, AttributeSet attrs) { super(context, attrs); LayoutInflater.from(context).inflate(R.layout.title, this); } } ","date":"2016-07-08","objectID":"/2016/07/08/first-line-code-android/:0:0","tags":["Android"],"title":"读《第一行代码——android》","uri":"/2016/07/08/first-line-code-android/"},{"categories":null,"content":"这里提供两种方案： ###方案一：Y = X ± X ÷ N 使用 Y = X ± X ÷ N 这个公式来预计项目耗时（《构建之法——第二版》P178） 参考了http://www.cnblogs.com/beasthunter/p/4902314.html 列出所有功能，重点是主要功能，次要功能也要列出来 主要功能按照每个以一个月的时间为基准进行估计，难度大的另加. 难度小的功能每个以两星期为基准进行估计 统计所有时间。根据团队的实际情况进行调整。如果是刚毕业的学生，则N为1；如果是已经有工作经验的人，则每在类似的项目工作一年N就加一。然后再根据实际情况，包括个人水平和其他能力，对N进行小幅度调整。 #####实例一：微软必应词典 列出功能。 主要功能。从其界面可以看出：词典，例句，翻译，应用 次要功能。在设置里可以看到一些：窗口和字体大小跟随系统DPI变化，主窗口查词时自动发音，双击Ctrl显示/隐藏主窗口，迷你窗口，取词，划译，代理…… ###方案二：乘π估计 可以结合上面的公式使用，也可以单独使用。但预估时间比较不准确。 用法就是：靠直觉大致估计项目总耗时，然后将得到的时间乘以π。所得的结果就是大致预估的值。 ","date":"2016-07-08","objectID":"/2016/07/08/time-consuming-prediction/:0:0","tags":["软件工程"],"title":"软件开发耗时估计","uri":"/2016/07/08/time-consuming-prediction/"},{"categories":null,"content":"每一天，只要头脑闲下来我就会思考各种事情。 这里的闲下来，有以下几种： 骑自行车或者走路的时候 排队的时候 吃饭的时候 注意力不集中的时候 …… 思考的范围也多变。大可以大到人生的哲学，宇宙的存在；小可以小到生活的琐事，原子的排列。 平时把那么多的时间拿来思考，有时候也会想把自己的想法写出来。但是有以下几个原因让我一直止于脑中思考： 写的时候会卡住 思考的时候可以一直延续下去，但如果想写出来思考就会卡住。 因为得等用笔写在纸上(基本上很少手写)，或者用键盘敲字，这时候就得等敲完字才能继续写下去。而在等待的期间，思考就暂停住了。思考一旦暂停，很难再继续。 所以我有时候会想，如果有这么一个设备，当你头脑在思考的时候，它可以把你思考的内容记录下来，那真是太棒了。 :) 灵感不是说来就来的 有时候是想写，可是没有灵感，不知道写什么才好。 而一般出去外面看到了什么东西，或者在外面走路、骑车、看书的时候，才会有灵感。毕竟这些思考都是需要一个触发条件的。 写作工具不太理想 要用什么工具写作是一个问题。 在PC上，我尝试过用为知笔记、Visual Studio Code，但是打开之后就没有写作的欲望了。 由于一般都是在户外才有灵感，所以得经常用手机。 大二的一年，是我个人目前为止思考（思想？）的最高峰。那时有很多思考，有一部分我用手机及时地记录下来了（还好记下来了！！） 挺不容易的。因为在大二之前，我也偶尔有些思考，而且我自认为这些思考中有不少的闪光点。然而由于没有记录下来，过后就忘了（好可惜啊！！）。所以大二的时候不断提醒自己，要把思考的内容记在手机上。 那时候试过几个笔记软件的Android版本。印象深刻的是“印象笔记”，软件启动太慢了！而且要写一篇笔记，还很麻烦。最后我是用MIUI的小米便签，除去第一次创建，之后一打开就能写，速度快！要知道，一旦软件启动慢，哪怕多等一秒，思考都有可能被断掉。 笔记软件功能太多实在是蛋疼。 发表到博客上。 这个是近期困扰我的一个问题。如果只是写在手机上或者电脑上，那还好。可是要发表出来，那就有点为难了。有以下几个原因： 我的思考都是特别主观的。基本上没有什么调查，凭的是我自己的总结和感觉。 我认为既然是发表到博客上，那要是篇幅太短，就很尴尬了。像上次发表的关于吴军博士某篇文章的看法，我自己感觉不太像是一篇博文的样子。 虽然我思考的时候，感觉内容特别多，但我最终写出来的都是一小段的零散的文字。因为我最终只能记得思考的结论，至于过程我就记不得了。关于这点，我想是因为我思考的跨度太大…很难逆推回去。有时候想要知道上一步都思考了什么，就得通过回忆和推理才能知道。 怕自己的幼稚被别人知道了，怕给别人留下不好的印象…关于这点我也不知道该说什么，囧 ……还有其他的原因，目前只想到这几点。 写的时候会被中断 在写的时候，会为了尽可能把所有情况写出来，或者文章想要更有逻辑性，或者选用词语觉得不恰当，而停下来想或者找。这时之前已经思考的要写下来的那些都被冲掉了，也就是因为想完整一些，导致把之后要写的给忘了，结果最终没写成。 这一点，我这次是先把想到的东西写出来，想到哪里写到哪里。词语用得不好？跳过。句子没有把想表达的意思展示出来？跳过！最终是先把大概的文章写出来了。想要修改，等全部写完再去修改。到时候有什么要补充的，再补充上去。 太高调了= = 不得不说，第四点的很多担心是多余的，因为只要“哎呀，反正不会有人来看我的博客啦”这样想着，也就释然了。然而我却当起了助教，增加了不少关注者的样子，囧。 不过这也不是没有解决方案：再开个新博客嘛~ 仔细想想，果然还是因为我太懒（逃 今天实在是受不了嘞！上次跟周筠老师说要开始写一些东西，但是迟迟没开始。不能再顾虑太多了，得开始行动起来！ 小插曲： 我刚刚午休结束，从床上下来。立马就开了电脑，尝试看看能不能写点什么。接着就是我的选择过程： ——用为知笔记试试？ ——不行不行，估计又写不下去了。 ——Visual Studio Code呢？ ——不行不行，有好几次写了一些，但是没敢发博客上，最终还是GG了。 ——诶，可以试试直接在博客园上面在线写！写完直接发表嘛。 ——看来可行，试试看！ 于是就有了这篇。 ——取个什么标题好呢？ ——不行，这样纠结下去就又什么都写不出来了。标题名就叫“写写写”好了！赶紧动手写文章，不然等下又放弃。 ","date":"2016-06-23","objectID":"/2016/06/23/just-write/:0:0","tags":["Daily"],"title":"写写写，写TMD","uri":"/2016/06/23/just-write/"},{"categories":null,"content":"网络上关于Git和GitHub的教程不少，但是这些教程有的命令太少不够用，有的命令太多，使得初期学习的时候需要额外花不少时间在一些当前用不到的命令上。 这篇文章主要的目标是用较少的时间学习Git和GitHub的基本使用。在足够一般使用的前提下，尽量减少命令。 如果需要其他命令，到时候再去其他地方了解就行了。 目录： 零、Git是什么 一、Git的主要功能：版本控制 二、概览 三、Git for Windows软件安装 四、本地Git的使用 五、Github与Git的关联 六、团队合作开发 七、Github的其他介绍 八、一些可能碰到的问题 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:0:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"零、Git是什么 我才不告诉你嘞 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:1:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"一、Git的主要功能：版本控制 版本： 想想你平时用的软件，在软件升级之后，你用的就是新版本的软件。你应该见过这样的版本号：v2.0 或者 1511（表示发布时为15年11月），如下图： 那么如果你修改并保存了一个文件，从版本管理的角度来说，你得到的是这个文件的新版本。 可是很多情况下，这种修改是不可逆的。你修改完之后，无法回到你修改前的样子。为了避免这种情况，有的人会把新版本的内容保存到一个新的文件里面。 由于 Git 更多地用于代码管理，举个程序员的例子。比如以下是计算机专业学生的作业： 这样存储多个文件夹，可能会造成混乱。你可能想保存以前写的代码，因为它们可能在以后会用到。但是更多的时候是，你不知道各个文件夹都做了什么修改。 这时候你需要一款软件帮你管理版本，它就是Git。 控制： 你可以用Git来对这些不同的版本进行控制。还可以很方便地查看两个不同版本之间的不同之处。 ——使用Git，你只保存最新的一份文件就可以了。 ——那我以前的文件怎么办？ ——可以用Git的 reset 帮你把文件回退到你想要的版本。 ——如果回去了，那我的最新版本呢？ ——还可以用 reflog 和 reset 的组合来还原。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:2:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"二、概览 所有命令前都要加 git，如表中的init是指 git init。 点击命令可直接跳转至本文第一次使用的地方。 以下命令都在命令行里执行。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:3:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"1.个人本地使用 行为 命令 备注 初始化 init 在本地的当前目录里初始化git仓库 clone 地址 从网络上某个地址拷贝仓库(repository)到本地 查看当前状态 status 查看当前仓库的状态。碰到问题不知道怎么办的时候，可以通过看它给出的提示来解决问题。这个命令执行的频率应该是其他命令的几倍，特别是新手 查看不同 diff 查看当前状态和最新的commit之间不同的地方 diff 版本号1 版本号2 查看两个指定的版本之间不同的地方。这里的版本号指的是commit的hash值 添加文件 add -A 这算是相当通用的了。在commit之前要先add 撤回修改的且还未stage的内容 checkout – . 这里用小数点表示撤回所有修改，在--的前后都有空格 提交 commit -m “提交信息” 提交信息最好能体现更改了什么 删除未tracked clean -xf 删除当前目录下所有没有track过的文件。不管它是否是.gitignore文件里面指定的文件夹和文件 查看提交记录 log 查看当前版本及之前的commit记录 reflog HEAD的变更记录 版本回退 reset –hard 版本号 回退到指定版本号的版本，该版本之后的修改都被删除。同时也是通过这个命令回到最新版本。需要reflog配合 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:3:1","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"2.个人使用远程仓库 行为 命令 备注 设置用户名 config –global user.name “你的用户名” 设置邮箱 config –global user.email “你的邮箱” 生成ssh key ssh-keygen -t rsa -C “你的邮箱” 这条命令前面不用加git 添加远程仓库 remote add origin 你复制的地址 设置origin 上传并指定默认 push -u origin master 指定origin为默认主机，以后push默认上传到origin上 提交到远程仓库 push 将当前分支增加的commit提交到远程仓库 从远程仓库同步 pull 在本地版本低于远程仓库版本的时候，获取远程仓库的commit 可以用一张图直观地看出以上主要的命令对仓库的影响。 图片引用自：Git introduction for CVS/SVN/TFS users 图片引用自：工作区和暂存区 - 廖雪峰的官方网站 （做了点修改） 对照查看两张图： workspace 即工作区，逻辑上是本地计算机，还没添加到repository的状态； staging 即版本库中的stage，是暂存区。修改已经添加进repository，但还没有作为commit提交，类似于缓存； Local repository 即版本库中master那个地方。到这一步才算是成功生成一个新版本； Remote repository 则是远程仓库。用来将本地仓库上传到网络，可以用于备份、共享、合作。本文将使用Github作为远程仓库的例子。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:3:2","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"三、Git for Windows软件安装 Git for Windows（又msysgit） 安装包可以到官方网站[1]下载，或者在github[2]下载。如果下载不下来，可以把链接复制下来用迅雷下载。如果用迅雷下载不放心，在下载完后去在github下载的那个地方查看SHA-256值，并和下载的文件对比，如果值一样就可以放心使用。 安装的时候一路点击Next就行了。 刚安装完打开后，窗口比较小。如果不太习惯，可以把它改大一些。 首先移到窗口右下角边缘，出现箭头后把窗口拉大。 点击窗口顶部左边的图标 -\u003e Options… -\u003e Window -\u003e Current size -\u003e OK 这样以后打开窗口都会是调整后的大小。 Git for Windows从2.8.0版本[3]开始，默认添加环境变量，所以环境变量部分就不用再手动配置了。（这句可以无视） ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:4:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"四、本地Git的使用 这里先不引入Github，而是在本地计算机上的操作。 打开命令行（cmd）或者在想要创建repository的地方右键鼠标并点击 Git Bash Here 打开窗口。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"1.新的仓库-》初始化 运行 git init 来初始化仓库，如下图： 它会创建一个隐藏的文件夹 .git 这里不去管它是用来干嘛的。关闭windows的显示隐藏的项目吧。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:1","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"2.文件的添加和提交 我在这个文件夹里面创建了一个 today.txt 的文件。并且不要脸地祝自己儿童节快乐： 这时我使用 git status 来查看有什么变化： 它告诉我有一个还未追踪的文件，并提示我可以使用 git add \u003cfile\u003e... 把它加进去。 但是我并不打算把所有命令都介绍一遍，所以我选择使用上面概览时所提到的 git add -A 命令。 嗯，什么提示都没有。没关系，我们再次使用 git status ： 状态变了！说明add成功。再看看它的提示 Changes to be committed ，也就是说现在可以执行commit了。下面一行则告诉你如何将文件从stage里移出，这里不管。 执行 git commit -m \"提交信息\" 将文件提交到repository里。提交信息用英文的双引号括起来。 这时运行 git log 就可以看到提交的记录了： 这样第一步就完成了。 也许你会奇怪：为什么要有一个add，直接commit不就行了？这是因为stage有很多用处，具体可以去查找相关资料。这里就算不了解问题也不大。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:2","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"3.文件的修改 接着我修改文件内容。改成祝大家儿童节快乐好了 (～￣▽￣)～ 我们用 git status 看看有什么变化： 这和之前的提示不一样了。之前是这个： 比较一下就会看到，之前的是添加新文件，当时文件还没被追踪（untracked），而这次是更改已经追踪（tracked）的文件。 现在我们通过git看看文件做了哪些变化，执行 git diff ： 它默认跟最新的一个commit进行比较。 红色（前面有减号-）表示删除，绿色（前面有加号+）表示添加。 因此，在git看来，我们是删除了原来那一行，并添加了新的两行。这在文件内容特别多的时候效果比较明显。 这个命令在以下情况可以使用： 你忘记改了什么，又想知道 别人发给你新版本，你想知道更改了什么 注：如果你用 windows 创建 txt 文件，并用自带文本编辑器来编辑文本，得到的编码是 GBK 。而 Git 读取文件时，使用 UTF-8 无 ROM 编码。因此会出现中文无法正常显示的情况。 假如我现在想撤销这些更改，执行 git checkout -- . 就行了： 恩，仍然没有任何提示。执行 git status 看看： 上一个status的提示已经不见咯。再来看看文件： 果然复原了！那么再次进行修改： 接着： git add -A git commit -m \"将[自己]改为[米娜桑]\" 用 git log 看看提交（commit）记录： 嗯。现在有两个提交了。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:3","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"4.版本回退！ 如果我写的是一篇很长的文章，并且在之前的版本基础上修改了一部分内容，生成一个新的commit，现在我发现我在修改的时候删掉了一部分内容，而这部分内容是我现在需要用到的，怎么办？版本回退！ 还是以刚才的文件为例，现在我试着将文件回退到第一个commit时的状态。但在这之前，我们看看这个文件夹里面的东西： .git 文件夹因为本来就是隐藏的，我在关了 显示隐藏的项目 的选项后，它就不显示了。 现在看到的是只有一个文件，而且是最新的一个版本。 文件的修改日期为 2016/6/1 21:52 从刚才的 git log ： 我们看到两行黄色部分是以 commit 开头的，后面接着一串字符。这一串字符是16进制的数，是一串哈希值。我们叫它版本号就行了。 开始回退，执行 git reset --hard 1df0573 （取版本号前7位就可以了）： 这里提示HEAD已经更改指向至1df0573了。此时文件： 其内容： 已经回到我的第一个版本的状态。 这里文件的修改日期被更改为我现在的时间 2016/6/2 19:29 这是由于文件的修改日期是由windows修改的，因为它检测到这个文件被修改了。而我们刚才从最新版本回退到现在这个版本，就像是我们手动修改了文件内容一样，事实上是由git来完成的。 其实可以不管上面这一段 现在再执行 git log ： 新版本的commit记录不见了！这就是 reset –hard 的力量，很好很强硬！ 现在已经看到了之前版本的内容，那么如何回到最新版呢？ 先执行 git reflog ： 可以看到HEAD的变化情况。 第一行表示当前HEAD所在的版本号是 1df0573 ，而之所以在这个版本号，是由于我们执行了reset命令。 看第二行，它告诉我们，这个HEAD所在的版本号是 ad93b89 ，这个版本号是在执行commit之后形成的。 此时我再用一次reset，将HEAD指向 ad93b89 ， 同时查看log ： git reset --hard ad93b89 git log 回到第一次reset前的状态了！ ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:4","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"5.清除未追踪的文件 通常在reset或者pull（后面会讲）之前要做两件事： 将新添加且为追踪的文件删除掉（比如编译程序后所产生的文件） 已追踪的文件已有修改，但又不需要这些修改，则将它们还原 还原已做修改的tracked文件，上面已经讲过。 现在看看如何用命令删除新加的文件。 首先我手动创建个文件，用来演示： 用checkout是没办法删除掉它的，使用 git clean -xf ： 这个命令的杀伤力比较大，它删除当前目录下所有没有track过的文件。不管它是否是.gitignore文件里面指定的文件夹和文件。当然，也有杀伤力比较小的，但这里就不介绍了。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:5","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"6.关于git status中文乱码问题 如果你的文件名是中文的，在使用git status时会乱码。如下图所示： 如果要使它显示为中文，在命令行里执行：git config --global core.quotepath false。 再使用 git status： 如果 git log 也会乱码，执行以下命令： git config --global i18n.commitencoding utf-8 git config --global i18n.logoutputencoding utf-8 请根据需要将后面的 utf-8 替换成你想要的编码。如果是团队项目，请确保所有成员的设置都一致。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:5:6","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"五、Github与Git的关联 上面的操作都是在本地计算机上产生影响的，一般也够用了。 如果你是程序员，想和其他人分享你的代码，或者合作开发，可以用Github。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:6:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"1）本地Git和Github的连接 到Github[4]注册账号。 本地配置用户名和邮箱（如果已经设置好，跳过该步）： git config --global user.name \"你的用户名\" git config --global user.email \"你的邮箱\" 如图所示： 或者你直接在config文件里改，位置在 C:\\Users\\你的用户名\\.gitconfig 。如下图所示，添加相应信息： 生成ssh key 运行 ssh-keygen -t rsa -C \"你的邮箱\" ，它会有三次等待你输入，直接回车即可。 将生成的ssh key复制到剪贴板，执行 clip \u003c ~/.ssh/id_rsa.pub （或者到上图提示的路径里去打开文件并复制）： 打开Github，进入Settings： 点击左边的 SSH and GPG keys ，将ssh key粘贴到右边的Key里面。Title随便命名即可。 点击下面的 Add SSH key 就添加成功了。 测试一下吧，执行 ssh -T git@github.com ： 嗯，这样就成功了！ 注： 对于 oschina 的 “码云” ，执行 ssh -T git@git.oschina.net 对于 coding 的 “码市” ，执行 ssh -T git@git.coding.net ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:6:1","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"2）创建远程仓库并与本地关联 创建远程仓库 首先是在右上角点击进入创建界面： 接着输入远程仓库名： 点击 Create repository 就创建好了。其他选项可以暂时不管。 将远程仓库和本地仓库关联起来 先到Github上复制远程仓库的SSH地址： 有两种方式可以关联，一种是SSH，一种是HTTPS。由于HTTPS比较慢，所以推荐使用SSH。 注意SSH的地址格式是这样开头的： git@github.com 运行 git remote add origin 你复制的地址 ： 如果你在创建 repository 的时候，加入了 README.md 或者 LICENSE ，那么 github 会拒绝你的 push 。你需要先执行 git pull origin master。 执行 git push -u origin master 将本地仓库上传至Github的仓库并进行关联： 关联已经完成！ 以后想在commit后同步到Github上，只要直接执行 git push 就行啦： 可以在Github上看到修改： ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:6:2","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"六、团队合作开发 关于团队合作开发，我在之前已经专门用一篇文章来说明了。 原文链接： GitHub团队项目合作流程 上文的目录： 零、前期准备 一、创建开发分支 二、Fork项目到个人的仓库 三、Clone项目到本地 四、和团队项目保持同步 五、push修改到自己的项目上 六、请求合并到团队项目上 七、团队项目负责人审核及同意合并请求 其中 零、一、七 是由团队项目负责人来完成的。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:7:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"七、Github的其他介绍 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:8:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"1）获取其他人的远程仓库 看到别人的代码，想要获取到本地计算机慢慢研究或者修改，可以用Git将其下载下来。 以我和一位同学合作的仓库为例，链接：schaepher/blogsbackup 这是我们用来备份博客园博客的一个小程序，主要用于助教备份学生的博客。 复制别人远程仓库的SSH。方法和上面关联git一样，进入仓库页面后，就能找到SSH地址。 执行 git clone 复制的SSH地址 整个项目42M，花了点时间才clone下来。 有一点要注意：这样直接clone别人的仓库后，不能push上自己的修改。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:8:1","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"2）另一种关联远程空仓库的方法 在知道了clone之后，你就可以更简单的创建并关联一个空仓库了。 在Github上创建仓库，上面有提到，这里不多讲 复制刚才创建的仓库的SSH clone到本地 这样可以不用再做关联了。省去了上面的 init 和 git remote add origin 以及 git push -u origin master 。 ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:8:2","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"3）使用GitHub的好处 全球最大的同♂性交友网站（逃 （严肃脸）很多牛逼程序员和牛逼公司的开源项目都放在这上面，有丰富的资源可以学习 别人（如HR）可以通过你的Github大致了解你的水平。《怎样花两年时间去面试一个人》 这篇文章的后面部分讲到了Github ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:8:3","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"八、一些可能碰到的问题 这篇文章基本只介绍主线操作，而在操作过程中，可能有误操作或者其他问题。我把这些问题集中放在另一篇博客里面（这里列出目录）。这样这篇文章不会显得太长。 一、 解决merge时出现的冲突 二、 回退一个merge 三、 获取某一commit的修改 四、 将低版本push到Github（删掉高版本Commit） ","date":"2016-06-05","objectID":"/2016/06/05/git-and-github/:9:0","tags":["Git"],"title":"Git和Github简单教程","uri":"/2016/06/05/git-and-github/"},{"categories":null,"content":"目录： 减少【.git】文件夹的大小和文件数 更换git for windows的文本编辑器 修改已经提交的commit说明 合并commit 解决merge时出现的冲突 回退一个merge 获取某一commit的修改 将低版本push到Github（删掉高版本Commit） ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:0","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"减少【.git】文件夹的大小和文件数 随着commit次数的增多，.git文件夹的文件数和文件夹大小都会不断增大。 虽然对于小项目，增大的速度极慢，文件夹也基本在10M左右。但如果你和我一样，想减少该文件夹的文件数目（通常不少），可以试试这个命令。当然，git是鼓励你多使用这个命令的。见：Git - git-gc Documentation Users are encouraged to run this task on a regular basis within each repository to maintain good disk space utilization and good operating performance. 如何减少？特别简单，就是你进入到一个repository，也就是你项目的根目录，执行 git gc 就行了。 我们来看看执行gc命令前后的对比： 运行 git gc 前： 运行 git gc 后： 文件夹大小变化不大。变化最大的是文件数目（2561 -\u003e 37)和文件夹数(274 -\u003e 18)。 要讲清楚这个命令，涉及到了git是如何存储你的commit。这就要说到快照（Snapshot）。 我们先看看git的官方说明：直接记录快照，而非差异比较 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。 那么问题来了：什么是快照？ 来实际感受一下： 首先我准备了一个大的文件，这样比较能说明问题。刚好今天整理一个文件，有55.3MB，就顺手拿来用了。 可以看到，源文件是55.3MB，而将其添加到仓库里后，仓库的大小变为46.3MB。这相当于是将整个文件复制到里面去。 如果这还不能说明问题，再来一次。我删除了文件里的一大部分内容，将其减小至20.5MB。再将其提交到仓库里面。 增加了19MB，跟20.5MB很接近。 你可以在 .git\\objects\\某个文件夹 里找到这两份快照。 于是我们可以得出一个近似的结论：在git中，一份快照就是你当前工作目录的状态。当你使用add命令时，它将你当前工作目录的文件进行压缩，形成一份快照。 不过，如果你有一些文件完全没有被修改过，它只保存指向之前版本的引用。这样可以减少快照所占用的空间。这里没有试验，但是官方对此有说明： To be efficient, if files have not changed, Git doesn’t store the file again, just a link to the previous identical file it has already stored. 那么这个时候我们再去执行 git gc ，会变成什么样呢？ 之前说的三个方面都有减少。 我们再看看.git\\objects里的文件夹（事先不知道它会删除文件夹，没有截图。。） 可以看到我们上面快照所在文件，即02和f7文件夹已经不在objects这个文件夹里面了。（info和pack两个文件夹一直都在那里） 点进pack文件夹： .git 文件夹的46.5M都在这里了。所以我们可以知道，git将之前两个文件夹的快照文件合并到一起了。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:1","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"更换git for windows的文本编辑器 git for windows默认使用vim作为文本编辑器，为此我专门写了篇vim的基本操作：vim编辑器的简单使用 如果你不想学习vim的使用，也可以把它换掉。 例如我想把它换成atom： 先找到启动atom的exe文件的路径。我的在 C:\\Users\\Schaepher\\AppData\\Local\\atom\\app-1.13.0\\atom.exe 启动git for windows，执行git config --global core.editor \"C:/Users/Schaepher/AppData/Local/atom/app-1.13.0/atom.exe --new-window --foreground --wait\" 注意，这里路径的斜杠与Windows显示的相反，这是Linux的路径格式。 后面一串参数--new-window --foreground --wait是由各编辑器自己指定的。如果不这样指定，执行git rebase -i commitId^的时候会直接退出编辑。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:2","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"修改已经提交的commit说明 先用git log查看commit信息： 我打算更改下面那个commit，使用git rebase -i 版本号^： 执行命令后，会进入这样的界面： 它把我们传入的版本号之上的commit条目都显示出来了，这里只关注我们要改的那一条。将第一个pick修改为reword，保存并退出。 过一会儿，它会再进入这样的界面： 将第一行的Android的ListView改为这个更改后的message，保存并退出。 再用git log查看： 不仅commit message被更改了，从被更改的commit开始，commit id都会重新生成。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:3","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"合并commit 先用git log查看commit信息： 如果你想把最近的四个commit合并成一个commit，有两种方法。一种是用git reset --soft d7ac，在git commit -m \"新的commit message\"，另一种是用git rebase。接下来讲第二种。 首先根据上图的commit id，我想把afe14f之后的commit合并到afe14f里面，执行 git rebase -i afe14f^。进入编辑界面： 根据提示，squash会把所在的commit合并到前一个commit上面。我们要合并到afe14f，所以修改后三个。而在合并之后，我们需要修改afe14f的commit message，所以使用reword。 你也可以用缩写，比如squash的缩写是s。而reword的缩写是s。 保存并退出，会进入下一个界面。修改第一行的commit message，即reword的那个message，为添加Android学习笔记，特别是ListView的介绍；添加对git commit的修改教程。如下图： 保存并退出。自动进入下一个界面： 此时要将其他三个message去掉，只要在那三行前面加#就行了。如下图： 保存并退出，等待git处理完成。 再次使用git log查看commit信息： 完成！ 这里貌似可以不使用reword，待实验。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:4","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"解决merge时出现的冲突 当你和其他团队成员对同一个文件进行修改后，merge的时候有可能会出现冲突。你可以打开每个冲突的文件，手工解决冲突；也可以借助冲突处理工具来解决冲突。这里分别介绍这两种方式： 手工解决冲突 冲突提示如下图所示： CONFLICT表示有冲突，在这一行的末尾，显示冲突文件。这里有两个文件冲突，分别是README.md和app.iml 这里以README.md为例，解决冲突： 被红框框住的符号 ======= 是冲突的分割线。 \u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD 和分割线之间的是本地的文本，分割线和 \u003e\u003e\u003e\u003e\u003e\u003e\u003e upstream/dev 之间的是远程分支的文本 你可以选择保留其中一个版本的文本，然后将三个冲突符号都删除。这样表示已解决冲突。如果你想同时保留两个版本，那么只需将冲突符号删除。 解决冲突后如下图所示： 借助冲突处理工具 个人认为Meld这个工具比较好用，Android Studio自带的冲突处理工具和它很相似。我用过tortoisegit的工具，感觉没有Meld好用，这里就不介绍了。 （1） 首先去Meld的官网下载安装文件并安装。-\u003e点此进入Meld官网 （2） 安装完后，打开你的git工具，比如msysgit。执行 git config --edit --global ，此时会打开一个配置文件。在文件最后添加以下四行： [merge] tool = meld [mergetool \"meld\"] path = e:/software/MeldMergeTool/Meld.exe 提示：path是根据你安装Meld的路径来决定的，同时要把路径中的 \\ 改成 / 。从上面可以看出我的安装路径为 e:\\software\\MeldMergeTool\\ 。 （3） 在merge的时候，如果出现冲突，运行命令 git mergetool 这时就会打开Meld。 （4） Meld的界面如下： 冲突的地方会显示红色，如果你想保留本地的代码，则点击左边的 → 箭头。 把所有红色（冲突）区域解决后，可以根据实际情况去解决绿色（添加）和灰色（更改）。 一般保存中间的修改就行。如上图红框处。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:5","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"回退一个merge 如果是merge一个GitHub的Pull Request，可以进入要回退的那个Pull Request，在下面有一个revert按钮，可以用来revert一个Pull Request。如下图红框处： 在命令行里revert （1）用 git log 看commit记录 现在我们要回退 commit 561dab （也就是图中第一个commit），该commit将Pull Request #113 merge到项目中。 （2）使用 git revert HEAD -m 1 命令回退 如果是非merge的回退，用 git revert 版本号 就行了。但是这里是对merge操作进行revert，需要加上参数 -m 。命令最后加个 1。 为什么要加上 1 呢？看上面（1）的图中的第二个红框，这个 1 对应红框中的 6a3c30c 版本。而如果填 2 ，则对应 b7831df 。 继续看log，会发现 6a3c30c 是merge这个Pull Request之前的状态。而 b7831df 则是当前版本之前的一个merge。 输入命令回车后，会跳出一个文本。 目前无视它就行。关闭文本，回到shell，回车。 回退成功！这个回退不会删除掉中间的commit记录，而是将这次revert作为一个commit加到commit记录上面。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:6","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"获取某一commit的修改 假设有commit a b c ，从左到右，c 为最新版。 这时你发现 b 的一个修改有问题，想回退到 a 。但是如果回退到 a ， c 的commit也会被取消。 这时可以用 git cherry-pick 版本号 这个命令获取 c 的commit。 下图是示例的log记录，从①可以看出，这里从②回退到⑤。 现在我想获取④的commit。使用 git cherry-pick 版本号 将选定版本的提交合并到当前版本。 ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:7","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"将低版本push到Github（删掉高版本Commit） 有时候会因为各种原因，想要回退版本。如果没有关联Github或者没有push上去，那问题不大。但是如果你已经push到Github上了，这时候就比较尴尬了，因为普通的push是会被Github拒绝的。虽然Github提供了Revert功能，但是这并不能完全消去一个commit。 先看看reset后被拒绝的样子： 解决方法就是： 先用 git reset --hard 版本号 回到你想要的版本 执行 git push --force 再看看Github： 当然，一般是推荐用 git push origin HEAD --force 的，能防止因为其他没配置好而产生错误。对我来说差别并不大…… ","date":"2015-11-16","objectID":"/2015/11/16/git-advanced/:1:8","tags":["Git"],"title":"Git的其他用法","uri":"/2015/11/16/git-advanced/"},{"categories":null,"content":"已在另一篇博客中写出关于以下问题的解决，点此进入： 同步团队项目到本地时出现冲突怎么办？ 项目负责人merge一个Pull Request后发现有错怎么回退？ ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:0:0","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"目录： 零、前期准备 一、创建开发分支 二、Fork项目到个人的仓库 三、Clone项目到本地 四、和团队项目保持同步 五、push修改到自己的项目上 六、请求合并到团队项目上 七、团队项目负责人审核及同意合并请求 注：其中 零、一、七 是由团队项目负责人来完成的。开发人员只要从 二 开始就行了。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:0","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"零、前期准备： 首先把队友直接push的权限关掉，即设置成Read。这样可以防止队友误操作，未经审核就把代码push到团队项目上。 Teams用来分配issue的时候会用到，所以保留下来，并不是没有用。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:1","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"一、创建开发分支 master分支一般用来发布稳定版本，dev分支（开发分支）用来发布开发版本。 输入分支名称后，下面会跳出Create branch，点击即可创建。 下面图片写的是develop，是因为我们这个项目已经有dev分支了。如果你们没有dev分支，那么名字改成dev即可。这个影响不大。 分支创建完毕后，会自动跳转到dev分支。由于dev分支是从master分支上创建的，因此内容与master分支一致。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:2","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"二、Fork项目到个人的仓库 点击右上角的Fork，并选择你的账号（一般在第一个）。就可以Fork团队项目到个人仓库啦。 Fork完成后 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:3","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"三、Clone项目到本地 首先是clone，clone的地址可以直接点击按钮复制（如下图）。 推荐使用SSH协议，用HTTP协议有时会出问题。 注意，这里clone的是你自己仓库里的项目 打开git命令行，输入指令和刚才复制的地址，回车即可克隆到本地 此时你只能看到master分支，并没有把dev分支clone下来。使用 git branch 命令查看本地分支，发现本地只有master分支。如下图的① 如上图的②，使用 git branch -a 查看所有分支，就能看到远程分支。 根据远程分支，我们可以创建一个新的本地分支dev，并把该项目的dev分支的内容放到本地dev分支。如上图③。 git checkout -b dev origin/dev 的意思是，创建一个dev分支（-b），并把远程dev分支（origin/dev）的内容放在该分支内。接着切换到该分支（checkout） 现在使用 git branch 可以查看两个分支，并且用 ls 或者 dir 就能看到dev分支的内容了。想切换回master分支的时候，再用 git checkout master 即可。 上面的操作完成后，你就可以在本地进行开发了。但是如果要将你修改完的代码合并到团队项目上，还需要进行下面的操作。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:4","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"四、和团队项目保持同步 首先查看有没有设置upstream，使用 git remote -v 命令来查看。如下图① 如果没有显示upstream，则使用 git remote add upstream 团队项目地址 命令。如上图② 接着再次使用 git remote -v ，如果如上图③，显示出了upstream，那么就设置好了 开始同步。首先执行 git fetch upstream 获取团队项目最新版本。如下图① 此时并没有把最新版本合并到你本地的分支上，因此还需要一步。如上图②，当前分支是dev分支，执行 git merge upstream/dev 命令后，会将源分支（upstream/dev）合并到当前分支（dev）。 如果你是在本地的master分支上开发，那么在使用该命令前，先切换到master分支。 merge的时候，有可能碰到冲突。需要解决冲突才能继续下面的操作。冲突的解决可以参考→ 冲突的解决 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:5","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"五、push修改到自己的项目上 解决冲突后，就可以使用 git push 命令将本地的修改同步到自己的GitHub仓库上了。 注意，在当前所在分支使用push，会push到与这个分支相关联的远程仓库分支。这里dev分支与origin/dev关联，因此push到GitHub上的dev分支。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:6","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"六、请求合并到团队项目上 首先到你的GitHub上，进入你Fork的仓库里。点击红框处的Pull request 下图左边红框，表示要合并到fzu2015/CourseManagement项目的dev分支。 下图右边红框，表示要从自己仓库的dev分支发起合并请求。 点击红框处的 Create pull request就可以发送合并请求了。 当然，在发送请求之前，你可以检查一下你都改了哪些东西。在上面那个页面往下拉，就可以看到两者的对比。如下图 以上操作结束后，团队成员的流程就结束了。最后一步交给团队项目负责人来完成。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:7","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"七、团队项目负责人审核及同意合并请求 首先进入GitHub的团队项目仓库中。此时右边的Pull requests显示当前项目有几个Pull request。点击进入查看。 选择一个Pull request 项目负责人审核有两个要注意的地方 一个是下图的①。一定要看清楚是合并到哪个分支。这里是从schaepher的dev分支合并到fzu2015的dev分支。 另一个是下图的②。点击进去后，就可以查看该Pull request对项目做了哪些修改。这样如果有问题，可以及时发现，并关闭该Pull request。 如果关闭了，一定要告诉队友，否则他可能会不知道。虽然也可以直接在下面发布Comment告诉他，但队友不一定看到。 如果没有问题，可以点击Merge pull request。这样就合并好了。 ","date":"2015-11-03","objectID":"/2015/11/03/git-teamwork/:1:8","tags":["Git","GitHub"],"title":"GitHub团队项目合作流程","uri":"/2015/11/03/git-teamwork/"},{"categories":null,"content":"编码字符集、字符编码 编码字符集： 表示某种编码所涉及到字符的集合。例如ASCII字符集、GB2312字符集。仅表示集合，集合元素（即字符）按照某种顺序排放，并编上序号。如Unicode。 字符编码： 把字符集中的字符 编码为 二进制，用来表示字符集中的字符，是字符集的实现方式。如UTF-8，UTF-16，UTF-32就是Unicode的实现。 一、欧美编码 发展关系： ASCII -\u003e EASCII -\u003e ISO 8859 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:0:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"ASCII 最初的ASCII只有七位（用于36位计算机），由ANSI协会于1963年公布首版标准。只能支持基础拉丁字符。 记得 ANSI C 标准吗？就是这个协会发布的。 注：我们现在通常说到ANSI编码，通常指的是平台的默认编码，例如 英文操作系统中是ISO-8859-1，中文系统是GBK 。 图片来自：https://en.wikipedia.org/wiki/ASCII wikipedia：Eventually, as 8-, 16- and 32-bit (and later 64-bit) computers began to replace 18- and 36-bit computers as the norm, it became common to use an 8-bit byte to store each character in memory, providing an opportunity for extended, 8-bit, relatives of ASCII. 而随着8位、16位、32位开始替代18位和36位计算机，此时ASCII可以从7位扩展到8位，扩展后的ASCII称为EASCII。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:1:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"EASCII（Extended ASCII） EASCII兼容ASCII，在其前面扩展一位。它解决了部份西欧语言的显示问题。此外还扩充了符号，包括制表符、计算符号、希腊字母和特殊的拉丁符号。 图片来自：http://blog.jobbole.com/86813/ 虽然它解决了西欧字符的编码问题，但是对于欧洲其他地方却没有办法。于是出现了ISO 8859。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:2:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"ISO 8859 ISO 8859是单字节编码，兼容ASCII。主要用于欧洲。 它本身不是一个标准，而是一系列标准，由15个字符集所组成。表示为ISO 8850-n (n = 1,2,3 …11,13…16，没有12)。 ISO 8859-1 有个别名：Latin-1，是西欧常用字符，包括德法两国的字母。 其00000000(0×00)-01111111(0x7F)范围段与ASCII保持一致， 而10000000(0×80)-11111111(0xFF)范围段被扩展用到不同的字符集。 二、中文编码 兼容关系 GB2312 \u003c GBK \u003c GB18030-2000 \u003c GB18030-2005 GB2312与BIG5有冲突 GB2312 有6763个汉字， GBK 有21003个汉字， GB18030-2000 有27533个汉字， GB18030-2005 有70244个汉字。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:3:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"GB 2312-1980 又简称为GB 2312或GB 2312-80。1980为该字符集发布年份。GB为国标（国家标准）拼音首字母。 GB 2312将ASCII里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的”全角”字符，而原来在127号以下的那些就叫”半角”字符。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:4:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"GBK GBK中英文都用两个字节来表示，兼容GB2312，加入对繁体字的支持。K为扩展的Kuo首字母。 windows用CP936来实现对GBK字符集的编解码。因此有些地方charset=windows-936就是指GBK。 GB 2312的代码页为：CP20936 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:5:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"GB 18030-2000 GBK的取代版本,在GBK基础上增加了CJK统一汉字扩充区A的汉字。 CJK: China Japan Korea，中日韩 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:6:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"GB 18030-2005 在GB 18030-2000的基础上加上了CJK统一汉字扩充区B的汉字，以及少数民族的文字。 GB 18030与GB 2312-1980完全兼容，与GBK基本兼容，支持GB 13000及Unicode的全部统一汉字，共收录汉字70244个。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:7:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"BIG5 与GB2312有冲突。繁体字符集。Big-\u003e大写。 BIG5是使用繁体中文（正体中文）社区中最常用的电脑汉字字符集标准，共收录13,060个汉字。BIG5虽普及于台湾、香港与澳门等繁体中文通行区，但长期以来并非当地的国家标准，而只是业界标准。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:8:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"BIG5-2003 2003年，Big5被收录到CNS11643中文标准交换码的附录当中，取得了较正式的地位。这个最新版本被称为Big5-2003。 Unicode 尽管人们能够在一台计算机上查阅不同语言的文档，但是一份文档无法同时使用多种编码。特别是在网络上，各个国家互相访问的时候，会出现乱码。Unicode就是用来解决这个问题的。 Unicode基于通用字符集（Universal Character Set）的标准来发展，它几乎涵盖了各个国家语言可能出现的符号和文字，并为它们编号。 Unicode中文范围 4E00-9FBF：CJK 统一表意符号。 Unicode就是一开始提到的编码字符集，而UTF-8、UTF-16、UTF-32就是字符编码，即Unicode规则字库的实现形式。 Unicode使用4字节的数字来表达每个字母、符号，或者表意文字。但是这样有一个问题：如果直接采用4字节的编码，那么就会浪费大量空间。例如，一个存储器本来可以存储800份以ASCII编码的文档，现在用Unicode编码，只能存储200份，这样大量的存储空间就被浪费了。 UTF-8巧妙地解决了这个问题。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:9:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"UTF-8 UTF-8使用可变长的编码，例如 ASCII 部分仍然使用一个字节，中文用两个字节……。也因为如此，它没有实现所有的 Unicode 的字符，它只实现了Unicode的Plane 0（Unicode共有16个Plane。Plane 0又称为BMP，Basic Multilingual Plane）。 UTF-8的最小编码单位为一个字节。一个字节的前1-3个bit为描述性部分，后面为实际序号部分。其编码规则如下： 如果字节以0开头，则代表当前字符为单字节字符。 如果字节以110开头，则代表当前字符为双字节字符。其第二个字节以10开头。 如果字节以1110开头，则代表当前字符为三字节字符。其第二、第三个字节以10开头。 如果字节以10开头，则代表当前字节为多字节字符（即上面那两点）的第二或第三个字节。 是不是感觉跟 IP 地址的 A, B, C, D 类的划分很像？这种分类方式应该可以推广到很多地方去。 UTF-8与中文： GB13000.1-1993 的字符集包含 20902 个汉字。Unicode 标准目前在基本平面上与 GB 13000 保持一致。采纳 UTF-16 方案作为未来实现 01H 到 0FH 共15个辅助平面的方式。其它方面与 GB 13000 基本一致。 UTF-8 与 GBK 的互相转换是通过查表来实现的。UTF-8 只是 Unicode 的一种实现，当它要转码时，要转换成 Unicode 码，查 Unicode 表。 关于查表，我去看了 Unicode 表，只有该字符集编码和对应的中文，并没有其他编码的码值。只有这样可是没办法转换的啊，因为计算机只能识别0和1。 于是我推测了一下：编码字符集存储在数据库。在数据库中，一个字符同时存储了几种编码方式所对应的码。在从 编码A 转换为 编码B 的时候，先在数据库中根据 编码A 寻找对应的字符，接着读取其在 B 中的编码。 UTF-8 的缺陷是无法表示 Plane 0 外的字符。而UTF-16可以应对这种情况。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:10:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"UTF-16/UCS-2 UCS = Universal Character Set UTF-16由UCS-2发展而来。UCS-2最初设计的时候只考虑到BMP（Plane 0）字符，因此使用固定2个字节长度，也就是说，它无法表示Unicode其他层面上的字符，而UTF-16为了解除这个限制，支持Unicode全字符集的编解码，采用了变长编码，最少使用2个字节，如果要编码BMP以外的字符，则需要4个字节结对。 UTF-8和UTF-16的变长是不一样的。UTF-8以一个字节为单位，扩展时，可以1-\u003e 2-\u003e 3字节；而UTF-16以两个字节为单位，扩展时只能2-\u003e 4字节。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:11:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"UTF-32/UCS-4 将UCS-4的BMP去掉前面的两个零字节就得到了UCS-2。在UCS-2的两个字节前加上两个零字节，就得到了UCS-4的BMP。而目前的UCS-4规范中还没有任何字符被分配在BMP之外。 本来UTF-32是UCS-4的一个子集。但现在它们几乎完全一样，不过UTF-32添加了额外的Unicode语义（可以编码更多的Unicode字符）。 ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:12:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"UTF-8 with BOM BOM的全称是Byte Order Mark，BOM是微软给UTF-8编码加上的，用于标识文件使用的是UTF-8编码，即在UTF-8编码的文件起始位置，加入三个字节“EF BB BF”。 这是微软特有的，Windows 的记事本（即新建文本文档，txt）就是用UTF-8 with BOM。UTF-8标准并不推荐包含BOM的方式。采用加BOM的UTF-8编码文件，对于一些只支持标准UTF-8编码的环境，可能导致问题。比如，网页第一行可能会显示一个“?”，明明正确的程序一编译就报语法错误，等等。 既然有如上缺点，为什么还要用BOM呢？因为计算机系统分为大端模式和小端模式。 简单说就是在处理多字节的时候，可能按照我们的逻辑排列字节，即把数字大的部分放在地址大的部分，数字小的放在地址小的部分，叫做小端模式；也可能反着来，叫做大端模式。 例如对于内存中的数据：e6 84 6c 4e（从左到右地址增大） 大端模式读取：e6 84 6c 4e 小端模式读取：4e 6c 84 e6 详细请看：大小端模式-百度百科 如果同一篇文档，在大端模式下编写，传到小端模式的机器上，就会读取错误。然而只要在文档的开始，告诉计算机所要读取的文档是在大端模式还是小端模式上写的，就不怕读取顺序出错了。 参考资料： 十分钟搞清字符集和字符编码 - 伯乐在线 字符集和字符编码（Charset \u0026 Encoding） - 伯乐在线 字符编码常识及问题解析 - 伯乐在线 关于字符编码，你所需要知道的 - 伯乐在线 中文字符集编码Unicode ,gb2312 , cp936 ,GBK，GB18030 - 博客园 ASCII - Wikipedia UTF-16 - Wikipedia UTF-32 - Wikipedia Byte order mark - Wikipedia 程序员趣味读物：谈谈Unicode编码 大小端模式-百度百科 Unicode、GB2312、GBK和GB18030中的汉字 and so on…… ","date":"2015-10-02","objectID":"/2015/10/02/encoding/:13:0","tags":["编码"],"title":"字符编码总结","uri":"/2015/10/02/encoding/"},{"categories":null,"content":"做自己的领导 “做自己的领导”这个概念是我近期自己想出来的。我在写这篇文章时，到网上大致搜索了一下，有一个和我定义很像的一个概念，叫“自我领导”。 自我领导指的是个体通过必要的自我指导和自我激励从而取得行为绩效的自我影响过程。 “做自己的领导”这个概念进入我大脑的背景是三月初的一次周会，领导要求开会时组长要把目标和完成情况列出来。开会时虽然组长有列出一些，但没有达到领导想要的样子。 我大致理解了领导想要的样子，然后确认一些细节。我之所以能快速理解，是因为我在通过不断思考和理解工程思想后，知道了想要高效率地把事情做好就必然需要掌握目标和完成情况。 在会议结束后，我做了个 Excel 表格，把我近期的某一项工作按照我理解的样子填进去，然后找领导确认。 时间段 目标 阶段性目标 当前状态 备注 2021Q3 XXX系统上线 1. XXX 1.1 修改XXX 1.2 发布XXX 1.1 修改XXX 2. XXX 2.1 XXX 3. XXX 3.1 XXX 领导指出我对他说的“目标”的理解有偏差。我列出的目标实际上是完成目标的事项，而目标应该是“解决XXX问题”。 我把原先的“目标”转为事项，然后在“目标”下面填写“解决XXX问题”。 时间段 目标 事项 阶段性目标 阶段性事项 当前状态 备注 2021Q3 解决XXX问题 XXX系统上线 1. 解决 X 问题 1. XXX 1.1 修改XXX 1.2 发布XXX 2. 解决 Y 问题 2. XXX 2.1 XXX 1.1 修改XXX 3. 解决 Z 问题 3. XXX 3.1 XXX 这样，就把“做什么事情”的视角转变为“为了解决什么问题而做什么事情”的视角。这才是领导应该关心的。 再次找领导确认前，我觉得这张表里缺少了一些很关键的信息。现在目标有了，事项也有了，但是什么时候完成呢？ 如果我不把什么时候完成写进去，由于通常会有 N 个事项需要完成，而我有时候会去选择自己喜欢做的事情，或者别人临时找我处理 bug，导致优先级混乱，没有优先完成重要且紧急的事情。 重要且紧急的事情对于我和领导来说，不一定总是一致的。那么我就需要和领导同步对一件事情的重要和紧急程度。这时可以通过加入“预计完成时间”，让查看这张表格的人知道我对一件事情的重要程度和紧急程度的理解，在必要情况下可以帮我调整。 另外我需要知道我自己给自己预计的时间和实际完成时间有多少差距，这是一个主要给我自己看的数据，但是领导也会关心。 时间段 目标 事项 阶段性目标 阶段性事项 预计完成时间 实际完成时间 当前状态 备注 2021Q3 解决XXX问题 XXX系统上线 1. 解决 X 问题 1. XXX 1.1 修改XXX 1.2 发布XXX 2021-03-10 - 2. 解决 Y 问题 2. XXX 2.1 XXX 2021-04-01 - 1.1 修改XXX 3. 解决 Z 问题 3. XXX 3.1 XXX 2021-04-20 - 在现代软件工程中，特别是像敏捷开发这种，要求做相当精细的预估。在上软件工程课的时候，我用的好像是半小时粒度的预估。在实际的工作中，这种方式其实还没有完全推广开来。 工作团队如果没有按照这种方式推进项目，就需要我们自己来把控。让自己清楚项目的发展方向，以及各个阶段的事项，做到心中有数。这就是“做自己的领导”。 领导确认没有问题后，我把近期的所有事项都填进去，发给组长（感觉直接发给领导不太合适）。组长可根据我的表格调整会议上的表格形式，但我没有明确说，只是问他我这样安排是否合理。组长让我按照我的安排来做。 实际上这张表主要是想让我自己看的，用来管理我自己的安排。但其附带作用是我可以定期发给我的组长，让他清楚我在做哪些事情，以及接下去的规划是什么。 职场中有一个“向上管理”的概念，向上管理指为了给公司、给上级及自己取得最好的结果而有意识地配合上级一起工作的过程。 虽然“做自己的领导”的出发点是管理自己，但实际上可以起到一定的“向上管理”的作用。 ","date":"0001-01-01","objectID":"/drafts/2021-life-thinking/:1:0","tags":null,"title":"","uri":"/drafts/2021-life-thinking/"},{"categories":null,"content":"家庭 家庭是一个无论如何都无法绕开的话题。特别是爱情和婚姻，在年龄增长到一定程度后，会更多地进入视野。 我曾经在不同的场合阐述我对于爱情的看法。这些看法不是一成不变的，它会随着我思考的增多而不断发展。 如果让我用尽可能简洁的话描述我对爱情的看法，那就是： 爱情是双方为了让对方不断变得更好的同时获得幸福。 我认为这是一个感性和理性融合的表述。这句话可以用世界的发展规律和逻辑进行一定程度上的证明，不过我想大多数人的第一反应是抗拒这种证明，认为是对爱情的亵渎。但如果能分阶段思考，加入感性后，不但能感受到原先想象中爱情的美好，还会感受到更多美好。如果在初期思考中加入太多情感因素，对爱情的基础过于乐观，后期反而很可能会给双方带来痛苦。 这可以解释为什么有的人拥有了“爱情”，却活得非常痛苦。双方或者其中的一方只是为了通过爱情向对方索取他们自己所想要的东西。我不太想说得太残酷，但是应该可以大致提一句。强者往往会自觉或不自觉地剥削弱者。强者个人需要通过不断地学习和反思自身，以此避免做出剥削弱者的事情。 爱情是对双方个人的促进，而婚姻则是对两个家庭的促进。从感性的角度讲，婚姻双方个人可以更容易做到一个人做不到的事情。从客观事实上讲，婚姻实际上是把双方的各种资源（智力、知识、物质财富等等）结合在一起，达到 1+1 \u003e 2 的效果，是一种正向的促进作用。 但是并不是所有人恋爱和结婚都是为了互相促进，大多数是平凡地把日子过下去，少部分人会主动去剥夺对方的资源，还有一部分人为了防止资源被对方剥夺而尽早地把自己的资源和自己绑定起来。 从客观上来讲，这是一种浪费，但是我并不反对，我也没有资格反对别人的这种选择。现在全球的社会本身就是以一种低效的方式运行着，它需要我们所有人的努力，去提高这个效率。最终人们能以更高的效率获得幸福。我庆幸自己出生在中国，这是一个在世界上朝着高效社会发展的最重要的贡献者。 由于当前的全球社会仍然处于以及仍将长期处于低效的运行阶段，所以我们需要自己去思考和把握。一旦有幸遇到一个也原意促进对方发展的伴侣，那将是一件非常幸福的事情。 具体到我个人的规划，前一段时间和同龄的同事出去吃火锅，他们问我想找什么样的女生。我说大致有三种类型，三种中的任何一种都可以。 第一种是能和我一起把小家庭照顾好的人。对于能够多大程度上使我变得更好，我不太在意。我能够让自己不断进步，而且也能促使她得到发展，所以条件是不阻止我向前发展以及能够在我的帮助下不断变得更好。一旦放松了核心的需求，那么相应的对其他方面会有相对比较高的要求，例如外貌。 第二种是能和我一起整合双方家庭甚至家族资源的人。小家庭能够获得足够的幸福，那么就可以去想办法为家族里的其他人做点什么。由于我已经具备和多种类型的人沟通的能力，所以能够为缓解或者消除家族内部矛盾提供一些帮助，我希望能把双方的家族团结起来。由于核心需求要求变高，其他方面的要求会降低，例如外貌。 第三种是能和我一起想办法为世界的发展多做一些贡献的人。为世界做贡献看起来是一个很大的命题，通常人们会认为必须是一个很大很重要的事情才是为世界的发展做贡献。但实际上每个人都在无意识地为世界的发展做贡献。如果不能理解，那么可以细细体会“人民群众是历史的创造者”这句话。第三种其实是把这种无意识转变为有意识。一旦有意识的去做这些事情，那么效率将会无意识地做这些事情来得高得多。得到的满足感也会多得多。 看到这里，通常会觉得我更想要第二种和第三种。但实际上对我来说，没有多少区别，毕竟有得就有失。而且我自身的价值有限，能通过等价交换得到的价值被我自身的价值所限制。 脱单 结婚 孩子 教育 ","date":"0001-01-01","objectID":"/drafts/2021-life-thinking/:2:0","tags":null,"title":"","uri":"/drafts/2021-life-thinking/"},{"categories":null,"content":"生死 意外的死亡已经没有那么可怕了。即使是很可能我的很多思考没有办法共享出去。 因为我知道，在中国，有无数的人有和我类似的思考，以及有无数的人思考得比我多。只是他们还没有开始传播，或者传播了但我还没有阅读到他们的思考。 人类总是会向前发展。就算其他国家想停止脚步，中国人仍然会不断前行。 ","date":"0001-01-01","objectID":"/drafts/2021-life-thinking/:3:0","tags":null,"title":"","uri":"/drafts/2021-life-thinking/"},{"categories":null,"content":"理论和实践 In theory, theory and practice are the same. In practice, they are not. 理论上，理论和实践是一样的。在实践中，理论和实践是不一样的。 傲慢地认为自己看的是高深的书、学的是最先进的思想，认为人民群众是愚蠢的，需要被“启蒙”，人民群众只需要按照拥有“最先进”的思想的人的想法来做就行了，这是“公知”们被批评的重要原因之一。 5 月份应该就是《资本论》的第一篇和第二篇的总结了。其实就是把里面的一些概念给理清楚。有些地方可能会在没有理解清楚的情况下得出不准确的结论。例如“劳动力的价值”在第六篇才有比较详细地说明，这样我在只看了第一篇和第二篇的情况下，容易因为丢失一些信息而得到错误的结论。我打算在后续了解清楚后再回来修正。 ","date":"0001-01-01","objectID":"/drafts/2021-life-thinking/:4:0","tags":null,"title":"","uri":"/drafts/2021-life-thinking/"},{"categories":null,"content":"索引 劳动力的使用就是劳动本身。—— P207 ","date":"0001-01-01","objectID":"/drafts/das-kapital/:1:0","tags":null,"title":"","uri":"/drafts/das-kapital/"},{"categories":null,"content":"反例 https://zhuanlan.zhihu.com/p/159044137 https://zhuanlan.zhihu.com/p/347898806 体力的消耗比较好理解，我们可以直观地看到搬重物多的人需要吃更多饭，我们自己在体力劳动后会感受到疲惫（体力下降）。但不能根据体力劳动的这个特性简单地去推导智力的消耗，得到智力的消耗会使得人变笨（智力下降）这样一种错误的结论。智力的消耗是指一个拥有特定知识积累的大脑在运用时所需的能量的消耗，因为大脑的运转也需要人体提供能量。 大脑的重量大致占人体重的 2%，但是它消耗的能量占人体的 20% - 25%。 这时我们面对一个问题：那些思考最前沿科技的人和只做一点点思考的人，排除基础代谢所消耗的能量的补充外，在能量摄取上，似乎没有显著的差异。我解释是，在知识积累的过程中，大脑消耗一部分能量把这些知识固化在脑中，使得大脑可以仅使用少量的能量就能够运用这些知识。 注意：这些内容不作为论证的依据。 劳动力是否能够表示为完成某件事所需要的能量？两个人完成同一件事所需要的能量是不一样的。我们之所以认为他们消耗的能量一样，是因为我们只看到了他们从接受这件事开始所消耗的能量。实际上他们为此所消耗的能量早在他们小时候开始学习知识的时候就开始了。 为具备生产某种使用价值而积累知识时所耗费的能量。如果一个数学教授去饭馆洗盘子，并不会因为他积累了很多数学领域的知识而获得远超于其他洗盘工人在洗盘子方面的收入。 ","date":"0001-01-01","objectID":"/drafts/das-kapital/:2:0","tags":null,"title":"","uri":"/drafts/das-kapital/"},{"categories":null,"content":"要先有以下镜像才能开启 k8s： REPOSITORY TAG IMAGE ID CREATED SIZE docker/desktop-storage-provisioner v1.1 e704287ce753 4 months ago 41.8MB docker/desktop-vpnkit-controller v1.0 79da37e5a3aa 5 months ago 36.6MB docker/desktop-kubernetes kubernetes-v1.16.5-cni-v0.7.5-critools-v1.15.0 a86647f0b376 6 months ago 279MB k8s.gcr.io/kube-apiserver v1.16.5 fc838b21afbb 6 months ago 159MB k8s.gcr.io/kube-proxy v1.16.5 0ee1b8a3ebe0 6 months ago 82.7MB k8s.gcr.io/kube-controller-manager v1.16.5 441835dd2301 6 months ago 151MB k8s.gcr.io/kube-scheduler v1.16.5 b4d073a9efda 6 months ago 83.5MB docker/kube-compose-controller v0.4.25-alpha1 129151cdf35f 9 months ago 35.6MB docker/kube-compose-api-server v0.4.25-alpha1 989749268895 9 months ago 50.7MB docker/kube-compose-installer v0.4.25-alpha1 2a71ac5a1359 9 months ago 42.3MB quay.io/kubernetes-ingress-controller/nginx-ingress-controller 0.26.1 29024c9c6e70 10 months ago 483MB k8s.gcr.io/etcd 3.3.15-0 b2756210eeab 11 months ago 247MB k8s.gcr.io/coredns 1.6.2 bf261d157914 11 months ago 44.1MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 2 years ago 742kB ","date":"0001-01-01","objectID":"/drafts/docker-desktop-k8s/:0:0","tags":null,"title":"","uri":"/drafts/docker-desktop-k8s/"},{"categories":null,"content":"硬件 键盘 USB Host Shield Mini MicroPython ESP32 开发板 CH340G （USB 转 TTL） ","date":"0001-01-01","objectID":"/drafts/esp32-bluetooth-keyboard/:1:0","tags":null,"title":"","uri":"/drafts/esp32-bluetooth-keyboard/"},{"categories":null,"content":"电源 键盘使用 USB 连接。参考 USB 供电： USB 3.0 的电压是5V，电流是 1000mA USB 2.0 的电压是5V，电流是 500mA USB Host Shield Mini 的工作电压是 3.3V 但可通过改造给 USB 设备另外接入 5.0V 的电源[[1][1]]。 ESP32 开发板可以选 3.3V 和 5V。 CH340G 支持输出 3.3V 和 5V。 键盘可使用 3.3V 供电： https://blog.csdn.net/jgagdwp/article/details/80647090 ","date":"0001-01-01","objectID":"/drafts/esp32-bluetooth-keyboard/:2:0","tags":null,"title":"","uri":"/drafts/esp32-bluetooth-keyboard/"},{"categories":null,"content":"测试 USB Host Shield Mini ","date":"0001-01-01","objectID":"/drafts/esp32-bluetooth-keyboard/:3:0","tags":null,"title":"","uri":"/drafts/esp32-bluetooth-keyboard/"},{"categories":null,"content":"ESP32 开发板 [1]: ","date":"0001-01-01","objectID":"/drafts/esp32-bluetooth-keyboard/:4:0","tags":null,"title":"","uri":"/drafts/esp32-bluetooth-keyboard/"},{"categories":null,"content":" db.statistics.aggregate([{ \"$match\": { \"date\": 1598198400 } }, { $group: { \"_id\": \"$date\", \"Traffic|0000\": { $sum: \"$0000.Traffic\" }, \"Traffic|0001\": { $sum: \"$0001.Traffic\" }, \"Traffic|0002\": { $sum: \"$0002.Traffic\" }, \"Traffic|0003\": { $sum: \"$0003.Traffic\" }, \"Traffic|0004\": { $sum: \"$0004.Traffic\" }, \"Traffic|0005\": { $sum: \"$0005.Traffic\" }, \"Traffic|0006\": { $sum: \"$0006.Traffic\" }, \"Traffic|0007\": { $sum: \"$0007.Traffic\" }, \"Traffic|0008\": { $sum: \"$0008.Traffic\" }, \"Traffic|0009\": { $sum: \"$0009.Traffic\" }, \"Traffic|0010\": { $sum: \"$0010.Traffic\" }, \"Traffic|0011\": { $sum: \"$0011.Traffic\" }, \"Traffic|0012\": { $sum: \"$0012.Traffic\" }, \"Traffic|0013\": { $sum: \"$0013.Traffic\" }, \"Traffic|0014\": { $sum: \"$0014.Traffic\" }, \"Traffic|0015\": { $sum: \"$0015.Traffic\" }, \"Traffic|0016\": { $sum: \"$0016.Traffic\" }, \"Traffic|0017\": { $sum: \"$0017.Traffic\" }, \"Traffic|0018\": { $sum: \"$0018.Traffic\" }, \"Traffic|0019\": { $sum: \"$0019.Traffic\" }, \"Traffic|0020\": { $sum: \"$0020.Traffic\" }, \"Traffic|0021\": { $sum: \"$0021.Traffic\" }, \"Traffic|0022\": { $sum: \"$0022.Traffic\" }, \"Traffic|0023\": { $sum: \"$0023.Traffic\" }, \"Traffic|0024\": { $sum: \"$0024.Traffic\" }, \"Traffic|0025\": { $sum: \"$0025.Traffic\" }, \"Traffic|0026\": { $sum: \"$0026.Traffic\" }, \"Traffic|0027\": { $sum: \"$0027.Traffic\" }, \"Traffic|0028\": { $sum: \"$0028.Traffic\" }, \"Traffic|0029\": { $sum: \"$0029.Traffic\" }, \"Traffic|0030\": { $sum: \"$0030.Traffic\" }, \"Traffic|0031\": { $sum: \"$0031.Traffic\" }, \"Traffic|0032\": { $sum: \"$0032.Traffic\" }, \"Traffic|0033\": { $sum: \"$0033.Traffic\" }, \"Traffic|0034\": { $sum: \"$0034.Traffic\" }, \"Traffic|0035\": { $sum: \"$0035.Traffic\" }, \"Traffic|0036\": { $sum: \"$0036.Traffic\" }, \"Traffic|0037\": { $sum: \"$0037.Traffic\" }, \"Traffic|0038\": { $sum: \"$0038.Traffic\" }, \"Traffic|0039\": { $sum: \"$0039.Traffic\" }, \"Traffic|0040\": { $sum: \"$0040.Traffic\" }, \"Traffic|0041\": { $sum: \"$0041.Traffic\" }, \"Traffic|0042\": { $sum: \"$0042.Traffic\" }, \"Traffic|0043\": { $sum: \"$0043.Traffic\" }, \"Traffic|0044\": { $sum: \"$0044.Traffic\" }, \"Traffic|0045\": { $sum: \"$0045.Traffic\" }, \"Traffic|0046\": { $sum: \"$0046.Traffic\" }, \"Traffic|0047\": { $sum: \"$0047.Traffic\" }, \"Traffic|0048\": { $sum: \"$0048.Traffic\" }, \"Traffic|0049\": { $sum: \"$0049.Traffic\" }, \"Traffic|0050\": { $sum: \"$0050.Traffic\" }, \"Traffic|0051\": { $sum: \"$0051.Traffic\" }, \"Traffic|0052\": { $sum: \"$0052.Traffic\" }, \"Traffic|0053\": { $sum: \"$0053.Traffic\" }, \"Traffic|0054\": { $sum: \"$0054.Traffic\" }, \"Traffic|0055\": { $sum: \"$0055.Traffic\" }, \"Traffic|0056\": { $sum: \"$0056.Traffic\" }, \"Traffic|0057\": { $sum: \"$0057.Traffic\" }, \"Traffic|0058\": { $sum: \"$0058.Traffic\" }, \"Traffic|0059\": { $sum: \"$0059.Traffic\" }, \"Traffic|0100\": { $sum: \"$0100.Traffic\" }, \"Traffic|0101\": { $sum: \"$0101.Traffic\" }, \"Traffic|0102\": { $sum: \"$0102.Traffic\" }, \"Traffic|0103\": { $sum: \"$0103.Traffic\" }, \"Traffic|0104\": { $sum: \"$0104.Traffic\" }, \"Traffic|0105\": { $sum: \"$0105.Traffic\" }, \"Traffic|0106\": { $sum: \"$0106.Traffic\" }, \"Traffic|0107\": { $sum: \"$0107.Traffic\" }, \"Traffic|0108\": { $sum: \"$0108.Traffic\" }, \"Traffic|0109\": { $sum: \"$0109.Traffic\" }, \"Traffic|0110\": { $sum: \"$0110.Traffic\" }, \"Traffic|0111\": { $sum: \"$0111.Traffic\" }, \"Traffic|0112\": { $sum: \"$0112.Traffic\" }, \"Traffic|0113\": { $sum: \"$0113.Traffic\" }, \"Traffic|0114\": { $sum: \"$0114.Traffic\" }, \"Traffic|0115\": { $sum: \"$0115.Traffic\" }, \"Traffic|0116\": { $sum: \"$0116.Traffic\" }, \"Traffic|0117\": { $sum: \"$0117.Traffic\" }, \"Traffic|0118\": { $sum: \"$0118.Traffic\" }, \"Traffic|0119\": { $sum: \"$0119.Traffic\" }, \"Traffic|0120\": { $sum: \"$0120.Traffic\" }, \"Traffic|0121\": { $sum: \"$0121.Traffic\" }, \"Traffic|0122\": { $sum: \"$0122.Traffic\" }, \"Traffic|0123\": { $sum: \"$0123.Traffic\" }, \"Traffic|0124\": { $sum: \"$0124.Traffic\" }, \"Traffic|0125\": { $sum: \"$0125.Traffic\" }, \"Traffic|0126\": { $sum: \"$0126.Traffic\" }, \"Traffic|0127\": { $sum: \"$0127.Traffic\" }, \"Traffic|0128\": { $sum: \"$0128.Traffic\" }, \"Traffic|0129\": { $sum: \"$0129.Traffic\" }, \"Traffic|0130\": { $sum: \"$0130.Traffic","date":"0001-01-01","objectID":"/drafts/mongodb/:0:0","tags":null,"title":"","uri":"/drafts/mongodb/"},{"categories":null,"content":"一个人的世界观、人生观和价值观会极大地影响他的选择。所以当尝试了解别人或者了解自己的时候，效率最高的是从三观开始。 “三观”这个词经常被用到，但是用这个词的人恐怕并不知道三观是哪三观，以及它们各自具体指的是什么。因此先把三观详细列出来，避免理解和讨论时出现偏差。 世界观是人们对整个世界的总的看法和根本观点 人生观是指对于人类生存的目的、价值和意义的看法 价值观是指人们在认识各种具体事物的价值的基础上，形成的对事物价值的总的看法和根本观点 三者的关系： 世界观决定人生观和价值观。世界观对人生观、价值观的指导作用，主要是通过揭示世界的发展变化规律，指明社会历史和人的发展方向，为人生目的、人生道路的选择提供保证和服务。价值观是世界观和人生观的现实体现。 https://zhuanlan.zhihu.com/p/64075089?utm_source=ZHShareTargetIDMore 我想了解自己，所以需要尝试总结自己的三观。如果不加以总结，会很困惑为何自己有时候会做出矛盾的事情（实际上这种矛盾是表面上的矛盾，在深层次的逻辑上并不矛盾）。 我是如何看待这个世界的？ 世界观的基本问题是精神和物质、思维和存在的关系问题，根据对这两者关系的不同回答，划分为两种根本对立的世界观基本类型，即唯心主义世界观和唯物主义世界观。—— 来自百度百科 拥有哪种世界观，会对人产生哪些影响？ 目前我并没有深入学习哲学，所以没有能力详细讨论。课本上的知识基本都还给老师了。 让我印象深刻的唯心主义世界观的一支是我在看《明朝那些事儿》所了解到的王阳明的心学（属于主观唯心主义）。我还专门记了四句箴言：无善无恶心之体，有善有恶意之动；知善知恶是良知，为善去恶是格物。 我选择在实践的基础上理解世界。这让我可以使用合适的方法改造极小范围的世界，即我的周围。主观唯心主义会让我把意志而不是实践作为主要的改造世界的方法。客观唯心主义会让我把注意力集中在一个虚无的对象上，以致在碰到难题时放弃思考，转而寻求慰藉。 对于我来说，可证伪性更加重要。这也是科学与玄学最根本的区别。 因此在深层次的两个对立世界观中，我选择唯物主义世界观。 我是如何看待自己的人生的？ 人生观主要是通过人生目的、人生态度和人生价值三个方面体现出来的。每个人的人生观在不同时期会发生变化。 在人类历史上曾出现过以下几种有代表性的人生观： 享乐主义人生观。它从人的生物本能出发，将人的生活归结为满足人的生理需要的过程，提出追求感官快乐，最大限度地满足物质生活享受是人生的唯一目的。 厌世主义人生观。宗教的厌世主义认为，人生是苦难的深渊，充满各种烦恼与痛苦，唯有脱俗灭欲，才能真正解脱。 禁欲主义人生观。它将人的欲望特别是肉体的欲望看作一切罪恶的根源，主张灭绝人欲，实行苦行主义。 幸福主义人生观。一种观点是强调个人幸福是人生的最高目的和价值；另一种观点是在强调个人幸福的同时，也强调他人幸福和社会公共幸福，认为追求公共幸福是人生的最高目的和价值所在。 乐观主义人生观。它认为社会发展的前途是光明的，人生的目的在于追求社会的文明和进步，在于追求真理，对人生抱着积极乐观的态度。 共产主义人生观。它是无产阶级的科学的人生观。它把人的生命活动历程看作是认识和改造客观世界的过程，把消灭资本主义，实现共产主义，为绝大多数人谋利益，看作是人生的崇高目的和最大幸福。人生的价值和意义在于对社会所尽的责任和所作的贡献，人生的最大价值和意义，在于努力为人民服务，无私地把自己的一切精力贡献给共产主义事业。 ","date":"0001-01-01","objectID":"/drafts/my-world/:0:0","tags":null,"title":"","uri":"/drafts/my-world/"},{"categories":null,"content":"我一直有再次长期全身心投入学习重要知识点的想法。除了感受学习本身的快乐，还可以让自己的基础更牢固，走得更远。 这个想法实践起来不容易，因为我还没确定能否承受它的风险。风险归根结底是我自身的问题，分为两方面：1、真的能够全身心投入，不受其他事情干扰吗？2、学习的结果是否能够帮助我获得更多幸福感？ 第一点我曾经做到了，不过没能持续。我至今还不清楚触发这种状态的充分条件。我的感受是前期一定要承受住一段时间的痛苦，超过一个点这种痛苦就会消失，接着就是精神高度集中，能够没有门槛地进入学习状态，从学习中获取远超于其他娱乐活动的快乐，以至于认为玩游戏看视频是在浪费时间。但是现在没法进入这种状态，所以还是会花一些时间在玩游戏或者看视频这种只能带来短暂快乐的事情上面。一旦可自由支配的时间增加，有可能把这些时间中的一部分花在这些娱乐上面。 上一次尝试进入这种状态是给自己增加阅读，确实状态变好了。但是当时意识到一个问题，就是如果不把这些东西输出成文字，就会一直处于零乱的状态，无法根据需要使用这些思考的结果。而且以后有可能忘记这些原先思考过的东西，到时就得重来，浪费时间。因此强行让自己大幅度减少阅读，增加输出。但是在输出过程中由于一段时间把关注点从输出文字转移到了输出工具上，导致工具上的问题影响到文字输出，整个状态就消失了。这就是为什么我后来专门买一台 surface go 放在工作的地方只用于写东西。 ","date":"0001-01-01","objectID":"/drafts/study/:0:0","tags":null,"title":"","uri":"/drafts/study/"},{"categories":null,"content":"当人们即将或者建立起较为紧密（好基友、合作伙伴、夫妻）的关系时，就需要花功夫去了解对方。 在只了解对方的一面时，如果这一面是好的，那么就会自动脑补其他面也是好的。但是在一般的关系中，人通常有保留地展现自己，这样无论是生活还是做事都会获得比较高的效率。但这仍然不是最高的效率。 在和他人建立起紧密关系后，展现自己的其他面，可以获得更高的效率。为什么要加这个条件？因为这受到当前的社会发展程度的限制。 人们往往都喜欢和了解并善待自己的人相处，于是在某些情况下，会通过某些细节确认对方是否愿意了解自己。 有一个经典的反例，即经常被批的“多喝热水”。它是描述客观事实的一种表达，也就是不需要了解对方是什么样的一个人，只需要了解客观的诸如“感冒”之类的信息就能做出回答。这在医生和病人的关系中没有问题，但在其他关系中就很难说了。在本应该在了解对方的基础上选择合适的表达，却选择了不需要了解对方就能使用的表达。这就使得一部份人认为这是一种“不关心我”的态度，也就是对待这件事的时候，仅考虑了“事”，没有考虑“人”。 安全感分为身体上的安全感和情感上的安全感，但归根接地还是身体上的安全感。即是否受到生存的威胁。 人类无法一下子就获得全部的真理，只能从最显著的真理开始了解，然后通过不断地探索去了解更多真理。这些最显著的具体真理需要以一定条件为前提才为真，当条件发生变化，则这些真理为架，另一些真理为真。 这就涉及到两种思考模式： A 类：真理只要满足 x 都成立 B 类：真理若想要成立，需要在客观因素为 X 时满足 x 条件；在客观因素为 Y 时满足 y 条件…… 对于 A 类人来说，客观因素的变化对真理成立的条件没有影响。当且仅当满足 x 条件时，真理才会成立。 问题在于不是所有人都愿意不断地探索。 在这些条件中，影响最大的因素是生产力发展程度。 ","date":"0001-01-01","objectID":"/drafts/think-people/:0:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":"情感因素 它提高了我们思考的效率。一件事情的思考角度有很多，因此优先思考更有可能发生的情况并加以验证就可以尽快地得到结论，减少身体和大脑的负担。 对于以前的人来说，减少身体和大脑的负担就更容易在生活资料匮乏的环境中存活下来。现在生活好了很多，不太需要担心因全面的调查和思考所带来的负担会威胁到生存，但整体的思维却很难和物质生活同步改变。 如果不去全面地了解和思考，那么思考多少个角度和思考到什么程度能得到接近事实的结论呢？每个人都会自带一套以自身生活经验为基础的标准。但是个人的经验是有局限性的，因为一个人生活的圈子很难发生巨大的变化，他对于一个事物的判断往往受到经常接触的人的影响。 例如一个长期生活在尔虞我诈的生活环境里的人，当一个人帮助他时，他首先想到的是这个人很可能是不怀好意。那么怎么证明这个人是不怀好意的？要从以前那些不怀好意的人的共性为基础，看这个人是不是有这些共性。问题是这些共性在客观事实上是否是一个人不怀好意的充分条件？一旦总结错了，就会误会他人，甚至造成不可挽回的损失。 按优先级思考本身不是问题，问题在于过早地得出结论。这样很可能就会舍弃了对其他可能性的了解和思考，容易得到不准确的结论。 ","date":"0001-01-01","objectID":"/drafts/think-people/:1:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":"情绪因素 情绪因素指的是我们在思考一件事情的时候，将以前经历类似事情时所产生的情绪加入思考中。 一个人在碰到一件事时，身体的一些激素发生作用，产生了情绪，并被记录下来。在下次思考类似事情的时候，会加入以前身体所记住的情绪。 ","date":"0001-01-01","objectID":"/drafts/think-people/:2:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":" 对一个人的了解本来只应该影响到思考可能性的优先顺序，但实际上人在思考的时候往往止步于思考高优先级可能性，直接不思考低优先级的可能性。 人是复杂的，日常生活或工作中通常只能接触到的一个人的某一面，但是人们通常只认为这一面就是这个人的全部。不同的人由于在不同场合接触同一个人，这就导致他们对同一个人所了解的信息不完全一致，甚至是有极大的不同。因此他们可能会得出相反的结论，并且对自己的结论深信不疑。以至于在出现与这个结论相反的信息时，本能地会拒绝接受和反驳。 在基于片面的信息做判断的时候，情感因素就会起到更大的作用。 想要完全地了解一个人，需要经过很多观察和总结。 现实中很难做到全面地观察。除非做一个全年无休的监控跟着对方。就算是家长和孩子、夫妻这样有着长时间相处的关系基础，也很难全部观察到。 在只能观察部分言行的情况下，需要结合其他人所述的关于这个人的经历和看法，通过逻辑推理和总结去进一步了解对方。 通常情况下，人们不会花那么大的功夫去了解一个人。但是如果不深入了解一个人，那么对于这个人的判断的准确性就会降低，也就容易判断错对方实际上想要的东西。 一些影视作品中的人物被评价为过于脸谱化，形象单一。这是因为在描写这些人物的时候，只对他的某一方面或者少数的几个方面进行描写。以至于观众对于这些人物的了解很少，也就对这些人物表现出较为单一的情绪。而有些作品则选择多个方面描写一个人，我们就说这个人的形象是立体的。例如《钢之炼金术师 FA》的“反派”，有些因为形象足够立体而受到欢迎。 对一个人的了解越全面（不是越多），对其言行的判断通常就能更加准确。对于事物也是如此。 而当我们对一个人或者一件事的了解很少的时候，或者以为足够多的时候，情感因素就会对判断产生较大的影响。 ","date":"0001-01-01","objectID":"/drafts/think-people/:3:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":"回到情感因素客观的一面 人往往会对其他人有一个大致的判断，即这个人是偏向于感性或者偏向于理性的。比如整个社会有一个偏见，认为女性往往是是感性的，而男性往往是理性的。 当人们评价一个人是感性的，就表示在评价者看来，这个人在对一件事情做判断的时候，会更多地受到情感因素的影响。而一个被评价为理性的人，在评价者看来，他在考虑事情的时候，更在乎事情的本身。 加入过多的情感因素，会扭曲事实。人类本身就是情感很强的生物，思考的时候难以避免加入情感因素，这可以使得在对世界理解达到一定程度前，更加容易生存下来。 ","date":"0001-01-01","objectID":"/drafts/think-people/:4:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":"微博 最近几个月接触了大量的网络言论，让我对从社会角度上看“人是如何思考的”产生了好奇。通过对自己过去思考问题方式的转变以及大量网络言论的分析，似乎看出了点东西。现在总结和分享出来，以便于进一步的探索。 ","date":"0001-01-01","objectID":"/drafts/think-people/:5:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":"信息来源 我主要在微博上观察。其他平台像知乎和百度贴吧，以前有用过一段时间。后来觉得占用的时间多，而且从推荐的内容能学到的东西很有限，就主要用于查找资料或者看漫画。微博也一样，只是这次为了搜集言论材料才下载 APP 看热搜。以前我都不看热搜，并且也删了很长一段时间微博 APP。不过会偶尔在电脑上登录 WEB 版微博浏览图片。 ","date":"0001-01-01","objectID":"/drafts/think-people/:6:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":"主要特征 这个大家其实都很清楚。微博上尤其是热搜的信息和评论，有不少带着强烈的负面情绪。这个是本文讨论的重点。 ","date":"0001-01-01","objectID":"/drafts/think-people/:7:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":" 我们似乎形成了一种共识，认为理性的男性会在与女朋友交流一件事情的时候，总是由于理性的影响而想给女朋友讲道理。这其实是一个误解。因为给想要被安慰而不是讲道理的人讲道理，本身是一个非理性的选择。理性是想要效率最大化，而这种行为是效率非常低的一种方式。 其中一部分人通过提前透支自己未来的劳动以获取超出与自身实力相匹配的资源。 ","date":"0001-01-01","objectID":"/drafts/think-people/:8:0","tags":null,"title":"","uri":"/drafts/think-people/"},{"categories":null,"content":" 避免重复设计轮子，学会重复制造轮子 Linux 与普通用户 ","date":"0001-01-01","objectID":"/drafts/titles/:0:0","tags":null,"title":"","uri":"/drafts/titles/"},{"categories":null,"content":"为什么要关注这些呢？其实准确地说，是只有从今年疫情之后才开始有这么多关注的。主要目的是为了防止以后被一些看似正确实则片面且负面的言论影响到日常生活的情绪。 2020 这一年发生了太多事情。由于新冠疫情，让很多国家的问题暴露出来。让很多人对西方国家——特别是美国——的认识得到了矫正。 随之而来的是对“公知”这一群体的讨论。公知一词原是对“公共知识分子”的简称，指的是那些比普通大众拥有更多知识并为人民群众提升见识，矫正错误认识的一类值得尊敬的群体。可见“公知”的原意是一个褒义词。 然而最近几年，“公知”一词逐渐变成了贬义词。如果你从十年前穿越过来，想夸一个通过公众平台发声的知识分子，称其为公知，那他会以为你在骂他。 如今大家讨论的是“公知说的话为什么越来越没有影响力了？甚至人人喊打？”。 大多数人的最直接的感受是现在知道了好多以前说外国好以诋毁中国的案例实际上都是谣言。 1。以前年龄小，对那些能写出长篇大论的人有一种自然而然的敬畏。而且所学知识量少，逻辑思维较为欠缺，不容易识别出文章里的逻辑漏洞。好在这两个问题比较容易解决，好好学习自然就能提高。 例如以前公知吹西方国家福利好，从出生安排到死亡。你会觉得西方很美好，再对比一下中国，对国家的恨意就上来了。然而学了政治经济课后，你可能会提出一个问题”这些福利用的钱是哪来的？”。如果你的老师宣扬那个言论，而你提出这个问题，脾气好的老师可能会愣住然后不搭理你，脾气不好的可能就直接把你赶出去。 等你工作了体会了“纳税“后，才能深刻体会到什么叫“福利“。想象一下自己在高福利的地方工作，越想越气，老子辛辛苦苦赚的钱要交那么多税去养一群懒人。当然，更高的福利肯定是没法仅由税收来支撑的。于是就有了国家向其他国家不断借钱维持高福利，直到国家实在无力偿还债务，宣布破产。这时候再回去过正常的日子肯定是更加难受的，社会问题一大堆。所以自从希腊出事后，很多人直观地感受到一味强调高福利的后果。公知也不太敢拿欧洲的高福利抨击我国了。 2。以前互联网的普及率并不高，能够获取高质量信息的方式有限。公知可以使用信息差来制造谣言。 现在移动互联网时代，普及率比以前高非常多。在这里特别感谢一下小米推出便宜而又足够用的手机硬件，以及优秀的 MIUI，让更多低收入人群买得起智能手机。同时高质量且更多基于事实的媒体也涌现出来，不再全是那些公知和资本操控的媒体了，例如观察者网。 这样在看待一件事的时候，我们有了更多的视角，通过更全面地了解事情而不是通过片面的或者编造的所谓事实得出“定体问“的可笑结论。 也就是以前公知常在阴阳怪气胡乱解读国家政策导致网民愤怒时说的“现在的人已经不像以前那么好骗了，政府隐瞒不住的“。然而他们没想到反转来得这么快。随着政府推动大学扩招，越来越多的人可以进入大学学习，提升知识水平和鉴别能力。最终公知们发现其实真正难骗网民的正是他们自己。而越来越多的人具备足够的知识可以理解国家政策了。真是讽刺。 公知的套路逐渐被人们所知。最严重的地方在于对不同主体采取双重标准，即“双标“。网友们有个神总结“这国怎？定体问！原是美，那没事“。 意思是如果一件事情出来，如果公知没有看是发生在哪个国家，以为是中国，就会先一顿分析和批评，最后总结“这个国家怎么了？一定是体制的问题“。结果后来发现事情发生在美国，便立即改口说那没事，顺便来一句“这体现了人家美国的自由和民主“。属实恶心。 另一个问题是公知会将一件事情的其中一小部分事实拿出来推出一个错误的结论，以贬低中国。例如抗美援朝的双方死伤人数。说我们死了十几万人，而美国才死一万多人。这个是不是事实？抛开美国为了面子提高死亡标准，如果以中国的标准计算会更多这件事不谈，这可以算作事实。但这不是完整的事实，交战双方的一边是中国志愿军和北朝鲜，另一边则是以美国和南朝鲜（或韩国）为主的16国联合国军队。而韩国死亡人数至少六十万。这样算下来，死亡人数可是联合国军队死亡人数比我们高很多。也因此，有网友调侃道“大家算死亡人数都不把韩国算上，嗯，韩国人不算人“。 还有一个问题是混淆概念。例如把抗美援朝和朝鲜战争划等号。抗美援朝的时间起点是在朝鲜战争的战线靠近鸭绿江的时候。这个时候开始，志愿军参战并把联合国军打退到 38 线以南。取得了军事上的胜利。而如果把抗美援朝和朝鲜战争划等号，就会得出始于 38 线止于 38 线，因此志愿军和联合国军（更过分的会只说美军）打成平手，以此贬低志愿军。 然而他们没有想到的是，就算退一千万步讲，打成平手，那也是美国输。为啥？假设我这么一个很少打乒乓球的菜鸟在和张继科的比赛中拿到平分。你真会觉得我俩打成平手？正常人的反应肯定是他输我一颗球都算我赢。而当年新中国刚成立，美国则是最强大的国家。联合国军无论是武器还是后勤都远远强于志愿军，而参战人数也是联合国军占优。这情况下打个平手都算他们输。要是志愿军有联合国军的十分之一的武器和后勤，联合国军都得直接全灭，那还能僵持在 38 线。 ","date":"0001-01-01","objectID":"/drafts/why/:0:0","tags":null,"title":"","uri":"/drafts/why/"},{"categories":null,"content":"绝对的优势地位 由于物质的组合具有随机性，无法确保不同物种能以相同的速度进化。而一旦有一个物种在一开始取得了大脑发展的优势，则这种优势会快速地不断放大，直至产生绝对的优势地位。其他物种则需要再经历相当长的时间继续通过变异以达到继续发展大脑的门槛。 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:1:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"其他 不重复地失败多少次，就等于成功了多少次。 实践的效率，自由探索的极限，指导性探索的缺点。 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:2:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"优势会限制自身 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:3:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"危机感促进的发展 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:4:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"站在巨人的肩膀上 一种是依赖于 DNA 遗传；另一种是依赖于图像、文字。 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:5:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"个人的上限 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:6:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"开源对世界发展的重大意义 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:7:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"人类命运共同体 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:8:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"人类是否肩负着一个未知的使命 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:9:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"以什么样的视角去看历史 ","date":"0001-01-01","objectID":"/drafts/world-outlook-not-use/:10:0","tags":null,"title":"","uri":"/drafts/world-outlook-not-use/"},{"categories":null,"content":"upstream cppver { consistent_hash $remote_addr; server 127.0.0.1:8090; } upstream gover { consistent_hash $remote_addr; server 127.0.0.1:8091; } upstream gover2 { consistent_hash $remote_addr; server 127.0.0.1:8092; } server { listen 8088; server_name test.com; location / { mirror /mirror; mirror /mirror2; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://cppver$request_uri; } location /mirror { internal; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://gover$request_uri; } location /mirror2 { internal; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://gover2$request_uri; } } ","date":"0001-01-01","objectID":"/1/01/01/nginx-mirror/:0:0","tags":null,"title":"","uri":"/1/01/01/nginx-mirror/"},{"categories":null,"content":"形而上、形而下 《周易》：在天成象，在地成形 《系辞》： 乾坤，其《易》之蕴邪？乾坤成列，而《易》立乎其中矣。乾坤毁，则无以见《易》。《易》不可见，则乾坤或几乎息矣。 是故形而上者谓之道，形而下者谓之器。化而裁之谓之变，推而行之谓之通，举而错之天下之民谓之事业。 [翻译]： 乾坤也就是天地，它是《易经》的精蕴呀，乾坤既成列于上下，《易经》的道理也就肇定于其中了。如果乾坤毁灭的话，则没有办法见到《易经》的道理了，《易经》的道理不可被知解的话，则天地乾坤之道也几乎要息灭了。 所以在形器之上，无形体度量，抽象不可形而为万物，所共由者，就叫做“道”；在形体之下，有形体可寻，是具体之物，就叫做“器”；将形上之道、形下之器，变化而裁制之以致用，就叫做“变”；推而发挥之，扩充之以实行于天下，谓之“通”；举而设施安置于天下的百姓，就叫做“事业”。 https://baike.baidu.com/item/%E7%B3%BB%E8%BE%9E 道 ↑ 没有具体的形体 形 器 ↓ 有具体的形体 哲学术语 哲学术语中的 形而上学 是日本明治时期著名哲学家井上哲次郎对 metaphysics 的翻译，指物理背后隐含的东西。晚清学者严复则采用了玄学这一翻译，后经清末留日学生将大批日制汉语（日本称和制汉语）带回国后，玄学这一译法渐渐被形而上学取代。 《系辞》的形而上指的是从天象推断“器”背后的东西。 metaphysics 的形而上指的是从地上的“器”来推断“器”背后的东西。 因此“形而上学”的翻译并不准确。但由于“形而上学”更容易被当时受教育程度普遍较低的中国民众所接受，因此这一词扎根在了汉语之中。 本体论的核心观点是：一切现象之外有一个终极的本体，支配着自然界的一切，世间万事万物都是这个永恒、终极的本体派生出来的产物。 神学会应用形而上学的分析方法，但是神学对信仰假设和科学实践是否相矛盾不深究。形而上学的假设一定不能同科学实践相矛盾。 ","date":"0001-01-01","objectID":"/1/01/01/philosophy/:0:0","tags":null,"title":"","uri":"/1/01/01/philosophy/"}]